--
-- PostgreSQL database dump
--

-- Dumped from database version 13.2
-- Dumped by pg_dump version 15.4

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: public; Type: SCHEMA; Schema: -; Owner: postgres
--

-- *not* creating schema, since initdb creates it


ALTER SCHEMA public OWNER TO postgres;

--
-- Name: timescaledb; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS timescaledb WITH SCHEMA public;


--
-- Name: EXTENSION timescaledb; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION timescaledb IS 'Enables scalable inserts and complex queries for time-series data';


SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: auth_group; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_group (
    id bigint NOT NULL,
    name text
);


ALTER TABLE public.auth_group OWNER TO postgres;

--
-- Name: auth_group_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_group_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_group_id_seq OWNER TO postgres;

--
-- Name: auth_group_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_group_id_seq OWNED BY public.auth_group.id;


--
-- Name: auth_group_permissions; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_group_permissions (
    id bigint NOT NULL,
    group_id bigint,
    permission_id bigint
);


ALTER TABLE public.auth_group_permissions OWNER TO postgres;

--
-- Name: auth_group_permissions_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_group_permissions_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_group_permissions_id_seq OWNER TO postgres;

--
-- Name: auth_group_permissions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_group_permissions_id_seq OWNED BY public.auth_group_permissions.id;


--
-- Name: auth_permission; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_permission (
    id bigint NOT NULL,
    content_type_id bigint,
    codename text,
    name text
);


ALTER TABLE public.auth_permission OWNER TO postgres;

--
-- Name: auth_permission_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_permission_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_permission_id_seq OWNER TO postgres;

--
-- Name: auth_permission_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_permission_id_seq OWNED BY public.auth_permission.id;


--
-- Name: auth_user; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_user (
    id bigint NOT NULL,
    password text,
    last_login timestamp with time zone,
    is_superuser boolean,
    username text,
    last_name text,
    email text,
    is_staff boolean,
    is_active boolean,
    date_joined timestamp with time zone,
    first_name text
);


ALTER TABLE public.auth_user OWNER TO postgres;

--
-- Name: auth_user_groups; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_user_groups (
    id bigint NOT NULL,
    user_id bigint,
    group_id bigint
);


ALTER TABLE public.auth_user_groups OWNER TO postgres;

--
-- Name: auth_user_groups_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_user_groups_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_user_groups_id_seq OWNER TO postgres;

--
-- Name: auth_user_groups_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_user_groups_id_seq OWNED BY public.auth_user_groups.id;


--
-- Name: auth_user_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_user_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_user_id_seq OWNER TO postgres;

--
-- Name: auth_user_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_user_id_seq OWNED BY public.auth_user.id;


--
-- Name: auth_user_user_permissions; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.auth_user_user_permissions (
    id bigint NOT NULL,
    user_id bigint,
    permission_id bigint
);


ALTER TABLE public.auth_user_user_permissions OWNER TO postgres;

--
-- Name: auth_user_user_permissions_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.auth_user_user_permissions_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.auth_user_user_permissions_id_seq OWNER TO postgres;

--
-- Name: auth_user_user_permissions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.auth_user_user_permissions_id_seq OWNED BY public.auth_user_user_permissions.id;


--
-- Name: django_admin_log; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_admin_log (
    id bigint NOT NULL,
    object_id text,
    object_repr text,
    action_flag smallint,
    change_message text,
    content_type_id bigint,
    user_id bigint,
    action_time timestamp with time zone
);


ALTER TABLE public.django_admin_log OWNER TO postgres;

--
-- Name: django_admin_log_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.django_admin_log_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.django_admin_log_id_seq OWNER TO postgres;

--
-- Name: django_admin_log_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.django_admin_log_id_seq OWNED BY public.django_admin_log.id;


--
-- Name: django_content_type; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_content_type (
    id bigint NOT NULL,
    app_label text,
    model text
);


ALTER TABLE public.django_content_type OWNER TO postgres;

--
-- Name: django_content_type_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.django_content_type_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.django_content_type_id_seq OWNER TO postgres;

--
-- Name: django_content_type_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.django_content_type_id_seq OWNED BY public.django_content_type.id;


--
-- Name: django_migrations; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_migrations (
    id bigint NOT NULL,
    app text,
    name text,
    applied timestamp with time zone
);


ALTER TABLE public.django_migrations OWNER TO postgres;

--
-- Name: django_migrations_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.django_migrations_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.django_migrations_id_seq OWNER TO postgres;

--
-- Name: django_migrations_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.django_migrations_id_seq OWNED BY public.django_migrations.id;


--
-- Name: django_session; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.django_session (
    session_key text NOT NULL,
    session_data text,
    expire_date timestamp with time zone
);


ALTER TABLE public.django_session OWNER TO postgres;

--
-- Name: home_blog; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.home_blog (
    sno bigint NOT NULL,
    title text,
    meta text,
    content text,
    thumbnail_img text,
    slug text,
    "time" date,
    category text,
    thumbnail_url text
);


ALTER TABLE public.home_blog OWNER TO postgres;

--
-- Name: home_blog_sno_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.home_blog_sno_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.home_blog_sno_seq OWNER TO postgres;

--
-- Name: home_blog_sno_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.home_blog_sno_seq OWNED BY public.home_blog.sno;


--
-- Name: home_home; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.home_home (
    sno bigint NOT NULL,
    title text,
    meta text,
    content text,
    thumbnail text,
    slug text,
    "time" date
);


ALTER TABLE public.home_home OWNER TO postgres;

--
-- Name: home_home_sno_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.home_home_sno_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.home_home_sno_seq OWNER TO postgres;

--
-- Name: home_home_sno_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.home_home_sno_seq OWNED BY public.home_home.sno;


--
-- Name: auth_group id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group ALTER COLUMN id SET DEFAULT nextval('public.auth_group_id_seq'::regclass);


--
-- Name: auth_group_permissions id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions ALTER COLUMN id SET DEFAULT nextval('public.auth_group_permissions_id_seq'::regclass);


--
-- Name: auth_permission id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_permission ALTER COLUMN id SET DEFAULT nextval('public.auth_permission_id_seq'::regclass);


--
-- Name: auth_user id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user ALTER COLUMN id SET DEFAULT nextval('public.auth_user_id_seq'::regclass);


--
-- Name: auth_user_groups id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups ALTER COLUMN id SET DEFAULT nextval('public.auth_user_groups_id_seq'::regclass);


--
-- Name: auth_user_user_permissions id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions ALTER COLUMN id SET DEFAULT nextval('public.auth_user_user_permissions_id_seq'::regclass);


--
-- Name: django_admin_log id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log ALTER COLUMN id SET DEFAULT nextval('public.django_admin_log_id_seq'::regclass);


--
-- Name: django_content_type id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_content_type ALTER COLUMN id SET DEFAULT nextval('public.django_content_type_id_seq'::regclass);


--
-- Name: django_migrations id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_migrations ALTER COLUMN id SET DEFAULT nextval('public.django_migrations_id_seq'::regclass);


--
-- Name: home_blog sno; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.home_blog ALTER COLUMN sno SET DEFAULT nextval('public.home_blog_sno_seq'::regclass);


--
-- Name: home_home sno; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.home_home ALTER COLUMN sno SET DEFAULT nextval('public.home_home_sno_seq'::regclass);


--
-- Data for Name: cache_inval_bgw_job; Type: TABLE DATA; Schema: _timescaledb_cache; Owner: postgres
--

COPY _timescaledb_cache.cache_inval_bgw_job  FROM stdin;
\.


--
-- Data for Name: cache_inval_extension; Type: TABLE DATA; Schema: _timescaledb_cache; Owner: postgres
--

COPY _timescaledb_cache.cache_inval_extension  FROM stdin;
\.


--
-- Data for Name: cache_inval_hypertable; Type: TABLE DATA; Schema: _timescaledb_cache; Owner: postgres
--

COPY _timescaledb_cache.cache_inval_hypertable  FROM stdin;
\.


--
-- Data for Name: hypertable; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.hypertable (id, schema_name, table_name, associated_schema_name, associated_table_prefix, num_dimensions, chunk_sizing_func_schema, chunk_sizing_func_name, chunk_target_size, compression_state, compressed_hypertable_id, replication_factor) FROM stdin;
\.


--
-- Data for Name: chunk; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.chunk (id, hypertable_id, schema_name, table_name, compressed_chunk_id, dropped) FROM stdin;
\.


--
-- Data for Name: dimension; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.dimension (id, hypertable_id, column_name, column_type, aligned, num_slices, partitioning_func_schema, partitioning_func, interval_length, integer_now_func_schema, integer_now_func) FROM stdin;
\.


--
-- Data for Name: dimension_slice; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.dimension_slice (id, dimension_id, range_start, range_end) FROM stdin;
\.


--
-- Data for Name: chunk_constraint; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.chunk_constraint (chunk_id, dimension_slice_id, constraint_name, hypertable_constraint_name) FROM stdin;
\.


--
-- Data for Name: chunk_data_node; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.chunk_data_node (chunk_id, node_chunk_id, node_name) FROM stdin;
\.


--
-- Data for Name: chunk_index; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.chunk_index (chunk_id, index_name, hypertable_id, hypertable_index_name) FROM stdin;
\.


--
-- Data for Name: compression_chunk_size; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.compression_chunk_size (chunk_id, compressed_chunk_id, uncompressed_heap_size, uncompressed_toast_size, uncompressed_index_size, compressed_heap_size, compressed_toast_size, compressed_index_size, numrows_pre_compression, numrows_post_compression) FROM stdin;
\.


--
-- Data for Name: continuous_agg; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.continuous_agg (mat_hypertable_id, raw_hypertable_id, user_view_schema, user_view_name, partial_view_schema, partial_view_name, bucket_width, direct_view_schema, direct_view_name, materialized_only) FROM stdin;
\.


--
-- Data for Name: continuous_aggs_hypertable_invalidation_log; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.continuous_aggs_hypertable_invalidation_log (hypertable_id, lowest_modified_value, greatest_modified_value) FROM stdin;
\.


--
-- Data for Name: continuous_aggs_invalidation_threshold; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.continuous_aggs_invalidation_threshold (hypertable_id, watermark) FROM stdin;
\.


--
-- Data for Name: continuous_aggs_materialization_invalidation_log; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.continuous_aggs_materialization_invalidation_log (materialization_id, lowest_modified_value, greatest_modified_value) FROM stdin;
\.


--
-- Data for Name: hypertable_compression; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.hypertable_compression (hypertable_id, attname, compression_algorithm_id, segmentby_column_index, orderby_column_index, orderby_asc, orderby_nullsfirst) FROM stdin;
\.


--
-- Data for Name: hypertable_data_node; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.hypertable_data_node (hypertable_id, node_hypertable_id, node_name, block_chunks) FROM stdin;
\.


--
-- Data for Name: metadata; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.metadata (key, value, include_in_telemetry) FROM stdin;
exported_uuid	7c241258-c1c5-43cb-8c3b-8187b7552282	t
\.


--
-- Data for Name: remote_txn; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.remote_txn (data_node_name, remote_transaction_id) FROM stdin;
\.


--
-- Data for Name: tablespace; Type: TABLE DATA; Schema: _timescaledb_catalog; Owner: postgres
--

COPY _timescaledb_catalog.tablespace (id, hypertable_id, tablespace_name) FROM stdin;
\.


--
-- Data for Name: bgw_job; Type: TABLE DATA; Schema: _timescaledb_config; Owner: postgres
--

COPY _timescaledb_config.bgw_job (id, application_name, schedule_interval, max_runtime, max_retries, retry_period, proc_schema, proc_name, owner, scheduled, hypertable_id, config) FROM stdin;
\.


--
-- Data for Name: auth_group; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_group (id, name) FROM stdin;
\.


--
-- Data for Name: auth_group_permissions; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_group_permissions (id, group_id, permission_id) FROM stdin;
\.


--
-- Data for Name: auth_permission; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_permission (id, content_type_id, codename, name) FROM stdin;
1	1	add_logentry	Can add log entry
2	1	change_logentry	Can change log entry
3	1	delete_logentry	Can delete log entry
4	1	view_logentry	Can view log entry
5	2	add_permission	Can add permission
6	2	change_permission	Can change permission
7	2	delete_permission	Can delete permission
8	2	view_permission	Can view permission
9	3	add_group	Can add group
10	3	change_group	Can change group
11	3	delete_group	Can delete group
12	3	view_group	Can view group
13	4	add_user	Can add user
14	4	change_user	Can change user
15	4	delete_user	Can delete user
16	4	view_user	Can view user
17	5	add_contenttype	Can add content type
18	5	change_contenttype	Can change content type
19	5	delete_contenttype	Can delete content type
20	5	view_contenttype	Can view content type
21	6	add_session	Can add session
22	6	change_session	Can change session
23	6	delete_session	Can delete session
24	6	view_session	Can view session
25	7	add_blog	Can add blog
26	7	change_blog	Can change blog
27	7	delete_blog	Can delete blog
28	7	view_blog	Can view blog
29	7	add_home	Can add home
30	7	change_home	Can change home
31	7	delete_home	Can delete home
32	7	view_home	Can view home
33	8	add_blog	Can add blog
34	8	change_blog	Can change blog
35	8	delete_blog	Can delete blog
36	8	view_blog	Can view blog
\.


--
-- Data for Name: auth_user; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_user (id, password, last_login, is_superuser, username, last_name, email, is_staff, is_active, date_joined, first_name) FROM stdin;
1	pbkdf2_sha256$390000$yIv4i27bp1PG9hZhCpi9Jp$rxh/oVdP9AGbQaPhKZ8q6hcJKX/eBCReuuAFprelExY=	2023-01-29 10:45:45.420702+00	t	ashish			t	t	2023-01-22 05:25:20.93894+00	
2	pbkdf2_sha256$260000$YjKG2vq3jze693mQnIv8tg$uNLZTKBHywTm7zHspc3qXs0+MeyBh9n25b4vp0iRRr0=	2023-09-27 14:39:53.745985+00	t	elenakapatsa		kapatsahelen@gmail.com	t	t	2023-09-27 14:39:39.368534+00	
\.


--
-- Data for Name: auth_user_groups; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_user_groups (id, user_id, group_id) FROM stdin;
\.


--
-- Data for Name: auth_user_user_permissions; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.auth_user_user_permissions (id, user_id, permission_id) FROM stdin;
\.


--
-- Data for Name: django_admin_log; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_admin_log (id, object_id, object_repr, action_flag, change_message, content_type_id, user_id, action_time) FROM stdin;
1	1	Best JavaScript Frameworks to Learn in 2023	1	[{"added": {}}]	8	1	2023-01-22 06:22:53.693611+00
2	2	Tailwind CSS vs Bootstrap Which one to Choose for your Next Project?	1	[{"added": {}}]	8	1	2023-01-22 07:15:12.064211+00
3	3	Top 10 VS Code Extensions Every Developer Should Use	1	[{"added": {}}]	8	1	2023-01-22 07:25:02.592007+00
4	1	Best JavaScript Frameworks to Learn in 2023	2	[{"changed": {"fields": ["Thumbnail img"]}}]	8	1	2023-01-22 10:43:42.821771+00
5	4	The Complete Guide to Become a Full Stack Developer in 2023	1	[{"added": {}}]	8	1	2023-01-22 10:46:44.985427+00
6	5	Best VS Code Alternatives you should try	1	[{"added": {}}]	8	1	2023-01-22 10:52:04.746025+00
7	1	Best JavaScript Frameworks to Learn in 2023	3		8	1	2023-01-22 10:58:04.572536+00
8	6	Make your Websites Faster with these Web Components	1	[{"added": {}}]	8	1	2023-01-23 13:12:00.194826+00
9	7	A complete Guide to Create Modern Websites	1	[{"added": {}}]	8	1	2023-01-24 07:24:54.719912+00
10	8	Best HTML, CSS & JS Cheatsheet for 2023	1	[{"added": {}}]	8	1	2023-01-24 07:30:14.129901+00
11	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["Category"]}}]	8	1	2023-01-25 13:15:27.707547+00
12	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["Content", "Remark"]}}]	8	1	2023-01-26 07:05:50.675039+00
13	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["Content"]}}]	8	1	2023-01-26 07:10:24.094854+00
14	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["body"]}}]	8	1	2023-01-26 07:28:39.172592+00
15	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["body"]}}]	8	1	2023-01-26 07:30:56.888262+00
16	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["body"]}}]	8	1	2023-01-26 07:32:57.274035+00
17	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["body"]}}]	8	1	2023-01-26 07:36:53.128789+00
18	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:11:52.248739+00
19	8	Best HTML, CSS & JS Cheatsheet for 2023	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:15:09.607476+00
20	8	Best HTML, CSS & JS Cheatsheet for 2023	3		8	1	2023-01-26 10:26:01.291869+00
21	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:32:42.245856+00
22	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:35:03.680122+00
23	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:35:49.831949+00
24	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:37:08.669082+00
25	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:41:18.44195+00
26	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:42:07.896348+00
27	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:42:27.393681+00
28	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:42:49.377608+00
29	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:46:21.289949+00
30	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:47:01.931009+00
31	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:47:15.935823+00
32	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:47:29.804828+00
33	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:48:04.92213+00
34	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:48:28.220889+00
35	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 10:48:58.713114+00
36	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:00:15.086118+00
37	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:01:17.096586+00
38	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:01:50.989144+00
39	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:02:08.836379+00
40	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:18:49.247908+00
41	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:37:14.0535+00
42	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:54:06.743341+00
43	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:57:05.901414+00
44	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:57:30.537089+00
45	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:58:05.655227+00
46	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:58:21.037278+00
47	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 11:59:36.110854+00
48	7	A complete Guide to Create Modern Websites	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 12:02:15.512735+00
49	6	Make your Websites Faster with these Web Components	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 12:05:20.944093+00
50	5	Best VS Code Alternatives you should try	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 12:05:27.095641+00
51	4	The Complete Guide to Become a Full Stack Developer in 2023	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 12:05:33.276978+00
52	3	Top 10 VS Code Extensions Every Developer Should Use	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 12:05:39.364017+00
53	2	Tailwind CSS vs Bootstrap Which one to Choose for your Next Project?	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 12:05:46.766755+00
54	7	A complete Guide to Create Modern Websites css	2	[{"changed": {"fields": ["Title"]}}]	8	1	2023-01-26 13:07:59.885173+00
55	6	Make your Websites Faster with these Web Components css	2	[{"changed": {"fields": ["Title"]}}]	8	1	2023-01-26 13:08:19.080176+00
56	5	Best VS Code Alternatives you should try css	2	[{"changed": {"fields": ["Title"]}}]	8	1	2023-01-26 13:08:29.325218+00
57	4	The Complete Guide to Become a Full Stack Developer in 2023 css	2	[{"changed": {"fields": ["Title"]}}]	8	1	2023-01-26 13:22:24.239496+00
58	3	Top 10 VS Code Extensions Every Developer Should Use css	2	[{"changed": {"fields": ["Title"]}}]	8	1	2023-01-26 13:22:36.687694+00
59	9	Tailwind CSS Introduction	1	[{"added": {}}]	8	1	2023-01-26 13:23:49.938632+00
60	9	Tailwind CSS Introduction	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 13:44:24.454113+00
61	9	Tailwind CSS Introduction	2	[{"changed": {"fields": ["content"]}}]	8	1	2023-01-26 13:45:27.226361+00
62	9	Tailwind CSS Introduction	2	[{"changed": {"fields": ["Category"]}}]	8	1	2023-01-26 13:47:36.750299+00
63	6	Make your Websites Faster with these Web Components css	2	[{"changed": {"fields": ["Category"]}}]	8	1	2023-01-29 10:46:14.601486+00
64	5	Best VS Code Alternatives you should try css	2	[{"changed": {"fields": ["Category"]}}]	8	1	2023-01-29 10:46:24.695542+00
65	4	The Complete Guide to Become a Full Stack Developer in 2023 css	2	[{"changed": {"fields": ["Category"]}}]	8	1	2023-01-29 10:47:13.658944+00
66	7	A complete Guide to Create Modern Websites css	2	[{"changed": {"fields": ["Category"]}}]	8	1	2023-01-30 12:48:03.006087+00
67	5	Best VS Code Alternatives you should try css	2	[{"changed": {"fields": ["Category"]}}]	8	1	2023-01-30 12:48:14.776375+00
68	9	Tailwind CSS Introduction	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	1	2023-01-31 05:33:49.027259+00
69	9	Tailwind CSS Introduction	2	[{"changed": {"fields": ["Thumbnail img"]}}]	8	1	2023-01-31 11:23:14.751376+00
70	7	A complete Guide to Create Modern Websites css	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	1	2023-01-31 11:26:41.550603+00
71	6	Make your Websites Faster with these Web Components css	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	1	2023-01-31 11:27:32.17534+00
72	5	Best VS Code Alternatives you should try css	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	1	2023-01-31 11:27:45.082156+00
73	4	The Complete Guide to Become a Full Stack Developer in 2023 css	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	1	2023-01-31 11:28:11.207252+00
74	3	Top 10 VS Code Extensions Every Developer Should Use css	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	1	2023-01-31 12:02:28.885558+00
75	2	Tailwind CSS vs Bootstrap Which one to Choose for your Next Project?	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	1	2023-01-31 12:03:00.030125+00
76	10	Обучение без учителя	1	[{"added": {}}]	8	2	2023-09-27 14:44:35.63328+00
77	234	Книга "Машинное обучение доступным языком"	3		8	2	2023-09-28 14:56:51.203262+00
78	233	(Untitled)	3		8	2	2023-09-28 14:56:51.208472+00
79	51	Стандартизация (Standartization)	2	[{"changed": {"fields": ["Meta", "content", "Category"]}}]	8	2	2023-09-28 15:01:49.08485+00
80	51	Стандартизация (Standartization)	2	[{"changed": {"fields": ["content"]}}]	8	2	2023-09-28 15:06:41.800327+00
81	247	Несбалансированный датасет (Imbalanced Dataset)	1	[{"added": {}}]	8	2	2023-10-01 09:45:09.434088+00
82	247	Несбалансированный датасет (Imbalanced Dataset)	2	[{"changed": {"fields": ["Thumbnail img", "Thumbnail url"]}}]	8	2	2023-10-01 09:47:07.489934+00
83	247	Несбалансированный датасет (Imbalanced Dataset)	2	[{"changed": {"fields": ["Thumbnail url"]}}]	8	2	2023-10-01 09:47:36.415108+00
84	48	Выброс (Outlier)	2	[{"changed": {"fields": ["Meta", "content", "Thumbnail url", "Category"]}}]	8	2	2023-10-01 09:51:41.903397+00
85	61	Обучение с частичным привлечением учителя\t(Semi-Supervised Learning)	2	[{"changed": {"fields": ["Meta", "content", "Thumbnail url", "Category"]}}]	8	2	2023-10-01 09:52:17.71472+00
86	147	Обнаружение мошеннических операций (Fraud Detection)	2	[{"changed": {"fields": ["Meta", "content", "Thumbnail url", "Category"]}}]	8	2	2023-10-01 09:53:52.602431+00
87	147	Обнаружение мошеннических операций (Fraud Detection)	2	[]	8	2	2023-10-01 09:54:42.591627+00
88	156	Техника переcэмплирования синтетического меньшинства (SMOTE)	2	[{"changed": {"fields": ["Meta", "content", "Thumbnail url", "Category"]}}]	8	2	2023-10-01 09:55:28.708246+00
89	247	Несбалансированный датасет (Imbalanced Dataset)	2	[]	8	2	2023-10-01 09:59:25.660495+00
90	248	Тепловая карта (Heat Map)	1	[{"added": {}}]	8	2	2023-10-07 10:21:13.001141+00
91	248	Тепловая карта (Heat Map)	2	[{"changed": {"fields": ["content"]}}]	8	2	2023-10-07 10:22:05.316732+00
92	248	Тепловая карта (Heat Map)	2	[{"changed": {"fields": ["content"]}}]	8	2	2023-10-07 10:23:08.154641+00
\.


--
-- Data for Name: django_content_type; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_content_type (id, app_label, model) FROM stdin;
1	admin	logentry
2	auth	permission
3	auth	group
4	auth	user
5	contenttypes	contenttype
6	sessions	session
7	home	home
8	home	blog
\.


--
-- Data for Name: django_migrations; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_migrations (id, app, name, applied) FROM stdin;
1	contenttypes	0001_initial	2023-01-16 05:47:26.150099+00
2	auth	0001_initial	2023-01-16 05:47:26.179775+00
3	admin	0001_initial	2023-01-16 05:47:26.20183+00
4	admin	0002_logentry_remove_auto_add	2023-01-16 05:47:26.217925+00
5	admin	0003_logentry_add_action_flag_choices	2023-01-16 05:47:26.230122+00
6	contenttypes	0002_remove_content_type_name	2023-01-16 05:47:26.256572+00
7	auth	0002_alter_permission_name_max_length	2023-01-16 05:47:26.276004+00
8	auth	0003_alter_user_email_max_length	2023-01-16 05:47:26.293772+00
9	auth	0004_alter_user_username_opts	2023-01-16 05:47:26.3075+00
10	auth	0005_alter_user_last_login_null	2023-01-16 05:47:26.32403+00
11	auth	0006_require_contenttypes_0002	2023-01-16 05:47:26.333304+00
12	auth	0007_alter_validators_add_error_messages	2023-01-16 05:47:26.346308+00
13	auth	0008_alter_user_username_max_length	2023-01-16 05:47:26.363242+00
14	auth	0009_alter_user_last_name_max_length	2023-01-16 05:47:26.381251+00
15	auth	0010_alter_group_name_max_length	2023-01-16 05:47:26.402378+00
16	auth	0011_update_proxy_permissions	2023-01-16 05:47:26.415587+00
17	auth	0012_alter_user_first_name_max_length	2023-01-16 05:47:26.432613+00
18	sessions	0001_initial	2023-01-16 05:47:26.448149+00
22	home	0001_initial	2023-01-22 06:04:43.366968+00
23	home	0002_rename_thumbnail_blog_thumbnail_img	2023-01-22 06:07:46.564677+00
24	home	0002_blog_category	2023-01-25 12:30:25.65606+00
25	home	0003_blog_remark	2023-01-26 07:01:12.817322+00
26	home	0004_remove_blog_remark	2023-01-26 07:12:18.307729+00
27	home	0005_blog_thumbnail_url	2023-01-31 05:32:33.574955+00
\.


--
-- Data for Name: django_session; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.django_session (session_key, session_data, expire_date) FROM stdin;
ys49e6gf5vc779onhglec1mx5lbwr8db	.eJxVjMsOwiAQRf-FtSE8BVy67zeQGRikaiAp7cr479qkC93ec859sQjbWuM2aIlzZhcm2el3Q0gPajvId2i3zlNv6zIj3xV-0MGnnul5Pdy_gwqjfuuQrE5KaBSSnJVAInuDSqMij8oo6aHojGCtysEBhGIQLckzuSK0Tuz9Ad5uOBU:1pJSrJ:WGyoKz8HOUg5NIUPNkdlHLG-2saiEvvrlc7zAn4KXS4	2023-02-05 05:25:37.709886+00
ghe6dhuf0mzq5f8tnb9z62y4zl1qd74l	.eJxVjMsOwiAQRf-FtSE8BVy67zeQGRikaiAp7cr479qkC93ec859sQjbWuM2aIlzZhcm2el3Q0gPajvId2i3zlNv6zIj3xV-0MGnnul5Pdy_gwqjfuuQrE5KaBSSnJVAInuDSqMij8oo6aHojGCtysEBhGIQLckzuSK0Tuz9Ad5uOBU:1pJwav:_QdCgppZh316gwoQTWRMBvJhhOYQXFrX1L14E1oYQo4	2023-02-06 13:10:41.766685+00
5ee55au1yvv42dgd8jvye9cpz9vvuvph	.eJxVjMsOwiAQRf-FtSE8BVy67zeQGRikaiAp7cr479qkC93ec859sQjbWuM2aIlzZhcm2el3Q0gPajvId2i3zlNv6zIj3xV-0MGnnul5Pdy_gwqjfuuQrE5KaBSSnJVAInuDSqMij8oo6aHojGCtysEBhGIQLckzuSK0Tuz9Ad5uOBU:1pM5Bx:V94kQ8ePiWCpb6iDyI2BHB6Gp9qesyFAE-tFHV0tAHI	2023-02-12 10:45:45.428701+00
2vuocgirvxygy1xbrtjcnyll3n1phfob	.eJxVjDsOwjAQBe_iGll2jH-U9DmDtetd4wBypDipEHeHSCmgfTPzXiLBtta0dV7SROIiBnH63RDyg9sO6A7tNss8t3WZUO6KPGiX40z8vB7u30GFXr81O-MJrHfnaLRxBjmQsqzRFcJcQoyBNVjMiiAbGLQibwsqY4uiQCzeH-wbOI8:1qlVhh:ykXZuQ1tT2D7pMczVikFHyeUxP-54HVisoCkJZZCnkw	2023-10-11 14:39:53.747694+00
\.


--
-- Data for Name: home_blog; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.home_blog (sno, title, meta, content, thumbnail_img, slug, "time", category, thumbnail_url) FROM stdin;
11	Скошенность (Skewness)		<p>Вы также можете ознакомиться с понятием с помощью <a href="https://youtu.be/Tbd743hJNwQ">видео</a>.</p><p><em>Скошенность</em> – это мера асимметрии распределения вероятностей вещественной случайной величины относительно ее среднего значения. </p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2020/12/skewness-types_distribution-types-2.jpg" class="kg-image" alt loading="lazy" width="2000" height="706" srcset="__GHOST_URL__/content/images/size/w600/2020/12/skewness-types_distribution-types-2.jpg 600w, __GHOST_URL__/content/images/size/w1000/2020/12/skewness-types_distribution-types-2.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/skewness-types_distribution-types-2.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/skewness-types_distribution-types-2.jpg 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Чтобы определить степень скошенности, используется следующая формула:</p><!--kg-card-begin: markdown--><p>$$z = \\frac{}{}, где$$<br>\n$$G\\space{–}\\space{скошенность,}$$<br>\n$$μ\\space{–}\\space{среднее,}$$<br>\n$$М_o\\space{–}\\space{мода,}$$<br>\n$$s\\space{–}\\space{стандартное}\\space{отклонение}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Компания, дающая велосипеды в аренду, собирала данные о частоте пользования услугой в течение 10 дней. Подсчитаем скошенность распределения величины "количество клиентов".</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/skewness-initial.jpeg" class="kg-image" alt loading="lazy" width="185" height="513"><figcaption>Исходный набор данных</figcaption></figure><p>Для этого определим среднее значение, моду и стандартное отклонение. Среднее вычисляется как сумма всех значений, разделенное на их количество.</p><!--kg-card-begin: markdown--><p>$$μ = \\frac{(7 + 5 + 3 + 3 + 4 + 1 + 1 + 2 + 3 + 8)}{10} = 3,7$$</p>\n<!--kg-card-end: markdown--><p>Мода – самое распространенное значение ряда. Бывают наборы чисел, где мод несколько или же ни одной. В нашем игрушечном примере самым распространенным числом стала 3.</p><p>Стандартное отклонение высчитывается в несколько простых шагов.</p><ul><li>Вычисление среднего значения ряда (в нашем случае, 3,7)</li><li>Вычитание из каждого значения ряда среднего и возведение результата в квадрат. Возведение необходимо, чтобы нейтрализовать разницу знаков, ведь порой значение ряда меньше среднего, а это создает отрицательную разность. Из полученного ряда вычислим среднее значение. К примеру, для первой ячейки мы получим:</li></ul><!--kg-card-begin: markdown--><p>$$(7 - 3.7)^2 = 10,89$$</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/skewness.jpeg" class="kg-image" alt loading="lazy" width="313" height="513"><figcaption>Вычисление стандартного отклонения</figcaption></figure><ul><li>Последний шаг – извлечение корня из этого среднего. Для 5,01 корнем будет число 2,23830292856. </li></ul><p>Ну что ж, мы получили все компоненты формулы скошенности, давайте вычислим этот коэффициент наконец.</p><!--kg-card-begin: markdown--><p>$$G = \\frac{(3,7 - 3)}{2,23830292856} = 0,3127369361$$</p>\n<!--kg-card-end: markdown--><p>Готово! Полученный коэффициент является положительным числом, значит, и распределение смещено влево. </p><p>Простейший способ вычислить скошенность автоматически – метод ```skew()``` библиотеки SciPy.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">&gt;&gt;&gt; from scipy.stats import skew\n&gt;&gt;&gt; skew([2, 8, 0, 4, 1, 9, 9, 0])\n0.2650554122698573</code></pre><figcaption>Вычисление коэффициента скошенности (Python + SciPy)</figcaption></figure><p>Фото: <a href="https://www.pexels.com/@dziana-hasanbekava/collections/">@dziana-hasanbekava</a></p>		skoshiennost	2020-11-30		
12	Курсы		<figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://go.redav.online/037447e98dc05c81"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Распродажа | GeekBrains</div><div class="kg-bookmark-description">Крутые программы обучения со скидками от GeekBrains</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://assets.website-files.com/5f75b134a1a14b83f4e06dfc/5f75b134a1a14b92e9e06e22_fav-icon-32.png" alt=""><span class="kg-bookmark-author">GeekBrains</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://assets.website-files.com/5f75b134a1a14b83f4e06dfc/62823766af2e2ea9e0c02ce2_r_sale.svg" alt=""></div></a></figure><h3 id="-c-scikit-learn">Введение в Машинное обучение c scikit-learn</h3><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/course-cover_-------------------------------scikit-learn.png" class="kg-image" alt loading="lazy" width="750" height="423" srcset="__GHOST_URL__/content/images/size/w600/2021/12/course-cover_-------------------------------scikit-learn.png 600w, __GHOST_URL__/content/images/2021/12/course-cover_-------------------------------scikit-learn.png 750w" sizes="(min-width: 720px) 720px"></figure><p>Уровень: начальный</p><p>Направления: статистика, Машинное обучение</p><p>Технологии: matplotlib, NumPy, os, Pandas, pandas_profiling, yfinance, Scikit-learn (SimpleImputer, SelectKBest, StandardScaler, PCA, RandomForestClassifier</p><div class="kg-card kg-button-card kg-align-center"><a href="https://www.youtube.com/playlist?list&#x3D;PLfac_zi0WUMx0NwzSxP_fCDcaWC-exB0Y" class="kg-btn kg-btn-accent">Смотреть на YouTube</a></div><hr><h3 id="tensorflow-">TensorFlow для начинающих</h3><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/course-cover_tensorflow---------------.png" class="kg-image" alt loading="lazy" width="750" height="423" srcset="__GHOST_URL__/content/images/size/w600/2021/12/course-cover_tensorflow---------------.png 600w, __GHOST_URL__/content/images/2021/12/course-cover_tensorflow---------------.png 750w" sizes="(min-width: 720px) 720px"></figure><p>Уровень: продвинутый</p><p>Направления: Машинное обучение, Глубокое обучение</p><p>Технологии: numpy, tensorboard, os, time, keras, matplotlib</p><div class="kg-card kg-button-card kg-align-center"><a href="https://www.youtube.com/playlist?list&#x3D;PLfac_zi0WUMxBiCMczdUGMsUCv2-z1MHF" class="kg-btn kg-btn-accent">Смотреть на YouTube бесплатно</a></div><hr><h3 id="-c-numpy-pandas">Статистический анализ c NumPy, Pandas</h3><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/course-cover_-----------------------numpy-pandas-500.png" class="kg-image" alt loading="lazy" width="889" height="501" srcset="__GHOST_URL__/content/images/size/w600/2021/12/course-cover_-----------------------numpy-pandas-500.png 600w, __GHOST_URL__/content/images/2021/12/course-cover_-----------------------numpy-pandas-500.png 889w" sizes="(min-width: 720px) 720px"></figure><p>Уровень: начальный, средний</p><p>Направления: статистика</p><p>Технологии: numpy, pandas, matplotlib, scipy, cufflinks, re, mailbox, csv, sklearn, seaborn, datetime, pylab, ipywidgets, IPython</p><div class="kg-card kg-button-card kg-align-center"><a href="https://www.udemy.com/course/data-science-numpy-pandas" class="kg-btn kg-btn-accent">Получить на Udemy</a></div><hr><h3 id="-pytorch">Ускоренный курс PyTorch</h3><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/course-cover_-----------------pytorch.png" class="kg-image" alt loading="lazy" width="750" height="423" srcset="__GHOST_URL__/content/images/size/w600/2021/12/course-cover_-----------------pytorch.png 600w, __GHOST_URL__/content/images/2021/12/course-cover_-----------------pytorch.png 750w" sizes="(min-width: 720px) 720px"></figure><p>Уровень: продвинутый</p><p>Направления: Машинное обучение</p><p>Технологии: PyTorch, NumPy, TensorBoard; PyTorch.nn</p><div class="kg-card kg-button-card kg-align-center"><a href="https://www.youtube.com/playlist?list&#x3D;PLfac_zi0WUMyG5tEJdmRPXY5sAaZiR3gj" class="kg-btn kg-btn-accent">Смотреть на YouTube</a></div>		courses	2020-12-01		
13	Случайная переменная (Random Variable)		<p>Случайная переменная – случайная величина, которая используется в теории вероятностей, статистике и <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a> для количественной оценки результатов случайного происшествия и принимают множество значений.</p><p>Случайные <a href="__GHOST_URL__/sluchainaia-vielichina/www.helenkapatsa.ru/priznak/">переменные</a> должны быть измеримыми и обычно являются действительными числами. Например, буква X может обозначать сумму чисел, полученных после броска трех кубиков. В этом случае X может быть 3 (1 + 1+ 1), 18 (6 + 6 + 6) или где-то между 3 и 18, поскольку наибольшая грань кубика равна 6, а наименьшая – 1.</p><p>Случайная величина может быть дискретной или непрерывной. Дискретные случайные величины принимают подсчитываемое число различных значений. Рассмотрим эксперимент, в котором монету бросают три раза. Если X представляет количество раз, когда монета выпадает орлом, то X – это дискретная случайная величина, которая может иметь только значения 0, 1, 2, 3 (от отсутствия орла в трех последовательных подбрасываниях монеты ко всем трем орлам). Для X невозможно другое значение.</p><p><em>Фото: <a href="https://www.amazon.com/QUICK-PICK-MAGIC/b/ref=bl_dp_s_web_15276350011?ie=UTF8&amp;node=15276350011&amp;field-lbr_brands_browse-bin=QUICK+PICK+MAGIC">@QUICK PICK MAGIC</a></em></p>		sluchainaia-vielichina	2020-12-02		
14	Словесные вложения (Word Embeddings)		<p>Словесные вложения – это векторное представление синтаксической единицы (слова или словосочетания), позволяющее уловить контекст. Такие векторы активно используются для широкого спектра задач, к примеру, для определения общей эмоциональной окраски текста.</p><p>Пример. Рассмотрим два похожих предложения: "Хорошего Вам дня" и "Отличного Вам дня". Компьютеру понять разницу в их значении довольно сложно, пока мы не создадим исчерпывающий словарь V = {Вам, дня, отличного, хорошего}.</p><p>Давайте создадим вектор <a href="__GHOST_URL__/bystroie-kodirovaniie/">быстрого кодирования</a> (One-Hot Encoding) для каждого элемента словаря V. Мы получим векторы из нулей и единиц, где единицы представляют место слова в V:</p><ul><li>Вам = [1, 0, 0, 0]</li><li>дня = [0, 1, 0, 0]</li><li>отличного = [0, 0, 1, 0]</li><li>хорошего = [0, 0, 0, 1]</li></ul><p>Если мы попытаемся визуализировать эти кодирования, мы сможем представить себе четырехмерное пространство, где каждое слово занимает одно из измерений и не имеет ничего общего с остальными (без проекции по другим измерениям). Это означает, что «Вам» и «дня» так же близки по значению, как «отличного» и «хорошего», а это неверно.</p><p>Наша цель состоит в том, чтобы слова с похожим контекстом занимали близкие пространственные позиции. С точки зрения математики, косинус угла между такими векторами должен стремиться к единице, то есть угол стремиться к нулю.</p><p>Здесь возникает идея <a href="__GHOST_URL__/raspriedieliennyie-priedstavlieniia/">распределенных представлений</a>. Мы интуитивно вводим некоторую зависимость одного слова от другого, чтобы устранить словарную независимость быстрого кодирования. С этим прекрасно справляется метод <a href="__GHOST_URL__/word2vec/">Word2Vec</a> и его разновидности – <a href="__GHOST_URL__/skip-gram/">скип-грам</a> (Skip-Gram) и <a href="__GHOST_URL__/nieprieryvnyi-mieshok-slov/">СBOW</a> (Continuous Bag of Words – "Обычный мешок слов"). </p><p>Посмотрим, как работает этот метод на Python. Импортируем библиотеку Gensim:</p><pre><code class="language-python">&gt;&gt;&gt; from gensim.models import Word2Vec</code></pre><p>Определим тренировочные данные:</p><pre><code class="language-python">&gt;&gt;&gt; sentences = [\n     ['это', 'первое', 'предложение', 'для', 'word2vec'],\n     ['это', 'второе', 'предложение'],\n     ['еще', 'одно', 'предложение'],\n     ['и', 'еще', 'одно'],\n     ['и', 'последнее', 'предложение']\n]</code></pre><p>Обучим модель:</p><pre><code class="language-python">&gt;&gt;&gt; model = Word2Vec(sentences, min_count = 1)\n&gt;&gt;&gt; print(model)\nWord2Vec(vocab=10, size=100, alpha=0.025)</code></pre><p>Выведем словарь:</p><pre><code class="language-python">&gt;&gt;&gt; words = list(model.wv.vocab)print(words)\n['это', 'первое', 'предложение', 'для', 'word2vec', 'второе', \n'еще', 'одно', 'и', 'последнее']</code></pre><p>Отобразим вектор для слова "предложение"</p><pre><code class="language-python">&gt;&gt;&gt; print(model['предложение'])\n[ 2.7951182e-04 -3.6876823e-03 -3.4739964e-03 -4.6593761e-03\n -6.8065652e-04  4.6762540e-03 -4.9465592e-03 -1.5376145e-03\n -3.8114607e-03  1.9782037e-03  4.8228186e-03 -4.4843061e-03\n -1.3968484e-03  3.9083948e-03 -3.8989992e-03 -3.7398134e-04\n -4.4471960e-04 -3.5032684e-03 -3.0881944e-03 -3.6933757e-03\n -3.1514678e-03 -7.4106758e-04 -3.2368137e-03  7.4947416e-04\n -3.8781634e-03 -4.5922254e-03 -1.8124420e-03 -4.7787088e-03\n -9.7891991e-04 -1.4414476e-03 -1.3748884e-03 -1.8673521e-03\n  2.7702458e-03  2.6762027e-03 -3.4182698e-03 -3.2053180e-03\n -3.4514379e-03  9.6735061e-04 -2.2312114e-03  1.9647963e-03\n  5.5354909e-04  8.5186592e-04 -3.7264288e-03  2.9388207e-03\n -2.3999424e-03 -1.4355985e-03  2.6428787e-04  3.1569401e-05\n -2.1141735e-03 -2.2317225e-03  5.8705662e-04 -2.2599476e-03\n -4.6893093e-03 -1.8836980e-03  1.2283499e-03  1.9487643e-03\n  2.5195950e-03  3.3758313e-03  2.0288548e-03 -1.4213409e-03\n  3.8691245e-03 -4.8152455e-03 -4.3285973e-03 -1.7564168e-03\n -3.2937850e-03  9.5582556e-04 -4.7819293e-03  4.0930249e-03\n  3.9580315e-03 -4.0846365e-03 -2.4885340e-03  4.3221661e-03\n  4.4720429e-03 -1.2656129e-03 -1.5065590e-03  2.2809014e-03\n  2.2248828e-03  3.2351266e-03 -2.7615167e-03 -8.9465710e-04\n  2.5687986e-03  1.3637041e-04  2.6037137e-03 -3.8954872e-03\n -1.4932255e-03 -1.1712565e-03  4.9797511e-03  1.6469969e-03\n  2.5363572e-04 -1.6010831e-03 -2.7519872e-03  3.0719070e-03\n  7.8561262e-04  8.4289414e-04 -2.1506855e-03 -2.5069705e-04\n  3.7172137e-04  2.2009613e-03 -8.7891234e-04  4.8939087e-03]</code></pre>		slovesnyye-vlozheniya	2020-12-03		
15	Скип-грамм (Skip Gram)		<p>Скип-грамм - это один из методов обучения без учителя, который используется для поиска близких по тематике слов для заданного слова. Алгоритм противоположен <a href="__GHOST_URL__/nieprieryvnyi-mieshok-slov/">CBOW</a>: здесь целевое слово вводится, а слова контекста выводятся. </p><p>Скип-грамм– это одна из моделей <a href="__GHOST_URL__/skip-gram/www.helenkapatsa.ru/word2vec/">word2vec</a>, и слова представлены в них как вектор в n-мерном пространстве (или n-мерном пространстве признаков), приближая похожие слова друг к другу. </p><p>Модель скип-граммы - одна из самых важных концепций в <a href="__GHOST_URL__/obrabotka-yestestvennogo-yazyka/">Обработке естественного языка (NLP)</a>, и мы должны понимать, как именно она работает.</p><p>Во-первых, нам нужно осознать, что компьютеры не понимают слов. Все, что понимает компьютер, – это числа. Поэтому нам нужно найти способ преобразовать каждое данное слово в число. Есть два способа сделать это – быстрое кодирование (One-Hot Encoding) и вектора вещественных чисел (Continuous Vectors).</p><h2 id="-">Быстрое кодирование</h2><p>Это способ представления <a href="__GHOST_URL__/katieghorialnaia-pieriemiennaia/">категориальных переменных</a> в векторных формах (состоящих из чисел, что понимают компьютеры). Первое, что нужно сделать, когда мы создаем такую кодировку для слова, присутствующего в нашем словаре, – это присвоить всем словам уникальные индексы. Это можно сделать, отсортировав слова в алфавитном порядке по возрастанию или убыванию или, возможно, расположив их в произвольном порядке. Самое важное здесь – индекс.</p><p>Допустим, исходный текст – скороговорка: "Даже шею, даже уши ты испачкал в черной туши. Становись скорей под душ. Смой с ушей под душем тушь. Смой и с шеи тушь под душем.". Тогда словарь будет выглядеть следующим образом: </p><pre><code class="language-python">{'в', 'даже', 'душ', 'и', 'испачкать', 'под', 'с', 'скорей', 'смыть', 'становиться', 'тушь', 'ты', 'уши', 'шея', 'черный', 'шея'}</code></pre><p>Для слова 'даже' быстрое кодирование выглядит так:</p><pre><code class="language-python">[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</code></pre><p>А для "черный" – так:</p><pre><code class="language-python">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</code></pre><p>Однако метод обладает рядом недостатков:</p><ul><li>Длина словаря может доходить до 1 миллиона элементов, что делает хранении данных крайне неэффективным</li><li>Такая кодировка не передает смысл слова: расстояние между кодировками любых двух слов всегда одинаково. Нам нужно кодирование, дающее представление о смысле слова. Здесь в игру вступают вещественные векторы.</li></ul><h2 id="--1">Вещественные векторы</h2><p>Такие векторы состоят из действительных чисел (а не только 0 или 1). В этом представлении нет фиксированного правила для длины таких векторов. Вы можете выбрать любую длину (любое количество функций) для представления слов, присутствующих в данном словаре.<br>Пример. Возьмем слова "Бэтмен", "Джокер", "Человек-паук" и "Танос". Вектора каждого из этих слов для трех характеристик супергероев выглядят следующим образом:</p><pre><code class="language-python">word2vec(‘Бэтмен’) = [0.9, 0.8, 0.2]\nword2vec(‘Джокер’) = [0.8, 0.3, 0.1]\nword2vec(‘Человек-паук’) = [0.2, .9, 0.8]\nword2vec(‘Танос’) = [0.3, 0.1, 0.9]</code></pre><p>Первый элемент этих векторов олицетворяет принадлежность к Вселенной DC. Обратите внимание: «Бэтмен» и «Джокер» имеют более высокие значения для своей 1-й функции, потому что принадлежат ко Вселенной DC.</p><p>Второй элемент в представлении word2vec здесь отвечает за "доброту" персонажа. Вот почему «Бэтмен» и «Человек-паук» имеют более высокие значения, а «Джокер» и «Танос» – меньшие.</p><p>Третий компонент векторов представляет наличие суперсилы. Все мы знаем, что «Бэтмен» и «Джокер» не обладают таковыми, и поэтому их векторы имеют небольшие числа на третьей позиции. Вот так вещественные вектора и улавливают значения слов.</p><h2 id="--2">Происхождение словесных вложений и вещественных векторов</h2><p>Словесные вложения и вещественные векторы не имеют изначального представления о числах, составляющих вектора, они обучаются на больших <a href="__GHOST_URL__/skip-gram/www.helenkapatsa.ru/tiekstovyi-blok/">текстовых блоках (corpuses)</a>. Вложения улавливают семантическое значение только тогда, когда обучаются на огромных объемах текста. Перед обучением вложения слов инициализируются случайным образом, и они вообще не имеют никакого смысла. Только когда модель обучена, вложения слов улавливают семантическое значение всех слов.</p><p>Первоначально вложения слов инициализируются случайным образом, и они не имеют никакого смысла, так же как ребенок не понимает разные слова. И только после того, как модель начала обучаться, векторы / вложения слов начинают улавливать значение слов, точно так же, как ребенок слышит и запоминает разные слова.</p><p>Вся идея глубокого обучения была вдохновлена человеческим мозгом. Чем больше он видит, тем больше понимает и учится.</p><h2 id="--3">Модель скип-граммов</h2><p>Модель скип-граммов, как и все другие модели word2vec, использует прием, который также используется во многих других алгоритмах <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Поскольку у нас нет предварительно подготовленных ярлыков, связанных со словами, это <a href="__GHOST_URL__/obuchieniie-s-chastichnym-privliechieniiem-uchitielia/">Обучение с частичным привлечением учителя (Semi-Supervised Learning)</a>, потому что у нас нет прямых ярлыков, связанных со словами, но мы используем соседние слова (контекстного слова в предложении) в качестве ярлыков.</p><p>Word2vec принимает на входе большой корпус текста и создает векторное пространство (пространство признаков), обычно состоящее из нескольких сотен измерений, причем каждому уникальному слову в корпусе назначается соответствующий вектор в пространстве. Это делается путем создания пар контекста и целевого слова, что дополнительно зависит от так называемого размера окна.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/skip-gram-word-pairs.png" class="kg-image" alt loading="lazy" width="1963" height="1199" srcset="__GHOST_URL__/content/images/size/w600/2020/12/skip-gram-word-pairs.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/skip-gram-word-pairs.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/skip-gram-word-pairs.png 1600w, __GHOST_URL__/content/images/2020/12/skip-gram-word-pairs.png 1963w" sizes="(min-width: 720px) 720px"></figure><p>На изображении выше показано, как создаются пары слов "контекст – цель", которые можно использовать для обучения модели скип-граммов. Мы используем параметр "размер окна" (в данном случае он равен двум), который учитывает одно слово слева и два справа от контекстного. Давайте изучим процесс подробнее.</p><ol><li>У нас есть предложение. Берем первое слово "русский": это наше контекстное слово. Мы составляем пары из этого контекстного слова со словами справа ("медленно" и "запрягает"). Таким образом, у нас есть 2 пары. Первая пара – "русский", "медленно", а вторая – "русский", "запрягает". Мы не можем соединить контекстное слово "русский" со словами слева, потому что слов там нет.</li><li>Теперь мы переходим ко второму контекстному слову ("медленно") и соединяем его со словом слева и двумя справа. На этот раз мы получим три пары: "медленно", "русский"; "медленно", "запрягает"; "медленно", "да".</li><li>Мы продолжаем создавать пары контекстно-целевых слов для всех возможных элементов датасета. По сути, мы здесь собираем пары слов, которые можно найти рядом друг с другом. Мы фиксируем контекст и собираемся использовать эту контекстную информацию в дальнейшем. Надеюсь, теперь понятно, почему этот алгоритм называется обучением с частичным привлечением учителя.</li></ol><h2 id="--4">Обучение</h2><p>На данный момент у нас есть готовый словарь из 10 000 слов, а также пары слов "контекст – цель". Мы берем вектор быстрого кодирования контекстного слова и передаем его в нейронную сеть. Эти входные данные умножаются на <a href="__GHOST_URL__/vies/">веса (Weights)</a> скрытых слоев, и в конце мы получаем выходной вектор 10 000 компонентов.</p><p>Обратите внимание: к выходному слою применена <a href="__GHOST_URL__/softmaks-funktsiia/">софтмакс-функция (Softmax</a>), которая преобразует выходной вектор нейронной сети в вектор вероятности, причем каждый компонент представляет вероятность появления слова в вблизи целевого.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/skip-gram-neurons.png" class="kg-image" alt loading="lazy" width="2000" height="1171" srcset="__GHOST_URL__/content/images/size/w600/2020/12/skip-gram-neurons.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/skip-gram-neurons.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/skip-gram-neurons.png 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/skip-gram-neurons.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Затем кросс-энтропия (Cross-Entropy) применяется для вычисления потерь и обновляет веса.</p><h2 id="--5">Применение</h2><p>Этот сниппет использует библиотеку Keras, автоматически генерирует слова и определяет, релевантны ли они.</p><pre><code class="language-python">from keras.preprocessing.sequence import skipgrams\n\n# Генерация скип-граммов с размером окна 10\nskip_grams = [skipgrams(wid, vocabulary_size = vocab_size, window_size = 10) for wid in wids]\n\n# Отображение семплов скип-грамма\npairs, labels = skip_grams[0][0], skip_grams[0][1]\nfor i in range(10):\n    print("({:s} ({:d}), {:s} ({:d})) -&gt; {:d}".format(\n          id2word[pairs[i][0]], pairs[i][0], \n          id2word[pairs[i][1]], pairs[i][1], \n          labels[i]))</code></pre><p>Мы можем увидеть, что релевантно друг другу, а что нет, на основе метки (0 или 1).</p><pre><code class="language-python">(james (1154), king (13)) -&gt; 1\n(king (13), james (1154)) -&gt; 1\n(james (1154), perform (1249)) -&gt; 0\n(bible (5766), dismissed (6274)) -&gt; 0\n(king (13), alter (5275)) -&gt; 0\n(james (1154), bible (5766)) -&gt; 1\n(king (13), bible (5766)) -&gt; 1\n(bible (5766), king (13)) -&gt; 1\n(king (13), compassion (1279)) -&gt; 0\n(james (1154), foreskins (4844)) -&gt; 0</code></pre>		skip-gramm	2020-12-06		
16	Категориальная переменная (Categorical Variable)		<p>Категориальная (номинальная) переменная – это <a href="__GHOST_URL__/katieghorialnaia-pieriemiennaia/www.helenkapatsa.ru/priznak/">переменная</a>, которая состоит из <a href="__GHOST_URL__/iarlyk/">Ярлыков (Label)</a>, и количество возможных значений часто ограничено.</p><p>Пример. Банк собирает данные о своих клиентах с целью определить, кто готов приобрести кредитный продукт. Для этого проводится анкетирование, с помощью которого выясняется:</p><ul><li>Образование индивида: законченное школьное образование, бакалавриат и проч. </li><li>День звонка: понедельник, вторник и проч.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/categorical-feature-1.jpg" class="kg-image" alt loading="lazy" width="893" height="431" srcset="__GHOST_URL__/content/images/size/w600/2020/12/categorical-feature-1.jpg 600w, __GHOST_URL__/content/images/2020/12/categorical-feature-1.jpg 893w" sizes="(min-width: 720px) 720px"><figcaption>Датасет банка: "Семейное положение" и "Образование" – категориальные переменные</figcaption></figure><p>Некоторые категории вроде "день звонка" имеют естественные взаимосвязи своих значений, например, порядок дней недели.</p><p>Стоит отметить, что если значения переменной являются ограниченным набором чисел (например, возраст), то это не категориальная, а <em>дискретная</em> <a href="__GHOST_URL__/chislovaia-pieriemiennaia/">числовая переменная</a>.</p><p>Если же мы имеем дело с логическим типом значений (да / нет), как столбец (workinkday) в примере ниже, то ее относят к <em>бинарным </em>категориальным переменным.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/statistics-sample-3.png" class="kg-image" alt loading="lazy" width="1343" height="246" srcset="__GHOST_URL__/content/images/size/w600/2020/12/statistics-sample-3.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/statistics-sample-3.png 1000w, __GHOST_URL__/content/images/2020/12/statistics-sample-3.png 1343w" sizes="(min-width: 720px) 720px"></figure><p>Фото: <a href="https://unsplash.com/@v2osk">@v2osk</a></p>		katieghorialnaia-pieriemiennaia	2020-12-06		
17	Числовая переменная (Numeric Variable)		<p>Числовая переменная – <a href="__GHOST_URL__/chislovaia-pieriemiennaia/www.helenkapatsa.ru/priznak/">переменная</a>, выраженная различными видами чисел.</p><ul><li><strong>Дискретная числовая переменная</strong><br>Мы говорим о дискретных данных, если их значения различны и раздельны. В основном, это информация, которую можно классифицировать. Пример: количество орлов в ста бросках монеты.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/numeric-variable-discrete.png" class="kg-image" alt loading="lazy" width="83" height="456"></figure><ul><li><strong>Непрерывная</strong> <strong>числовая переменная</strong><br>Непрерывные данные – это так называемые замеры. Количество возможных значений столбца невозможно подсчитать, поскольку минимальная разница в одну тысячную уже делает значение уникальным. Пример: рост человека.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/numeric-variable-continuous.png" class="kg-image" alt loading="lazy" width="161" height="490"></figure><ul><li><strong>Интервальная числовая переменная</strong><br>Значения интервалов представляют собой упорядоченные элементы, которые имеют одинаковую разницу. Пример: температурные интервалы. Такие данные не имеют нулевого значения, то есть, возвращаясь к нашему примеру, мы не можем выбрать значение "никакая температура". Это делает интервальные переменные особым классом, к которой неприменимы определенные принципы вычислительной статистики.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/numeric-variable-interval.png" class="kg-image" alt loading="lazy" width="118" height="456"></figure><ul><li><strong>Коэффициенты</strong><br>Коэффициенты также являются упорядоченными элементами, имеющими одинаковую разницу. Они подобны интервальным переменным с той лишь разницей, что имеют абсолютный ноль. Пример: длина доски, значение может быть равно нулю.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/numeric-variable-coefficient.png" class="kg-image" alt loading="lazy" width="70" height="456"></figure><p>Фото: <a href="https://unsplash.com/@sbk202">@sbk202</a></p>		chislovaia-pieriemiennaia	2020-12-06		
18	Тензор (Tensor)		<p>Тензор – это многомерный ряд. Это массив чисел, расположенных на сетке с переменным числом осей. </p><pre><code class="language-python">     t111, t121, t131     t112, t122, t132     t113, t123, t133\nT = (t211, t221, t231),  (t212, t222, t232),  (t213, t223, t233)\n     t311, t321, t331     t312, t322, t332     t313, t323, t333</code></pre><p>Тензор – это список из списков. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nfrom numpy import tensordot\nfrom numpy import array</code></pre><p>В приведенном ниже примере тензор 3 x 3 x 3 инициализируется как массив библиотеки NumPy. Мы сначала определяем строки, затем список строк, и затем список списков строк.</p><pre><code class="language-python">T = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])</code></pre><p>Как и в случае с матрицами, мы можем выполнять поэлементные арифметические операции с тензорами.</p><h2 id="-">Сложение</h2><pre><code class="language-python">A = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nC = A + B\nprint(C)</code></pre><p>Сложение сгенерирует такой тензор C:</p><pre><code class="language-python">[[[ 2  4  6]\n  [ 8 10 12]\n  [14 16 18]]\n\n [[22 24 26]\n  [28 30 32]\n  [34 36 38]]\n\n [[42 44 46]\n  [48 50 52]\n  [54 56 58]]]</code></pre><h2 id="--1">Вычитание</h2><pre><code class="language-python">A = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nD = A - B\nprint(D)</code></pre><p>Вычитание – такой тензор D:</p><pre><code class="language-python">[[[0 0 0]\n  [0 0 0]\n  [0 0 0]]\n\n [[0 0 0]\n  [0 0 0]\n  [0 0 0]]\n\n [[0 0 0]\n  [0 0 0]\n  [0 0 0]]]</code></pre><h2 id="--2">Произведение</h2><p>Поэлементное умножение одного тензора на другой такой же размерности приводит к созданию нового тензору такой же размерности. Такой частный случай перемножения с одноразмерными матрицами / тензорами называют произведением Адамара.</p><pre><code class="language-python">A = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nE = A * B\nprint(E)</code></pre><p>Такой код сгенерирует тензор E, который выглядит следующим образом:</p><pre><code class="language-python">[[[  1   4   9]\n  [ 16  25  36]\n  [ 49  64  81]]\n\n [[121 144 169]\n  [196 225 256]\n  [289 324 361]]\n\n [[441 484 529]\n  [576 625 676]\n  [729 784 841]]]</code></pre><p>Если же дан тензор A с q измерений и тензор B с r измерений, их произведение будет новым тензором с q + r размерностями. Например:</p><pre><code class="language-python">A = array([1, 2, 3])\nB = array([3, 4])\nF = tensordot(A, B, axes=0)\nprint(F)</code></pre><p>Результатом будет такой тензор F:</p><pre><code class="language-python">[[ 3  4]\n [ 6  8]\n [ 9 12]]</code></pre><h2 id="--3">Деление</h2><pre><code class="language-python">A = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nG = A / B\nprint(G)</code></pre><p>Такое поэлементное деление одноразмерных тензоров сгенерирует F:</p><pre><code class="language-python">[[[ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]]\n\n [[ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]]\n\n [[ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]]]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1l-3ppVv9i7d_98M4dUlCGMa133fUJeGO?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@salty_sandals">@salty_sandals</a></p>		tenzor	2020-12-08		
19	Обучение без учителя (Unsupervised Learning)		<p>Обучение без учителя – это метод <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, при котором <a href="__GHOST_URL__/modiel/">Модель (Model)</a> обучается на Неразмеченных данных (Unlabeled Data). </p><p><a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Обучение с учителем (Supervised Learning)</a> предполагает, что учебные данные размечены вручную, и модель получает четкий ответ на вопрос, к какой категории принадлежит то или иное <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a>:</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/unsupervised-learning-7.png" class="kg-image" alt loading="lazy" width="1484" height="915" srcset="__GHOST_URL__/content/images/size/w600/2021/02/unsupervised-learning-7.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/unsupervised-learning-7.png 1000w, __GHOST_URL__/content/images/2021/02/unsupervised-learning-7.png 1484w"><figcaption>Метрики опухолей (сверху) и клиентская база</figcaption></figure><p>Задачей же Unsupervised Learning может быть обнаружение групп похожих примеров в данных, и это называется Кластеризацией (Clustering), или определение того, как данные распределяются в пространстве, и это известно как Оценка плотности (Density Estimation). Такие алгоритмы позволяют выполнять более сложные задачи обработки по сравнению с обучением с учителем, хоть и являются менее предсказуемыми.</p><p>Обучение без учителя:</p><ul><li>Находит в данных неизвестные закономерности</li><li>Помогает найти полезные для категоризации <a href="__GHOST_URL__/priznak/">Признаки (Features)</a></li><li>Стоит дешевле (т.е. подготовка неразмеченных данных)</li><li>Не обязывает определять число классов </li></ul><p>К примеру, в семье с ребенком есть еще и собака, которую он узнает среди остальных живых существ. Друг семьи приводит с собой другую собаку и пытается подружить своего четвероногого друга с малышом. Малыш раньше не видел этого пса, но узнает характерные черты (два уха, два глаза, ходьба на четырех ногах) и идентифицирует новое животное как собаку. Это простейшая аналогия обучения без учителя, при условии, что гость не указал ребенку, что его компаньон – пес.</p><p>Наряду с этим выделяют еще три разновидности обучения:</p><ul><li>Обучение с учителем (Supervised Learning)</li><li><a href="__GHOST_URL__/obuchieniie-s-chastichnym-privliechieniiem-uchitielia/">Обучение с частичным привлечением учителя (Semi-Supervised Learning)</a></li><li><a href="__GHOST_URL__/obuchieniie-s-podkrieplieniiem/">Обучение с подкреплением (Reinforcement Learning)</a></li></ul><h3 id="pca">PCA</h3><p>Посмотрим, как работает Unsupervised Learning на примере <a href="__GHOST_URL__/mietod-ghlavnykh-komponient/">Анализа главных компонент (PCA)</a>. Для начала импортируем Scikit-learn и Matplotlib:</p><pre><code class="language-python">from sklearn import datasets\nfrom sklearn import decomposition\nimport matplotlib.pyplot as plt</code></pre><p>Загрузим датасет с помощью встроенного метода <code>load_digits()</code>:</p><pre><code class="language-python">digits = datasets.load_digits()\nX = digits.data\ny = digits.target</code></pre><p>Мы используем такое разделение данных на X и y, поскольку так создатели организовали данные. Если посмотреть, что эти компоненты из себя представляют, мы увидим два массива:</p><pre><code class="language-python">X</code></pre><p>Каждый из рядов – это набор чисел, характеризующий яркость пиксела на сетке 8 х 8:</p><pre><code class="language-python">array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n       ...,\n       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n       [ 0.,  0., 10., ..., 12.,  1.,  0.]])</code></pre><p>y, в свою очередь, – это список целевых значений. Нам предстоит научить модель ассоциировать наборы пикселей с цифрами.</p><pre><code class="language-python">y</code></pre><pre><code class="language-python">array([0, 1, 2, ..., 8, 9, 8])</code></pre><p>Выстроим пикселы в сетки обратно и отобразим в оттенках серого:</p><pre><code class="language-python"># Зададим размер холста\nplt.figure(figsize = (16, 6))\n\nfor i in range(10):\n    # Организуем полотно с двумя рядами по 5 графиков\n    plt.subplot(2, 5, i + 1)\n\n    # Каждый ряд X превратим в матрицу 8 х 8 и используем черно-белую гамму\n    plt.imshow(X[i,:].reshape([8,8]), cmap = 'gray');</code></pre><p>Вот такая магия – из чисел в картинки:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/unsupervised-learning-digits.jpg" class="kg-image" alt loading="lazy" width="1001" height="441" srcset="__GHOST_URL__/content/images/size/w600/2021/02/unsupervised-learning-digits.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/02/unsupervised-learning-digits.jpg 1000w, __GHOST_URL__/content/images/2021/02/unsupervised-learning-digits.jpg 1001w" sizes="(min-width: 720px) 720px"></figure><p>Метод главных компонент сократит количество измерений с 64 (длина каждого ряда) до 2, и с этой точки зрения сможет произвести кластеризацию, т.е. распознавание той или иной цифры в наборе данных: </p><pre><code class="language-python">pca = decomposition.PCA(n_components = 2)\nX_reduced = pca.fit_transform(X) # Передадим данные модели\n\nplt.figure(figsize = (12, 10))\n\n\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n\t\t\t# Цветов столько же, сколько и цифр\n\t\t\tc = y, \n            \n\t\t\t# Настроим уникальный цвет каждой цифре  \n            edgecolor = 'none', \n            \n            # Сделаем точки на 70% непрозрачными \n            # Для общего восприятия картины\n            alpha = 0.7, \n            \n            # Зададим размер точки\n            s = 40,\n            \n\t\t\t# Используем предустановленную цветовую схему\n            cmap = plt.cm.get_cmap('nipy_spectral', 10))\n            \n# Цветовая шкала справа от графика как легенда            \nplt.colorbar()\n\n# MNIST (Modified National Institute of Standards and Technology) –\nбаза рукописных цифр\nplt.title('MNIST. Проекция PCA');</code></pre><p>Теперь мы лучше понимаем, почему капча как идея существует столько лет: компьютеры по-прежнему имеют весьма туманное представление о цифрах, и распознать наверняка одну от другой не в состоянии:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/unsupervised-learning-pca.png" class="kg-image" alt loading="lazy" width="737" height="674" srcset="__GHOST_URL__/content/images/size/w600/2021/02/unsupervised-learning-pca.png 600w, __GHOST_URL__/content/images/2021/02/unsupervised-learning-pca.png 737w" sizes="(min-width: 720px) 720px"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1W7ZSlehXaUYXoMbi6any927WhxUrr12s?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@gabrielperelman">@gabrielperelman</a></p>		niekontroliruiemoie-obuchieniie	2020-12-09		
20	Вектор (Vector)		<p>Вектор - это кортеж из одного или нескольких значений-скаляров. Векторы строятся из компонентов, которые являются обычными числами. Вы можете думать о векторе как о списке чисел, а о векторной алгебре как об операциях, выполняемых над числами в списке.</p><p>Векторы являются основополагающим элементом линейной алгебры и используются в Науке о данных при описании <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> для создания нейросети.</p><!--kg-card-begin: markdown--><p>$$v = (v_1, v_2, v_3)$$</p>\n<!--kg-card-end: markdown--><p>Обычно в <a href="Scikit-learn">Машинном обучении (ML)</a> целевую переменную  представляют как вектор 'y'.</p><p>Векторы легче понять с использованием геометрической аналогии: вектор представляет точку или координату в n-мерном пространстве, где n – количество измерений, например 2. Его также можно рассматривать как линию от начала векторного пространства с направлением и величиной. А вот скаляр не имеет направления.</p><p>Теперь, когда мы знаем, что такое вектор в линейной алгебре, давайте посмотрим, как инициализировать его на Python.</p><h2 id="-">Инициализация вектора</h2><p>Вектор принято инициализировать как массив NumPy. Например, мы определяем вектор длиной в три элемента и состоящего из целых чисел 1, 2 и 3. Для начала импортируем все необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nfrom numpy import array</code></pre><pre><code class="language-python">v = array([1, 2, 3])\nprint(v)</code></pre><p>Отобразится вектор так:</p><pre><code class="language-python">[1 2 3]</code></pre><h2 id="--1">Векторная арифметика</h2><p>В этом разделе вы познакомитесь с векторно-скалярной арифметикой, в которой все операции выполняются поэлементно между двумя векторами одинаковой длины, в результате чего получается новый вектор такой же длины.</p><h2 id="--2">Сложение векторов</h2><p>Два вектора равной длины можно сложить вместе.</p><pre><code class="language-python">c = a + b</code></pre><p>Новый вектор имеет ту же длину, что и два других. Каждый элемент нового вектора – сумма элементов с тем же индексом; например:</p><pre><code class="language-python">a + b = (a1 + b1, a2 + b2, a3 + b3)</code></pre><p>Или, по-другому:</p><pre><code class="language-python">c[0] = a[0] + b[0]\nc[1] = a[1] + b[1]\nc[2] = a[2] + b[2]</code></pre><p>Мы можем складывать векторы прямо в Python, суммируя массивы. В примере ниже определяются два вектора с тремя элементами в каждом, которые затем складываются.</p><pre><code class="language-python">a = array([1, 2, 3])\nprint(a)\nb = array([1, 2, 3])\nprint(b)\nc = a + b\nprint(c)</code></pre><p>При вычислении сначала отображаются два "родительских" вектора, а затем новый вектор-сумма.</p><pre><code class="language-python">[1 2 3]\n[1 2 3]\n[2 4 6]</code></pre><h2 id="--3">Вычитание вектора</h2><p>Один вектор можно вычесть из другого, такой же длины.</p><pre><code class="language-python">c = a - b</code></pre><p>Как и при сложении, новый вектор имеет ту же длину, что и родительские векторы, и каждый элемент нового вектора – разница между элементами векторов с одинаковыми индексами.</p><pre><code class="language-python">a - b = (a1 - b1, a2 - b2, a3 - b3)</code></pre><p>Разницу можно представить и так:</p><pre><code class="language-python">c[0] = a[0] - b[0]\nc[1] = a[1] - b[1]\nc[2] = a[2] - b[2]</code></pre><p>Массивами NumPy можно напрямую оперировать следующим образом:</p><pre><code class="language-python">a = array([1, 2, 3])\nprint(a)\nb = array([0.5, 0.5, 0.5])\nprint(b)\nc = a - b\nprint(c)</code></pre><p>В примере мы определяем два вектора с тремя элементами в каждом, а затем вычитает первый из второго. При выполнении кода сначала отображаются два исходных вектора, затем печатается новый, который является разницей.</p><pre><code class="language-python">[1 2 3]\n[ 0.5 0.5 0.5]\n[ 0.5 1.5 2.5]</code></pre><h2 id="--4">Векторное умножение</h2><p>Два вектора одинаковой длины можно перемножить.</p><pre><code class="language-python">c = a * b</code></pre><p>Как и в случае с сложением и вычитанием, эта операция выполняется поэлементно, чтобы получить новый вектор той же длины:</p><pre><code class="language-python">a * b = (a1 * b1, a2 * b2, a3 * b3)</code></pre><p>Умножение можно представить и так:</p><pre><code class="language-python">ab = (a1b1, a2b2, a3b3)</code></pre><p>или так:</p><pre><code class="language-python">c [0] = a [0] * b [0]\nc [1] = a [1] * b [1]\nc [2] = a [2] * b [2]</code></pre><p>NumPy позволяет ускорить эту вычислительную операцию:</p><pre><code class="language-python">a = array([1, 2, 3])\nprint(a)\nb = array([1, 2, 3])\nprint(b)\nc = a * b\nprint(c)</code></pre><p>В примере определяются два вектора с тремя элементами в каждом, а затем векторы перемножаются. На выводе сначала – два исходных вектора, затем – результирующий.</p><pre><code class="language-python">[1 2 3]\n[1 2 3]\n[1 4 9]</code></pre><h2 id="--5">Векторное деление</h2><p>Один вектор можно поделить на другой при условии равенства их длин.</p><pre><code class="language-python">c = a / b</code></pre><p>Как и другие арифметические операции, эта выполняется поэлементно, чтобы получить новый вектор такой же длины.</p><pre><code class="language-python">a / b = (a1 / b1, a2 / b2, a3 / b3)</code></pre><p>Корректно и такое представление:</p><pre><code class="language-python">c[0] = a[0] / b[0]\nc[1] = a[1] / b[1]\nc[2] = a[2] / b[2]</code></pre><p>Мы можем выполнить эту операцию с помощью NumPy.</p><pre><code class="language-python">a = array([1, 2, 3])\nprint(a)\nb = array([1, 2, 3])\nprint(b)\nc = a / b\nprint(c)</code></pre><p>В примере определяются два вектора с тремя элементами в каждом, а затем первый делится на второй.</p><p>При выполнении кода сначала отображаются два родительских вектора, а затем вектор-частное.</p><pre><code class="language-python">[1 2 3]\n[1 2 3]\n[1. 1. 1.]</code></pre><h2 id="-dot-product-">Векторное перемножение (Dot Product)</h2><p>Векторное перемножение – это сумма перемноженных элементов векторов одинаковой длины. Название Dot Product происходит от символа, используемого для его обозначения.</p><pre><code class="language-python">c = a . b</code></pre><p>Скалярное произведение рассчитывается следующим образом:</p><pre><code class="language-python">a . b = (a1 * b1 + a2 * b2 + a3 * b3)</code></pre><p>Мы можем вычислить скалярное произведение, используя функцию dot() библиотеки NumPy.</p><pre><code class="language-python">a = array([1, 2, 3])\nprint(a)\nb = array([1, 2, 3])\nprint(b)\nc = a.dot(b)\nprint(c)</code></pre><p>На выходе мы увидим два исходных вектора, затем скалярное скалярное произведение.</p><pre><code class="language-python">[1 2 3]\n[1 2 3]\n14</code></pre><h2 id="--6">Векторно-скалярное умножение</h2><p>Вектор может быть умножен на скаляр. В результате мы получим масштабированный вектор, где каждый новый элемент кратен исходному.</p><p>Для упрощения обозначений мы будем использовать строчную букву «s» для скалярного значения.</p><pre><code class="language-python">c = s * v</code></pre><p>Умножение выполняется поэлементно, чтобы получить новый масштабированный вектор той же длины.</p><pre><code class="language-python">s * v = (s * v1, s * v2, s * v3)</code></pre><p>или</p><pre><code class="language-python">c[0] = a[0] * s\nc[1] = a[1] * s\nc[2] = a[2] * s</code></pre><p>NumPy прекрасно справляется и с этой операцией:</p><pre><code class="language-python">a = array([1, 2, 3])\nprint(a)\ns = 0.5\nprint(s)\nc = s * a\nprint(c)</code></pre><p>В примере сначала инициализируется вектор, а затем он умножается на скаляр. Выводом будут исходный вектор, скаляр и результат их перемножения.</p><pre><code class="language-python">[1 2 3]\n0.5\n[ 0.5 1. 1.5]</code></pre><p>Точно так же выполняются векторно-скалярные сложение, вычитание и деление.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1WzeQ9y7x6KOB414hyl9h88akpepgb47L?usp=sharing">здесь</a>.</p><p><em>Фото: <a href="https://unsplash.com/@pawel_czerwinski">@pawel_czerwinski</a></em></p>		viektor	2020-12-10		
21	Целевая переменная (Target Variable)		<p>Целевая (зависимая) переменная – признак <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, который предстоит предсказывать <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Зависимой ее называют, поскольку в ходе <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">Разведочного анализа данных (EDA)</a> выявляется <a href="__GHOST_URL__/korrieliatsiia/">Корреляция (Correlation)</a> между одной или несколькими переменными-предикторами (Predictor Variable) и рассматриваемым целевым признаком.</p><p>Пример. Банк собирает данные о своих клиентах и хочет выяснить, сколько заемщиков не смогут выполнить свои обязательства. Для этого была сформирована обширная таблица с историческими данными, и на изображении представлена ее часть:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/target-variable..png" class="kg-image" alt loading="lazy" width="626" height="280" srcset="__GHOST_URL__/content/images/size/w600/2021/01/target-variable..png 600w, __GHOST_URL__/content/images/2021/01/target-variable..png 626w"></figure><p>Целевым признаком в соответствии с задачей является "Невыполнение кредитных обязательств". Мы передаем такую таблицу модели (Model) в качестве "образовательного материала". Иными словами, показываем модели, как выглядит профиль клиента, выплачивающего начисляемые проценты, и как выглядит профиль должника. Это фаза тренировки модели (Model Training).</p><p>Впоследствии на фазе тестирования (Model Testing) модель получает неполную таблицу (в столбце с целевой переменной пусто) с новыми данными, которых намеренно не поступало на фазе обучения. <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> как бы создает передаваемую копию этой части данных и намеренно опустошает столбец с целевой переменной, чтобы протестировать предсказательную способность обученной модели.  В случае с банком тестовая часть датасета выглядит так:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/target-variable-test-2.png" class="kg-image" alt loading="lazy" width="626" height="196" srcset="__GHOST_URL__/content/images/size/w600/2021/01/target-variable-test-2.png 600w, __GHOST_URL__/content/images/2021/01/target-variable-test-2.png 626w"></figure><p>Модель генерирует ряд предсказаний целевой переменной, который сравнивается с оригинальной, полной версией тестовых данных. Такая последовательность применима не ко всем типам данных, ко <a href="__GHOST_URL__/vriemiennoi-riad/">Временным рядам (Time Series)</a> применима соответствующая <a href="__GHOST_URL__/kross-validatsiia/">Кросс-валидация (Cross Validation)</a>.</p><h3 id="-">Разновидности целевых переменных</h3><p>Существует несколько видов таргет-признаков:</p><ul><li><u>Качественная бинарная</u> (как в примере с кредитами): возможными предсказаниями являются "да" или "нет". На самом деле, с тестовыми данными модель генерирует ряд вероятностей от нуля до единицы, которые округляются в бóльшую или меньшую сторону в зависимости от порога (Threshold). В нашем "банковском" примере порогом решили установить число 0,5, и "конвертируются" обратно предсказания таким образом:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/target-variable-test-predictions-1.png" class="kg-image" alt loading="lazy" width="340" height="196"></figure><ul><li><u>Качественная полиномиальная</u>, где возможными классами являются три и более значений, но их число все равно сильно ограничено. К примеру, магазин электроники прогнозирует, насколько высока вероятность покупателя совершить приобретение в этом месяце (0 – низкая, 1 – средняя, 2 – высокая).</li><li><u>Количественная дискретная:</u> модель не занимается классификацией, а предсказывает количество чего-либо. Например, туроператор прогнозирует количество путевок, которые приобретет клиент в последующие 10 лет, и их число варьируется от 1 до, скажем, 70.</li><li><u>Количественная непрерывная</u>: дата-сайентист прогнозирует вещественные значения, например, цены подержанных автомобилей.</li></ul><h3 id="--1">Модель и тип целевой переменной</h3><p>В зависимости от того, какую переменную мы предсказываем, вид модели Машинного обучения меняется, и диаграмма ниже создана, чтобы упростить выбор:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/01/models-and-target-variable-types-1.png" class="kg-image" alt loading="lazy" width="2000" height="774" srcset="__GHOST_URL__/content/images/size/w600/2021/01/models-and-target-variable-types-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/models-and-target-variable-types-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/models-and-target-variable-types-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/01/models-and-target-variable-types-1.png 2400w"></figure><p>Фото: <a href="https://unsplash.com/@jrarce">@jrarce</a></p>		tsielievaia-pieriemiennaia	2020-12-10		
22	Машинное обучение (ML)		<p>Машинное обучение (сокращенно ML) – это наука о том, как заставить компьютеры выполнять объемную вычислительную задачу без явного программирования.</p><p>Классическим алгоритмам дают точные и полные правила для выполнения задачи, алгоритмам Машинного обучения – данные для создания модели. Таким образом, алгоритм может выполнить свою задачу, если модель была скорректирована с учетом данных. Мы говорим, что «подгоняем модель к данным» или «модель должна быть обучена на данных».</p><p>Проиллюстрируем это на простом примере. Предположим, мы хотим спрогнозировать цену дачного дома на основе его площади, размера придомового участка и количества комнат.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/machine-learning-house-price-elements-1.png" class="kg-image" alt loading="lazy" width="1578" height="527" srcset="__GHOST_URL__/content/images/size/w600/2020/12/machine-learning-house-price-elements-1.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/machine-learning-house-price-elements-1.png 1000w, __GHOST_URL__/content/images/2020/12/machine-learning-house-price-elements-1.png 1578w" sizes="(min-width: 720px) 720px"></figure><p>Мы могли бы попытаться построить классический алгоритм, который решает эту проблему. Этот алгоритм возьмет три <a href="__GHOST_URL__/priznak/">Признака (Feature)</a> дома и вернет прогнозируемую цену на основе явного правила. Но на практике эта формула часто неизвестна.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/machine-learning-formula_explicit-formula.png" class="kg-image" alt loading="lazy" width="2000" height="375" srcset="__GHOST_URL__/content/images/size/w600/2020/12/machine-learning-formula_explicit-formula.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/machine-learning-formula_explicit-formula.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/machine-learning-formula_explicit-formula.png 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/machine-learning-formula_explicit-formula.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Явно заданная формула оценки стоимости дома</figcaption></figure><p>Для нашего примера ценообразования классический подход будет заключаться в явном программировании формулы, которая определяет цену дома в зависимости от трех рассматриваемых нами характеристик.</p><p>Однако мы хотим автоматизировать этот процесс и построить нейронный алгоритм, ведь он будет корректировать формулу каждый раз, когда появляются новые примеры цен на жилье. В целом, Машинное обучение невероятно полезно для сложных задач, когда мы располагаем неполной или слишком обильной информацией для программирования вручную. В этих случаях мы можем предоставить информацию, которая у нас есть, для нашей модели, и позволить ей «изучить» недостающую. Затем алгоритм будет использовать статистические методы для извлечения недостающих знаний.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/machine-learning-formula-coefficients_unknown-coefficients.png" class="kg-image" alt loading="lazy" width="2000" height="386" srcset="__GHOST_URL__/content/images/size/w600/2020/12/machine-learning-formula-coefficients_unknown-coefficients.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/machine-learning-formula-coefficients_unknown-coefficients.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/machine-learning-formula-coefficients_unknown-coefficients.png 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/machine-learning-formula-coefficients_unknown-coefficients.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Коэффициенты A, B и C, которые модели Машинного обучения предстоит определить</figcaption></figure><p>Машинное обучение способно выполнять широкий спектр задач: от автоматизации оценки стоимости, до модификации изображений, от помощи на письме до обработки звука. Чтобы получить первое представление о программном коде ней, рассмотрим следующий пример: мы обрабатываем любую мелодию так, чтобы тембр изменился и стал похож на скрипку, флейту или трубу на выбор. Звук (сэмпл) – это комбинация звуков разной частоты, и эти данные тоже подходят для обработки <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сетью (Neural Network)</a>.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/machine-learning-soundwave-1.png" class="kg-image" alt loading="lazy" width="1392" height="266" srcset="__GHOST_URL__/content/images/size/w600/2020/12/machine-learning-soundwave-1.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/machine-learning-soundwave-1.png 1000w, __GHOST_URL__/content/images/2020/12/machine-learning-soundwave-1.png 1392w" sizes="(min-width: 1200px) 1200px"><figcaption>Визуализация ритмичного звукового образца с помощью Ableton Live</figcaption></figure><p>Запустите ячейки <a href="https://colab.research.google.com/drive/1PZOky5t3rcnepPNM6H439eV4aiufGUPH?usp=sharing">этого ноутбука</a>: в нем код по умолчанию скрыт специальной разметкой. Он позволяет произвести Дифференцируемую цифровую обработку сигнала (Differentiable Digital Signal Processing, сокр. DDSP), которая и видоизменит характер звучания.</p><p>Нажимайте все кнопки запуска ячеек последовательно, и тогда код исполнится с предустановленными аудиозаписями и инструментами. На второй ячейке в режиме 'Record' звук будет записываться с помощью встроенного микрофона, так что развлекайтесь, сыграйте или спойте что-нибудь простое! Чтобы впустить Машинное обучение в свою жизнь сразу и полностью, дважды кликните на запускаемую ячейку, и отобразится код.</p><p>Фото: <a href="https://unsplash.com/@draufsicht">@draufsicht</a></p>		mashinnoie-obuchieniie	2020-12-10		
23	Google Colab		<p>Colab – это среда разработки Python, которая запускается в браузере с помощью Google Cloud.</p><p>Это прекрасный способ начать обучение без затрат на настройку вашего компьютера. <a href="__GHOST_URL__/data-saiientist/">Дата-сайентисты (Data Scientist)</a> предпочитают этот продукт по целому ряду причин:</p><ul><li>Ноутбуками (рабочими файлами с кодом) можно делиться так, как это принято во всех продуктах Google.</li><li>Работая со средой в браузере, вы подключаетесь к так называемой удаленной среде выполнения, тем самым компенсируете маломощность своего компьютера, если таковая имеется. </li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/google-colab-specs.png" class="kg-image" alt loading="lazy" width="433" height="288"></figure><ul><li>Передовая политика развития продукта. Вкусы дата-сайентистов разнятся, однако огромное количество людей ценит умение Google определить развивающиеся потребности пользователей, к примеру, быстрое перемещение ячеек по ноутбуку стрелками бокового меню.</li><li>Локальное подключение Google Диска. Чтобы использовать файлы, загруженные в облачное хранилище, следуйте инструкции в одноименном разделе <a href="https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA">документации</a>. </li><li>Длительное время подключения к среде. Когда Вы запустите библиотеку, перебирающую различные <a href="__GHOST_URL__/modiel/">Модели (Model)</a> одну за одной, волей-неволей отвлечетесь в процессе обучения моделей. Здесь на помощь придет резервирование среды на 12 часов во время исполнения задачи. Если же вы хотите задерживать остановку среды вне зависимости от кода, откройте панель разработчика своего браузера (например, Google Chrome DevTools), перейдите в раздел "Console" и введите следующий код:</li></ul><pre><code class="language-python">function KeepClicking(){\n   console.log("Clicking");\n   document.querySelector("colab-toolbar-button#connect").click()\n}setInterval(KeepClicking, 60000)</code></pre><ul><li>Автосохранение. Ошибки компиляции – едва ли не обязательная часть процесса, в сочетании с нестабильным сетевым соединением может провоцировать зависание страницы. Автосохранение в Colab достаточно частое, чтобы спасти ваши усилия в 99% случаев.</li><li>Зачатки визуального изящества. Помимо неспешно нарастающей популярности HTML-разметки, Colab поддерживает "темный" режим экрана, и это наиболее бесшовный способ усовершенствовать свое рабочее пространство в сравнении с Jupyter Notebook. GIF ниже – это создание кликабельной ссылки на тот или иной раздел ноутбука.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/Google-Colab-Section-Linking-2.gif" class="kg-image" alt loading="lazy" width="690" height="388"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1XZVJKOo0kfenVdBarwn8mFLvm7sNQfaC?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@samina">@Samina Hussain</a></p>		google-colab	2020-12-11		
77	Критерий хи-квадрат (Chi-Square Statistic)		<p>Критерий хи-квадрат (χ<sup>2</sup>, критерий согласия Пирсона) – это метрика, которая измеряет, насколько <a href="__GHOST_URL__/modiel/">Модель (Model)</a> сопоставима с фактическими наблюдаемыми данными. Вычисляется с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$χ_c^2 = \\sum_{} \\frac{(O_i - E_i)^2}{E_i}, где$$<br>\n$$c\\space{–}\\space{степень}\\space{свободы,}$$<br>\n$$χ^2\\space{–}\\space{Критерий}\\space{хи-квадрат,}$$<br>\n$$O_i\\space{–}\\space{реальное}\\space{i-e}\\space{наблюдение,}$$<br>\n$$E_i\\space{–}\\space{ожидаемое}\\space{i-e}\\space{наблюдение}$$</p>\n<!--kg-card-end: markdown--><p>Данные, используемые при вычислении этой <a href="__GHOST_URL__/statistika/" rel="noopener noreferrer">Статистики (Statistics)</a>, должны быть случайными, необработанными, взаимоисключающими, взятыми из независимых переменных и взятыми из достаточно большой <a href="__GHOST_URL__/vyborka/" rel="noopener noreferrer">Выборки (Sample)</a>. Например, результаты подбрасывания монеты соответствуют этим критериям.</p><p>При проверке гипотез часто используется критерий Хи-квадрат. Статистика сравнивает размер любых расхождений между ожидаемыми и фактическими результатами, учитывая размер выборки и количество переменных. Для этих тестов используются <a href="__GHOST_URL__/degrees-of-freedom/">Степени свободы (Degrees of Freedom)</a>, чтобы определить, можно ли отклонить определенную <a href="__GHOST_URL__/nulievaia-ghipotieza/">Нулевую гипотезу (Null Hypothesis)</a> на основе общего количества переменных и выборок в эксперименте. Как и в случае с любой другой статистикой, чем больше размер выборки, тем надежнее результаты.</p><p>Существует два основных вида тестов хи-квадрат: тест на независимость, который задает вопрос о взаимоотношениях, например: «Есть ли связь между полом студента и выбором курса?»; и тест согласия, который спрашивает что-то вроде «Насколько хорошо монета в моей руке соответствует теоретически "честной" монете?»</p><h3 id="%D0%BD%D0%B5%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D1%8C">Независимость</h3><p>При изучении взаимосвязи между полом учащегося и выбранным курсом можно использовать критерий χ<sub>2</sub> на независимость. Для проведения этого теста исследователь собирал данные по двум выбранным переменным (пол и выбранные курсы), а затем сравнивал частоту, с которой учащиеся мужского и женского пола выбирали среди предлагаемых классов, используя формулу, приведенную выше, и специальную статистическую таблицу.</p><p>Если нет взаимосвязи между полом и выбором курса (то есть, если они независимы), то следует ожидать, что фактическая частота, с которой студенты мужского и женского пола выбирают каждый предлагаемый курс, будет примерно равной. Число учащихся женского пола на любом выбранном курсе должно быть примерно равным доле студентов мужского в выборке. Тест на независимость может охарактеризовать разницу между фактическим наблюдением и теоретическим ожиданием.</p><h3 id="%D0%B0%D0%B4%D0%B5%D0%BA%D0%B2%D0%B0%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8">Адекватность модели</h3><p>Критерий Хи-квадрат предоставляет способ проверить, насколько хорошо выборка соответствует характеристикам <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population)</a>. Мы не будем использовать выборку, если она не соответствует ожидаемым свойствам интересующей нас совокупности.</p><p>Пример. Рассмотрим воображаемую монету с вероятностью выпадения орла или решки ровно 50/50 и реальную монету, которую вы подбрасываете 100 раз. Если эта реальная монета имеет "справедливую" форму, то она также будет иметь равную вероятность приземления с обеих сторон, и ожидаемый результат подбрасывания монеты: орел выпадет 50 раз, и решка столько же. В этом случае критерий может сказать нам, насколько хорошо фактические результаты 100 подбрасываний монеты сравниваются с теоретической моделью, согласно которой честная монета даст результат 50/50. Фактический бросок может составить 50/50, 60/40 или даже 90/10. Чем дальше фактические результаты от 50/50, тем меньше соответствие этого набора бросков теоретическому ожиданию 50/50 и тем более вероятно, что эта монета на самом деле несправедлива.</p><h3 id="%D0%BA%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9-%D1%85%D0%B8-%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82-%D0%B8-scipy">Критерий Хи-квадрат и SciPy</h3><p>Критерий можно вычислить с помощью функции SciPy. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\n\nimport scipy\nfrom scipy.stats import chisquare</code></pre><p>Инициируем множества <code>X</code> и <code>y</code>, которые являются <a href="__GHOST_URL__/priediktor/">Предикторами (Predictor Variable)</a> и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> соответственно:</p><pre><code class="language-python">X = np.array([[1, 0, 0, 0, 1],\n           [1, 1, 0, 1, 1],\n           [1, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 1],\n           [1, 0, 0, 0, 1],\n           [1, 0, 1, 1, 1],\n           [0, 1, 1, 0, 0],\n           [1, 0, 1, 1, 1],\n          [1, 1, 1, 1, 0]])\ny = np.array([1, 0, 0, 0, 1, 1, 1, 1, 0, 1])</code></pre><p>Переформатируем целевую переменную с помощью метода <code>vstack()</code>^ то есть превратим массивы <code>1-y</code> и <code>y</code> в вертикальные массивы. Выполним <a href="__GHOST_URL__/viektor/#-dot-product-">Векторное перемножение (Dot Product)</a> <code>X</code> и <code>Y</code> и посмотрим на результат:</p><pre><code class="language-python">Y = np.vstack([1 - y, y])\nobserved = np.dot(Y, X)\nobserved</code></pre><p>Это наблюдаемые частоты признаков для каждого класса, то есть Таблица сопряжённости (Contingency Table):</p><pre><code class="language-python">array([[3, 1, 1, 2, 2],\n       [4, 2, 3, 2, 4]])</code></pre><p>Теперь вычислим ожидаемые значения:</p><pre><code class="language-python">feature_count = X.sum(axis = 0)\nclass_prob = Y.mean(axis = 1)\nexpected = np.dot(feature_count.reshape(-1, 1), class_prob.reshape(1, -1)).T\nexpected</code></pre><p>Ожидаемые частоты выглядят так:</p><pre><code class="language-python">array([[2.8, 1.2, 1.6, 1.6, 2.4],\n       [4.2, 1.8, 2.4, 2.4, 3.6]])</code></pre><p>Наконец проведем тест Хи-квадрат, и для этого создадим два объекта <code>score</code> – результаты теста, и <code>pval</code> – <a href="__GHOST_URL__/p-znachieniie/">P-значение (P-Value)</a>:</p><pre><code class="language-python">score, pval = chisquare(observed, expected)\nscore</code></pre><p>Реальные записи довольно плохо соответствуют ожидаемым, и это легко заметить по среднему низкому значению теста. Интересно, что создатели предполагают отображение 8 знаков после запятой, потому третий элемент ряда, "закончившийся" после третьего знака, так забавно выглядит:</p><pre><code class="language-python">array([0.02380952, 0.05555556, 0.375     , 0.16666667, 0.11111111])</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/11Bc3t1nSjA72qaSYa8kST9hsWmeeDFiN?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/c/chi-square-statistic.asp#:~:text=A%20chi%2Dsquare%20(%CF%872,from%20a%20large%20enough%20sample.&amp;text=Chi%2Dsquare%20tests%20are%20often%20used%20in%20hypothesis%20testing.">Adam Hayes</a></p><p>Фото: <a href="https://unsplash.com/@onefabian">@onefabian</a></p>		kritierii-khi-kvadrat	2021-04-08		
24	Модель (Model)		<p>Модель – это абстракция, которая может прогнозировать, выполнять действия со входными значениями или преобразовывать их. Она может быть одним числом, таким как <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее значение (Mean)</a> <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, которое часто используется в качестве базовой модели, выражением с многочленами или набором правил (например, дерева решений), которые определяют, как получить результат.</p><p>В общем, модель определяется набором правил и гиперпараметров, которые определяют ее структуру и возможности. Их необходимо оптимизировать для выполнения поставленной задачи. Гиперпараметром может быть степень полинома или глубина дерева решений. </p><h3 id="-">Разновидности моделей</h3><p>Список существующих моделей не ограничивается 11, но их используют чаще всего:</p><ol><li><a href="https://helenkapatsa.ghost.io/ghost/#/editor/post/5fd4fdb7d1e7890039b7ee59/">Бустинг (Boosting)</a></li><li>[Машина] градиентного бустинга (GBM)</li><li><a href="__GHOST_URL__/dierievo-rieshienii/">Дерево решений (Decision Tree)</a></li><li><a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейная регрессия (Linear Regression)</a></li><li><a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/">Логистическая регрессия (Logistic Regression)</a></li><li><a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метод k-ближайших соседей (kNN)</a></li><li><a href="__GHOST_URL__/mietod-k-sriednikh/">Метод k-средних (K-Means)</a></li><li><a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a></li><li>Наивный байесовский классификатор (Naive Bayes)</li><li>Регрессионный сплайн (Regression Spline)</li><li><a href="__GHOST_URL__/sluchainyi-lies/">Случайный лес (Random Forest)</a></li></ol><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/01/target-variable-MODEL.png" class="kg-image" alt loading="lazy" width="2000" height="949" srcset="__GHOST_URL__/content/images/size/w600/2021/01/target-variable-MODEL.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/target-variable-MODEL.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/target-variable-MODEL.png 1600w, __GHOST_URL__/content/images/2021/01/target-variable-MODEL.png 2280w"></figure><p>На диаграмме видно, что модели повторяют свою классификацию за <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевыми переменными (Target Variable)</a>.</p><p>Фото: <a href="https://unsplash.com/@stanleydai">@stanleydai</a></p>		modiel	2020-12-12		
25	Статистика (Statistics)		<ol><li>Определение Википедии:</li></ol><blockquote>Статистика – это отрасль знаний, наука, в которой излагаются общие вопросы сбора, измерения, мониторинга, анализа массовых статистических (количественных или качественных) данных и их сравнение; изучение количественной стороны массовых общественных явлений в числовой форме.</blockquote><p>2. Статистическая метрика. Пример использования: "У нас есть 50 независимых значений, и мы хотим вычислить одну-единственную <em>статистику</em> – среднее значение". </p><p>Чтобы получить первое представление о том, какое место занимает современная статистика в <a href="__GHOST_URL__/statistika/Scikit-learn">Машинном обучении (ML)</a>, рассмотрим следующий пример: мы предсказываем количество клиентов сервиса аренды велосипедов на основании исторических данных компании. </p><p>Для начала установим специальную библиотеку <a href="__GHOST_URL__/pandas-profiling/">pandas-profiling</a>:</p><pre><code class="language-python">!pip install pandas-profiling</code></pre><p>Затем импортируем несколько вспомогательных библиотек: </p><pre><code class="language-python">import pandas as pd # Вычитывание файла датасета\nimport pandas_profiling # Составление статистического профиля датасета</code></pre><p>Импортируем данные и посмотрим, c каким многообразием бесшовно работает метод <code><strong>describe()</strong></code>: </p><pre><code class="language-python"># Разрешите скачивать файл с помощью 'dl=1' после вопросительного знака-разделителя\ndf = pd.read_csv('https://www.dropbox.com/s/m47eybsrwi4y9zv/hour.csv?dl=1')\ndf.describe()</code></pre><p>Обратите внимание: столбцы состоят из целых чисел (Integer – season), Вещественных чисел (Real Number – temp), бинарной категориальной переменной (workingday) и из <a href="__GHOST_URL__/vriemiennoi-riad/">Временных рядов (Time Series</a> – dteday).</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/statistics-sample-2.png" class="kg-image" alt loading="lazy" width="1343" height="246" srcset="__GHOST_URL__/content/images/size/w600/2020/12/statistics-sample-2.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/statistics-sample-2.png 1000w, __GHOST_URL__/content/images/2020/12/statistics-sample-2.png 1343w" sizes="(min-width: 720px) 720px"></figure><p>Вычислим основные показатели описательной статистики с помощью этого метода. Мы получим классический набор статистик Выборки (Sample) – Количество уникальных значений (Count), <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее (Mean)</a>, <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>, Минимум (Minimum), Квантили (Quantile) .25 (25%), .5 (50%), .75 (75%), Максимум (Maximum). </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/statistics-describe-stats.png" class="kg-image" alt loading="lazy" width="1741" height="389" srcset="__GHOST_URL__/content/images/size/w600/2020/12/statistics-describe-stats.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/statistics-describe-stats.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/statistics-describe-stats.png 1600w, __GHOST_URL__/content/images/2020/12/statistics-describe-stats.png 1741w" sizes="(min-width: 720px) 720px"></figure><p>Запустим профайлер Pandas, передав <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> в качестве аргумента функции ProfileReport(). Вручную запрашивать такие данные, как объем выборки, количество <a href="__GHOST_URL__/priznak/">признаков</a> (Number of variables), <a href="__GHOST_URL__/propusk/">пропусков</a> (Missing cells) и проч. теперь нет необходимости, и Ваши драгоценные умственные ресурсы теперь можно направить на <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">Разведочный анализ данных (EDA)</a>.</p><pre><code class="language-python">pandas_profiling.ProfileReport(df)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/statistics-pandas-stats.png" class="kg-image" alt loading="lazy" width="2000" height="709" srcset="__GHOST_URL__/content/images/size/w600/2020/12/statistics-pandas-stats.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/statistics-pandas-stats.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/statistics-pandas-stats.png 1600w, __GHOST_URL__/content/images/2020/12/statistics-pandas-stats.png 2341w" sizes="(min-width: 720px) 720px"></figure><p>В разделе "Предупреждения" ('Warnings') Вы найдете переменные с высокой <a href="__GHOST_URL__/korrieliatsiia/">Корреляцией (Correlation)</a>; некоторые из них можно в дальнейшем исключить при обучении модели, <a href="__GHOST_URL__/moshchnost/">Мощностью (Cardinality)</a> и проч. Во вкладке "Репродукция" ('Reproduction') – сводку профайлинга (дата начала и окончания, длительность, версию pandas-profiling).</p><p>Профайлер пробегается и по каждой переменной: к примеру, для temp (Нормализованная температура в градусах Цельсия) мы проверяем:</p><ul><li>Нормальность распределения с помощью графика слева. Это необходимо, чтобы в дальнейшем использовать параметрический алгоритм при обучении модели. </li><li>Индикация пропусков (Missing) – важный шаг при подготовке датасета: обнаружив и правильно заполнив пустующие ячейки, Вы "спасете" ценные неполные данные от удаления и в конечном итоге улучшите предсказательную способность модели.</li><li>Вероятно, параметр "Бесконечность" ('Infinite'), рассчитываемый только для вещественных чисел, отыскивает сильно выделяющиеся значения, которыми в некоторых источниках принято обозначать пропуски.</li><li>Среднее значение (Mean)</li><li>Минимум (Minimum)</li><li>Максимум (Maximum)</li><li>Количество нулей (Zeros). Нулями также порой обозначают пропуски.</li><li>Размер памяти (Memory size). Для больших датасетов метрика учитывается, поскольку влияет на скорость обучения модели.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/statistics-pandas-variable-profile.png" class="kg-image" alt loading="lazy" width="2000" height="501" srcset="__GHOST_URL__/content/images/size/w600/2020/12/statistics-pandas-variable-profile.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/statistics-pandas-variable-profile.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/statistics-pandas-variable-profile.png 1600w, __GHOST_URL__/content/images/2020/12/statistics-pandas-variable-profile.png 2339w" sizes="(min-width: 720px) 720px"></figure><p>Следующий интересный раздел – "Корреляции" ('Correlations'). В Машинном обучении принято использовать пять ее типов – r (коэффициент корреляции Пирсона), p (к. к. Спирмана), <a href="http://localhost:8890/lab#correlations_tab-kendall">τ</a> ("тау", к. к. Кендалла), φk ("фи-ка", к. фи-корреляции), φc ("фи-Крамер", к. к. Крамера). Чем ярче (краснее / синее) ячейка, тем сильнее выражена корреляция. Диагональные ячейки игнорируются, поскольку являются результатом расчета коэффициента между переменной и ее копией.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/statistics-pandas-pearson-correlation.png" class="kg-image" alt loading="lazy" width="2000" height="1071" srcset="__GHOST_URL__/content/images/size/w600/2020/12/statistics-pandas-pearson-correlation.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/statistics-pandas-pearson-correlation.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/statistics-pandas-pearson-correlation.png 1600w, __GHOST_URL__/content/images/2020/12/statistics-pandas-pearson-correlation.png 2106w" sizes="(min-width: 720px) 720px"></figure><p>Эти и многие другие статистики Вы с легкостью можете получить с помощью замечательной библиотеки pandas-profiling. Теперь Вы знаете, в чем разница между "статистикой" и "статистиками".</p><p>Ноутбук с кодом, не требующим дополнительной настройки в Jupyter Lab 2.1.5, можно скачать по <a href="https://drive.google.com/file/d/1GAehinQx65Fwq13lkX6MVYDfNVIPmjZE/view?usp=sharing">ссылке</a>.</p><p>Фото: <a href="https://unsplash.com/@jeremybishop">@jeremybishop</a></p>		statistika	2020-12-12		
26	Линейная регрессия (Linear Regression)		<p>Линейная регрессия – один из наиболее широко используемых подходов, используемых для моделирования взаимосвязи между двумя или более переменными. Его можно применять где угодно, от прогнозирования продаж  до предсказания урожайности.В этом посте мы рассмотрим, что такое линейная регрессия, как она работает, и создадим модель <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (Machine Learning)</a> для прогнозирования средней продолжительности жизни человека на основе ряда факторов.</p><h3 id="-"><strong>Что такое линейная регрессия?</strong></h3><p>Линейная регрессия в Машинном обучении – это подход к моделированию отношений между <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> и одной или несколькими "предсказывающими" переменными (Predictor Variable). Проще говоря, это «линия наилучшего соответствия», которая помогает спрогнозировать положение других точек в будущем. Ниже приведен пример линии, которая наилучшим образом соответствует точкам данных. Создавая прямую наилучшего соответствия, Вы сможете также выявить выбросы (Outlier). Предположим, что этот график представляет цену бриллиантов в зависимости от веса. Если мы посмотрим на голубую точку, мы увидим, что этот конкретный бриллиант переоценен.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/linear-regression_standart-2.png" class="kg-image" alt loading="lazy" width="600" height="408" srcset="__GHOST_URL__/content/images/2021/01/linear-regression_standart-2.png 600w"></figure><p>Математики и статисты десятилетиями совершенствуют алгоритмы, которые помогают ответить на вопрос: как же найти наилучшую линию? Давайте познакомимся со способами коррекции направления линии.</p><h3 id="--1"><strong>Как работает простая линейная регрессия</strong></h3><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/linear-regression_correction.png" class="kg-image" alt loading="lazy" width="600" height="408" srcset="__GHOST_URL__/content/images/2021/01/linear-regression_correction.png 600w"></figure><p>Мы собираемся сосредоточиться на простой линейной регрессии. Линия наилучшего соответствия или уравнение, представляющее данные, находится следующим образом:</p><ul><li>Мы строим линию и вычисляем расстояние между точкой и ее проекцией на линию. Расстояние от точки до линии соответствия обозначается здесь красной или зеленой <em>тонкой</em> линией.</li><li>Расстояния от точки до линии соответствия одного цвета возводятся в квадрат и суммируются.</li></ul><p>Мы строим еще одну линию и повторяем вышеописанные шаги. Как только возведенные в квадрат расстояния суммированы, мы сравниваем это значение с предыдущей суммой и выбираем наименьшее. Это сравнение еще называют минимизацией квадратичного расстояния. Этот регламент оставляет немало вопросов: насколько удалять от предыдущей следующую линию? Сколько линий стоит перебрать, прежде чем выбрать наилучшую? В качестве примера выше показаны две «линии наилучшего соответствия»: красная и зеленая. Обратите внимание: суммарная длина тонких зеленых линий гораздо больше таких же красных, что говорит о большей <a href="__GHOST_URL__/oshibka/">Ошибке (Error)</a> в предсказании положения точки в будущем. Если Вы хотите использовать простую линейную регрессию, используйте класс LinearRegression библиотеки Scikit-learn. Импортируем его и NumPy:</p><pre><code class="language-python">from sklearn.linear_model import LinearRegression\nimport numpy as np</code></pre><h3 id="--2">Полиномиальная<strong> линейная регрессия</strong></h3><p>Простая линейная регрессия полезна, когда вы хотите найти уравнение, которое представляет переменную-предиктор (x) и целевую переменную (y). Но что если у Вас много предикторов? Цена автомобиля, определенно, зависит от множества факторов, таких как мощность, размер автомобиля и стоимость бренда. Тогда на сцену выходит полиномиальная (множественная) регрессия. Она используется для объяснения взаимосвязи между зависимой переменной и более чем одной независимой переменной. Вы наверняка уже встречали красочные графики с плоскостями, теперь давайте разберемся, как это работает:</p><pre><code class="language-python">import matplotlib\nfrom matplotlib import cm # Для окрашивания плоскости\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport scipy, scipy.optimize\nfrom mpl_toolkits.mplot3d import Axes3D # Для трехмерного графика</code></pre><p>Итак, мы импортировали все необходимое для работы с трехмерными графиками, теперь определим их внешний вид:</p><pre><code class="language-python"># Параметры графика\ngraphWidth = 8\ngraphHeight = 6\nnumberOfContourLines = 16</code></pre><p>Теперь создадим игрушечный датасет и специальную агрегирующую переменную data, которая будет собирать данные воедино и разъединять, когда требуется:</p><pre><code class="language-python"># Сгенерированные игрушечные данные\nxData = np.array([971.0, 691.0, 841.0, 970.0, 755.0, 684.0, 938.0, 956.0, 658.0, 838.0, 879.0, 752.0, 690.0, 970.0, 964.0, 966.0, 901.0, 671.0, 660.0, 666.0, 765.0, 831.0, 899.0, 668.0, 969.0, 967.0, 651.0, 929.0, 805.0, 812.0, 936.0, 650.0, 964.0, 719.0, 654.0, 646.0, 932.0, 827.0, 917.0, 945.0, 724.0, 956.0, 966.0, 969.0, 968.0, 967.0, 718.0, 966.0, 812.0, 649.0, 645.0, 675.0, 959.0, 966.0, 962.0, 967.0, 956.0, 757.0, 964.0, 817.0, 666.0, 812.0, 902.0, 969.0, 661.0, 962.0, 752.0, 802.0, 670.0, 663.0, 966.0, 967.0, 773.0, 663.0, 818.0, 917.0, 952.0, 834.0, 516.0, 547.0, 846.0, 458.0, 490.0, 835.0, 579.0, 472.0, 557.0, 652.0, 471.0, 455.0, 837.0, 842.0, 832.0, 675.0, 529.0, 509.0, 533.0, 493.0, 572.0, 695.0, 464.0, 846.0, 845.0, 505.0, 833.0, 544.0, 550.0, 594.0, 486.0, 847.0, 471.0, 533.0, 497.0, 838.0, 832.0, 830.0, 847.0, 844.0, 837.0, 831.0, 671.0, 844.0, 824.0, 841.0, 532.0, 576.0, 852.0, 471.0, 496.0, 839.0, 587.0, 478.0, 565.0, 657.0, 481.0, 463.0, 841.0, 842.0, 832.0, 682.0, 532.0, 509.0, 539.0, 497.0, 574.0, 704.0, 472.0, 850.0, 849.0, 512.0, 834.0, 540.0, 542.0, 603.0, 481.0, 847.0, 472.0, 529.0, 496.0, 836.0, 570.0, 588.0, 837.0, 474.0, 781.0, 842.0, 855.0, 846.0, 845.0, 518.0, 854.0, 585.0, 531.0, 539.0, 536.0])\n\nyData = np.array([956.0, 825.0, 963.0, 731.0, 939.0, 879.0, 523.0, 962.0, 880.0, 962.0, 536.0, 942.0, 902.0, 954.0, 662.0, 959.0, 550.0, 798.0, 836.0, 778.0, 945.0, 959.0, 532.0, 880.0, 783.0, 733.0, 833.0, 526.0, 955.0, 956.0, 959.0, 863.0, 714.0, 924.0, 778.0, 849.0, 523.0, 957.0, 960.0, 559.0, 925.0, 959.0, 955.0, 760.0, 953.0, 952.0, 921.0, 713.0, 955.0, 838.0, 819.0, 781.0, 956.0, 950.0, 714.0, 937.0, 955.0, 947.0, 739.0, 957.0, 864.0, 957.0, 531.0, 896.0, 796.0, 954.0, 945.0, 955.0, 762.0, 878.0, 951.0, 953.0, 951.0, 877.0, 959.0, 958.0, 609.0, 791.0, 496.0, 786.0, 597.0, 615.0, 574.0, 432.0, 805.0, 599.0, 793.0, 344.0, 617.0, 615.0, 792.0, 456.0, 807.0, 328.0, 504.0, 543.0, 494.0, 644.0, 803.0, 319.0, 611.0, 690.0, 471.0, 543.0, 392.0, 774.0, 783.0, 812.0, 597.0, 478.0, 627.0, 508.0, 576.0, 799.0, 803.0, 421.0, 534.0, 645.0, 791.0, 422.0, 321.0, 790.0, 384.0, 803.0, 520.0, 797.0, 563.0, 629.0, 581.0, 441.0, 809.0, 602.0, 797.0, 354.0, 625.0, 621.0, 796.0, 463.0, 806.0, 333.0, 511.0, 543.0, 501.0, 648.0, 804.0, 323.0, 620.0, 689.0, 483.0, 554.0, 396.0, 767.0, 777.0, 806.0, 596.0, 479.0, 625.0, 506.0, 574.0, 411.0, 801.0, 811.0, 426.0, 626.0, 811.0, 809.0, 515.0, 805.0, 804.0, 651.0, 564.0, 795.0, 589.0, 576.0, 495.0])\n\nzData = np.array([-2.0, -105.0, -26.0, 44.0, -69.0, -65.0, 60.0, -22.0, -77.0, -24.0, 58.0, -36.0, -66.0, -3.0, 34.0, -8.0, 57.0, -82.0, -98.0, -90.0, -55.0, -23.0, 60.0, -61.0, 29.0, 36.0, -72.0, 61.0, -44.0, -47.0, -27.0, -73.0, 40.0, -37.0, -107.0, -89.0, 68.0, -32.0, -38.0, 63.0, -54.0, -33.0, 16.0, 34.0, 3.0, 15.0, -61.0, 54.0, -39.0, -72.0, -77.0, -97.0, -16.0, 0.0, 45.0, 11.0, -9.0, -57.0, 47.0, -37.0, -82.0, -15.0, 63.0, 21.0, -73.0, 4.0, -55.0, -23.0, -87.0, -74.0, 24.0, -1.0, -46.0, -59.0, -47.0, -18.0, 41.0, 18.0, -104.0, -25.0, 18.0, -55.0, -64.0, 55.0, -35.0, -56.0, -25.0, 63.0, -46.0, -70.0, 16.0, 59.0, -17.0, 78.0, -86.0, -102.0, -113.0, -41.0, -53.0, 68.0, -56.0, 28.0, 24.0, -88.0, 42.0, -59.0, -35.0, -38.0, -79.0, 48.0, -65.0, -113.0, -73.0, 4.0, -8.0, 63.0, 28.0, 23.0, 25.0, 48.0, 74.0, 7.0, 45.0, 11.0, -92.0, -38.0, 29.0, -69.0, -87.0, 56.0, -31.0, -60.0, -29.0, 59.0, -43.0, -53.0, -4.0, 50.0, -5.0, 74.0, -89.0, -84.0, -116.0, -53.0, -42.0, 46.0, -69.0, 32.0, 36.0, -83.0, 57.0, -64.0, -36.0, -18.0, -94.0, 52.0, -72.0, -87.0, -77.0, 44.0, -57.0, -33.0, 53.0, -76.0, -33.0, -12.0, 15.0, 9.0, -6.0, -70.0, 43.0, -58.0, -100.0, -78.0, -97.0])\n\n# Инициализируем специальный список data, чтобы передавать его целиком или по частям\ndata = [xData, yData, zData]</code></pre><p>Теперь определим функцию-построитель:</p><pre><code class="language-python">def SurfacePlot(func, data, fittedParameters):\n    # Зададим размер графика\n    f = plt.figure(figsize = (graphWidth, graphHeight), dpi = 100)\n\n    # Отобразим сетку\n    matplotlib.pyplot.grid(True)\n\n    # Зададим трехмерное пространство\n    axes = Axes3D(f)\n\n    # Снова раздробим данные натрое для комбинирования моделей x- и yModel \n    x_data = data[0]\n    y_data = data[1]\n    z_data = data[2]\n\n    # min() и max() найдут для каждого ряда наименьшее \n    # и наибольшее значения, а linspace() возвратит список \n    # чисел с равным интервалом (20).\n    xModel = np.linspace(min(x_data), max(x_data), 20)\n    yModel = np.linspace(min(y_data), max(y_data), 20)\n\n    # meshgrid() вернет матрицу координат.\n    X, Y = np.meshgrid(xModel, yModel)\n\n    Z = func(np.array([X, Y]), *fittedParameters)\n\n    axes.plot_surface(X, Y, Z, \n                      # Аргументы rstride и cstride устанавливают \n                      # шаг, используемый для выборки входных данных \n                      # при создания графика.\n                      rstride = 1, cstride = 1, \n                      cmap = cm.coolwarm, # Градиентное окрашивание \n                      linewidth = 1, antialiased = True) # Толщина линий\n\n\t# Вспомогательная точечная диаграмма\n\taxes.scatter(x_data, y_data, z_data)\n    \n    # Название графика и осей координат\n    axes.set_title('График плоскости') \n    axes.set_xlabel('X')\n    axes.set_ylabel('Y')\n    axes.set_zlabel('Z')\n\n    plt.show()</code></pre><p>И наконец построим плоскостный график полиномиальной линейной регрессии:</p><pre><code class="language-python">fittedParameters, pcov = scipy.optimize.curve_fit(func, [xData, yData], zData, p0 = initialParameters)\nSurfacePlot(func, data, fittedParameters)</code></pre><p>Мы получили такую прелестную графику:</p><figure class="kg-card kg-image-card"><img src="https://avatars.mds.yandex.net/get-zen_doc/2804475/pub_5ff2d2e6fe4e686f6ade97d5_5ff317ccf906b1687213c0c1/orig" class="kg-image" alt loading="lazy"></figure><p>Итак, вы получили первое представление о линейной регрессии. Реальные данные зачастую демонстрируют нелинейные отношения между переменными, и тогда в игру вступает нелинейная регрессия. </p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1QXvlGB8Q0tAFrYSK63yug8Kb6c0W1pPc?usp=sharing" rel="noopener noreferrer">здесь</a>.</p>		linieinaia-rieghriessiia	2020-12-12		
27	Степени свободы (Degrees of Freedom)		<p>Степени свободы (D<sub>f</sub>, C) – это количество параметров (точек контроля) <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. Они указывают количество независимых значений, которые могут изменяться в ходе анализа без нарушения каких-либо ограничений.</p><p>Пример. </p><ul><li>Рассмотрим Выборку (Sample) данных, состоящую для простоты из пяти положительных целых чисел. Значения могут быть любыми числами без известной связи между ними. Эта выборка данных теоретически должна иметь пять степеней свободы.</li><li>Четыре числа в выборке - это {3, 8, 5 и 4}, а среднее значение всей выборки данных равно 6.</li><li>Это должно означать, что пятое число равно 10. Иначе быть не может. У пятого значения нет свободы варьироваться.</li><li>Таким образом, степень свободы для этой выборки данных равна 4.</li></ul><p>Формула степени свободы выглядит следующим образом:</p><!--kg-card-begin: markdown--><p>$$D_f = N - 1$$</p>\n<!--kg-card-end: markdown--><p>где</p><p>D_f – степень свободы</p><p>N – количество значений </p><p>Математически степени свободы часто представляют, используя греческую букву "ню", которая выглядит так: ν. Вы наверняка встретите и такие сокращения: 'd.o.f.', 'dof', 'd.f.' или просто 'df'. </p><h2 id="-">Степени свободы в статистике </h2><p>Степени свободы в статистике – это количество значений, используемых при вычислении <a href="__GHOST_URL__/degrees-of-freedom/www.helenkapatsa.ru/priznak/">переменной</a>.</p><p>Степени свободы = Количество независимых значений - Количество статистик</p><p>Пример. У нас есть 50 независимых значений, и мы хотим вычислить одну-единственную <a href="__GHOST_URL__/statistika/">статистику</a> "среднее". Согласно формуле, степеней свободы будет 50 - 1 = 49.</p><h2 id="--1">Степени свободы в Машинном обучении</h2><p>В прогностическом моделировании, степени свободы часто относятся к количеству параметров, включая данные, используемые при вычислении ошибки модели. Наилучший способ понять это – рассмотреть модель <a href="__GHOST_URL__/degrees-of-freedom/www.helenkapatsa.ru/linieinaia-rieghriessiia/">линейной регрессии</a>. </p><p>Рассмотрим модель линейной регрессии для <a href="Категориальная переменная, ">Датасета (Dataset)</a> с двумя входными переменными. Нам потребуется один коэффициент в модели для каждой входной переменной, то есть модель будет иметь еще и два параметра. </p><!--kg-card-begin: markdown--><p>$$\\hat{y} = x_1 * β_1 + x_2 * β_2$$</p>\n<!--kg-card-end: markdown--><p>где</p><p>y – целевая переменная<br>x_1, x_2 – входные переменные<br>β_1, β_2 – параметры модели</p><p>Эта модель линейной регрессии имеет две степени свободы, потому что есть два параметра модели, которые должны быть оценены на основе обучающего датасета. Добавление еще одного столбца к данным (еще одной входной переменной) добавит модели еще одну степень свободы. Сложность обучения модели линейной регрессии описывается степенью свободы, например, "модель четвертой степени сложности" означает наличие четырех входных переменных, а также степень свободы, равную четырем.</p><h2 id="--2">Степени свободы для ошибки линейной регрессии </h2><p>Количество обучающих примеров имеет значение и влияет на количество степеней свободы регрессионной модели. Представьте, что мы создаем модель линейной регрессии на базе датасета, состоящего из ста строк.</p><p>Сравнивая предсказания модели с реальными выходными значениями, мы минимизируем ошибку. Итоговая ошибка модели имеет одну степень свободы для каждого ряда за вычетом количества параметров. В нашем случае ошибка модели 98 степеней свободы (100 рядов - 2 параметра). </p><h2 id="--3">Итоговые степени свободы для линейной регрессии</h2><p>Конечные степени свободы для модели линейной регрессии рассчитываются как сумма степеней свободы модели плюс степени свободы ошибки модели. В нашем примере это 100 (2 степени свободы модели + 98 степеней свободы ошибки). Как вы уже заметили, степеней свободы столько, сколько рядов в датасете.</p><p>Теперь рассмотрим набор данных из 100 строк, но теперь у нас есть 70 входных переменных. Это означает, что модель имеет еще и 70 коэффициентов, что дает нам d.o.f. ошибки, равной 30 (100 строк - 70 коэффициентов). d.o.f. самой модели по-прежнему равен ста.</p><h2 id="--4">Отрицательные степени свободы</h2><p>Что происходит, когда у нас больше столбцов, чем строк данных? Отрицательные значения вполне допустимы здесь. Например, у нас может быть 100 строк данных и 10 000 переменных, к примеру, маркеры генов для 100 пациентов. Следовательно, модель линейной регрессии будет иметь 10 000 параметров, то есть модель будет иметь 10 000 степеней свободы. </p><p>Тогда степени свободы рассчитываются следующим образом:</p><p>Степень свободы модели = Количество независимых значение - Количество параметров = 100 - 10 000 = -9 900</p><p>В свою очередь, степени свободы модели линейной регрессии будут следующими:</p><p>Степени свободы модели линейной регрессии  = Степени свободы модели - Степени свободы ошибки модели = 10 000 - 9 900 = 100</p><p>Фото: <a href="https://unsplash.com/@mickeyoneil">@mickeyoneil</a></p>		degrees-of-freedom	2020-12-13		
28	Квантиль (Quantile)		<p>Квантиль (иногда фрактиль) – граница деления Выборки (Sample) или Совокупности (Population) на равные по размеру смежные подгруппы. Это также относится к разделению распределения вероятностей на области равного размера:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/quantile-1.jpg" class="kg-image" alt loading="lazy" width="2000" height="1143" srcset="__GHOST_URL__/content/images/size/w600/2021/01/quantile-1.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/01/quantile-1.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/quantile-1.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2021/01/quantile-1.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Площади многоугольников, разделенные пунктирными проекциями, равны</figcaption></figure><p>Медиана – это тоже квантиль, которая расположена так, чтобы ровно половина данных (0,5) была левее медианы. Медиана делит распределение на две равные области, поэтому ее еще называют 2-м квантилем.</p><p>Квартили также являются квантилями; они делят распределение на четыре равные части. Тоже верно и для процентилей, децилей и квинтилей, разделяющих распределение на 100, 10 и 5 равных частей соответственно.</p><p>Пример. Найдем 20% самых маленьких чисел в списке:</p><pre><code class="language-python">lst = [110, 59, 127, 91, 14, 22, 78, 79, 91, 13, 116, 59, 88, 10, 114, 107, 11, 124, 74, 9, 128, 98, 68, 88, 123, 99, 6, 56, 9, 106, 26, 124, 82, 25, 2, 57, 28, 17, 4, 94]</code></pre><p>Шаг 1. Упорядочим данные от наименьшего к наибольшему:</p><pre><code class="language-python">lst.sort()\nprint('Сортированный список:', lst)</code></pre><pre><code class="language-python">Сортированный список: [2, 4, 6, 9, 9, 10, 11, 13, 14, 17, 22, 25, 26, 28, 56, 57, 59, 59, 68, 74, 78, 79, 82, 88, 88, 91, 91, 94, 98, 99, 106, 107, 110, 114, 116, 123, 124, 124, 127, 128]</code></pre><p>Шаг 2. Подсчитаем, сколько <a href="Быстрое кодирование, ">Наблюдений (Observation)</a> в совокупности:</p><pre><code class="language-python">n = len(lst)\nprint(n)</code></pre><pre><code class="language-python">40</code></pre><p>Шаг 3: Определим значение, отсекающее 20% значений от остальных, с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$Пограничное\\space{наблюдение} = q × (n + 1), где$$<br>\n$$q\\space{–}\\space{размер}\\space{квантили,}$$<br>\n$$n\\space{–}\\space{число}\\space{наблюдений}$$</p>\n<!--kg-card-end: markdown--><p>Размер квантили – это величина, характеризующая количество измерений, входящих в обозначенную часть совокупности. В нашем случае, это первые 20%, то есть 0,2.</p><!--kg-card-begin: markdown--><p>$$Пограничное\\space{наблюдение} = 0,2 × (40 + 1) = 8,2$$</p>\n<!--kg-card-end: markdown--><p>Дело за малым – отфильтруем список по условию "меньше 8,2":</p><pre><code class="language-python">small = [x for x in lst if x &lt; 8.2]\nprint(small)</code></pre><pre><code class="language-python">[2, 4, 6]</code></pre><h3 id="-">Квантили в Машинном обучении</h3><p>Помимо прочего, квантили используются в т.н. Квантильной регрессии (Quantile Regression) и делают ее оценки более устойчивыми к выбросам.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1HmJT0I0N_Kj8igW-vWKOngt4KfYqt1Xc?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@thmsvrbrggn">@thmsvrbrggn</a></p>		kvantil	2020-12-13		
29	Стандартное отклонение (Standard Deviation)		<p>Стандартное отклонение (σ, s) – это мера разброса в наборе числовых данных. Выражаясь простыми словами, насколько далеко от <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Cреднего арифметического (Mean)</a> находятся точки данных. Его также можно назвать мерой центральной тенденции: чем меньше стандартное отклонение, тем более «сгруппированы» данные вокруг центра (среднего). Чем отклонение больше, тем больше разброс значений. </p><h3 id="-">Стандартное отклонение в статистике</h3><p>Метрика рассчитывается с помощью следующей формулы:</p><!--kg-card-begin: markdown--><p>$$σ = \\sqrt{\\frac{Σ_{i=1}^n(x_i - \\bar{X})^2}{n}}, где$$<br>\n$$σ\\space(малая\\spaceсигма)\\space–\\spaceстандартное\\spaceотклонение$$<br>\n$$Σ\\space–\\spaceсумма$$<br>\n$$x\\space–\\space{i-й}\\spaceэлемент\\spaceвыборки$$<br>\n$$\\bar{X}\\space–\\spaceсреднее\\spaceзначение\\spaceвыборки$$<br>\n$$n\\space–\\spaceколичество\\spaceэлементов\\spaceв\\spaceвыборке$$</p>\n<!--kg-card-end: markdown--><p>Пример. Мы располагаем <a href="__GHOST_URL__/vyborka/">Выборкой (Sample)</a> из 10 наблюдений, где указано, сколько килограммов томатов собрали дачники в этом месяце:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/standart-deviation.png" class="kg-image" alt loading="lazy" width="132" height="456"></figure><p>Средним значением выборки будет 7,7:</p><!--kg-card-begin: markdown--><p>$$\\bar{X} = (5 + 7 + 8 + 11 + 12 + 7 + 5 + 4 + 10 + 8) / 10 = 7,7$$</p>\n<!--kg-card-end: markdown--><p>Следуя формуле, вычислим квадрат разницы между i-м элементом выборки и средним значением. К примеру, для первого вхождения это будет:</p><!--kg-card-begin: markdown--><p>$$x_i – \\bar{X} = (5 - 7,7)^2 = 7,29$$</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/standart-deviation-difference-5.png" class="kg-image" alt loading="lazy" width="229" height="494"></figure><p></p><p>Причина, по которой мы возводим разницы в квадрат, заключается в том, что большие отклонения от среднего как бы "наказываются" более сурово. Возведение в квадрат также приводит одинаковому учету отклонений в обоих направлениях (положительном и отрицательном), то есть расстояние от среднего значения у отрицательного и положительного числа будет рассчитано верно в обоих случаях.</p><p>Суммой значений правого столбца является число 64,1. Итак, согласно формуле стандартное отклонение будет равно:</p><!--kg-card-begin: markdown--><p>$$σ = \\frac{64,1}{10} = 6,41$$</p>\n<!--kg-card-end: markdown--><h3 id="--1">Стандартное отклонение в Машинном обучении</h3><p>Представьте, что перепись "томатного" населения приобрела более широкие масштабы, и исследователи собрали данные о целом климатическом поясе. Мало тех, кто собрал по 2 килограмма, и тех, кто собрал 50. В среднем, садоводы собирали 25 кг. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/standard-deviation_------------------1.png" class="kg-image" alt loading="lazy" width="306" height="378"></figure><p>При создании модели прогнозирования урожая стандартное отклонение уточняет наши предположения с помощью следующих принципов:</p><ul><li>С вероятностью 68% следующее наблюдение будет лежать в пределах одного отклонения от среднего (25 ± 6,41), то есть в диапазоне 18,59 - 31,41 кг.</li><li>С вероятностью 95% следующий дачник сообщит, что собрал томатов. в пределах двух стандартных отклонений от среднего значения (25 ± 2 × 6,41), то есть 12,18 – 37,82 кг.</li><li>С вероятностью 99% размер урожая будет лежать в пределах 3 отклонений (25 ± 3 × 6,41): 5,77 – 44,23 кг.</li></ul><h3 id="-statistics">Библиотека Statistics</h3><p>Рассчитывание стандартного отклонения выполняется мгновенно с помощью библиотеки statistics:</p><pre><code class="language-python">import statistics \nsample = [1, 2, 3, 4, 5] \nstatistics.stdev(sample)</code></pre><p>На выводе получаем следующее:</p><pre><code class="language-python">1.5811388300841898</code></pre><p>Фото: <a href="https://unsplash.com/@danielodowd">@danielodowd</a></p>		standartnoie-otklonieniie	2020-12-13		
30	Временная метка (Timestamp)		<p>Временная метка – набор кодированных символов, характеризующих время того или иного события в программировании.</p><p>Это специальный формат временной метки в модуле Python <code>datetime</code>:</p><pre><code class="language-python">dt = datetime.datetime(2012, 5, 1)</code></pre><p>А это – временная метка на языке библиотеки Pandas:</p><pre><code class="language-python">ts = pd.DatetimeIndex([dt])[0]\n&lt;Timestamp: 2012-05-01 00:00:00&gt;</code></pre><p>А здесь – временная метка на языке библиотеки NumPy:</p><pre><code class="language-python">dt64 = np.datetime64(dt)\nnumpy.datetime64('2012-05-01T01:00:00.000000+0100')</code></pre><p>В отличие от <a href="__GHOST_URL__/vriemiennoi-riad/">Временного ряда (Time Series)</a>, который характеризует столбец <a href="__GHOST_URL__/datafrieim/">Датафрейма (DataFrame)</a>, временная метка характеризует отдельную ячейку этого столбца.</p><h3 id="-datetime">Временная метка и datetime</h3><p>Правильно подготовить <a href="__GHOST_URL__/priznak/">Признак (Feature)</a>, выраженный временной меткой, можно с помощью библиотеки datetime. Для начала импортируем все необходимые надстройки:</p><pre><code class="language-python">import pandas as pd\nimport datetime as dt</code></pre><p>Подгрузим небольшой <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>, содержащий данные о покупках в магазине сувениров:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/2ly8sx4ehwqe78o/gift-shop-purchases.csv?dl=1')\ndf.head()</code></pre><p>Наш набор выглядит следующим образом:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/cardinality-datetime-dataset.png" class="kg-image" alt loading="lazy" width="856" height="273" srcset="__GHOST_URL__/content/images/size/w600/2021/03/cardinality-datetime-dataset.png 600w, __GHOST_URL__/content/images/2021/03/cardinality-datetime-dataset.png 856w"></figure><p>Зачастую Pandas поверхностно "обходится" с признаками датафрейма и интерпретирует их как объекты. Посмотрим, к каким типам данных он относит временной ряд <code>InvoiceDate</code>, выраженный временной меткой. Для этого используем встроенный метод <code>Pandas.dtypes</code>:</p><pre><code class="language-python">df.dtypes</code></pre><p>Удивительно, но факт: помимо причисления столбца временной метки к объектам, Pandas к тому же решил интерпретировать подобным образом и другие признаки – номер счета (InvoiceNo), артикул (StockCode), страну (Country). Возможно, это делается намеренно, чтобы <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> обратил внимание на очевидные несоответствия и скорее переназначил(-а) признакам типы:</p><pre><code class="language-python">InvoiceNo       object\nStockCode       object\nDescription     object\nQuantity         int64\nInvoiceDate     object\nUnitPrice      float64\nCustomerID     float64\nCountry         object\ndtype: object\n</code></pre><p>Восстановим справедливость, по крайней мере, к признаку "Дата выставления счета" и изменим тип на временную метку методом <code>dt.to_datetime()</code>, который умен настолько, что даже не потребует <a href="__GHOST_URL__/rieghuliarnoie-vyrazhieniie/">Регулярного выражения (Regular Expression)</a> с описанием формата даты:</p><pre><code class="language-python">df['InvoiceDate'] =  pd.to_datetime(df['InvoiceDate'])\ndf.dtypes</code></pre><p>Тип столбца обновился. Система самостоятельно преобразовала объект во временную метку, и признак превратился во <a href="__GHOST_URL__/vriemiennoi-riad/">Временной ряд (Time Series)</a>.</p><pre><code class="language-python">InvoiceNo              object\nStockCode              object\nDescription            object\nQuantity                int64\nInvoiceDate    datetime64[ns]\nUnitPrice             float64\nCustomerID            float64\nCountry                object\ndtype: object</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1WCv0QAnEGv6MOSgWbl5iZq9l6FFmwaEI?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@crisaur">@crisaur</a></p>		vriemiennaia-mietka	2021-03-27		
31	Признак (Feature)		<p>Признак (переменная, фича, атрибут, столбец, функция, фактор) – это объективная характеристика, характерная черта или свойство, которое может быть определено или измерено.</p><p>Представьте, что Вы имеете дело с таблицей, состоящей из столбцов, рядов и ячеек: это данные о классификация еще не родившихся детей со здоровым и больным сердцем на основании данных УЗИ. Столбец C – индикатор наличия болезни, где ноль означает, что плод здоров.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/feature-table-1.png" class="kg-image" alt loading="lazy" width="181" height="260"><figcaption>Универсальные табличные данные</figcaption></figure><h2 id="-">Статистика</h2><p>С точки зрения <a href="__GHOST_URL__/statistika/">Статистики (Statistics)</a>, мы рассматриваем некую гипотетическую функцию f, где с помощью <em>входных (input)</em> переменных X пытаемся предсказать <em>выходную (output) Y</em>. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/feature-statistics.png" class="kg-image" alt loading="lazy" width="181" height="260"><figcaption>Статистические данные для предсказания Y</figcaption></figure><p>Иными словами, мы используем независимые (independent) переменные X для предсказания зависимого (dependent) признака Y:</p><!--kg-card-begin: markdown--><p>$$Y = f(X_1, X_2)$$</p>\n<!--kg-card-end: markdown--><p>Если же речь идет о компьютеризированной вычислительной статистике, строка часто описывает сущность (например, человека) и <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a> о ней. Столбцы строки часто называют атрибутами наблюдения. Тогда при моделировании проблемы и прогнозировании мы ссылаемся на <em>входные (input)</em> и <em>выходные (output)</em> атрибуты.</p><h2 id="--1">Машинное обучение</h2><p>Вот мы и добрались до <a href="__GHOST_URL__/priznak/Scikit-learn">Машинного обучения (ML)</a>, где всем известные термины статистики обрели новое прочтение и были снова переименованы, на сей раз в предиктор (Predictor Variable) и целевую переменную (Target Variable). Теперь мы работаем с бо́льшими объемами данных, но принципы похожи: ссылаясь на предсказательные данные, мы определяем ценность каждой из таких переменных в формировании целевого признака. </p><!--kg-card-begin: markdown--><p>$$Y = A * X_1 + B * X_2 + C$$</p>\n<!--kg-card-end: markdown--><p>Ссылаясь на пример со здоровьем плода, каковы частота сердечных сокращений и количество движений плода у здорового, а какие – у больного ребенка?</p><h2 id="--2">Признаки и типы</h2><p>Помимо вышеуказанной классификации признаков, существуют еще и другая перспектива: <em>типы данных</em> (числовые, категориальные временные, текстовые):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/feature-data-types.png" class="kg-image" alt loading="lazy" width="802" height="414" srcset="__GHOST_URL__/content/images/size/w600/2020/12/feature-data-types.png 600w, __GHOST_URL__/content/images/2020/12/feature-data-types.png 802w" sizes="(min-width: 720px) 720px"><figcaption>Слева направо: числовые, категориальные, текстовые, временные, числовые данные</figcaption></figure><p>Фото: <a href="https://unsplash.com/@davidclode">@davidclode</a></p>		priznak	2020-12-13		
32	Выборка (Sample)		<p>Выборка (сэмпл) – часть Cовокупности (Population), подмножество точек ее данных. Процесс получения набора сэмплов называют Сэмплингом (Sampling).</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/sample-1.png" class="kg-image" alt loading="lazy" width="622" height="450" srcset="__GHOST_URL__/content/images/size/w600/2021/01/sample-1.png 600w, __GHOST_URL__/content/images/2021/01/sample-1.png 622w"></figure><p>Первый шаг статистического анализа – определить, является ли набор данных, с которым Вы имеете дело, генеральной совокупностью или Выборкой. Совокупность – это набор всех элементов, представляющих интерес для вашего исследования.</p><h3 id="-vs-">Совокупность vs. Выборка</h3><p>Совокупность – это наблюдаемые нами объекты: люди, события, животные и т.д. У нее есть некоторые параметры, такие как <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее значение (Mean)</a>, <a href="__GHOST_URL__/miediana/">Медиана (Median)</a>, <a href="__GHOST_URL__/moda/">Мода (Mode)</a>, <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a> и другие.</p><p>Выборка – это случайное подмножество совокупности. Когда она слишком велика и затрудняет анализ, пригождается выборка. Перечисленные выше метрики также можно рассчитать и для выборки, но называться они будут статистиками, обозначаться и рассчитываться по-своему:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/sample-symbols.png" class="kg-image" alt loading="lazy" width="422" height="162"><figcaption>Обозначения: параметры совокупности и статистики выборки</figcaption></figure><h3 id="-">Центральная предельная теорема</h3><p>Говорят, что это самая важная теорема статистики и математики. Она может быть очень мощной при оценке проблем и ситуаций. Центральная предельная теорема (Central Limit Theorem) утверждает, что распределение выборки будет нормальным независимо от анализируемой совокупности.</p><p>Пример. Мы располагаем данными о любителях видеоигр:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/sample-gamers.png" class="kg-image" alt loading="lazy" width="404" height="431"></figure><p>Расположим данные на двумерной плоскости: ось X отвечает за возраст игрока, y – за количество пройденных игр:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/sample_x-y-axis-1.png" class="kg-image" alt loading="lazy" width="451" height="450"></figure><p>Возьмем 50 сэмплов из нашей совокупности вместо одного:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/sample_multiple-samples-3.png" class="kg-image" alt loading="lazy" width="451" height="450"></figure><p>Для каждого сэмпла вычислим среднее значение. Конечно, не все они будут равны между собой. Мы получим целый список средних значений, и если сгруппировать их в зависимости от попадания в тот или иной десяток, то график будет выглядеть следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/sample-mean-histogram.png" class="kg-image" alt loading="lazy" width="2000" height="811" srcset="__GHOST_URL__/content/images/size/w600/2021/01/sample-mean-histogram.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/sample-mean-histogram.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/sample-mean-histogram.png 1600w, __GHOST_URL__/content/images/2021/01/sample-mean-histogram.png 2058w" sizes="(min-width: 720px) 720px"></figure><p>Это выборочное распределение выборочного среднего. Кривая тяготеет к кривой нормального распределения, несмотря на то, что в отдельно взятых сэмплах среднее значение может сильно отличаться от "центрального".</p><p>Замечательно проиллюстрирован этот принцип на <a href="http://onlinestatbook.com/stat_sim/sampling_dist/index.html">onlinestatbook.com</a>: из генеральной совокупности случайным образом вычленяются равные выборки, и для каждой из них вычисляется среднее. Каждое такое среднее становится элементом третьего графика "Распределение средних", агрегирующего средние значения. В конечном итоге, после 10 тысяч повторений, кривая распределения средних становится нормальной:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/sample-onlinestatbook.gif" class="kg-image" alt loading="lazy" width="690" height="388"></figure><p>Фото: <a href="https://unsplash.com/@raphaelfyi">@raphaelfyi</a></p>		vyborka	2020-12-13		
33	Мощность (Cardinality)		<p>Мощность – 1. Количество уникальных значений Признака (Feature). Понятие "Высокая мощность" применимо, как следствие, к признакам с уникальным обширным набором возможных значений. 2.Число вершин Графа (Graph) Нейронной сети (Neural Network).</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/cardinality.png" class="kg-image" alt loading="lazy" width="1500" height="706" srcset="__GHOST_URL__/content/images/size/w600/2021/03/cardinality.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/cardinality.png 1000w, __GHOST_URL__/content/images/2021/03/cardinality.png 1500w" sizes="(min-width: 1200px) 1200px"><figcaption>Большая мощность признаков InvoiceNo – InvoiceDate в pandas_profiling</figcaption></figure><p><u>Мощность одного признака.</u> Например, если список A имеет конечное число элементов, его мощность – это просто количество элементов. Если A = [2,4,6,8,10], то мощность равна 5.</p><p><u>Мощность пар признаков.</u> Пример. Мы выделили уникальные значения двух признаков датасета:</p><p><code>A = [1, 2, 3, 4, 5], B = [3, 7]</code></p><p>Объединенная мощность A и B (обозначается так: A ⋃ B, это оператор присоединения множеств) равна 7, поскольку <code>A + B = [1, 2, 3, 4, 5, 7]</code>, и в этом новом ряду 6 элементов. Мощность A ⋂ B равна 1, так как A пересекается с B (A ⋂ B) лишь одним элементом – числом 3.</p><h2 id="-">Высокая мощность</h2><h3 id="--1">Конструирование признаков</h3><p>Если "мощный" признак является <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевым (Target Variable)</a>, и не только, это значительно усложняет задачу <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, и тогда применяется Конструирование признаков (Feature Engineering) с целью сократить число возможных классов. Например, столбец с индексами преобразовывается в города:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/cardinality-2.png" class="kg-image" alt loading="lazy" width="606" height="514" srcset="__GHOST_URL__/content/images/size/w600/2021/03/cardinality-2.png 600w, __GHOST_URL__/content/images/2021/03/cardinality-2.png 606w"></figure><h3 id="--2">Отсекание редких классов</h3><p>Другой подход к урегулированию числа возможных классов – это сохранить самые распространенные значения и отсечь редкие с помощью специального порога, или сделать редкие классы одним большим.</p><h3 id="--3">Кодировка числом вхождений</h3><p>Еще один интересный способ сокращения мощности признака – Кодировка числом вхождений (Count Encoding). Мы заменяем каждое категориальное значение количеством раз, которое оно встречается в <a href="__GHOST_URL__/dataset/">Датасете (Dataset)</a>. Например, если значение A встречается 10 раз, то каждое такое вхождение будет заменено числом 10. </p><h2 id="--4">Средняя мощность</h2><p>Если число уникальных значений не столь велико, можно сгенерировать Фиктивные переменные (Dummy Variable), чтобы сделать возможной загрузку такого датасета в <a href="__GHOST_URL__/modiel/">Модель (Model)</a>.</p><h2 id="--5">Низкая мощность</h2><p>Признаки с двумя уникальными значениями обладают низкой мощностью. Если такой признак является целевым, задача классификации становится Бинарной (Binary Classification).</p><p>Фото: <a href="https://unsplash.com/@dan_stark">@dan_stark</a></p>		moshchnost	2020-12-13		
34	pandas_profiling		<p>Профайлер Pandas (pandas_profiling) – библиотека для создания статистического отчета о <a href="__GHOST_URL__/datafrieim/">Датафрейме (DataFrame)</a>. На момент создания статьи определяет:</p><ul><li>Типы <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a></li><li>Число Уникальных значений (Distinct Value), <a href="__GHOST_URL__/propusk/">Пропусков (Omission)</a></li><li>Квантильные <a href="__GHOST_URL__/statistika/">Статистики (Statistics)</a>: Минимум (Minimum), Максимум (Maximum), <a href="__GHOST_URL__/kvantil/">Квантили (Quantile)</a></li><li>Описательные статистики: <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее значение (Mean)</a>, <a href="__GHOST_URL__/moda/">Моду (Mode)</a>, <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>, <a href="__GHOST_URL__/skoshiennost/">Скошенность (Skewness)</a></li><li>Самые частые значения</li><li>Размер задействованной датафреймом и признаком памяти</li></ul><p>А также строит графики:</p><ul><li><a href="__GHOST_URL__/gistoghramma/">Гистограммы (Histogram)</a> распространенных значений признака</li><li>Матрицы <a href="__GHOST_URL__/korrieliatsiia/">Корреляции (Correlation)</a></li><li><a href="__GHOST_URL__/tochechnaya-diagramma/">Точечную диаграмму (Scatterplot)</a> взаимодействия признаков </li></ul><h3 id="-">Пример</h3><p>Чтобы установить какой-то внешний модуль, а профайлер Pandas к ним относится, используют специальную команду:</p><pre><code class="language-python">!pip install pandas-profiling</code></pre><p>Теперь его можно импортировать, как и любую другую библиотеку, директивой <code>import</code>:</p><pre><code class="language-python">import pandas as pd\nimport pandas_profiling</code></pre><p>Итак, мы возьмем набор данных о потребителях банковских кредитных продуктов:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/lsa6q8iqtmfd6ob/eda-bank-dataset.csv?dl=1', sep = ';')\ndf.head()</code></pre><p>Создадим датафрейм из csv-файла. Наш набор выглядит вот так:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_dataset.png" class="kg-image" alt loading="lazy" width="2000" height="459" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_dataset.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_dataset.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_dataset.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_dataset.png 2340w" sizes="(min-width: 1200px) 1200px"></figure><p>Запросим расширенную статистическую аналитику. Чтобы вызвать профайлер Pandas, вызовем встроенную функцию ProfileReport(). Нам понадобится чуть меньше минуты, прежде чем выделенные мощности рассчитают сводку:</p><pre><code class="language-python">pandas_profiling.ProfileReport(df)</code></pre><p>Итак. Мы получили обширный документ, и вкладки здесь – это якорные ссылки на раздел длинного HTML-документа. Пролистаем отчет, и посмотрим, какую пользу можно извлечь из каждого раздела.</p><h3 id="overview">Overview</h3><p>В разделе «Сводка» (Overview) находится набор полезных метрик: число признаков, наблюдений, пустых строк, пустых ячеек вообще, и их доля в документе, количество рядов-дубликатов и их доля в датасете:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_overview.png" class="kg-image" alt loading="lazy" width="2000" height="709" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_overview.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_overview.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_overview.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_overview.png 2340w" sizes="(min-width: 1200px) 1200px"></figure><p>Далее по списку – общий объем занимаемой файлом памяти, и «вес» каждого наблюдения. В правой части вкладки – число категориальных и числовых признаков. Эти метрики нужны, чтобы подготовить датасет к загрузке в модель.</p><p>Подраздел «Воспроизведение» (Reproduction) содержит записи о начале о окончании анализа, длительности расчетов, версии профайлера, файла .yaml со скриптом, с помощью которого операционная система и проходится по датасету:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_reproduction.png" class="kg-image" alt loading="lazy" width="2000" height="603" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_reproduction.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_reproduction.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_reproduction.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_reproduction.png 2340w" sizes="(min-width: 1200px) 1200px"></figure><p>В «Предупреждениях» (Warnings) можно ознакомиться с обнаруженными особенностями данных, которые повлияют на предсказательную способность будущей модели. Система уже сейчас обнаружила дубликаты. Нашла она также и признаки с высокой степенью корреляции, а это значит, что некоторые из них могут быть удалены из <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a> еще до начала обучения <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. Нули для признака "Предыдущий контакт" – это норма (разговор с человеком случился в первый раз), однако профайлер все равно уведомляет, что признак "переполнен" нулевыми значениями.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_warnings.png" class="kg-image" alt loading="lazy" width="2000" height="550" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_warnings.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_warnings.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_warnings.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_warnings.png 2340w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="variables">Variables</h3><p>Наконец, первая секция изучена, приступим к обзору Переменных (Variable). Каждый из признаков датасета последовательно характеризуется: тут и доля уникальных значений 'Unique (%)', и пропуски (Missing), и задействованная память (Memory size):</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_variables.png" class="kg-image" alt loading="lazy" width="2000" height="950" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_variables.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_variables.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_variables.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_variables.png 2340w" sizes="(min-width: 1200px) 1200px"></figure><p>Интересно, когда я знакомилась с профайлером, для каждого признака резюме выглядело по-другому. Это означает, что стандартизации здесь еще не наступила, и создатели все еще экспериментируют, чтобы понять потребности пользователей.</p><p>Для числовых признаков  (например, "Возраст") сводка весьма обширна: здесь и среднее арифметическое, и минимум-максимум, и доля нулевых значений. График призван продемонстрировать долю нулевых значений переменной. Их здесь нет. </p><p>Для столбца "Работа" – категориального признака на горизонтальной  гистограмме приводятся самые часто встречающиеся типы профессий. </p><h3 id="interactions">Interactions</h3><p>Едем дальше. Этот необычный график – видоизмененная точечная диаграмма. В <a href="https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/pages/advanced_usage.html#interactions">документации</a> ее гордо величают 'Hexagonal binned plot' (гексагональный группированный график). Она рассматривает каждую пару числовых признаков с целью обнаружить некоторые особенности данных:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_interactions------.png" class="kg-image" alt loading="lazy" width="1504" height="1085" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_interactions------.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_interactions------.png 1000w, __GHOST_URL__/content/images/2021/03/pandas_profiling_interactions------.png 1504w" sizes="(min-width: 1200px) 1200px"></figure><p>Для пары "Возраст – Длительность [телефонного разговора]", например, обнаружилось, что банк выбрал фокус-группой молодых людей (30 - 40 лет).</p><h3 id="correlations">Correlations</h3><p>Корреляционные матрицы – излюбленный инструмент статистического анализа, который позволяет выявить наиболее влиятельные признаки на ранней стадии, и даже идентифицировать неважные:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_correlation-matrice-------2.png" class="kg-image" alt loading="lazy" width="1679" height="1645" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_correlation-matrice-------2.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_correlation-matrice-------2.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_correlation-matrice-------2.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_correlation-matrice-------2.png 1679w" sizes="(min-width: 720px) 720px"></figure><p>Чем краснее и синее ячейка, тем более ярко выражена корреляция между признаками. Тут настает пора для самых неочевидных побочных эффектов: немного отстранившись от конечной цели исследования датасета – "научиться предсказывать, возьмет ли клиент кредит", мы обнаружим интересные взаимосвязи самых разных показателей. Чем ниже Европейская межбанковская ставка предложения, тем больше сотрудников готова нанять компания. Компания звонит "прогретым" клиентам в определенные дни недели. На одной такой матрице можно узнать о политике ведения бизнеса больше, чем за множество веб-страниц.</p><h3 id="missing-values">Missing values</h3><p>Наш датасет прекрасно подготовлен, а это трудоемкое и дорогое занятие. В нем Пропуски (Omission) обозначены по-особенному, не 'NaN' (Not a Number), иначе столбчатая диаграмма выглядела бы иначе. В таких случаях при подготовке данных будет применена особая стратегия поиска нетипичных пропусков, но вернемся к профайлеру.</p><h3 id="sample">Sample</h3><p>С разделом Sample все довольно просто: это выборка из 10 первых и последних наблюдений:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_sample.png" class="kg-image" alt loading="lazy" width="2000" height="950" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_sample.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_sample.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_sample.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_sample.png 2340w" sizes="(min-width: 1200px) 1200px"></figure><p>Иногда это полезно, чтобы получить представление о том, сортированы ли какие-либо признаки или нет, чтобы правильно разделить датасета на тренировочную и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовую части (Test Data)</a>.</p><h3 id="duplicate-rows">Duplicate rows</h3><p>Система обнаружила все дубликаты и подсчитала число вхождений в столбце count:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/pandas_profiling_duplicate_rows.png" class="kg-image" alt loading="lazy" width="2000" height="950" srcset="__GHOST_URL__/content/images/size/w600/2021/03/pandas_profiling_duplicate_rows.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/pandas_profiling_duplicate_rows.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/pandas_profiling_duplicate_rows.png 1600w, __GHOST_URL__/content/images/2021/03/pandas_profiling_duplicate_rows.png 2340w" sizes="(min-width: 1200px) 1200px"></figure><p>Это лишь поверхностный обзор профайлера: наметанный глаз сможет извлечь из такого набора метрик куда больше пользы. Надеюсь, эта замечательная библиотека покроет "первичные статистические потребности" любого исследователя данных. А реализовать оставшиеся запросы не составит большого труда.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://www.kaggle.com/helenkapatsa/pandas-profiling">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@lena_pixl">@lena_pixl</a></p>		pandas-profiling	2020-12-13		
35	Корреляция (Correlation)		<p>Корреляция (причинность, взаимозависимость) – мера взаимосвязи переменных друг с другом. В <a href="__GHOST_URL__/korrieliatsiia/Scikit-learn">Машинном обучении (ML)</a> зачастую подразумевается как взаимосвязь <a href="__GHOST_URL__/korrieliatsiia/www.helenkapatsa.ru/priznak/">переменной</a>-предиктора и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">целевой переменной (Target Variable)</a>. </p><p>В статистике корреляция – метод, который определяет, как одна переменная изменяется по отношению к другой, поскольку в большинстве случаев полезно выражать одну тему с точки зрения ее отношений с другими.</p><p>Самой распространенной в Машинном обучении по праву считается корреляция Пирсона и высчитывается это коэффициент следующим образом:</p><!--kg-card-begin: markdown--><p>$$r_x = \\frac{n * Σ(x_i * y_i) - Σx_i * Σy_i}{\\sqrt{(n * Σ{x_i^2} - (Σx_i)^2) - (n * Σ{y_i^2} - (Σy_i)^2)}}$$<br>\n$$n\\space{–}\\space{количество}\\space{наблюдений,}$$<br>\n$$x_i\\space{–}\\space{значения,}\\space{принимаемые}\\space{первой}\\space{переменной,}$$<br>\n$$y_i\\space{–}\\space{значения},\\space{принимаемые}\\space{второй}\\space{переменной}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Двадцати школьникам были даны тесты на наглядно-образное и вербальное мышление. Измерялось среднее время решения заданий теста в секундах. Психолога интересует вопрос: существует ли взаимосвязь между временем решения этих задач? Переменная X – среднее время решения наглядно-образных, а переменная Y – среднее время решения вербальных заданий.</p><p>Представим данные в виде таблицы, где время решения наглядно-образных задач – X, вербальных – Y, а n – количество учеников. Снабдим ее дополнительными расчетными метриками – произведением X и Y, а также X и Y в квадрате по отдельности. Дополнительно вычислим сумму значений каждого столбца.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/correlation-2.png" class="kg-image" alt loading="lazy" width="420" height="964"></figure><p>Выполнив подстановку, мы получим:</p><!--kg-card-begin: markdown--><p>$$r_x = \\frac{20 * 20 089 - 731 * 518}{\\sqrt{(20 * 27 873 - 731^2) - (20 * 16 000 - 518^2)}} = 0,669$$</p>\n<!--kg-card-end: markdown--><p>Отлично! Мы получили сильную корреляцию, поскольку принято разграничивать ее типы следующими интервалами:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/correlation-coefficients.png" class="kg-image" alt loading="lazy" width="288" height="164"></figure><p>Чем лучше ученик решает наглядно-образные задачи, тем лучше он справляется и с вербальными. Визуализируем это <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a>, отсортировав для наглядности таблицу от большего к меньшему по столбцу X.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/correlation-chart-table.jpg" class="kg-image" alt loading="lazy" width="204" height="880"></figure><p>Построим <a href="__GHOST_URL__/chat-bot-chatbot/">Точечную диаграмму (Scatterplot)</a> на базе этой таблицы:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/correlation-3.png" class="kg-image" alt loading="lazy" width="916" height="518" srcset="__GHOST_URL__/content/images/size/w600/2020/12/correlation-3.png 600w, __GHOST_URL__/content/images/2020/12/correlation-3.png 916w" sizes="(min-width: 720px) 720px"></figure><p>Чем больше времени уходит на задачу первого типа, тем больше и на задачу второго. Это корреляция положительная, поскольку коэффициент равен положительному числу, и линия имеет уклон вправо.</p><p>Существует несколько классификаций корреляции:</p><ul><li>по выраженности (сильная, средняя, низкая)</li><li>по знаку (положительная, отрицательная)</li><li>по форме (линейная, нелинейная)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/correlation-nonlinear-correlation-1.png" class="kg-image" alt loading="lazy" width="1349" height="756" srcset="__GHOST_URL__/content/images/size/w600/2020/12/correlation-nonlinear-correlation-1.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/correlation-nonlinear-correlation-1.png 1000w, __GHOST_URL__/content/images/2020/12/correlation-nonlinear-correlation-1.png 1349w" sizes="(min-width: 720px) 720px"><figcaption>Нелинейная корреляция</figcaption></figure><p>В Машинном обучении принято рассматривать пять типов такой взаимозависимости – r (коэффициент корреляции Пирсона), p (к. к. Спирмана), <a href="http://localhost:8890/lab#correlations_tab-kendall">τ</a> ("тау", к. к. Кендалла), φk ("фи-ка", к. фи-корреляции), φc ("фи-Крамер", к. к. Крамера). </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/statistics-pandas-pearson-correlation-1.png" class="kg-image" alt loading="lazy" width="2000" height="1071" srcset="__GHOST_URL__/content/images/size/w600/2020/12/statistics-pandas-pearson-correlation-1.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/statistics-pandas-pearson-correlation-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/statistics-pandas-pearson-correlation-1.png 1600w, __GHOST_URL__/content/images/2020/12/statistics-pandas-pearson-correlation-1.png 2106w" sizes="(min-width: 720px) 720px"><figcaption>Матрица корреляции, вычисленная с помощью библиотеки pandas-profiling</figcaption></figure><p>Чем ярче (краснее / синее) ячейка, тем сильнее выражена корреляция. Диагональные ячейки игнорируются, поскольку являются результатом расчета коэффициента между переменной и ее копией.</p><h2 id="-">Корреляция и причинность</h2><p>Стоит знать, что вышеупомянутые термины означают разные вещи в <a href="__GHOST_URL__/statistika/">статистике</a> и Машинном обучении. Причинность означает, что одна переменная является причиной появления другой, как, например, зарплата работника напрямую зависит от количества отработанных часов. А вот корреляция, в свою очередь, не означает, что изменение одной переменной вызовет изменение значений другой переменной, а лишь показывает, существует ли связь между ними.</p><p>Фото: <a href="https://unsplash.com/@anniespratt">@anniespratt</a></p>		korrieliatsiia	2020-12-13		
36	Prophet		<p>Prophet – это библиотека для создания <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (Machine Learning)</a> на базе Scikit-learn. Ее характерная особенность – это использование временных рядов для предсказания целевой <a href="__GHOST_URL__/prophet/www.helenkapatsa.ru/priznak/">переменной</a>. Тренировочный <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> подготавливается таким образом, чтобы входными данными был временной ряд и исторические значения целевой переменной:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/prophet-stock-price.png" class="kg-image" alt loading="lazy" width="202" height="246"></figure><p>Давайте попробуем предсказать стоимость акции компании "Магнит" (тикер $MGNT.ME). Для этого экспортируем csv-файл со вкладки "Исторические данные" (Historical Data) на <a href="https://finance.yahoo.com/quote/MGNT.ME?p=MGNT.ME&amp;.tsrc=fin-srch">finance.yahoo.com</a> со всей историей наблюдений. Для этого в настройке "Временной интервал" (Time Period) установим 'Max'. </p><p>Создадим ноутбук на <a href="https://colab.research.google.com/">colab.research.google.com</a> и импортируем для начала необходимые библиотеки. Мы используем классический набор из NumPy, Pandas и Matplotlib для подготовки данных и их визуализации. Pylab позволит нам тонко настроить внешний вид конечного графика, а MinMaxScaler – произвести <a href="__GHOST_URL__/normalizatsiia/">Нормализацию (Normalization)</a> данных. Класс add_changepoints_to_plot – это специальная надстройка, позволяющая добавить так называемые точки разворота:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/prophet----------------.png" class="kg-image" alt loading="lazy" width="2000" height="727" srcset="__GHOST_URL__/content/images/size/w600/2020/12/prophet----------------.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/prophet----------------.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/prophet----------------.png 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/prophet----------------.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Точки разворота (Pivot Points) показывают, когда стоимость акции сменила тенденцию</figcaption></figure><pre><code class="language-python">import numpy as np\nimport pandas as pd\nimport pylab\nimport matplotlib.pyplot as plt\n\nfrom fbprophet import Prophet\nfrom fbprophet.plot import add_changepoints_to_plot\n\n# Для нормализации данных\nfrom sklearn.preprocessing import MinMaxScaler </code></pre><p>Зададим графику "опрятный" внешний вид:</p><pre><code class="language-python">plt.rcParams["figure.figsize"] = (7.5, 5) # Размер полотна в дюймах\npylab.rc('figure', figsize = (10, 7))\n\nplt.style.use('seaborn') # Предустановленная цветовая палитра\n\n# Универсальные переменные для стилизации графика\nMEDIUM_SIZE = 12\nBIGGER_SIZE = 16\n\n# Размер шрифта\nplt.rc('axes', titlesize = BIGGER_SIZE)     # Оси\nplt.rc('axes', labelsize = BIGGER_SIZE)     # Ярлыки x и y\nplt.rc('xtick', labelsize = MEDIUM_SIZE)    # Отметки оси x\nplt.rc('ytick', labelsize = MEDIUM_SIZE)    # Отметки оси y\nplt.rc('figure', titlesize = BIGGER_SIZE)   # Заголовок</code></pre><h2 id="-">Препроцессинг</h2><p>Прежде чем отправлять модель обучаться, мы произведем препроцессинг и слегка преобразуем датасет. Для простоты воспроизведения кода загрузим датасет на Dropbox и разрешим его скачивание, задав параметр 'dl=1' в конце ссылки.</p><pre><code class="language-python">training_dataset = pd.read_csv(\n    'https://www.dropbox.com/s/4qoitqcxmxo1n8x/MGNT.ME.csv?dl=1')\n\ntraining_dataset.head()</code></pre><p>Наши данные выглядят следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/prophet-training-dataset.png" class="kg-image" alt loading="lazy" width="742" height="246" srcset="__GHOST_URL__/content/images/size/w600/2020/12/prophet-training-dataset.png 600w, __GHOST_URL__/content/images/2020/12/prophet-training-dataset.png 742w" sizes="(min-width: 720px) 720px"></figure><pre><code class="language-python"># .iloc() выбирает все ряды (':') столбцов \n# с индексами c 1-го по 2-й [1:2] и копируем выбранные значения\n# в новый объект training_data\ntraining_data = training_dataset.iloc[:, 1:2].values\n\n# Приведем все значения к диапазону от 0 до 1 (нормализация)\nsc = MinMaxScaler(feature_range = (0, 1))  \n\n# Загрузим данные в модель\ntraining_data = sc.fit_transform(training_data)\n\n# Выполним масштабирование\nX_train = []\ny_train = []\n\n# Отсечем первые 60 наблюдений, чтобы исключить период роста\nfor i in range(60, len(training_data)):\n    X_train.append(training_data[i-60:i, 0])\n    y_train.append(training_data[i, 0])\n\n# Преобразуем результат в ряд NumPy для точности\nX_train = np.array(X_train)\ny_train = np.array(y_train)    \n\n# Зададим явные измерения тренировочных данных для Prophet\nX_train = X_train.reshape((len(training_data)-60), 60, 1)</code></pre><h2 id="--1">Прогнозирование</h2><pre><code class="language-python">stock = pd.read_csv(\n'https://www.dropbox.com/s/4qoitqcxmxo1n8x/MGNT.ME.csv?dl=1',\n                    header = 0,         # Индекс заголовка – ноль\n                    index_col = 0,      # Считывание с нулевого столбца\n                    parse_dates = True) # Обнаружить временные метки\n\t\t\t\t\t\nstock.head()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/prophet-stock-1.jpg" class="kg-image" alt loading="lazy" width="712" height="246" srcset="__GHOST_URL__/content/images/size/w600/2020/12/prophet-stock-1.jpg 600w, __GHOST_URL__/content/images/2020/12/prophet-stock-1.jpg 712w"></figure><p>Prophet работает с двумя столбцами, один из которых – временная метка, потому вторым мы выберем "Стоимость акции на момент открытия биржи" (Open):</p><pre><code class="language-python">stock = stock["Open"]</code></pre><p> Произведем переиндексацию согласно требованиям этой библиотеки Facebook:</p><pre><code class="language-python">data_train = stock.reset_index() # Теперь индекс – порядковые номера\n\n# Столбцы называем "Штамп даты" (datestamp) и "Целевая переменная" (y)\ndata_train.columns = ['ds', 'y'] </code></pre><p>Скормим модели тренировочные данные и запросим предсказание на 2020-2021 гг. включительно:</p><pre><code class="language-python">model = Prophet()     # Инициализация Prophet()\nmodel.fit(data_train) # Загрузка данных в модель\n\n# Предсказание на 730 дней вперед\nfuture = model.make_future_dataframe(periods = 730) \npredict = model.predict(future)</code></pre><p>Визуализируем результат с помощью Matplotlib:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/prophet-mgnt.png" class="kg-image" alt loading="lazy" width="839" height="536" srcset="__GHOST_URL__/content/images/size/w600/2020/12/prophet-mgnt.png 600w, __GHOST_URL__/content/images/2020/12/prophet-mgnt.png 839w" sizes="(min-width: 720px) 720px"></figure><p>Скачать ноутбук, исполняемый без дополнительной настройки на момент написания статьи, можно по <a href="https://colab.research.google.com/drive/1aO9Ib_Nxi9jWZ8DdzFRsPsWdunrrTWs7?usp=sharing">ссылке</a>.</p><p>Фото: <a href="https://unsplash.com/@svfedo">@svfedo</a></p>		prophet	2020-12-18		
37	Матрица (Matrix)		<p>Матрица – это таблица из n × m (например, 3 × 3) чисел, заключенная в квадратные скобки. Мы можем складывать и вычитать матрицы одного размера, умножать одну матрицу на другую, если их измерения совместимы, и умножать всю матрицу на константу. Вектор - это матрица с одной строкой или столбцом. Основная идея заключается в том, что это двумерная сетка чисел.</p><p>В чем же отличие между матрицей и тензором? Последний часто рассматривается как <em>обобщенная</em> матрица, то есть это может быть одномерная матрица (вектор), трехмерная матрица (что-то вроде куба чисел), даже ноль-мерная матрица (одно число) или более высокая размерная структура, которую трудно визуализировать. Размерность тензора называется его <em>рангом</em>.</p><h3 id="-">Инициализация</h3><p>В приведенном ниже примере матрица инициализируется как набор рядов NumPy:</p><pre><code class="language-python">from numpy import array\nA = array([[1, 2, 3], [4, 5, 6]])\nprint(A)</code></pre><p>Отобразится матрица следующим образом:</p><pre><code class="language-python">[[1 2 3]\n [4 5 6]]</code></pre><p>Как и в случае с <a href="__GHOST_URL__/matritsa/www.helenkapatsa.ru/tenzor/">тензорами</a>, мы можем выполнять поэлементные арифметические операции с матрицами.</p><h3 id="--1">Сложение</h3><pre><code class="language-python">from numpy import array\nA = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nC = A + B\nprint(C)</code></pre><p>Сложение сгенерирует такую матрицу C:</p><pre><code class="language-python">[[[ 2  4  6]\n  [ 8 10 12]\n  [14 16 18]]\n\n [[22 24 26]\n  [28 30 32]\n  [34 36 38]]\n\n [[42 44 46]\n  [48 50 52]\n  [54 56 58]]]</code></pre><h3 id="--2">Вычитание</h3><pre><code class="language-python">from numpy import array\n\nA = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nD = A - B\nprint(D)</code></pre><p>Вычитание – такую матрицу D:</p><pre><code class="language-python">[[[0 0 0]\n  [0 0 0]\n  [0 0 0]]\n\n [[0 0 0]\n  [0 0 0]\n  [0 0 0]]\n\n [[0 0 0]\n  [0 0 0]\n  [0 0 0]]]</code></pre><h3 id="--3">Произведение</h3><p>Поэлементное умножение одной матрицы на другую такой же размерности приводит к созданию новой матрицы такой же размерности. Такой частный случай перемножения с одноразмерными матрицами / тензорами называют произведением Адамара.</p><pre><code class="language-python">from numpy import array\nA = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nE = A * B\nprint(E)</code></pre><p>Такой код сгенерирует матрицу E, котора выглядит следующим образом:</p><pre><code class="language-python">[[[  1   4   9]\n  [ 16  25  36]\n  [ 49  64  81]]\n\n [[121 144 169]\n  [196 225 256]\n  [289 324 361]]\n\n [[441 484 529]\n  [576 625 676]\n  [729 784 841]]]</code></pre><p>Если же дана матриц A с q измерений и матрица B с r измерений, их произведение будет новой матрицей с q + r размерностями. Например:</p><pre><code class="language-python">from numpy import array\nfrom numpy import tensordot\nA = array([1, 2, 3])\nB = array([3, 4])\nF = tensordot(A, B, axes = 0)\nprint(F)</code></pre><p>Результатом будет такая матрица F:</p><pre><code class="language-python">[[ 3  4]\n [ 6  8]\n [ 9 12]]</code></pre><h3 id="--4">Деление</h3><pre><code class="language-python">from numpy import array\n\nA = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nB = array([\n  [[1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]],\n  ])\nG = A / B\nprint(G)</code></pre><p>Такое поэлементное деление одноразмерных матриц сгенерирует матрицу F:</p><pre><code class="language-python">[[[ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]]\n\n [[ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]]\n\n [[ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]]]</code></pre><h3 id="confusion-matrix">Confusion Matrix</h3><p>Матрицы используются в <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a> повсеместно, но не только в обобщенном виде. М<em>атрица ошибок (Confusion Matrix)</em> дает нам целостное представление о том, насколько хорошо работает наша классификационная модель и какие ошибки она допускает:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/matrix-confusion-1.png" class="kg-image" alt loading="lazy" width="564" height="185"></figure><p>О том, что означает каждая ячейка матрицы ошибок и как она работает, читайте в отдельной статье.</p><p>Фото: <a href="https://unsplash.com/@ngbates">@ngbates</a></p>		matritsa	2020-12-19		
38	Черный ящик (Black Box)		<p>Черный ящик – метафорическое название системы, где пользователь лишь подает данные на входе и, не понимая принципов функционирования ее компонентов, получает данные на выходе.</p><p>Настоящая проблема <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (Machine Learning)</a> заключается в том, что компоненты модели зачастую сложны и недостаточно хорошо поняты. Путаница возникает из-за неправильных представлений людей о том, как работают эти системы. </p><h3 id="-">Эксперимент Скиннера</h3><p>В качестве простого примера рассмотрим мысленный эксперимент Скиннера: вам предоставляется панель управления автомобилем с переключателями и кнопками и результатами нажатия (запуск двигателя, стабилизация автомобиля, резкий старт). Нажимая кнопки, Вы можете наблюдать за соответствующей "реакцией" автомобиля, но не можете заглянуть внутрь, чтобы увидеть, как эта система работает. Очевидно, кнопка "Запустить / остановить двигатель" включает и выключает его. Однако в случае, если система достаточно сложна, определить простым перебором, как работает коробка, станет крайне затруднительно.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2020/12/goh-rhy-yan-y8CtjK0ej6A-unsplash--1-.jpg" class="kg-image" alt loading="lazy" width="2000" height="1500" srcset="__GHOST_URL__/content/images/size/w600/2020/12/goh-rhy-yan-y8CtjK0ej6A-unsplash--1-.jpg 600w, __GHOST_URL__/content/images/size/w1000/2020/12/goh-rhy-yan-y8CtjK0ej6A-unsplash--1-.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/goh-rhy-yan-y8CtjK0ej6A-unsplash--1-.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/goh-rhy-yan-y8CtjK0ej6A-unsplash--1-.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Фото: <a href="https://unsplash.com/@gohrhyyan">@gohrhyyan</a></figcaption></figure><p>Теперь представьте, что Вы заглянули под капот транспортного средства. Вам даже дали его принципиальную электросхему со всеми компонентами и их взаимосвязями. Более того, ни один из компонентов не является сложным сам по себе; все построено на простых и понятных резисторах и конденсаторах. Теперь у Вас есть не только доступ к полной спецификации компонентов, но и возможность проводить эксперименты над каждой частью механизма, чтобы увидеть реакцию каждого из них на нажатие той или иной кнопки.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/Electrical-Blueprint-min-1080x721.jpeg" class="kg-image" alt loading="lazy" width="1080" height="721" srcset="__GHOST_URL__/content/images/size/w600/2020/12/Electrical-Blueprint-min-1080x721.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2020/12/Electrical-Blueprint-min-1080x721.jpeg 1000w, __GHOST_URL__/content/images/2020/12/Electrical-Blueprint-min-1080x721.jpeg 1080w" sizes="(min-width: 720px) 720px"></figure><p>Хочется думать, что, имея всю эту информацию под рукой, Вы сможете дать хорошее объяснение того, как работает панель управления. Ведь каждый отдельный компонент понятен, и никакой скрытой информации нет. К сожалению, сложность возникает из-за <em>взаимодействия </em>многих простых компонентов. Даже если Вы знаете общее назначение системы, объяснить, почему ее части расположены именно в таком порядке, тоже довольно сложно. Особенно если компонент выполняет сразу несколько функций.</p><h3 id="xgboost">XGBoost</h3><p>Ниже Вы увидите классический пример Черного ящика – XGBoost. Направленная на лаконичность, библиотека позволяет обучить модель всего за несколько строк кода. Мы располагаем входными данными (train / test) и получаем конечные (xqboost_start2) так быстро, что новички принимают эту краткость за простоту:</p><pre><code class="language-python">import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\nfor feature in df_train.columns:\n    if 'cat' in feature:\n        gr_feature = df_train.groupby(df_train[feature])['loss'].mean()\n        df_train[feature] = df_train[feature].map(gr_feature)\n        df_test[feature] = df_test[feature].map(gr_feature)\n\n       \ny_train = np.log(df_train['loss'] + 200)\nx_train = df_train.drop(['loss', 'id'], axis = 1)\nx_test = df_test.drop(['id'], axis = 1)\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, \n                                                      test_size = 0.15, \n                                                      random_state = 345)\n\nd_train = xgb.DMatrix(x_train, y_train)\nd_valid = xgb.DMatrix(x_valid, y_valid)\nd_test = xgb.DMatrix(x_test)\n\nxgb_params = {\n    'learning_rate'   : 0.02,\n    'max_depth'       : 7,\n    'min_child_weight': 1,\n    'subsample'       : 0.7,\n    'colsample_bytree': 0.7,\n\n    'objective': 'reg:linear',\n    'silent'   : 1,\n    'seed'     : 120\n}\n\ndef xg_eval_mae(yhat, dtrain):\n    y = dtrain.get_label()\n    return 'mae', mean_absolute_error(np.exp(y), np.exp(yhat))\n    \nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nclf = xgb.train(xgb_params, d_train, 2500, watchlist, early_stopping_rounds=30, \n                verbose_eval=20, feval=xg_eval_mae, maximize=False)\n\np_test = np.exp(clf.predict(d_test)) - 200\n\nsub = pd.DataFrame()\nsub['id'] = df_test['id']\nsub['loss'] = p_test\nsub.to_csv('xqboost_start2.csv', index=False)</code></pre><p>Даже если каждые отдельные класс, переменная, функция изучены с помощью документации, простое объяснение принципу работы XGBoost откроется новоиспеченному дата-сайентисту весьма нескоро.</p><p>Здесь самое место ленинским фразам в духе "Учиться, учиться и еще раз учиться!", однако обойдемся без очевидных назиданий. Разучить 200 базисных терминов Машинного обучения – вполне подъемная задача, обучиться построению сложных мыслительных конструкций из них – тоже, и Вы это, уверена, сделаете рано или поздно. Что важно и часто игнорируется, так это внимательное отношение к себе. Оптимизация темпа освоения материала, правильный отдых, комьюнити – вот что подарит весьма устойчивый рост компетенции, и Черные ящики пооткрываются один за одним.</p><p>Фото: <a href="https://unsplash.com/@iyamiphotography">@iyamiphotography</a></p>		chiernyi-iashchik	2020-12-19		
39	Наука о данных (DS)		<p>Определение Википедии:</p><blockquote>Наука о данных (Data Science) – это раздел информатики, изучающий проблемы анализа, обработки и представления данных в цифровой форме. Объединяет методы по обработке данных в условиях больших объёмов.</blockquote><p>Зачем нужна эта наука? Это не про построение сложных <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> и потрясающую визуализацию. И даже не про код. Наука о данных – это способ принести своей компании как можно больше пользы, дохода. Польза бывает разной: </p><ul><li>Программа, использующая <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a> для решения конкретной задачи</li><li>Инсайты – этакие озарения о неочевидных взаимосвязях между событиями и явлениями</li><li>Набор рекомендаций </li></ul><p>Чтобы осуществить подобное, Вам и понадобится инструментарий для построения сложных моделей, визуализации данных и создания якода. Ваша задача как дата-сайентиста – решать проблемы компании, используя данные, и стек здесь дело второстепенное.</p><p>Если вы погружались в тему, то наверняка встречали неправильное представление об этой науке. Главная причина тому – разница между легко продаваемым у СМИ и действительно востребованным в бизнес-среде. Большая пятерка (Amazon, Apple, Facebook, Google, Microsoft) действительно использует <a href="__GHOST_URL__/bolshiie-dannyie/">Большие данные (Big Data)</a> для улучшения своих продуктов, но до момента взрывного роста популярности понятия использовался термин "Глубинный анализ данных" (Data Mining). </p><p>В своей статье "От глубинного анализа к развертыванию баз данных" Вильям Кливленд, профессор университета Пердью, сослался на процесс обнаружения полезной информации. Он хотел поднять статистику на новый уровень, скомбинировав компьютерные вычисления и глубинный анализ. Выражаясь проще, он сделал статистику более технологичной, чтобы дать ход инновациям нового уровня. Тогда он окрестил свое детище 'Combo Data Science'.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://images.unsplash.com/photo-1551288049-bebda4e38f71?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fGFuYWx5dGljc3xlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" class="kg-image" alt="Speedcurve Performance Analytics" loading="lazy" width="4810" height="3207" srcset="https://images.unsplash.com/photo-1551288049-bebda4e38f71?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fGFuYWx5dGljc3xlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=600 600w, https://images.unsplash.com/photo-1551288049-bebda4e38f71?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fGFuYWx5dGljc3xlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1000 1000w, https://images.unsplash.com/photo-1551288049-bebda4e38f71?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fGFuYWx5dGljc3xlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1600 1600w, https://images.unsplash.com/photo-1551288049-bebda4e38f71?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDF8fGFuYWx5dGljc3xlbnwwfHx8&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2400 2400w" sizes="(min-width: 720px) 720px"><figcaption>Фото: <a href="https://unsplash.com/@lukechesser?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Luke Chesser</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption></figure><p>К тому времени заиграла методика 'Web 2.0' – идеология развития систем в интернете, основанная на принципе, что чем больше пользователей задействовано в работе над проектом, тем лучше он развивается и более жизнеспособен. В проектах на основе web 2.0 именно пользователям принадлежит важная роль в наполнении проектов контентом, а также проверки уже существующей там информации, ее исправления и дополнения. Так что активная пользовательская деятельность – посты, лайки, комментарии, шейры, выгрузки – все виды цифровых следов, помогли создать новую экосистему, которой мы с Вами сегодня пользуемся. Обилие данных стало ее последствием, и обычных вычислительных технологий стало не хватать. </p><p>Софт специально для управления, извлечения пользы из массивных объемов информации и стали называть "Большими данными". Человечество создало параллельные вычислительные системы, адаптировало целые языки программирования, такие как Python, под нужды Науки о данных. Полагаю, такой минимализм его синтаксиса – это дань специалистам,  работающих с большими объемами информации и нуждающимся в минимизации всего остального.</p><p>Чтобы бизнес мог структурировать скопившийся за долгие годы данные, науку условно сгруппировали по приложениям: <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственный интеллект (AI)</a>, <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокое обучение (DL)</a>, оптимизация и тестирование, эксперименты, категоризация, очистка и подготовка данных, программирование потоков, сбор данных и прочими компонентами.</p>		nauka-o-dannykh	2020-12-21		
40	Матрица ошибок (Confusion Matrix)		<p>Матрица ошибок – это метрика производительности классифицирующей модели <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.</p><p>Когда мы получаем данные, то после очистки и предварительной обработки, первым делом передаем их в модель и, конечно же, получаем результат в виде вероятностей. Но как мы можем измерить эффективность нашей модели? Именно здесь <a href="__GHOST_URL__/matritsa-oshibok/,%20%D1%87%D0%B5%D1%80%D1%82%20%D0%B2%D0%BE%D0%B7%D1%8C%D0%BC%D0%B8,">матрица</a> ошибок и оказывается в центре внимания. </p><p>Матрица ошибок – это показатель успешности классификации, где классов два или более. Это таблица с 4 различными комбинациями сочетаний прогнозируемых и фактических значений.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/matrix-confusion-2.png" class="kg-image" alt loading="lazy" width="564" height="185"></figure><p>Давайте рассмотрим значения ячеек (истинно позитивные, ошибочно позитивные, ошибочно негативные, истинно негативные) с помощью "беременной" аналогии.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/confusion-matrix-error-types.png" class="kg-image" alt loading="lazy" width="512" height="512"></figure><p><br><strong>Истинно позитивное предсказание (True Positive, сокр. TP)</strong><br>Вы предсказали положительный результат, и женщина действительно беременна.</p><p><strong>Истинно отрицательное</strong> <strong>предсказание (True Negative, TN)</strong><br>Вы предсказали отрицательный результат, и мужчина действительно не беременен.</p><p><strong>Ошибочно положительное</strong> <strong>предсказание (ошибка типа I, False Positive, FN)</strong><br>Вы предсказали положительный результат (мужчина беременен), но на самом деле это не так.</p><p><strong>Ошибочно отрицательное</strong> <strong>предсказание (ошибка типа II, False Negative, FN)</strong><br>Вы предсказали, что женщина не беременна, но на самом деле она беременна.</p><p>Давайте разберемся в матрице ошибок с помощью арифметики.</p><p>Пример. Мы располагаем датасетом пациентов, у которых диагностируют рак. Зная верный диагноз (столбец целевой переменной "Y на самом деле"), хотим усовершенствовать диагностику с помощью модели Машинного обучения. Модель получила тренировочные данные, и на тестовой части, состоящей из 7 записей (в реальных задачах, конечно, больше) и изображенной ниже, мы оцениваем, насколько хорошо прошло обучение.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/confusion-matrix-example-1.png" class="kg-image" alt loading="lazy" width="615" height="416" srcset="__GHOST_URL__/content/images/size/w600/2020/12/confusion-matrix-example-1.png 600w, __GHOST_URL__/content/images/2020/12/confusion-matrix-example-1.png 615w"></figure><p><br>Модель сделала свои предсказания для каждого пациента и записала вероятности от 0 до 1 в столбец "Предсказанный Y". Мы округляем эти числа, приводя их к нулю или единице, с помощью порога, равного 0,6 (ниже этого значения – ноль, пациент здоров). Результаты округления попадают в столбец "Предсказанная вероятность": например, для первой записи модель указала 0,5, что соответствует нулю. В последнем столбце мы анализируем, угадала ли модель. </p><p>Теперь, используя простейшие формулы, мы рассчитаем <a href=" (Recall)">Отзыв (Recall)</a>, точность результата измерений (Precision), точность измерений (Accuracy), и наконец поймем разницу между этими метриками.</p><h3 id="-">Отзыв</h3><p>Из всех положительных значений, которые мы предсказали правильно, сколько на самом деле положительных? Подсчитаем, сколько единиц в столбце "Y на самом деле" (4), это и есть сумма TP + FN. Теперь определим с помощью "Предсказанной вероятности", сколько из них диагностировано верно (2), это и будет TP. </p><!--kg-card-begin: markdown--><p>$$Отзыв = \\frac{TP}{TP + FN} = \\frac{2}{2 + 2} = \\frac{1}{2}$$</p>\n<!--kg-card-end: markdown--><h3 id="-precision-">Точность результата измерений (Precision)</h3><p>В этом уравнении из неизвестных только FP. Ошибочно диагностированных как больных здесь только одна запись.</p><!--kg-card-begin: markdown--><p>$$Точность\\spaceрезультата\\spaceизмерений = \\frac{TP}{TP + FP} = \\frac{2}{2 + 1} = \\frac{2}{3}$$</p>\n<!--kg-card-end: markdown--><h3 id="-accuracy-">Точность измерений (Accuracy)</h3><p>Последнее значение, которое предстоит экстраполировать из таблицы – TN. Правильно диагностированных моделью здоровых людей здесь 2.</p><!--kg-card-begin: markdown--><p>$$Точность\\spaceизмерений = \\frac{TP + TN}{Всего\\spaceзначений} = \\frac{2 + 2}{7} = \\frac{4}{7}$$</p>\n<!--kg-card-end: markdown--><h3 id="f-">F-мера точности теста</h3><p>Эти метрики полезны, когда помогают вычислить F-меру – конечный показатель эффективности модели.</p><!--kg-card-begin: markdown--><p>$$F-мера = \\frac{2 * Отзыв * Точность\\spaceизмерений}{Отзыв + Точность\\spaceизмерений} = \\frac{2 * \\frac{1}{2} * \\frac{2}{3}}{\\frac{1}{2} + \\frac{2}{3}} = 0,56$$</p>\n<!--kg-card-end: markdown--><p>Наша скромная модель угадывает лишь 56% процентов диагнозов, и такой результат, как правило, считают промежуточным и работают над улучшением точности модели.</p><h3 id="sklearn">SkLearn</h3><p>С помощью замечательной библиотеки Scikit-learn мы можем мгновенно определить множество метрик, и матрица ошибок – не исключение. </p><pre><code class="language-python">from sklearn.metrics import confusion_matrix\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\nconfusion_matrix(y_true, y_pred)</code></pre><p>Выводом будет ряд, состоящий из трех списков: </p><pre><code class="language-python">array([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])</code></pre><p>Значения диагонали сверху вниз слева направо [2, 0, 2] – это число верно  предсказанных значений.</p><p>Фото: <a href="https://unsplash.com/@opeleye">@opeleye</a></p>		matritsa-oshibok	2020-12-22		
41	Разведочный анализ данных (EDA), ч. 1		<p>Разведочный анализ данных (Exploratory Data Analysis) – предварительное исследование <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> с целью определения его основных характеристик, взаимосвязей между признаками, а также сужения набора методов, используемых для создания <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (Machine Learning)</a>.</p><p>Давайте рассмотрим, на какие этапы EDA разбивают. Для этого мы используем данные банка, который автоматизирует выдачу кредитов своим клиентам. В реальной жизни получить такой датасет – довольно дорогое удовольствие, по карману зачастую это только среднему и крупному бизнесу. К счастью, мы располагаем обширным набором переменных (столбцов):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-dataset-description-2.png" class="kg-image" alt loading="lazy" width="864" height="1356" srcset="__GHOST_URL__/content/images/size/w600/2020/12/eda-dataset-description-2.png 600w, __GHOST_URL__/content/images/2020/12/eda-dataset-description-2.png 864w" sizes="(min-width: 720px) 720px"></figure><p>Теперь стало немного понятнее, почему менеджеры по продажам, звонящие из банков, ведут себя так странно? Они располагают именно таким набором данных о Вас.</p><p>Довольно увесистый датасет для восприятия, особенно если учесть, что записей в нем – более 40 тысяч. Однако приступим! Для начала импортируем датасет и посмотрим на "шапку". Параметр 'sep' используется, чтобы указать на нестандартный разделитель, в данном случае – точку с запятой, которая используется в Apple Numbers.</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/62xm9ymoaunnfg6/bank-full.csv?dl=1', sep=';')\ndf.head()</code></pre><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2020/12/eda-sample.png" class="kg-image" alt loading="lazy" width="2000" height="276" srcset="__GHOST_URL__/content/images/size/w600/2020/12/eda-sample.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/eda-sample.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/eda-sample.png 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/eda-sample.png 2400w"></figure><p>Итак, нам предстоит пройти несколько этапов разведочного анализа, и среди них будут взаимоисключающие, потому его придется импортировать несколько раз.</p><h3 id="-">Удаление дубликатов</h3><p>Дублирующие записи не только искажают статистические показатели датасета, но и снижают качество обучения модели, потому удалим полные дублирующие вхождения. Для начала уточним, сколько записей в датасете с помощью свойства <code><strong>Pandas.DataFrame.shape</strong></code>:</p><pre><code class="language-python">&gt;&gt;&gt; df.shape\n(41188, 21)</code></pre><p>Удалим дублирующие записи с помощью Pandas.drop_duplicates() и обновим данные о размере данных:</p><pre><code class="language-python">&gt;&gt;&gt; df = df.drop_duplicates()\n&gt;&gt;&gt; df.shape\n(41176, 21)</code></pre><p>Pandas нашел и удалил 12 дубликатов. Хоть число и небольшое, все же качество данные мы повысили.</p><h3 id="--1">Обработка пропусков</h3><p>Стоит помнить, что в случае, если пропусков у признака слишком много (более 70%), такой признак удаляют. Проверим, насколько полны наши признаки: метод <code><strong>isnull()</strong></code><strong> </strong>пройдется по каждой ячейке каждого столбца и определит, кто пуст, а кто нет, составив датафрейм такого же размера, состоящий из True / False. Метод <code><strong>mean()</strong></code><strong> </strong>суммирует все значения True, определит концентрацию пропусков в каждом столбце. На 100 мы умножаем, чтобы получить значение в процентах:</p><pre><code class="language-python">df.isnull().mean() * 100</code></pre><p>Среди всех признаков слишком много пропусков оказалось в переменной 'День':</p><!--kg-card-begin: markdown--><p>Возраст                                0.000000<br>\nРабота                                 0.801438<br>\nСемейный статус                        0.194288<br>\nОбразование                            4.201477<br>\nКредитный дефолт                       0.000000<br>\nИпотека                                0.000000<br>\nЗайм                                   0.000000<br>\nКонтакт                                0.000000<br>\nМесяц                                  0.000000<br>\nДень недели                            0.000000<br>\nДлительность                           0.000000<br>\nКампания                               0.000000<br>\nДень                                  96.320672<br>\nПредыдущий контакт                     0.000000<br>\nДоходность                             0.000000<br>\nКолебание уровня безработицы           0.000000<br>\nИндекс потребительских цен             0.000000<br>\nИндекс потребительской уверенности     0.000000<br>\nЕвропейская межбанковская ставка       0.000000<br>\nКоличество сотрудников в компании      0.000000<br>\ny                                      0.000000<br>\ndtype: float64</p>\n<!--kg-card-end: markdown--><p>Таким образом, переменная подлежит удалению с помощью drop():</p><pre><code class="language-python">df = df.drop(columns=['День'])</code></pre><p>Существует несколько способов обозначить пропуски, и зачастую создатели датасета не описывают данные в достаточной мере, и определять, как обозначены пропуски, приходится вручную. Из встреченных доселе обозначений приведу следующие:</p><ul><li>NaN / NaT (упрощенно: "не число" / "не время")</li><li>Пустая ячейка</li><li>Для числовых признаков – радикальный выброс. К примеру, для столбца "День" это число 999.</li><li>Маркер или нестандартный символ</li></ul><p>Встроенные методы Pandas позволяют с легкостью справиться с первыми двумя разновидностями таких пробелов. Разберемся для начала с категориальными переменными, объединив их в один вектор. Список получится совсем уж нелогичный, но это не столь важно в данной ситуации: мы лишь ищем способы обозначения пропуска.</p><pre><code class="language-python">&gt;&gt;&gt; column_values = df[['Работа', 'Семейный статус', 'Образование', 'Контакт', 'Месяц', 'День недели', 'Доходность']].values.ravel()\n&gt;&gt;&gt; unique_values =  pd.unique(column_values)\n&gt;&gt;&gt; print(unique_values)\n\n['Самозанятый' 'Не женат / не замужем' 'Университетская степень'\n 'Городской телефон' 'Октябрь' 'Пятница' 'Отсутствует' 'Преддприниматель'\n 'Женат / замужем' 'Голубой воротничок' 'Базовое (9 классов)' 'Менеджер'\n 'Высшая школа' 'Базовое (4 класса)' 'Техник' 'Профессиональный курс'\n 'Разведен(-а)' 'Неизвестно' 'Сотовый телефон' 'Август' 'Понедельник'\n 'Студент' 'Домохозяйка' 'Обслуживающий персонал' 'Базовое (6 классов)'\n 'Пенсионер' 'Четверг' 'Вторник' 'Не присутствует' 'Июль' 'Среда' 'Июнь'\n 'Неграмотный' 'Май' 'Ноябрь' 'Присутствует' 'Cамозанятый' 'Декабрь'\n 'Март' 'Апрель' 'Сентябрь']</code></pre><p>Из общего списка уникальных значений этих переменных пропуски обозначаются словом "Неизвестно". Для числовых переменных пропуски – число 999 или пустая ячейка.</p><p>Процесс обработки пропусков, к счастью, можно сократить с помощью <code><strong>sklearn.impute.SimpleImputer</strong></code><strong>. </strong>Мы выбираем все категориальные переменные и применяем стратегию "[вставить вместо пропуска] самое распространенное значение":</p><pre><code class="language-python">from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n\ndf["Работа"] = imputer.fit_transform(df["Работа"].values.reshape(-1,1))[:,0]\n\ndf["Семейный статус"] = imputer.fit_transform(df["Семейный статус"].values.reshape(-1,1))[:,0]\n\ndf["Образование"] = imputer.fit_transform(df["Образование"].values.reshape(-1,1))[:,0]\n\ndf["Месяц"] = imputer.fit_transform(df["Месяц"].values.reshape(-1,1))[:,0]\n\ndf["День недели"] = imputer.fit_transform(df["День недели"].values.reshape(-1,1))[:,0]\n\ndf["Доходность"] = imputer.fit_transform(df["Доходность"].values.reshape(-1,1))[:,0]</code></pre><p>Такой код можно сократить еще с помощью <a href="__GHOST_URL__/paiplain/">Пайплайнов (Pipeline)</a>, однако здесь обработаем каждую переменную построчно.</p><p>Признаки, принадлежащие к булевому типу данных, обрабатываются алгоритмом тем же образом. Целевую переменную Y мы не обрабатываем (если в этом столбце есть пропуски, такие строки стоит удалить):</p><pre><code class="language-python">imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n\ndf["Кредитный дефолт"] = imputer.fit_transform(df["Кредитный дефолт"].values.reshape(-1,1))[:,0]\n\ndf["Ипотека"] = imputer.fit_transform(df["Ипотека"].values.reshape(-1,1))[:,0]\n\ndf["Займ"] = imputer.fit_transform(df["Займ"].values.reshape(-1,1))[:,0]</code></pre><p>Подобным образом заполняются пустоты в числовых переменных, только стратегия теперь – "вставить среднее значение".</p><pre><code class="language-python">imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n\ndf["Возраст"] = imputer.fit_transform(df["Возраст"].values.reshape(-1,1))[:,0]\n\ndf["Длительность"] = imputer.fit_transform(df["Длительность"].values.reshape(-1,1))[:,0]\n\ndf["Кампания"] = imputer.fit_transform(df["Кампания"].values.reshape(-1,1))[:,0]\n\ndf["Предыдущий контакт"] = imputer.fit_transform(df["Предыдущий контакт"].values.reshape(-1,1))[:,0]\n\ndf["Колебание уровня безработицы"] = imputer.fit_transform(df["Колебание уровня безработицы"].values.reshape(-1,1))[:,0]\n\ndf["Индекс потребительских цен"] = imputer.fit_transform(df["Индекс потребительских цен"].values.reshape(-1,1))[:,0]\n\ndf["Индекс потребительской уверенности"] = imputer.fit_transform(df["Индекс потребительской уверенности"].values.reshape(-1,1))[:,0]\n\ndf["Европейская межбанковская ставка"] = imputer.fit_transform(df["Европейская межбанковская ставка"].values.reshape(-1,1))[:,0]\n\ndf["Количество сотрудников в компании"] = imputer.fit_transform(df["Количество сотрудников в компании"].values.reshape(-1,1))[:,0]</code></pre><p>Метод <code><strong>isnull()</strong></code><strong> </strong>пробегается по каждой ячейке каждого признака и определяет, пустая ли ячейка, возвращая True / False. Метод <code><strong>mean()</strong></code><strong> </strong></p><h3 id="--2">Обнаружение аномалий</h3><p>Самый легкий способ обнаружить выбросы – визуальный. Мы построим разновидность графика "ящик с усами" для одной из числовых переменных –  "Возраст":</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-boxplot.png" class="kg-image" alt loading="lazy" width="151" height="411"></figure><p>Скучковавшиеся окружности в верхней части изображения – и есть аномалии, и от них, как правило, избавляются с помощью квантилей:</p><pre><code class="language-python">q = df["Возраст"].quantile(0.99)\nq2 = df["Длительность"].quantile(0.99)\nq3 = df["Кампания"].quantile(0.99)\nq5 = df["Предыдущий контакт"].quantile(0.99)\nq6 = df["Колебание уровня безработицы"].quantile(0.99)\nq7 = df["Индекс потребительских цен"].quantile(0.99)\nq8 = df["Индекс потребительской уверенности"].quantile(0.99)\nq9 = df["Европейская межбанковская ставка"].quantile(0.99)\nq10 = df["Количество сотрудников в компании"].quantile(0.99)\n\ndf[df["Возраст"] &lt; q]\ndf[df["Длительность"] &lt; q2]\ndf[df["Кампания"] &lt; q3]\ndf[df["Предыдущий контакт"] &lt; q5]\ndf[df["Колебание уровня безработицы"] &lt; q6]\ndf[df["Индекс потребительских цен"] &lt; q7]\ndf[df["Индекс потребительской уверенности"] &lt; q8]\ndf[df["Европейская межбанковская ставка"] &lt; q9]\ndf[df["Количество сотрудников в компании"] &lt; q10]</code></pre><p>Во <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-2/">второй части статьи</a> о разведочном анализе Вы узнаете про:</p><ul><li>Одномерный анализ (описательная статистика, важность признаков (Feature Importance)</li><li>Одномерный анализ (парный анализ, уменьшение размерности, стандартизация, <a href="__GHOST_URL__/normalizatsiia/">Нормализация (Normalization)</a></li></ul><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1fYtVbamX9gxicgdpsCPk_HlECw-O_TJs?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@f7photo">@f7photo</a></p>		razvedochnyy-analiz-dannykh-chast-1	2020-12-26		
42	Разведочный анализ данных (EDA), ч. 2		<p>Разведочный анализ данных (Exploratory Data Analysis) – предварительное исследование <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> с целью определения его основных характеристик, взаимосвязей между <a href="__GHOST_URL__/p/36ec9a2f-dae2-4672-af4e-e322d634189c/www.helenkapatsa.ru/priznak/">Признаками (Feature)</a>, а также сужения набора методов, используемых для создания <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.</p><p>Итак, в <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">первой части статьи</a> мы познакомились со следующими этапами разведочного анализа:</p><ul><li>Удаление дубликатов</li><li>Обработка пропусков</li><li>Обнаружение аномалий</li></ul><p>И теперь продолжим глубже знакомиться с особенностями датасета.</p><h2 id="-">Одномерный анализ</h2><h3 id="--1">Описательная статистика</h3><p>Прежде чем применять те или иные методы обучения, нам необходимо удостовериться, что они применимы к текущему датасету. Раздел описательной статистики включает в себя проверку на нормальность распределения и определение прочих статистических метрик. С этим нам поможет замечательная библиотека <a href="__GHOST_URL__/pandas-profiling/">pandas-profiling</a>. Установим самую свежую версию во избежание ошибок:</p><pre><code class="language-python">!pip install pandas_profiling --upgrade</code></pre><p>Запустим профайлер и передадим df в качестве аргумента:</p><pre><code class="language-python">from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(df)\nprofile</code></pre><p>Профайлер высчитывает основные статистические метрики для каждой переменной и датасета в целом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-pandas-profiling.png" class="kg-image" alt loading="lazy" width="2000" height="481" srcset="__GHOST_URL__/content/images/size/w600/2020/12/eda-pandas-profiling.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/eda-pandas-profiling.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/eda-pandas-profiling.png 1600w, __GHOST_URL__/content/images/2020/12/eda-pandas-profiling.png 2104w" sizes="(min-width: 720px) 720px"></figure><p>К примеру, в признаке "Длительность" мы вычислили:</p><ul><li>Количество уникальных значений (Distinct)</li><li>Количество пропусков (Missing)</li><li>Вероятно, параметр "Бесконечность" ('Infinite'), рассчитываемый только для вещественных чисел, отыскивает сильно выделяющиеся значения, которыми иногда обозначают пропуски.</li><li>Среднее значение (Mean)</li><li>Минимум (Minimum)</li><li>Максимум (Maximum)</li><li>Количество нулей (Zeros)</li><li>Память, задействованная этой переменной (Memory Size)</li><li>Нормальность распределения (график)</li></ul><p>Следующий интересный раздел – "Корреляции" ('Correlations'). Чем ярче (краснее / синее) ячейка, тем сильнее выражена корреляция между парой признаков. Диагональные ячейки игнорируются, поскольку являются результатом расчета коэффициента между переменной и ее копией.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-pandas-correlation.png" class="kg-image" alt loading="lazy" width="2000" height="1073" srcset="__GHOST_URL__/content/images/size/w600/2020/12/eda-pandas-correlation.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/eda-pandas-correlation.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/eda-pandas-correlation.png 1600w, __GHOST_URL__/content/images/2020/12/eda-pandas-correlation.png 2106w" sizes="(min-width: 720px) 720px"></figure><p>Профайлер вычленил из датасета только числовые признаки, и потому матрица имеет размер 11 x 11. К примеру, "колебание уровня безработицы" и "европейская межбанковская ставка" сильно коррелируют друг с другом, но поскольку эти признаки второстепенны, в дальнейшем их можно объединить на этапе инжиниринга признаков (Feature Engineering). Зачастую целевая переменная не сильно коррелирует с предикторами.</p><h3 id="--2">Важность признаков</h3><p>Прежде чем произвести инжиниринг признаков и сократить объем входных данных, стоит определить, какие признаки имеют первостепенную значимость, и в этом нам поможет Scikit-Learn и критерий Хи-квадрат (Chi-Squared Test).:</p><pre><code class="language-python">from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX = df[['Возраст', 'Длительность', 'Кампания', 'День', 'Предыдущий контакт', 'Индекс потребительских цен', 'Европейская межбанковская ставка', 'Количество сотрудников в компании']]\ny = df.iloc[:, -1]\n\nbestfeatures = SelectKBest(score_func = chi2, k = 'all')\nfit = bestfeatures.fit(X, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\nfeatureScores = pd.concat([dfcolumns, dfscores], axis = 1)\nfeatureScores.columns = ['Specs', 'Score']  \nprint(featureScores.nlargest(10, 'Score'))  </code></pre><p>Неожиданно, но самым важным признаком оказалась длительность разговора и день звонка. Люди склонны брать кредитные продукты, если им позвонили в определенный день недели и разговор длился оптимальное время.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-feature-importance.png" class="kg-image" alt loading="lazy" width="424" height="340"></figure><h2 id="--3">Многомерный анализ</h2><h3 id="--4">Рассмотрение парных особенностей</h3><p>Чего только не создаст комьюнити в Науке о данных! Для нужд разведочного анализа крайне кстати будет и попарные графики, и здесь на помощь приходит другой великолепный класс - <code><strong>seaborn.pairplot()</strong></code><strong>. </strong>Каждая из переменных ляжет в основу одной из осей двумерного точечного графика, и так, пока все пары признаков не будут отображены. Сократим названия длинных переменных, чтобы уместить их на скромном отведенном пространстве:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2020/12/eda-pairplot-1.png" class="kg-image" alt loading="lazy" width="1869" height="1760" srcset="__GHOST_URL__/content/images/size/w600/2020/12/eda-pairplot-1.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/eda-pairplot-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/eda-pairplot-1.png 1600w, __GHOST_URL__/content/images/2020/12/eda-pairplot-1.png 1869w"></figure><h3 id="--5">Уменьшение размерности, стандартизация</h3><p>Рассмотрев признаки по отдельности и попарно, мы пришли к выводу, что некоторые признаки могут быть как бы объединены с помощью специальной техники – <a href="__GHOST_URL__/mietod-ghlavnykh-komponient/">Анализ главных компонент (PCA)</a>. Итак, давайте создадим заменяющий столбец, который представляет эти признаки в равной мере и тем самым уменьшим размер данных.</p><pre><code class="language-python"># Создадим список признаков, подлежащих уменьшению\nfeatures = ['Колебание уровня безработицы', 'Индекс потребительских цен', 'Индекс потребительской уверенности', 'Европейская межбанковская ставка']\n\n# Выбираем сокращаемые признаки и целевой\nx = df.loc[:, features].values\ny = df.loc[:,['y']].values</code></pre><p>Выполняем <a href="__GHOST_URL__/standartizatsiia/">Стандартизацию (Standartization)</a> x, и это впоследствии станет частью тренировочных данных:</p><pre><code class="language-python">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n\nx = StandardScaler().fit_transform(x)\npd.DataFrame(data = x, columns = features).head()\n\n</code></pre><p><code><strong>StandardScaler()</strong></code><strong> </strong>на месте заменяет данные на их стандартизированную версию, и мы получаем признаки, где все значения как бы центрованы относительно нуля. Такое преобразование необходимо, чтобы сократить нагрузку на вычислительную систему компьютера, который будет обучать модель:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-standart-scaler.png" class="kg-image" alt loading="lazy" width="698" height="252" srcset="__GHOST_URL__/content/images/size/w600/2020/12/eda-standart-scaler.png 600w, __GHOST_URL__/content/images/2020/12/eda-standart-scaler.png 698w"></figure><p>Анализ главных компонент (Principal Component Analysis) представляет собой метод уменьшения размерности больших наборов данных путем преобразования большого набора переменных в меньший с минимальными потерями информативности.</p><pre><code class="language-python">from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\nprincipalDf.head()</code></pre><p>Мы получили два принципиальных компонента и путем такого сокращения понижаем размерность датасета без потерь.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-pca-1.png" class="kg-image" alt loading="lazy" width="366" height="235"></figure><hr><h3 id="--6">Нормализация</h3><p>Еще один шаг, не затронутый в примере выше, – это <a href="__GHOST_URL__/normalizatsiia/">Нормализация (Normalization)</a>, и порой приходится выбирать между ею и стандартизацией. Мы нормализуем те же признаки, характеризующие состояние экономики и потому загрузим датасет в исходном виде еще раз:</p><pre><code class="language-python">from sklearn import preprocessing\n\ndf = pd.read_csv('https://www.dropbox.com/s/62xm9ymoaunnfg6/bank-full.csv?dl=1', sep=';')\n\n# Выберем признаки, выраженные вещественными числами и подлежащие \n# нормализации\nfeatures = ['Колебание уровня безработицы', 'Индекс потребительских цен', 'Индекс потребительской уверенности', 'Европейская межбанковская ставка']\nx = df.loc[:, features].values\n\n# Инициализируем нормализатор \nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled) </code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/eda-normalization.png" class="kg-image" alt loading="lazy" width="385" height="218"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1fYtVbamX9gxicgdpsCPk_HlECw-O_TJs?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@yassine_khalfall">@yassine_khalfall</a></p>		razvedochnyy-analiz-dannykh-chast-2	2020-12-26		
43	Быстрое кодирование (One-Hot Encoding)		<p>Быстрое кодирование (One-Hot Encoding) – процесс, с помощью которого <a href="__GHOST_URL__/katieghorialnaia-pieriemiennaia/">категориальные переменные</a> преобразуются в подходящую ​​алгоритмам <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> форму.</p><p>Первое, что вы делаете при создании любой <a href="__GHOST_URL__/modiel/">Модели (Model)</a>, – это, как правило, предварительная подготовка данных (Data Preparation). Мы еще не достигли такого уровня ИИ, чтобы просто передать модели таблицу и ожидать точных предсказаний. <br>Большая часть предварительной обработки – это кодирование в понятный компьютеру язык чисел. Отсюда и название 'encode', что буквально означает «преобразовать в [компьютерный] код». Существует множество различных способов кодирования, таких как Ярлычное (Label Encoding) или Быстрое кодирование. </p><p>Предположим, мы работаем с категориальными данными (кошки и собаки):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/one-hot-encoding.png" class="kg-image" alt loading="lazy" width="210" height="456"></figure><p>Легко догадаться, что столбец "Категория" представляет в виде чисел столбец "Вид", и кодирование просто означает присвоение числа 1 собакам, а 2 – кошкам. Получив числа, компьютер теперь может их обработать. Но есть проблема: числа имеют естественные упорядоченные отношения (3 &gt; 1), и это дает бо́льшим числам бо́льший вес. </p><p>Пример. Представьте, что у вас есть 3 категории продуктов: яблоки, курица и брокколи. Используя Ярлычное кодирование, вы должны присвоить каждому из них номер, чтобы разделить на категории: яблоки – 1, курица – 2 и брокколи – 3. Теперь, если Вашей модели предстоит рассчитать <a href="__GHOST_URL__/sriednieie-znachieniie/#-">среднее арифметическое (Mean)</a>, она по умолчанию сделает так: 1+ 3 = 4/2 = 2. В соответствии с Вашей моделью, нечто среднее между яблоками и курицей – это брокколи. Обнаруженные корреляции будут совершенно неверны.</p><p>Вместо того, чтобы обозначать вещи целыми положительными числами (1..3), мы воспользуемся бинарным стилем категоризации, 'One-Hot' – это как раз про это. Визуализируем разницу между ярлыком и Быстрым кодированием. Акцентируйте внимание на разнице:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/one-hot-encoding-ohe-vs-le------.png" class="kg-image" alt loading="lazy" width="1650" height="428" srcset="__GHOST_URL__/content/images/size/w600/2020/12/one-hot-encoding-ohe-vs-le------.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/one-hot-encoding-ohe-vs-le------.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/one-hot-encoding-ohe-vs-le------.png 1600w, __GHOST_URL__/content/images/2020/12/one-hot-encoding-ohe-vs-le------.png 1650w" sizes="(min-width: 720px) 720px"></figure><p><br>Наши категории раньше были строками, теперь они столбцы. Однако наш числовой <a href="__GHOST_URL__/bystroie-kodirovaniie/www.helenkapatsa.ru/priznak/">признак (Feature)</a> – калории, остался прежним. Ячейка A2 ('1') таблицы справа сообщит компьютеру категорию продукта правильным способом. </p><p>Этот способ не всегда лучше Ярлычного кодирования, хотя бы потому, что является низкоэффективным способом хранения данных (число столбцов резко увеличивается). Просто у него своя специализация.</p><h3 id="onehotencoder">OneHotEncoder</h3><p>Посмотрим, как <a href="__GHOST_URL__/data-saiientist/">Дата-сайентисты (Data Scientist)</a> научились мгновенно выполнять Быстрое кодирование:</p><!--kg-card-begin: markdown--><p>import numpy as np<br>\nimport pandas as pd</p>\n<!--kg-card-end: markdown--><p> Импортируем <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>:</p><!--kg-card-begin: markdown--><p>df = pd.read_csv('<a href="https://www.dropbox.com/s/orj5fgqzqsgsi87/googleplaystore.csv?dl=1">https://www.dropbox.com/s/orj5fgqzqsgsi87/googleplaystore.csv?dl=1</a>')<br>\ndf.head()</p>\n<!--kg-card-end: markdown--><p><br>Это датасет приложений с Google Play Store, включающий название, категорию, рейтинг, стоимость и прочие параметры.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/one-hot-encoding-apps.png" class="kg-image" alt loading="lazy" width="1398" height="292" srcset="__GHOST_URL__/content/images/size/w600/2020/12/one-hot-encoding-apps.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/one-hot-encoding-apps.png 1000w, __GHOST_URL__/content/images/2020/12/one-hot-encoding-apps.png 1398w" sizes="(min-width: 720px) 720px"></figure><p>Для простоты восприятия опустим все столбцы, кроме автоиндекса, названия и категории ('CATEGORY'):</p><!--kg-card-begin: markdown--><p>df = df.drop(columns=['Rating', 'Reviews', 'Size', 'Installs', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver'])<br>\ndf.head()</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/one-hot-encoding-apps-simple.png" class="kg-image" alt loading="lazy" width="372" height="275"></figure><p>Теперь займемся кодированием. Используем встроенный метод get_dummies(), чтобы получить такую же таблицу, как и в примере с пищей:</p><!--kg-card-begin: markdown--><p>s = pd.Series(df.Category)<br>\npd.get_dummies(s)</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2020/12/one-hot-encoding-dummies.png" class="kg-image" alt loading="lazy" width="2000" height="284" srcset="__GHOST_URL__/content/images/size/w600/2020/12/one-hot-encoding-dummies.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/one-hot-encoding-dummies.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/one-hot-encoding-dummies.png 1600w, __GHOST_URL__/content/images/size/w2400/2020/12/one-hot-encoding-dummies.png 2400w"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/18kAMOAmPAIL6-hFlLqp2WrKXhuUIcc5g?usp=sharing">здесь</a>. </p><p>Фото: <a href="https://unsplash.com/@davidclode">@davidclode</a></p>		bystroie-kodirovaniie	2021-01-01		
44	Проклятие размерностей (Curse of Dimensionality)		<p>Проклятие размерностей – одна из крупнейших проблем Машинного обучения (Machine Learning), которая гласит: чем выше размерность, тем более разреженны данные. Иными словами, по мере роста количества признаков объем данных, которые нам нужно обобщить, растет экспоненциально.</p><p>Пример. Легко поймать гусеницу, движущуюся в трубе (1 размер). Собаку сложнее поймать, если она бегает по самолету (два измерения). Гораздо труднее охотиться на птиц, у которых теперь есть дополнительное измерение, в которое они могут перемещаться. Если мы представим призраков существами из более высоких измерений, их будет еще труднее поймать.</p><p>Еще пример. Предположим, у нас есть две точки на прямой, 0 и 1. Эти две точки находятся на расстоянии друг от друга, равном единице:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/curse-of-dimensionality_one-dimension-1.png" class="kg-image" alt loading="lazy" width="700" height="230" srcset="__GHOST_URL__/content/images/size/w600/2021/01/curse-of-dimensionality_one-dimension-1.png 600w, __GHOST_URL__/content/images/2021/01/curse-of-dimensionality_one-dimension-1.png 700w"></figure><p>Теперь мы вводим вторую ось Y – второе измерение. Положение точек определяется теперь списком из двух чисел – (0,0) и (1,1). Расстояние между точками теперь подсчитывается с помощью Евклидова расстояния. Помните извечный школьный способ вычисления гипотенузы – теорему Пифагора? Сумма квадратов длин катетов равна квадрату длины гипотенузы. Евклидово расстояние и вычисляется с помощью этого правила:</p><!--kg-card-begin: markdown--><p>$$d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} = \\sqrt{(1 - 0)^2 + (1 - 0)^2} = \\sqrt{2} ≈ 1,414, где$$<br>\n$$d\\space{–}\\space{расстояние}\\space{между}\\space{точками,}$$<br>\n$$x_1,\\space{x_2}\\space{–}\\space{координаты}\\space{точек}\\space{по}\\space{оси}\\space{X,}$$<br>\n$$y_1,\\space{y_2}\\space{–}\\space{координаты}\\space{точек}\\space{по}\\space{оси}\\space{Y.}$$</p>\n<!--kg-card-end: markdown--><p>В двумерном пространстве наши точки выглядят следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/curse-of-dimensionality_two-dimensions-1.png" class="kg-image" alt loading="lazy" width="700" height="565" srcset="__GHOST_URL__/content/images/size/w600/2021/01/curse-of-dimensionality_two-dimensions-1.png 600w, __GHOST_URL__/content/images/2021/01/curse-of-dimensionality_two-dimensions-1.png 700w"></figure><p>Расстояние увеличилось, и вместо 1 равно теперь примерно 1,41! В трехмерном пространстве две точки будут на расстоянии √3. К тому времени, когда мы достигнем 10-мерного пространства, две точки будут находиться на расстоянии более 3 целых (≈3,16) друг от друга. Посмотрите, как элегантно <a href="https://www.kaggle.com/residentmario">@residentmario</a> иллюстрирует этот принцип с помощью Matplotlib:</p><pre><code class="language-python">import matplotlib.pyplot as plt\nimport numpy as np</code></pre><pre><code class="language-python">plt.style.use('fivethirtyeight') # Стиль графика\n\nplt.figure(figsize = (12, 6)) # Размер полотна\nplt.title("Евклидово расстояние для $[0]^{1 x n}$ и $[1]^{1 x n}$, если $n \\in \\{1, \\ldots, 10\\}$") \nplt.xlabel('$n$')\nplt.ylabel('$|| x - y ||_2$')\nplt.plot(range(1, 10), np.sqrt(range(1, 10)))</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/curse-of-dimensionality-euclidian-distance.png" class="kg-image" alt loading="lazy" width="826" height="429" srcset="__GHOST_URL__/content/images/size/w600/2021/01/curse-of-dimensionality-euclidian-distance.png 600w, __GHOST_URL__/content/images/2021/01/curse-of-dimensionality-euclidian-distance.png 826w" sizes="(min-width: 720px) 720px"></figure><h3 id="-">Машинное обучение</h3><p>Чтобы получить статистически обоснованный и надежный результат, экспоненциально наращивают количество данных, необходимых для подтверждения результата. Процесс часто базируется на обнаружении областей, в которых <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> образуют <a href="__GHOST_URL__/klaster/">Кластеры (Cluster)</a> со схожими свойствами. Однако в данных большой размерности все объекты кажутся разреженными и непохожими во многих отношениях, что мешает эффективно применять такие стратегии.</p><p>Если дистанционные метрики, такие как <a href="__GHOST_URL__/ievklidovo-rasstoianiie/">Евклидово расстояние (Euclidean Distance)</a>, используются в наборе данных со слишком большим количеством измерений, все наблюдения (Observation) становятся примерно равноудаленными друг от друга. К примеру, метод K-средних (K-Means) использует меру расстояния для количественной оценки сходства между наблюдениями. Если все расстояния примерно равны, тогда все наблюдения кажутся одинаково похожими (или одинаково разными), и никакие значимые кластеры не могут быть сформированы.</p><p>Объем пространства увеличивается с невероятной скоростью относительно количества измерений. Даже 10 измерений могут стать "проклятием".<br>Короче говоря, по мере роста числа измерений, Евклидово расстояние между точкой и ее ближайшим соседом, а также дальним соседом изменяется неочевидным образом.<br><br>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1-yJQKY6ZblTLgoF6QVX4STaFZJYSSBMD?usp=sharing">здесь</a>. </p><p>Фото: <a href="https://unsplash.com/@eprouzet">@eprouzet</a></p>		prokliatiie-razmiernostiei	2021-01-07		
45	Нормальное распределение (Normal Distribution)		<p>Нормальное распределение (распределением Гаусса или Гаусса — Лапласа) – распространенная разновидность непрерывного распределения вероятностей для случайной величины. </p><p>Помните колоколообразную кривую? Вот эту:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/normal-distribution.png" class="kg-image" alt loading="lazy" width="700" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normal-distribution.png 600w, __GHOST_URL__/content/images/2021/01/normal-distribution.png 700w"></figure><p>Долгое время она служила главным критерием профессиональной оценки сотрудников американских учреждений, и равнодушных не оставляла, ведь от нее зависело, как себя позиционирует человек и его начальство.</p><p>Нормальное распределение – это ключевая концепция <a href="__GHOST_URL__/statistika/">Статистики (Statistics)</a> и основа <a href="__GHOST_URL__/nauka-o-dannykh/">Науки о данных (Data Science)</a>. При выполнении <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">Разведочного анализа данных (EDA)</a> мы сначала стремимся найти их распределение вероятностей, и наиболее распространенный ее вид – нормальное распределение.</p><p>Посмотрите на распределение вероятностей окупить инвестиции в фондовый индекс S&amp;P 500:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/normal-distribution_s-p-500--return-2.png" class="kg-image" alt loading="lazy" width="700" height="328" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normal-distribution_s-p-500--return-2.png 600w, __GHOST_URL__/content/images/2021/01/normal-distribution_s-p-500--return-2.png 700w"></figure><p>Да-да, вероятность "выйти в ноль" выше остальных! Также справедливо утверждение, что вероятность потерять больше как бы тает вместе с отрицательным процентом возврата. Белой непрерывной линией обозначено предсказание кривой нормального распределения. Прочие наблюдения, такие как вес при рождении и показатель IQ, часто следуют нормальному распределению подобным образом.</p><p>Еще одна причина, по которой нормальное распределение становится важным для <a href="__GHOST_URL__/data-saiientist/">Дата-сайентистов (Data Scientist)</a> – это Центральная предельная теорема (Central Limit Theorem). Эта теорема объясняет магию математики и является основой методов проверки гипотез.</p><p>В этой статье мы поймем важность и различные свойства нормального распределения, а изучим, как использовать эти свойства для проверки нормальности наших данных.</p><h3 id="-">Свойства нормального распределения</h3><p>Кривая стандартного нормального распределения симметрична относительно <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднего арифметического (Mean)</a>, <a href="__GHOST_URL__/miediana/">Медианы (Median)</a> и <a href="__GHOST_URL__/moda/">Моды (Mode)</a>. Более того, также являются нормальным распределением произведение двух нормальных распределений и их сумма. Магия, не правда ли? Существуют и другие, более сложные закономерности, пока обойдемся самыми понятными. </p><h3 id="--1">Эмпирическое правило</h3><p>Вы слышали об эмпирическом правиле? Оно часто используется в статистике и гласит: "68,27% наблюдений случайной Выборки (Sample) лежат в пределах одного <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">Стандартного отклонения (Standard Deviation)</a>, 95,45% – в пределах двух, а 99,73 – в пределах трех стандартных отклонений от среднего":</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/standard-deviation-updated.png" class="kg-image" alt loading="lazy" width="306" height="378"></figure><p>Это правило позволяет нам идентифицировать Выбросы (Outlier) и очень полезно при Проверке на нормальность (Normality Test).</p><h3 id="--2">Стандартное нормальное распределение</h3><p>Стандартное нормальное распределение – это частный случай нормального распределения, когда среднее значение равно нулю и стандартное отклонение равно единице. Любое нормальное распределение мы можем преобразовать его в стандартное, используя формулу:</p><!--kg-card-begin: markdown--><p>$$z = \\frac{x - μ}{σ}, где$$<br>\n$$z\\space{–}\\space{,}$$<br>\n$$x\\space{–}\\space{значение выборки,}$$<br>\n$$μ\\space{–}\\space{среднее,}$$<br>\n$$σ\\space{–}\\space{стандартное}\\space{отклонение}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Есть два интерна: Левин и Ричардс. Левин набрал 65 баллов на экзамене по терапии, а Ричардс – 80 баллов на экзамене по кожной венерологии. Верно ли, что Ричардс учился лучше, чем Левин?</p><p>Нет, потому что манера поведения людей в терапии отличается от того, как люди проявляют себя в кожной венерологии. Таким образом, прямое сравнение простым сравнением оценок некорректно.</p><p>Теперь предположим, что отметки теста по терапии подчиняются нормальному распределению со средним значением 60 и стандартным отклонением 4. С другой стороны, отметки о кожвенерологии подчиняются нормальному распределению со средним значением 79 и стандартным отклонением 2.</p><p>Нам нужно будет вычислить <a href="__GHOST_URL__/standartizovannaia-otsienka/">Стандартизированную оценку (Z-score)</a> путем стандартизации обоих этих распределений:</p><!--kg-card-begin: markdown--><p>$$z_{Левин} = \\frac{65 - 60}{4} = 1,25$$<br>\n$$z_{Ричардс} = \\frac{80 - 79}{2} = 0,5$$</p>\n<!--kg-card-end: markdown--><p>Таким образом, Левин набрал 1,25 стандартного отклонения выше среднего, в то время как Ричардс – только 0,5. Следовательно, Левин показал себя лучше:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/normal-distribution_levin-vs-richards.png" class="kg-image" alt loading="lazy" width="700" height="328" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normal-distribution_levin-vs-richards.png 600w, __GHOST_URL__/content/images/2021/01/normal-distribution_levin-vs-richards.png 700w"></figure><h3 id="--3">Асимметричное распределение<br></h3><p>Нормальное распределение – это симметрично, что означает, что его "хвосты" слева и справа – зеркальные отображения друг друга. Но это не относится к большинству реальных наборов данных. Как правило, мы будем иметь дело со скошенными асимметричными распределениями. </p><h3 id="--4">Визуальная оценка нормальности</h3><p>Для таких целей принято использовать три вида графиков:</p><ul><li><a href="__GHOST_URL__/gistoghramma/">Гистограмма (Histogram)</a>, которая отображает частоту "попадания" наблюдения (Observation) в том или ином вертикальном "Ящике" (Bin). В этом примере распределение скошено вправо.</li><li>График <a href="__GHOST_URL__/yadernoye-sglazhivaniye/">Ядерного сглаживания (KDE)</a>. Является сглаженной версией гистограммы, где непрерывная кривая – ядро, также отображает каждое наблюдение. </li><li>График квантиль-квантиль (Q-Q Plot). Если значения располагаются по примерно прямой линии под углом 45 градусов, то данные распределяются нормально. На нашем графике видно, что значения данных имеют тенденцию немного отклоняться от линии под углом 45°, особенно на концах, что может указывать на то, что набор данных не распределяется нормально.</li></ul><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/normal-distribution_histgram-kde-qq-1.png" class="kg-image" alt loading="lazy" width="2000" height="312" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normal-distribution_histgram-kde-qq-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/normal-distribution_histgram-kde-qq-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/normal-distribution_histgram-kde-qq-1.png 1600w, __GHOST_URL__/content/images/2021/01/normal-distribution_histgram-kde-qq-1.png 2100w"><figcaption>Слева направо: гистограмма, График KDE, Q-Q Plot</figcaption></figure><p>Для оценки нормальности распределения также используют <a href="__GHOST_URL__/skoshiennost/">Скошенность (Skewness)</a> и <a href="__GHOST_URL__/ekstsiess/">Эксцесс (Kurtosis)</a>.</p><h3 id="-python">Нормальное распределение и Python</h3><p>Посмотрим, как выглядит код, визуализирующий распределение и заодно рассчитывающий основные метрики <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport pandas as pd\nimport seaborn as sns</code></pre><p>Загрузим данные:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/ezugw0xgfp8pzut/%D0%9E%D1%86%D0%B5%D0%BD%D0%BA%D0%B8.csv?dl=1')\ndf.head()</code></pre><p>Определим функцию, которая пройдется по всем столбцам датасета, рассчитает основные статистические метрики (среднее, минимум, максимум и т.д.):</p><pre><code class="language-python">def UVA_numeric(data):\n    var_group = data.columns # Список столбцов\n    size = len(var_group) # Количество столбцов (3)\n    plt.figure(figsize = (7 * size, 3), dpi = 400) # Параметры графика\n\n    # Применяем расчеты к каждому столбцу\n    for j,i in enumerate(var_group):\n        \n        # Рассчитываем основные статистические метрики\n        mini = data[i].min()\n        maxi = data[i].max()\n        ran = data[i].max()-data[i].min() # Диапазон значений\n        mean = data[i].mean()\n        median = data[i].median()\n        st_dev = data[i].std() # Стандартное отклонение\n        skew = data[i].skew() # Скошенность \n        kurt = data[i].kurtosis() # Эксцесс\n\n        # Расчет точек стандартного отклонения\n        points = mean - st_dev, mean + st_dev\n\n        # Построим график с каждым из трех наборов даннных\n        #Plotting the variable with every information\n        plt.subplot(1, size, j+1)\n        sns.distplot(data[i], hist = True, kde=  True)\n        \n        sns.lineplot(points, [0,0], color = 'black', label = "std_dev")\n        sns.scatterplot([mini,maxi], [0,0], color = 'orange', label = "min/max")\n        sns.scatterplot([mean], [0], color = 'red', label = "mean")\n        sns.scatterplot([median], [0], color = 'blue', label = "median")\n        plt.xlabel('{}'.format(i), fontsize = 20)\n        plt.ylabel('density')\n        plt.title('Стандартное отклонение = {}; Эксцесс = {};\\n Скошенность = {}; Разброс, шаг гистограммы = {}\\n Среднее = {}; Медиана = {}'.format((round(points[0],2),round(points[1],2)),\n                                                                                 round(kurt,2),                             \t\t\t\t\t\t\tround(skew,2),\n        (round(mini,2),round(maxi,2),round(ran,2)),\n        round(mean,2),\n        round(median,2)))</code></pre><p>Построим тройной график:</p><pre><code class="language-python">UVA_numeric(df)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/normal-distribution-marks------.png" class="kg-image" alt loading="lazy" width="2000" height="419" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normal-distribution-marks------.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/normal-distribution-marks------.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/normal-distribution-marks------.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/01/normal-distribution-marks------.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1TYUuT2CDLg9s9E70n-2DuCp7q6rJGsl-?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@changlisheng">@changlisheng</a></p>		normalnoie-raspriedielieniie	2021-01-09		
46	Доля правильных ответов (Accuracy)		<p>Доля правильных ответов (ДПО) – это коэффициент, характеризующий верность прогноза <a href="__GHOST_URL__/modiel/">Модели (Model)</a>, отношение правильно спрогнозированных <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> к общему их количеству. Рассчитывается показатель с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$ДПО\\space{измерений}\\space{=}\\space{\\frac{\\space{Количество}\\space{верных}\\space{предсказаний}}{Общее\\space{количество}\\space{предсказаний}}}$$</p>\n<!--kg-card-end: markdown--><p>Для бинарной классификации (Binary Classification) ДПО также может быть рассчитана с помощью <a href="__GHOST_URL__/matritsa-oshibok/">Матрицы ошибок (Confusion Matrix)</a> следующим образом:</p><!--kg-card-begin: markdown--><p>$$ДПО\\space{=}\\space{\\frac{ИП + ИН}{ИП + ИН + ОП + ОН}},\\space{где}$$<br>\n$$ИП\\space{–}\\space{истинно}\\space{позитивные}\\space{предсказания}$$<br>\n$$ИН\\space{–}\\space{истинно}\\space{негативные}\\space{предсказания}$$<br>\n$$ОП\\space{–}\\space{ошибочно}\\space{позитивные}\\space{предсказания}$$<br>\n$$ОН\\space{–}\\space{ошибочно}\\space{негативные}\\space{предсказания}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Попробуем рассчитать accuracy модели, которая различает  злокачественные (положительный класс) и доброкачественные (отрицательный класс) опухоли. По результатам последовавшей медицинской проверки оказалось, что модель была права не везде:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/accuracy.png" class="kg-image" alt loading="lazy" width="360" height="245"></figure><p>Согласно формуле, доля правильных ответов будет равна:</p><!--kg-card-begin: markdown--><p>$$ДПО\\space{измерений}\\space{=}\\space{\\frac{1 + 90}{1 + 90 + 1 + 8}}\\space{=}\\space{0,91}$$</p>\n<!--kg-card-end: markdown--><p>По меркам современной "успеваемости" моделей <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, мы добились, на первый взгляд, удовлетворительного результата. Но так ли все просто?</p><p>На 100 наблюдений 91 доброкачественных и 9 злокачественных.</p><ul><li>Из 91 доброкачественных опухолей 90 обнаружены</li><li>Из 9 злокачественных только 1 диагностирована. Иными словами, 8 из 9 злокачественных опухолей не диагностированы. <strong>И в этом суть.</strong></li></ul><p>Поскольку способность модели найти злокачественные опухоли – ключевая задача модели, то эффективность в 1/9 (≈0,11) не может быть признана достаточной.</p><p>Сама по себе доля правильных ответов не дает полной картины, когда вы работаете с несбалансированным по классам датасетом (Imbalanced Dataset), подобным этому, где количество отрицательных значительно превышает количество положительных меток. Тогда полную картину составят accuracy в сочетании с другими показателями: Точность результата измерений (Precision), <a href="__GHOST_URL__/otzyv/">Отзыв (Recall)</a>, критерий F1 (F1 Score).</p><p>Фото: <a href="https://unsplash.com/@bermixstudio">@bermixstudio</a></p>		tochnost-izmierienii	2021-01-10		
47	Среднее значение (Average)		<p>Среднее значение (μ – "мю", x̅ ) – мера центральной тенденции, служащая для описания множества значений одним-единственным числом. Меру  можно охарактеризовать несколькими метриками: <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Cреднее (Mean)</a>, <a href="__GHOST_URL__/miediana/">Медиана (Median)</a>, Мода (Mode). В <a href="__GHOST_URL__/nauka-o-dannykh/">Науке о данных (Data Science)</a> широкое применение получили следующие его разновидности: арифметическое, геометрическое и гармоническое средние значения.</p><h2 id="-">Среднее арифметическое</h2><p>Среднее арифметическое  (μ для совокупности, x̄ для выборки; англ. Arithmetic Mean) – показатель описательной статистики, сумма элементов <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, разделенная на их количество. Рассчитывается с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$μ = \\frac{Σ_{i=1}^n a_i}{n}, где$$<br>\n$$μ\\space{–}\\space{среднее,}$$<br>\n$$Σ_{i=1}^n a_i\\space{–}\\space{сумма}\\space{всех}\\space{элементов}\\space{выборки},$$<br>\n$$n\\space{–}\\space{количество}\\space{наблюдений}$$</p>\n<!--kg-card-end: markdown--><p>По умолчанию рассматривают именно с среднее <em>арифметическое</em>, остальные разновидности среднего рассматривают реже:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/mean-1.png" class="kg-image" alt loading="lazy" width="1316" height="702" srcset="__GHOST_URL__/content/images/size/w600/2021/01/mean-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/mean-1.png 1000w, __GHOST_URL__/content/images/2021/01/mean-1.png 1316w" sizes="(min-width: 720px) 720px"><figcaption>Разновидности среднего значения</figcaption></figure><p>В данной статье рассматриваются простые средние значения без Весовой функции (Weight Function).</p><p>Пример. Для небольшого списка [1, 6, 3, 2] средним арифметическим будет:</p><!--kg-card-begin: markdown--><p>$$μ = \\frac{1 + 6 + 3 + 2}{4} = \\frac{12}{4} = 3$$</p>\n<!--kg-card-end: markdown--><p>Понятие используется в Науке о данных множеством способов:</p><ul><li>В сочетании с другими показателями описательной статистики для первичного представления о <a href="__GHOST_URL__/priznak/">признаке (Feature)</a></li><li>Для визуальной оценки скошенности признака:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/mean_skewness.png" class="kg-image" alt loading="lazy" width="660" height="359" srcset="__GHOST_URL__/content/images/size/w600/2021/01/mean_skewness.png 600w, __GHOST_URL__/content/images/2021/01/mean_skewness.png 660w"><figcaption>Данные скошены влево, и одного Среднего для описания признака уже недостаточно</figcaption></figure><ul><li>Для индикации Выбросов (Outlier) и проч.</li></ul><h3 id="-statistics">Среднее арифметическое и библиотека statistics</h3><p>Рассчитать среднее автоматически позволит библиотека statistics. Установим библиотеку и импортируем ее для начала:</p><pre><code class="language-python">!pip install statistics</code></pre><pre><code class="language-python">import statistics</code></pre><p>Инициализируем список:</p><pre><code class="language-python"># Выборка: высота плодовых деревьев  \nlst = [5, 16, 1, 12, 20, 5, 17, 2, 11, 3, 16, 15, 6, 9]\n\nx = statistics.mean(lst) \nprint('Арифметическое среднее: %.3f' % x)</code></pre><pre><code class="language-python">Арифметическое среднее: 9.857</code></pre><h2 id="--1">Среднее геометрическое </h2><p>Среднее геометрическое (Geometric Mean) – корень N-й степени из произведения всех значений:</p><!--kg-card-begin: markdown--><p>$$x̅_{geom} = \\sqrt[n]{x_1 × x_1 ×... × x_n},\\space{где}$$<br>\n$$x̅_{geom}\\space{–}\\spaceсреднее\\space{геометрическое,}$$<br>\n$$x_n\\space{–}\\space{n-й}\\space{элемент}\\space{выборки}$$</p>\n<!--kg-card-end: markdown--><p>Если Выборка (Sample) содержит два значения, мы извлекаем квадратный корень из перемноженных элементов. Для трех значений используется кубический корень и так далее.</p><p>Пример. Как построить квадрат той же площади, что и прямоугольник 2 x 18? Вычислим среднее геометрическое:</p><!--kg-card-begin: markdown--><p>$$x̅_{geom} = \\sqrt[2]{2 × 18} = 6$$</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/mean_rectangle-square.png" class="kg-image" alt loading="lazy" width="352" height="132"><figcaption>Площади равны</figcaption></figure><p>Наш квадрат будет иметь ту же площадь (36), и ребра, равные 6.</p><p>В <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a> Критерий G-Mean (Geometric Mean) – это Среднее геометрическое, определяющее качество классификации большинства и меньшинства. Низкий G-Mean-критерий является признаком плохой работы <a href="__GHOST_URL__/modiel/">Модели (Model)</a> в Бинарной классификации (Binary Classification) для положительных случаев.</p><h3 id="-scipy">Среднее геометрическое и SciPy</h3><p>Среднее геометрическое можно вычислить с помощью функции SciPy <code><strong>gmean()</strong></code>:</p><pre><code class="language-python">from scipy.stats import gmean</code></pre><pre><code class="language-python"># Инициализируем список данных\ndata = [1, 2, 3, 40, 50, 60, 0.7, 0.88, 0.9, 1000]\n\n# Применим функцию\ny = gmean(data)\nprint('Среднее геометрическое: %.3f' % y)</code></pre><pre><code class="language-python">Среднее геометрическое: 7.246</code></pre><h2 id="--2">Среднее гармоническое</h2><p>Среднее гармоническое (Harmonic Mean) – количество значений, поделенное на сумму обратных величин:</p><!--kg-card-begin: markdown--><p>$$x̅_{harmonic} = \\frac{N}{\\frac{1}{x_1} + \\frac{1}{x_2} + ... + \\frac{1}{x_n}},\\space{где}$$<br>\n$$x̅_{harmonic}\\space{–}\\space{среднее}\\space{гармоническое,}$$<br>\n$$x_n\\space{–}\\space{n-й}\\space{элемент}\\space{выборки}$$</p>\n<!--kg-card-end: markdown--><p>В Машинном обучении Критерий F1 ( F1 Score), показатель оценки эффективности модели, – это Среднее гармоническое <a href="__GHOST_URL__/tochnost-izmierienii/">Точности измерений (Accuracy)</a> и <a href="__GHOST_URL__/otzyv/">Отзыва (Recall)</a>.</p><h3 id="-scipy-1">Среднее гармоническое и SciPy</h3><p>Среднее гармоническое значение можно вычислить с помощью функции SciPy <code><strong>hmean()</strong></code>.</p><pre><code class="language-python">from scipy.stats import hmean</code></pre><pre><code class="language-python"># Инициализируем список\ndata = [0.11, 0.22, 0.33, 0.44, 0.55, 0.66, 0.77, 0.88, 0.99]\n\nz = hmean(data)\nprint('Среднее гармоническое: %.3f' % z)</code></pre><pre><code class="language-python">Среднее гармоническое: 0.350</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1u8a8jXH5Hq-tHvfmR2WBeLRcrkygQ9Go?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@peterluo0113">@peterluo0113</a></p>		sriednieie-znachieniie	2021-01-13		
48	Выброс (Outlier)	Выброс – это наблюдение, удаленное от других в выборке. Другими словами, это Наблюдение (Observation), которое расходится с общей закономерностью Выборки (Sample).	<p>Выброс &ndash; это наблюдение, удаленное от других в выборке. Другими словами, это <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a>, которое расходится с общей закономерностью Выборки (Sample).</p>\r\n<figure class="kg-card kg-image-card kg-card-hascaption"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier.png" alt="" width="472" height="310" loading="lazy" />\r\n<figcaption>Выбросы обозначены голубым цветом</figcaption>\r\n</figure>\r\n<p>Стоит различать это определение с Несбалансированным датасетом (Imbalanced Dataset). Хоть в определениях и есть некоторые сходства, однако несбалансированный набор данных с точки зрения <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> &ndash; это меньший размер выборки одного класса в сравнении с другим.</p>\r\n<h3 id="-">Источники выбросов</h3>\r\n<p>Появление таких наблюдений может быть вызвано:</p>\r\n<ul>\r\n<li>Различиями в способах измерений (например, изменилась чувствительность датчика)</li>\r\n<li>Экспериментальными ошибками (регламент эксперимента совершенствуется на ходу)</li>\r\n<li>Новыми процессами (появление другого человека в наблюдении за поведением одного единственного)</li>\r\n</ul>\r\n<p>Выбросы могут быть результатом ошибки во время сбора данных или индикатором расхождения наблюдений. Потому их надлежит исключить из <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>. Однако Дата-сайентисты (Data Scientist) могут столкнуться с трудностями во время разграничения выбросов и нормальных значений, потому и не спешат исключать то или иное наблюдение.</p>\r\n<h3 id="--1">Разновидности выбросов</h3>\r\n<p>Выделяют 3 типа выбросов:</p>\r\n<ul>\r\n<li><strong>Глобальные: </strong>наблюдение далеко выходит за пределы всего набора данных. Пример: в классе все ученики &ndash; сверстники, но попадается запись об учащемся в возрасте 500 лет.</li>\r\n<li><strong>Условные</strong>: наблюдения считаются аномальными с учетом контекста. Пример: экономические показатели резко страны падают из-за мирового экономического кризиса, и на какое-то время нормой становятся более низкие показатели.</li>\r\n<li><strong>Коллективные</strong>: набор наблюдений, близких друг к другу и имеющих близкие аномальные значения. Подмножество точек считается аномальным, если эти значения как совокупность значительно отклоняются от всего набора данных, но значения отдельных точек данных сами по себе не являются аномальными ни в контекстном, ни в глобальном смысле:</li>\r\n</ul>\r\n<figure class="kg-card kg-image-card kg-card-hascaption"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier.jpg" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/01/outlier.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/01/outlier.jpg 1000w, __GHOST_URL__/content/images/2021/01/outlier.jpg 1374w" alt="" width="1374" height="583" loading="lazy" />\r\n<figcaption>Стоимость акции $MOMO</figcaption>\r\n</figure>\r\n<h3 id="--2">Почему так важно идентифицировать выбросы?</h3>\r\n<p>Алгоритмы Машинного обучения чувствительны к диапазону и распределению значений атрибутов. Выбросы могут ввести в заблуждение <a href="__GHOST_URL__/modiel/">Модель (Model)</a>, что приведет к увеличению времени обучения, меньшей Точности (Accuracy) и, в конечном итоге, к худшим результатам.</p>\r\n<h3 id="--3">Визуальные методы обнаружения выбросов</h3>\r\n<p>Выбросы легко обнаружить с помощью следующих графиков:</p>\r\n<ul>\r\n<li>Ящик с усами (Boxplot) &ndash; это метод анализа одного или нескольких <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, которые используют <a href="__GHOST_URL__/miediana/">Медиану (Median)</a>, а также квантили 0.25, 0.75:</li>\r\n</ul>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier-boxplot_boxplot.jpg" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/01/outlier-boxplot_boxplot.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/01/outlier-boxplot_boxplot.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/outlier-boxplot_boxplot.jpg 1600w, __GHOST_URL__/content/images/2021/01/outlier-boxplot_boxplot.jpg 1627w" alt="" width="1627" height="2062" loading="lazy" /></figure>\r\n<ul>\r\n<li>Гистограмма (Histogram)</li>\r\n</ul>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier-histogram.jpg" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/01/outlier-histogram.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/01/outlier-histogram.jpg 1000w, __GHOST_URL__/content/images/2021/01/outlier-histogram.jpg 1496w" alt="" width="1496" height="663" loading="lazy" /></figure>\r\n<ul>\r\n<li><a href="__GHOST_URL__/chat-bot-chatbot/">Точечная диаграмма (Scatterplot)</a></li>\r\n</ul>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier-scatterplot-1.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/01/outlier-scatterplot-1.png 600w, __GHOST_URL__/content/images/2021/01/outlier-scatterplot-1.png 947w" alt="" width="947" height="480" loading="lazy" /></figure>\r\n<h3 id="--4">Математические методы обнаружения выбросов</h3>\r\n<p>Наряду с визуальными методами мы также можем использовать некоторые математические функции:</p>\r\n<ul>\r\n<li><a href="__GHOST_URL__/standartizovannaia-otsienka/">Стандартизованная оценка (Z-Score)</a>, которая характеризует меру наблюдаемого значения. В большинстве случаев используется пороговое значение: если значение Z-оценки больше или меньше 3 или -3 соответственно, эта точка данных будет классифицирована как выброс.</li>\r\n<li>Межквартильный размах (Interquartile Range), разница между верхним и нижним квартилями.</li>\r\n</ul>\r\n<p>Это далеко не полный список методов для поиска выбросов.</p>\r\n<h3 id="-scikit-learn">Выбросы и библиотека Scikit-learn</h3>\r\n<p>Выбросы можно найти с помощью Scikit-learn. Начнем с импорта необходимых библиотек:</p>\r\n<pre><code class="language-python">import numpy as np\r\nimport pandas as pd\r\n\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\n\r\nimport seaborn as sns\r\n\r\nimport sklearn\r\nfrom sklearn import datasets\r\n\r\nimport scipy\r\nfrom scipy import stats</code></pre>\r\n<p>Затем мы загрузим "Бостонский датасет" о ценах на недвижимость:</p>\r\n<pre><code class="language-python"># Используем встроенную функцию load_boston\r\nboston = datasets.load_boston(return_X_y = False)\r\n\r\n# Преобразуем данные в датафрейм\r\nboston_df = pd.DataFrame(boston.data)\r\n\r\n# Присвоим названиям столбцов заранее подготовленный список feature_names\r\nboston_df.columns = boston.feature_names\r\nboston_df.head()</code></pre>\r\n<p>Мы будем работать со следующим <a href="__GHOST_URL__/datafrieim/">Датафреймом (DataFrame)</a>:</p>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier-dataset-head.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/01/outlier-dataset-head.png 600w, __GHOST_URL__/content/images/2021/01/outlier-dataset-head.png 850w" alt="" width="850" height="220" loading="lazy" /></figure>\r\n<p>Названия признаков имеют следующие значения:</p>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier-dataset-description.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/01/outlier-dataset-description.png 600w, __GHOST_URL__/content/images/2021/01/outlier-dataset-description.png 856w" alt="" width="856" height="721" loading="lazy" /></figure>\r\n<p>Отобразим ящик с усами для одного из признаков &ndash; расстояния от бостонских центров занятости:</p>\r\n<pre><code class="language-python">sns.boxplot(x = boston_df['DIS'])</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier-boxplot.png" alt="" width="352" height="262" loading="lazy" /></figure>\r\n<p>Теперь &ndash; точечную диаграмму:</p>\r\n<pre><code class="language-python">fig, ax = plt.subplots(figsize = (16, 8))\r\nax.scatter(boston_df['INDUS'], boston_df['TAX'])\r\nax.set_xlabel('INDUS')\r\nax.set_ylabel('TAX')\r\nplt.show()</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/outlier-scatterplot.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/01/outlier-scatterplot.png 600w, __GHOST_URL__/content/images/2021/01/outlier-scatterplot.png 947w" alt="" width="947" height="480" loading="lazy" /></figure>\r\n<p>Обратимся к математическим методам обнаружения выбросов и начнем со Стандартизированной оценки:</p>\r\n<pre><code class="language-python"># Получим абсолютное значение (модуль) чисел признаков\r\nz = np.abs(stats.zscore(boston_df))\r\nprint(z)</code></pre>\r\n<p>Мы получим полный перечень стандартизированных оценок для каждого значения признака:</p>\r\n<pre><code class="language-python">[[0.41978194 0.28482986 1.2879095  ... 1.45900038 0.44105193 1.0755623 ]\r\n [0.41733926 0.48772236 0.59338101 ... 0.30309415 0.44105193 0.49243937]\r\n [0.41734159 0.48772236 0.59338101 ... 0.30309415 0.39642699 1.2087274 ]\r\n ...\r\n [0.41344658 0.48772236 0.11573841 ... 1.17646583 0.44105193 0.98304761]\r\n [0.40776407 0.48772236 0.11573841 ... 1.17646583 0.4032249  0.86530163]\r\n [0.41500016 0.48772236 0.11573841 ... 1.17646583 0.44105193 0.66905833]]</code></pre>\r\n<p>Сузим область поиска и отсечем нормальные значения:</p>\r\n<pre><code class="language-python"># Отсечем значения, лежашие в пределах трех стандартных отклонений\r\nthreshold = 3\r\nprint(np.where(z &gt; 3))</code></pre>\r\n<p>Список значительно сузился:</p>\r\n<pre><code class="language-python">(array([ 55,  56,  57, 102, 141, 142, 152, 154, 155, 160, 162, 163, 199,\r\n       200, 201, 202, 203, 204, 208, 209, 210, 211, 212, 216, 218, 219,\r\n       220, 221, 222, 225, 234, 236, 256, 257, 262, 269, 273, 274, 276,\r\n       277, 282, 283, 283, 284, 347, 351, 352, 353, 353, 354, 355, 356,\r\n       357, 358, 363, 364, 364, 365, 367, 369, 370, 372, 373, 374, 374,\r\n       380, 398, 404, 405, 406, 410, 410, 411, 412, 412, 414, 414, 415,\r\n       416, 418, 418, 419, 423, 424, 425, 426, 427, 427, 429, 431, 436,\r\n       437, 438, 445, 450, 454, 455, 456, 457, 466]), array([ 1,  1,  1, 11, 12,  3,  3,  3,  3,  3,  3,  3,  1,  1,  1,  1,  1,\r\n        1,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  5,  3,  3,  1,  5,\r\n        5,  3,  3,  3,  3,  3,  3,  1,  3,  1,  1,  7,  7,  1,  7,  7,  7,\r\n        3,  3,  3,  3,  3,  5,  5,  5,  3,  3,  3, 12,  5, 12,  0,  0,  0,\r\n        0,  5,  0, 11, 11, 11, 12,  0, 12, 11, 11,  0, 11, 11, 11, 11, 11,\r\n       11,  0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]))</code></pre>\r\n<p>Удалим из датасета значения, чей Z-критерий меньше 3:</p>\r\n<pre><code class="language-python">clean_boston_df = boston_df\r\nclean_boston_df = clean_boston_df[z &lt; 3].all(axis = 1)\r\ndisplay(clean_boston_df.shape)</code></pre>\r\n<p>Размер датасета слегка изменился:</p>\r\n<pre><code class="language-python">(6478,)</code></pre>\r\n<p>Рассмотрим еще один способ &ndash; межквартильный размах:</p>\r\n<pre><code class="language-python"># Первая квантиль\r\nQ1 = boston_df.quantile(0.25)\r\n\r\n# Третья квантиль\r\nQ3 = boston_df.quantile(0.75)\r\n\r\n# Межквантильное расстояние\r\nIQR = Q3 - Q1\r\nprint(IQR)</code></pre>\r\n<p>Применив метод <code><strong>quantile()</strong></code><strong> </strong>к датасету, мы получили список межквартильных размахов для каждого признака датасета:</p>\r\n<pre><code class="language-python">CRIM         3.595038\r\nZN          12.500000\r\nINDUS       12.910000\r\nCHAS         0.000000\r\nNOX          0.175000\r\nRM           0.738000\r\nAGE         49.050000\r\nDIS          3.088250\r\nRAD         20.000000\r\nTAX        387.000000\r\nPTRATIO      2.800000\r\nB           20.847500\r\nLSTAT       10.005000\r\ndtype: float64</code></pre>\r\n<p>Очистим набор данных с помощью специального условия:</p>\r\n<pre><code class="language-python"># Это причудливое выражение &ndash; прекрасный способ отфильтровать значения, выходящие за пределы \r\n# трех межквартильных размахов (по полторы с каждой стороны), причем с обеих сторон. \r\n# Мы намеренно отыскиваем значения, выходящие за эти пределы и инвертируем их тильдой ('~'),\r\n# тем самым выделяя нормальные значения в отдельный временный список.\r\nclean_iqr_boston_df = boston_df[~((boston_df &lt; (Q1 - 1.5 * IQR)) | (boston_df &gt; (Q3 + 1.5 * IQR))).any(axis = 1)]\r\nclean_iqr_boston_df.shape</code></pre>\r\n<pre><code class="language-python">(274, 13)</code></pre>\r\n<p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1EZXSr6xov3qIDDOwdknSgnKWIiR4z06V?usp=sharing">здесь</a>.</p>\r\n<p>Попробуйте наши курсы по Машинному обучению на <a href="https://www.udemy.com/user/helen-kapatsa/">Udemy</a>.</p>\r\n<p>Фото: <a href="https://unsplash.com/@woblack">@woblack</a></p>		vybros	2021-01-15	Статистика	https://images.unsplash.com/photo-1536303100418-985cb308bb38?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3570&q=80
49	Нормализация (Normalization)		<p>Нормализация (Max-Min Normalization, Min-Max Scaling) – техника преобразования значений <a href="__GHOST_URL__/priznak/">признака (Feature)</a>, масштабирующая значения таким образом, что они располагаются в диапазоне от 0 до 1. Вычисляется каждый нормализованный элемент признака с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$x_{i,norm} = \\frac{x_i - x_{min}}{x_{max} - x_{min}}, где$$<br>\n$$x_{i,norm}\\space{–}\\space{нормализованный}\\space{элемент}\\space{признака,}$$<br>\n$$x_{min}\\space{–}\\space{наименьший}\\space{элемент}\\space{признака,}$$<br>\n$$x_i\\space{–}\\space{i-й}\\space{непреобразованный}\\space{элемент,}$$<br>\n$$x_{max}\\space{–}\\space{наибольший}\\space{элемент}$$</p>\n<!--kg-card-end: markdown--><p>Цель такого преобразования – изменить значения числовых столбцов в наборе данных так, чтобы сохранить различия их диапазонов. В <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (Machine Learning)</a> <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> требует нормализации, когда признаки имеют разные диапазоны и тем самым способствуют искажению восприятия взаимоотношений между Переменными-предикторами (Predictor Variable) и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>.</p><h3 id="-sklearn">Нормализация и SkLearn</h3><p>Нормализовать можно с помощью функции SkLearn. Импортируем MinMaxScaler библиотеки Scikit-learn:</p><pre><code class="language-python">import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler</code></pre><p>Загрузим наш игрушечный датасет и дополним команду: разделителем  является точка с запятой (sep = ';'):</p><pre><code class="language-python"># Импортируем датасет\ndf = pd.read_csv('https://www.dropbox.com/s/62xm9ymoaunnfg6/bank-full.csv?dl=1', sep = ';')\ndf.head()</code></pre><p>Мы будем нормализовать <a href="__GHOST_URL__/chislovaia-pieriemiennaia/">Числовые переменные (Numeric Variable)</a> датасета о клиентах банка – потребителях кредитных продуктов:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/01/normalization.png" class="kg-image" alt loading="lazy" width="2000" height="247" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normalization.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/normalization.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/normalization.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/01/normalization.png 2400w"></figure><p>Определим, какие признаки являются числовыми, чтобы впоследствии их нормализовать:</p><pre><code class="language-python">df.dtypes</code></pre><pre><code class="language-python">Возраст                                 int64\nРабота                                 object\nСемейный статус                        object\nОбразование                            object\nКредитный дефолт                       object\nИпотека                                object\nЗайм                                   object\nКонтакт                                object\nМесяц                                  object\nДень недели                            object\nДлительность                            int64\nКампания                                int64\nДень                                    int64\nПредыдущий контакт                      int64\nДоходность                             object\nКолебание уровня безработицы          float64\nИндекс потребительских цен            float64\nИндекс потребительской уверенности    float64\nЕвропейская межбанковская ставка      float64\nКоличество сотрудников в компании     float64\ny                                      object\ndtype: object</code></pre><p>Для простоты восприятия сузим датасет и создадим его копию, состоящую только из признаков, выраженных числами:</p><pre><code class="language-python"># Выберем встретившиеся числовые типы\nnumerics = ['int64', 'float64']\n\n# Создадим копию датасета\nnewdf = df.select_dtypes(include = numerics)</code></pre><p>Теперь приступим непосредственно к нормализации:</p><pre><code class="language-python"># Инициализация нормализатора\nscaler = MinMaxScaler()\n\n# Передача датасета и преобразование\nscaler.fit(newdf)\nscaled_features = scaler.transform(newdf)\n\n# Конвертация в табличный формат.\ndf_MinMax = pd.DataFrame(data = scaled_features, \ncolumns = ["Возраст", \n          "Длительность",\n          "Кампания",\n          "День",\n          "Предыдущий контакт", \n          "Колебание уровня безработицы", \n          "Индекс потребительских цен",\n          "Индекс потребительской уверенности",\n          "Европейская межбанковская ставка",\n          "Количество сотрудников в компании"])\n\ndf_MinMax.head()</code></pre><p>Результирующий нормализованный датасет приобрел следующий вид и готов к дальнейшей обработке и загрузке в <a href="__GHOST_URL__/modiel/">Модель (Model)</a>:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/01/normalization-normalized.png" class="kg-image" alt loading="lazy" width="1284" height="277" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normalization-normalized.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/normalization-normalized.png 1000w, __GHOST_URL__/content/images/2021/01/normalization-normalized.png 1284w"></figure><p>Сравните, как повлияла нормализация на восприятие отношений между признаками "Возраст" и "Длительность [телефонного разговора с менеджером по продажам]". Это окажет существенное влияние на работу <a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метода k-ближайших соседей (kNN)</a> и других его собратьев, учитывающих расстояние между точками. Мы отобразим на графике первые пять <a href="__GHOST_URL__/normalizatsiia/%D0%92%D1%8B%D0%B1%D0%BE%D1%80%D0%BA%D0%B0,">Наблюдений (Observation)</a> датасета в исходном виде и преобразованном:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/normalization-1.png" class="kg-image" alt loading="lazy" width="1952" height="858" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normalization-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/normalization-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/normalization-1.png 1600w, __GHOST_URL__/content/images/2021/01/normalization-1.png 1952w" sizes="(min-width: 720px) 720px"></figure><h3 id="-">Нормализация и стандартизация</h3><p><a href="__GHOST_URL__/standartizatsiia/">Стандартизация (Standartization)</a> является разновидностью нормализации с использованием <a href="__GHOST_URL__/standartizovannaia-otsienka/">Стандартизованной оценки (Z-Score)</a> и как бы центрирует наблюдения относительно нуля. Расстояние от нуля, то есть <a href="__GHOST_URL__/dispiersiia-sluchainoi-vielichiny/">Дисперсия случайной величины (Variance)</a> не превышает единицы:</p><pre><code class="language-python"># Исходные данные\n[[0, 0], [0, 0], [1, 1], [1, 1]]\n\n# Стандартизованные данные\n[[-1, -1], [-1, -1], [1, 1], [1, 1]]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1eWcZkfzjmYGscU5dravSwJcnXeqn9sLg?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@bharathrajn89">@bharathrajn89</a></p>		normalizatsiia	2021-01-21		
50	Пайплайн (Pipeline)		<p>Пайплайн – 1. Последовательные стадии преобразования данных, предшествующие их загрузке в <a href="__GHOST_URL__/modiel/">Модель (Model)</a>. 2. Класс библиотеки Scikit-learn, последовательно применяющий к исходным данным настраиваемые преобразования. 3. Автоматизируемая последовательность обучения и оптимизации модели в PyTorch и других библиотеках.</p><h2 id="-">Пайплайн как последовательность</h2><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/01/pipeline.png" class="kg-image" alt loading="lazy" width="320" height="318"></figure><h3 id="--1">Извлечение</h3><p>Этот этап включает сбор данных из Интернета или баз данных и конвертация в определенные форматы.</p><p>Например, приложение Apple. Здоровье позволяет экспортировать свои медданные в формате .xml, и чтобы взаимодействовать с данными в дальнейшем, мы можем использовать библиотеку <a href="https://github.com/tdda/applehealthdata">applehealthdata</a> и конвертировать их в .csv:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/--------------2021-01-26---11.06.06.png" class="kg-image" alt loading="lazy" width="2000" height="997" srcset="__GHOST_URL__/content/images/size/w600/2021/01/--------------2021-01-26---11.06.06.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/--------------2021-01-26---11.06.06.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/--------------2021-01-26---11.06.06.png 1600w, __GHOST_URL__/content/images/2021/01/--------------2021-01-26---11.06.06.png 2250w" sizes="(min-width: 720px) 720px"><figcaption>Результат конверсии файлов "экспорт.xml", "export_cda.xml" Apple. Здоровье в .csv</figcaption></figure><h3 id="--2">Скраббинг </h3><p>Это очистка данных и самый трудоемкий этап и требует наибольших усилий. Он делится на два этапа: </p><p><u>Изучение</u></p><ul><li>Выявление ошибок</li><li>Поиск <a href="__GHOST_URL__/propusk/">Пропусков (Omission)</a></li><li>Поиск повреждений данных</li></ul><p><u>Очистка</u></p><ul><li>Замена или заполнение отсутствующих значений</li><li>Исправление повреждений</li></ul><h3 id="-eda-">Разведочный анализ данных (EDA)</h3><p>Когда данные достигают этого этапа, они не содержат ошибок и пропущенных значений и, следовательно, подходят для поиска закономерностей с использованием визуализации.</p><p>Великолепная библиотека для всестороннего разведочного анализа – <a href="__GHOST_URL__/pandas-profiling/">pandas-profiling</a>. На изображении ниже отдельно взятый признак <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> о клиентах банка – длительность телефонного разговора, выраженный Вещественным числом (Real Number), исследуется с разных точек зрения: количества уникальных значений (Distinct), пропусков (Missing), Слишком удаленных значений (Infinite), <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднего арифметического (Mean)</a>, Минимума (Minimum), Максимума (Maximum), нулевых значений (Zeros), используемой признаком памяти (Memory size).</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/01/eda-pandas-profiling-1.png" class="kg-image" alt loading="lazy" width="2000" height="481" srcset="__GHOST_URL__/content/images/size/w600/2021/01/eda-pandas-profiling-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/eda-pandas-profiling-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/eda-pandas-profiling-1.png 1600w, __GHOST_URL__/content/images/2021/01/eda-pandas-profiling-1.png 2104w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="--3">Моделирование</h3><p>Это этап обработки, где на сцену выходит <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a>. Модели – это не что иное, как обнаруженные статистические закономерности, которые используются для принятия лучших бизнес-решений.</p><p>На изображении ниже – <a href="__GHOST_URL__/modiel-kholta-vintiersa/">Модель Хольта-Винтера (Holt-Winters Model)</a>, предсказывающая энергопотребление с помощью Датасета (Dataset) компании Duquesne Light Company. Прежде чем использовать модель на реальных задачах, мы производим оценку ее предсказательной способности. Мы производим предсказание для прошлого (в данном случае, 2017 - 2018 гг.) и сравниваем с реальным энергопотреблением. Предсказание, обозначенное розовым, довольно сильно отличается от объективной реальности (Ground Truth), так что модель предстоит перенастраивать. Нередко для самых актуальных проблем современности удается добиться лишь посредственной предсказательной способности (70-80%), и каждый следующий процент Дата-сайентистам (Data Scientist) приходится отвоевывать. </p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/01/newplot--4-.png" class="kg-image" alt loading="lazy" width="1173" height="525" srcset="__GHOST_URL__/content/images/size/w600/2021/01/newplot--4-.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/newplot--4-.png 1000w, __GHOST_URL__/content/images/2021/01/newplot--4-.png 1173w"><figcaption>Визуализация предсказания энергопотребления</figcaption></figure><h3 id="--4">Интерпретация</h3><p>Это перефразирование <a href="__GHOST_URL__/insait/">Инсайтов (Insight)</a> Вашей модели, ключевой этап, на котором с использованием психологических методов, специфических знаний о бизнесе и навыков презентации Вы можете преподнести свою модель заказчику и широкой аудитории вообще.</p><h3 id="--5">Пересмотр</h3><p>По мере изменения бизнес-ситуации появляются новые переменные, которые могут снизить качество существующих моделей. Потому периодические обновления – неотъемлемая часть работы.</p><p>Наука о данных – это не просто уникальные алгоритмы Машинного обучения, но и решения, которые вы предоставляете на их основе. Очень важно убедиться, что пайплайн выдержит возможные незначительные изменения формы данных, и вы точно определяете бизнес-проблематику, чтобы впоследствии предлагать точные решения.</p><h2 id="-scikit-learn">Пайплайн Scikit-learn</h2><p>В узком смысле пайплайн – это модуль <strong><code>sklearn.pipeline</code></strong>, который позволяет автоматизировать предварительные преобразования данных перед обучением модели. Посмотрите, как использует пайплайны <a href="https://www.kaggle.com/baghern">@baghern</a> с целью идентифицировать авторов по их стилю (<a href="https://www.dropbox.com/s/pvqawholxi3l2uq/pipeline-author-identification.csv?dl=0">оригинал на английском</a>). Импортируем все необходимые библиотеки:</p><pre><code class="language-python">import nltk\nnltk.download('stopwords') # Для удаления брани\n\nimport numpy as np \nimport pandas as pd\n\nimport sklearn \nfrom sklearn.pipeline import FeatureUnion # Для удаления брани\nfrom sklearn.model_selection import train_test_split\n\n# Для дополнительных преобразований числовых и текстовых признаков\nfrom sklearn.base import BaseEstimator, TransformerMixin \n\n# Ансамблевый классификатор "Случайный лес"\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Непосредственно пайплайн для подготовки датафрейма\nfrom sklearn.pipeline import Pipeline\n\n# TfidVectorizer конвертирует датасет в матрицу Меры оценки важности слова в контексте документа (TF-IDF)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Стандартизация дополнительного признака\nfrom sklearn.preprocessing import StandardScaler</code></pre><p>Импортируем данные, которые для Вашего удобства залиты на Dropbox. Чтобы повторить это позднее, установите параметр <code>dl=1</code> после вопросительного знака в конце ссылки, это разрешит скачивание файла:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/pvqawholxi3l2uq/pipeline-author-identification.csv?dl=1')\n\n# Опустим пустые ячейки\ndf.dropna(axis = 0)\n\n# Зададим индекс\ndf.set_index('id', inplace = True)\ndf.head()</code></pre><p>Для того, чтобы сконцентрировать внимание на пайплайнах, мы будем использовать датасет с выполненным Конструированием признаков (Feature Engineering). Последнее добавило <a href="__GHOST_URL__/datafrieim/">Датафрейму (Dataframe)</a> следующие признаки:</p><ul><li>PROCESSED (тот же текст, но без ругани)</li><li>LENGTH (количество символов)</li><li>WORDS (количество слов)</li><li>WORDS_NOT_STOPWORDS (количество небранных слов)</li><li>AVG_WORD_LENGTH (Средняя длительность слова)</li><li>COMMAS (Количество запятых)</li></ul><p>Как просто, на первый взгляд, определить авторский стиль! Всего-то определить, насколько автор склонен ругаться, да подсчитать запятые... Но все не так просто. Мы будем работать с таким датафреймом:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/01/pipeline-1.png" class="kg-image" alt loading="lazy" width="986" height="241" srcset="__GHOST_URL__/content/images/size/w600/2021/01/pipeline-1.png 600w, __GHOST_URL__/content/images/2021/01/pipeline-1.png 986w"></figure><pre><code class="language-python"># Разделим признаки на числовые (numeric_features) и остальные (features)\nfeatures = [c for c in df.columns.values if c  not in ['id', 'text', 'author']]\nnumeric_features = [c for c in df.columns.values if c  not in ['id', 'text', 'author', 'processed']]\n\n# Выберем целевую переменную – мы хотим идентифицировать автора поста по его стилю\ntarget = 'author'\n\n# Разделим датасет на тренировочную и тестовую части случайным образом в пропорции 67:33\nX_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size = 0.33, random_state = 42)\nX_train.head()</code></pre><p>Тренировочные данные подобраны случайным образом, и это становится заметно с помощью индекса:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/01/pipeline-x_train.png" class="kg-image" alt loading="lazy" width="903" height="245" srcset="__GHOST_URL__/content/images/size/w600/2021/01/pipeline-x_train.png 600w, __GHOST_URL__/content/images/2021/01/pipeline-x_train.png 903w"></figure><p>Зададим вспомогательные функции, выбирающие из датафрейма текстовые и числовые признаки для автоматизации преобразований в пайплайнах:</p><pre><code class="language-python">class TextSelector(BaseEstimator, TransformerMixin):\n    # Функция, выбирающая каждый текстовый признак датасета \n    # для дополнительных преобразований\n    \n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y = None):\n        return self\n\n    def transform(self, X):\n        return X[self.key]\n    \nclass NumberSelector(BaseEstimator, TransformerMixin):\n    # Функция, выбирающая каждый числовой признак датасета \n    # для дополнительных преобразований\n       \n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y = None):\n        return self\n\n    def transform(self, X):\n        return X[[self.key]]</code></pre><p>Создадим пайплайн, последовательно выбирающий каждый столбец, затем выполним преобразование в матрицу TF-IDF только для этого столбца и вернем результат. Для каждого использованного слова мы определим Меру оценки его важности в контексте документа (TF-IDF):</p><pre><code class="language-python">text = Pipeline([\n                ('selector', TextSelector(key = 'processed')),\n                ('tfidf', TfidfVectorizer(stop_words = 'english'))\n            ])\n\n# Передадим модели часть данных\ntext.fit_transform(X_train)</code></pre><p>Компилятор уведомляет нас, что матрица существенно выросла и состоит из 13 с лишним тысяч столбцов вещественных чисел, причем некоторые ячейки пустуют (sparse). В сжатом виде ячеек, несущих значения, чуть больше 148 тысяч.</p><pre><code class="language-python">&lt;13117x21516 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 148061 stored elements in Compressed Sparse Row format&gt;</code></pre><p>Стандартизуем признак "длина" [поста] с использованием селектора числовых признаков:</p><pre><code class="language-python">length =  Pipeline([\n                ('selector', NumberSelector(key = 'length')),\n                ('standard', StandardScaler())\n            ])\n\n# Передадим модели еще часть\nlength.fit_transform(X_train)</code></pre><p>Пайплайн показывает нам, что ряд, прошедший Стандартизацию (Standartization), выглядит так:</p><pre><code class="language-python">array([[-0.50769254],\n       [ 0.88000324],\n       [ 2.24907223],\n       ...,\n       [-0.46112557],\n       [-0.14447015],\n       [-0.39593181]])</code></pre><p>Объединим признаки, характеризующие стиль автора, с помощью FeatureUnion:</p><pre><code class="language-python">feats = FeatureUnion([('text', text), # Текст поста              \n                      ('length', length), # Длина поста\n                      ('words', words), # Список уникальных использованных слов\n                      ('words_not_stopword', words_not_stopword), # Очищенный список без ругательств\n                      ('avg_word_length', avg_word_length), # Средняя длина слова\n                      ('commas', commas)]) # Количество запятых\n\n\n# Объединим результаты нескольких преобразованных переменных в единый набор данных. \n# Мы сделаем конвейер для каждой переменной, затем объединим их.\nwords =  Pipeline([\n                ('selector', NumberSelector(key = 'words')),\n                ('standard', StandardScaler())\n            ])\nwords_not_stopword =  Pipeline([\n                ('selector', NumberSelector(key = 'words_not_stopword')),\n                ('standard', StandardScaler())\n            ])\navg_word_length =  Pipeline([\n                ('selector', NumberSelector(key = 'avg_word_length')),\n                ('standard', StandardScaler())\n            ])\ncommas =  Pipeline([\n                ('selector', NumberSelector(key = 'commas')),\n                ('standard', StandardScaler()),\n            ])\n</code></pre><p>Теперь прогоним объединение признаков, характеризующих уникальность автора, через пайплайн:</p><pre><code class="language-python">feature_processing = Pipeline([('feats', feats)])\n\n# Передадим модели еще часть данных\nfeature_processing.fit_transform(X_train)</code></pre><p>Несмотря на то, что форма матрицы осталась прежней, ее наполненность увеличилась до 213+ тысяч:</p><pre><code class="language-python">&lt;13117x21521 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 213646 stored elements in Compressed Sparse Row format&gt;</code></pre><p>Используем сгенерированные признаки, характеризующие авторский стиль, для предсказания# с помощью классификатора "случайный лес":</p><pre><code class="language-python">pipeline = Pipeline([\n    ('features', feats),\n    ('classifier', RandomForestClassifier(random_state = 42)),\n])\n\n# Теперь модель получила все учебные данные\npipeline.fit(X_train, y_train)\n\n# Определим точность\npreds = pipeline.predict(X_test)\nnp.mean(preds == y_test)</code></pre><p><a href="__GHOST_URL__/tochnost-izmierienii/">Аккуратность (Accuracy)</a> достигла 67%, и для начала это хороший результат. Но для реальных проектов этого недостаточно. Интуитивное ощущение, что такого скромного набора метаданных для идентификации автора среди миллионов других, оказалось верным.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1doPtCmGzT8ms1YXUqudTHw7Q0wZahg8d?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@xoforoct">@xoforoct</a></p>		paiplain	2021-01-26		
51	Стандартизация (Standartization)	Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillu	<p>Стандартизация (Standartization, Z-score Normalization) &ndash; техника преобразования значений <a href="__GHOST_URL__/priznak/">признака (Feature)</a>, адаптирующая признаки с разными диапазонами значений к <a href="__GHOST_URL__/modiel/">Моделям (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, использующих дистанцию для прогнозирования. Эта разновидность нормализации с использованием <a href="__GHOST_URL__/standartizovannaia-otsienka/">Стандартизированной оценки (Z-Score)</a> преобразует значения таким образом, что из каждого <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> каждого Признака вычитается <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее значение (Mean)</a> и результат делится на <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a> этого признака. Как правило, каждый стандартизованный элемент признака вычисляется следующим образом:</p>\r\n<!--kg-card-begin: markdown-->\r\n<p>$$x_{i,станд.} = \\frac{x_i - &mu;}{&sigma;}, где$$<br />$$x_{i,станд.}\\space{&ndash;}\\space{стандартизованный}\\space{элемент}\\space{признака,}$$<br />$$x_i\\space{&ndash;}\\space{исходный}\\space{элемент,}$$<br />$$&mu;\\space{&ndash;}\\space{среднее}\\space{арифметическое,}$$<br />$$&sigma;\\space{&ndash;}\\space{стандартное}\\space{отклонение}$$</p>\r\n<!--kg-card-end: markdown-->\r\n<p>Такое преобразование необходимо, поскольку признаки датасета могут иметь большие различия между своими диапазонами, и для моделей Машинного обучения, основанных на вычислении дистанции между точками на графике как основу прогнозирования: <a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метод k-ближайших соседей (kNN)</a>, <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a>, <a href="__GHOST_URL__/dierievo-rieshienii/">Дерево решений (Decision Tree)</a> и проч., это спровоцирует искаженное восприятие данных.</p>\r\n<p>Z-оценка &ndash; один из самых популярных методов стандартизации, однако не единственный. После стандартизации все столбцы будут иметь среднее значение, равное нулю, стандартное отклонение, равное единице, и, следовательно, одинаковый масштаб влияния на модель.</p>\r\n<h3 id="-">Нормализовать или стандартизировать?</h3>\r\n<p>"<a href="__GHOST_URL__/normalizatsiia/">Нормализация (Normalization)</a> или стандартизация?" &ndash; извечный вопрос среди новичков. Первую удобно использовать, когда мы знаем, что распределение данных не соответствует Гауссову, то есть нормальному:</p>\r\n<figure class="kg-card kg-image-card kg-card-hascaption"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/normal-distribution_s-p-500--return_s-p-500--return-1.png" srcset="__GHOST_URL__/content/images/size/w600/2021/01/normal-distribution_s-p-500--return_s-p-500--return-1.png 600w, __GHOST_URL__/content/images/2021/01/normal-distribution_s-p-500--return_s-p-500--return-1.png 700w" alt="" width="700" height="328" loading="lazy" />\r\n<figcaption>Нормальное распределение возврата инвестиций S&amp;P 500</figcaption>\r\n</figure>\r\n<p>Это может быть полезно в алгоритмах, которые не предполагают никакого распределения. Стандартизация же полезна в случаях, когда данные соответствуют гауссовскому распределению. В отличие от нормализации, стандартизация не имеет ограничивающего диапазона:</p>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/standartization.png" srcset="__GHOST_URL__/content/images/size/w600/2021/01/standartization.png 600w, __GHOST_URL__/content/images/2021/01/standartization.png 701w" alt="" width="701" height="401" loading="lazy" /></figure>\r\n<p>Даже если у вас есть <a href="__GHOST_URL__/vybros/">Выбросы (Outlier)</a> в ваших данных, стандартизация не повлияет на них. Однако выбор между типами преобразований будет зависеть от бизнес-задачи и выбранного алгоритма Машинного обучения. Не существует жесткого правила, которое подскажет, когда нормализовать или стандартизировать данные. Вы всегда можете начать с подгонки моделей к необработанным, нормализованным и стандартизованным данным и сравнить их производительность для достижения наилучших результатов.</p>\r\n<p>Хорошая практика &ndash; это стандартизировать <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>, причем только <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a>. <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевая переменная (<u>Target Variable)</u></a> остается в неизменном виде, и это позволяет избежать утечки данных в процессе тестирования модели.</p>\r\n<h3 id="-scikit-learn">Стандартизация и Scikit-learn</h3>\r\n<p>Стандартизацию можно выполнить с помощью функции <code>Sklearn.preprocessing.StandardScaler()</code>. Мы будем использовать упрощенный пример <a href="https://www.kaggle.com/discdiver">@discdiver</a> (<a href="https://www.kaggle.com/discdiver/guide-to-scaling-and-standardizing">статья на английском</a>). Для начала импортируем библиотеки:</p>\r\n<pre><code class="language-python">import matplotlib # Для графиков\r\nimport matplotlib.pyplot as plt\r\n\r\nimport numpy as np \r\nimport pandas as pd # Для датафреймов\r\nimport seaborn as sns # Для графиков\r\n\r\nimport sklearn # Для стандартизации\r\nfrom sklearn import preprocessing</code></pre>\r\n<p>Сгенерируем датасет из случайных чисел</p>\r\n<pre><code class="language-python">df = pd.DataFrame({ \r\n    # Экспоненциальное распределение, 10 &ndash; "резкость" экспоненты, 1000 &ndash; размер\r\n    'exponential': np.random.exponential(10, 1000), \r\n\r\n    # Нормальное распределение, 10 &ndash; среднее значение р., 2 &ndash; стандартное отклонение, 1000 &ndash; количество сэмплов\r\n    'normal_p': np.random.normal(10, 2, 1000) \r\n})</code></pre>\r\n<p>Зададим параметры холста Seaborn, название и визуализируем кривые распределения:</p>\r\n<pre><code class="language-python">fig, (ax1) = plt.subplots(ncols = 1, figsize = (10, 8))\r\nax1.set_title('Оригинальные распределения')\r\n\r\n# kdeplot() (KDE &ndash; оценка плотности ядра) &ndash; специальный метод для графиков распределений\r\nsns.kdeplot(df['exponential'], ax = ax1)\r\nsns.kdeplot(df['normal_p'], ax = ax1)</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/standartization-initial-distribution-------1.png" srcset="__GHOST_URL__/content/images/size/w600/2021/01/standartization-initial-distribution-------1.png 600w, __GHOST_URL__/content/images/2021/01/standartization-initial-distribution-------1.png 672w" alt="" width="672" height="546" loading="lazy" /></figure>\r\n<p>Случается, данные подаются в стандартизированном виде без уведомления, и чтобы ненароком не выполнить лишнюю, не меняющую картины стандартизацию, убедитесь, что среднее арифметическое признаков не равно нулю. Инициализируем стандартизатор, подготовим датасет и выполним стандартизацию:</p>\r\n<pre><code class="language-python"># Инициализируем стандартизатор\r\ns_scaler = preprocessing.StandardScaler()\r\n\r\n# Копируем исходный датасет\r\ndf_s = s_scaler.fit_transform(df)\r\n\r\n# Копируем названия столбцов, которые теряются при использовании fit_transform()\r\ncol_names = list(df.columns)\r\n\r\n# Преобразуем промежуточный датасет в полноценный датафрейм для визуализации\r\ndf_s = pd.DataFrame(df_s, columns = col_names)\r\n\r\n# Расположим распределения на одном полотне\r\nfig, (ax1) = plt.subplots(ncols = 1, figsize = (10, 8))\r\n\r\n# Зададим название графика\r\nax1.set_title('После стандартизации')\r\n\r\nsns.kdeplot(df_s['exponential'], ax = ax1)\r\nsns.kdeplot(df_s['normal_p'], ax = ax1)</code></pre>\r\n<p>Мы получили преобразованные кривые, и теперь их масштабы влияния на выбранную модель будут соответствовать друг другу.</p>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/01/standartization-standartized-distribution-1.jpg" srcset="__GHOST_URL__/content/images/size/w600/2021/01/standartization-standartized-distribution-1.jpg 600w, __GHOST_URL__/content/images/2021/01/standartization-standartized-distribution-1.jpg 659w" alt="" width="659" height="546" loading="lazy" /></figure>\r\n<p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1EF_7PFB3QbJmavUsdaIWjD8ElONN_DYX?usp=sharing">здесь</a>.</p>\r\n<p>Фото: <a href="https://unsplash.com/@jasonortego">@jasonortego</a></p>		standartizatsiia	2021-01-29	Статистика	\N
52	Датасет (Dataset)		<p>Датасет  – 1. Набор исследуемых данных, располагаемый на нескольких компьютерах одновременно ввиду большого объема. 2. Выборка из такого массивного объема данных, созданная с целью продемонстрировать тот или иной принцип или концепцию <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>:</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/eda-sample.png" class="kg-image" alt loading="lazy" width="2000" height="276" srcset="__GHOST_URL__/content/images/size/w600/2021/02/eda-sample.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/eda-sample.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/eda-sample.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/02/eda-sample.png 2400w"><figcaption>Датасет банка о потребителях кредитных продуктов</figcaption></figure><p>Датасеты – основа <a href="__GHOST_URL__/nauka-o-dannykh/">Науки о данных (Data Science)</a>, материал, на котором основаны все исследования. В контексте науки принято рассматривать два их типа: традиционные и <a href="__GHOST_URL__/bolshiie-dannyie/">Большие данные (Big Data)</a>.</p><h2 id="-">Традиционные и Большие данные</h2><p>Традиционные данные структурированы и хранятся в базах, управляемых с одного компьютера; это табличное представление, содержащее числовые или текстовые значения. На самом деле, эпитет «традиционный» мы вводим для ясности: это помогает подчеркнуть различия.</p><p>Большие данные, в свою очередь, массивнее, чем традиционные, как в контексте разнообразия (числа, текст, изображения, аудио, видео и проч.), так и скорости извлечения и вычисления в реальном времени, и объема (тера-, пета-, эксабайты и проч.). Большие данные обычно распределяются по компьютерной сети. Так что учебные, "игрушечные" датасеты, с помощью которых мы осваиваем модели и окололежащие особенности Машинного обучения, это метонимия (перенос наименования с одного предмета или явления на другой на основе смежности).</p><h2 id="--1">Виды датасетов</h2><p>Наука разделяет датасеты на три категории: </p><h3 id="--2">Простая запись</h3><p>Это самая простая форма не имеет явной связи между строками-<a href="__GHOST_URL__/nabliudieniie/">Наблюдениями (Observation)</a> или столбцами-<a href="__GHOST_URL__/priznak/">Признаками (Feature)</a>, и каждая строка имеет одинаковый набор характеристик. Данные записи обычно хранятся либо в файлах (форматы .csv, .parquet), либо в реляционных базах данных:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/dataset-record.png" class="kg-image" alt loading="lazy" width="621" height="280" srcset="__GHOST_URL__/content/images/size/w600/2021/02/dataset-record.png 600w, __GHOST_URL__/content/images/2021/02/dataset-record.png 621w"></figure><p>Существует несколько подвидов простых записей:</p><ul><li><strong>Транзакционные данные:</strong> например, покупки в супермаркете. Чаще всего это двоичные признаки, указывающие, был ли предмет куплен или нет:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/dataset-market-basket-data.png" class="kg-image" alt loading="lazy" width="445" height="246"></figure><ul><li><strong>Матрица данных:</strong> если все объекты в коллекции имеют один и тот же фиксированный набор числовых признаков, то последние можно рассматривать как <a href="__GHOST_URL__/viektor/">Векторы (Vector)</a> в многомерном пространстве. Набор таких записей можно интерпретировать как <a href="__GHOST_URL__/matritsa/">Матрицу (Matrix)</a> m × n, где имеется m строк, по одной для каждого объекта, и n столбцов, по одной для каждого признака. Следовательно, мы можем применять стандартные матричные операции для преобразования данных и управления ими. Матрица является стандартным форматом для большинства статистических данных:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/dataset-matrix.png" class="kg-image" alt loading="lazy" width="360" height="246"></figure><ul><li><strong>Матрица разреженных данных </strong>(иногда также матрицей данных документа): особая разновидность матрицы данных, в которой признаки одного типа и асимметричны; т.е. важны только ненулевые значения:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/dataset-sparse-matrix-1.png" class="kg-image" alt loading="lazy" width="701" height="263" srcset="__GHOST_URL__/content/images/size/w600/2021/02/dataset-sparse-matrix-1.png 600w, __GHOST_URL__/content/images/2021/02/dataset-sparse-matrix-1.png 701w"></figure><h3 id="--3">Графы</h3><ul><li><strong>Данные со связями между объектами</strong>: отношения между объектами фиксируются связями:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/dataset-lineked-web-pages-1.png" class="kg-image" alt loading="lazy" width="2000" height="1696" srcset="__GHOST_URL__/content/images/size/w600/2021/02/dataset-lineked-web-pages-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/dataset-lineked-web-pages-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/dataset-lineked-web-pages-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/02/dataset-lineked-web-pages-1.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Часть графа статьи "Большие данные" на Википедии</figcaption></figure><ul><li><strong>Структурированные графы</strong>: узловые компоненты взаимосвязаны друг с другом определенным образом:</li></ul><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/dataset-graph-3.png" class="kg-image" alt loading="lazy" width="1001" height="401" srcset="__GHOST_URL__/content/images/size/w600/2021/02/dataset-graph-3.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/dataset-graph-3.png 1000w, __GHOST_URL__/content/images/2021/02/dataset-graph-3.png 1001w"><figcaption>Граф социальной сети</figcaption></figure><h3 id="--4">Упорядоченные записи</h3><p>Некоторые данные упорядочены во времени или пространстве. Их можно разделить на следующие типы:</p><ul><li><strong>Последовательные данные</strong> состоят из набора отдельных объектов, таких как слова или буквы. Здесь нет временных меток; вместо этого есть позиции в упорядоченной последовательности:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/dataset-sequence.png" class="kg-image" alt loading="lazy" width="638" height="638" srcset="__GHOST_URL__/content/images/size/w600/2021/02/dataset-sequence.png 600w, __GHOST_URL__/content/images/2021/02/dataset-sequence.png 638w"><figcaption>Геном</figcaption></figure><ul><li><strong><a href="__GHOST_URL__/vriemiennoi-riad/">Временной ряд (Time Series)</a> –</strong> это особый тип последовательных данных, в которых каждая запись представляет собой временной ряд, то есть серию измерений, выполненных во времени:</li></ul><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/dataset-time-series.png" class="kg-image" alt loading="lazy" width="2000" height="265" srcset="__GHOST_URL__/content/images/size/w600/2021/02/dataset-time-series.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/dataset-time-series.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/dataset-time-series.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/02/dataset-time-series.png 2400w"><figcaption>Временной ряд энергопотребления с сезоными скачками</figcaption></figure><ul><li><strong>Пространственные данные</strong> имеют координаты:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/dataset-map.png" class="kg-image" alt loading="lazy" width="1520" height="1305" srcset="__GHOST_URL__/content/images/size/w600/2021/02/dataset-map.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/dataset-map.png 1000w, __GHOST_URL__/content/images/2021/02/dataset-map.png 1520w" sizes="(min-width: 720px) 720px"><figcaption>Трафик аэропортов США</figcaption></figure><h2 id="--5">Атрибуты датасета</h2><p>Выделяют три основные характеристики датасета:</p><ul><li>Размерность (Dimensionality) – это количество признаков в наборе данных. Если таковых много (т.н. "высокая размерность"), тогда проанализировать такой набор данных будет сложнее. Эту проблему называют <a href="__GHOST_URL__/prokliatiie-razmiernostiei/">Проклятием размерности (Curse of Dimensionality<u>)</u></a>. </li><li>Разреженность (Sparsity) – черта, характеризующая заполненность датасета, т.е. доля ячеек, заполненных ненулевыми значениями. Для некоторых наборов данных с асимметричными функциями, большинство признаков имеют значения 0; во многих случаях менее 1% записей не равны нулю:</li></ul><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/one-hot-encoding-dummies.png" class="kg-image" alt loading="lazy" width="2000" height="284" srcset="__GHOST_URL__/content/images/size/w600/2021/02/one-hot-encoding-dummies.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/one-hot-encoding-dummies.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/one-hot-encoding-dummies.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/02/one-hot-encoding-dummies.png 2400w"><figcaption>Разреженные данные как результат Быстрого кодирования (OHE)</figcaption></figure><ul><li>Разрешение (Resolution) – это возможность обнаружить то или иное явление в случае, если данные подробны ровно настолько, сколько этого требует задача. Например, изменение атмосферного давления по часам отражает перемещение циклона, причем в масштабе месяцев такие явления незаметны. В статистике это называют <a href="__GHOST_URL__/paradoks-simpsona/">Парадоксом Симпсона (Simpson Paradox)</a>.</li></ul><h3 id="--6">Специальные методы датасетов</h3><p>Для образовательных целей, как правило, достаточно игрушечных, небольших датасетов, и некоторые библиотеки подготавливают свои наборы данных для ускорения.</p><p>Встроенный метод библиотеки Pandas <code>read_csv()</code> позволяет преобразовать файл в <a href="__GHOST_URL__/datafrieim/">Датафрейм (Dataframe)</a>, и это один из самых распространенных способов подгрузки данных в код:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/j04e6thkqmk02z1/LPL.csv?dl=1')</code></pre><p>Метод позволяет также указать тип разделителя (<code>sep = ':'</code>), кодировку (<code>encoding = 'utf-8'</code>) и многие другие параметры загрузки.</p><p>У некоторых обширных библиотек вроде Scikit-learn также встречаются собственные методы, позволяющие быстро импортировать встроенные датасеты, прекрасно подходящие для демонстрации работы классов, функций, интерфейсов и других своих объектов.</p><pre><code class="language-python">from sklearn.datasets import load_digits\ndigits = load_digits()</code></pre><p>Помимо таких встроенных датасетов, данные для обучения нейросетей предоставляет еще и сайт kaggle.com.</p><p>С перечнем других встроенных наборов данных в Scikit-learn можно ознакомиться по <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets">ссылке</a>.</p><p>Фото: <a href="https://unsplash.com/@conscious_design">@conscious_design</a></p>		dataset	2021-02-03		
53	Метод K-ближайших соседей\t(kNN)		<p>Метод K-ближайших соседей – это алгоритм <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, который используют для решения задач классификации и регрессии.</p><p>Алгоритм <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Контролируемого обучения (Supervised Learning)</a>, в отличие от Неконтролируемого (Unsupervised Learning), полагается на размеченные входные данные для получения соответствующего результата с новыми данными без <a href="__GHOST_URL__/iarlyk/">Ярлыков (Label)</a>.</p><p>Представьте, что компьютер – это ребенок, а мы – это учителя. Обучая ребенка узнавать лошадь, мы покажем ему несколько разных картинок, причем на некоторых из них действительно изображены эти животные, а остальные могут быть изображениями чего угодно (кошек, собак и проч.). Когда мы видим лошадь, мы говорим «лошадь!», а когда видим другого зверя – «Нет, не лошадь!» Спустя какое-то количество изображений ребенок (компьютер) будет правильно (в большинстве случаев) узнавать парнокопытное на картинках. Это контролируемое Машинное обучение с целью классификации изображений по видам животных.</p><p>Задача классификации имеет дискретное значение на выходе. Например, «любит пиццу с ананасами» и «не любит пиццу с ананасами» дискретны. Здесь нет никакого промежуточного варианта: </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/k-nearest-neighbors.png" class="kg-image" alt loading="lazy" width="199" height="490"><figcaption>Классификация любителей пиццы на основе возраста (1 – любит)</figcaption></figure><p><br>На этом изображении – базовый пример данных классификации. У нас есть Переменная-предиктор (Predictor Variable), или набор таковых, и репрезентативные метки 1 и 0. Мы попытаемся предсказать, любит ли человек пиццу с ананасами, используя его возраст.</p><p>Алгоритм kNN предполагает, что похожие <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a>, в данном случае потребители пиццы, существуют в непосредственной близости:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/k-nearest-neighbors-2.png" class="kg-image" alt loading="lazy" width="703" height="568" srcset="__GHOST_URL__/content/images/size/w600/2021/02/k-nearest-neighbors-2.png 600w, __GHOST_URL__/content/images/2021/02/k-nearest-neighbors-2.png 703w"><figcaption>Результат классификации kNN – сегментированные кластеры наблюдений</figcaption></figure><p>В большинстве случаев похожие наблюдения расположены близко друг к другу. kNN улавливает идею сходства (иногда называемого расстоянием, близостью или близостью), благодаря, как правило, вычислению Евклидова расстояния (Euclidean Distance) между точками на графике. </p><h3 id="-">Стандартная последовательность</h3><ol><li>Загрузите данные</li><li>Выберите число k, характеризующее количество соседей в кластере</li><li>Вычислите Евклидово расстояние между всеми точками попарно. Отсортируйте получившийся набор расстояний от наименьшего к наибольшему. Затем выберите первые k записей из отсортированной коллекции.</li><li>Для задач классификации определите Моду (Mode) – наиболее распространенную метку кластера.</li></ol><h3 id="-k-scikit-learn">Метод k-ближайших соседей и Scikit-learn </h3><p>Для демонстрации работы метода используем специальный игрушечный датасет, который генерируется с помощью встроенного метода <code>make_moons</code>. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import plotly\nimport plotly.graph_objects as go\n\nimport numpy as np\n\nimport sklearn\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier</code></pre><p>Загрузим игрушечный датасет. make_moons сгенерирует два перекрещивающихся полукруга на двумерном графике для демонстрации возможностей классификатора:</p><pre><code class="language-python"># Переменная повлияет на сглаженность разделительных границ\nmesh_size = .2\n\n# Параметр повлияет на ширину каждой из зон на графике\nmargin = 0.25\n\n# noise добавляет реалистичный шум \nX, y = make_moons(noise = 0.3, random_state = 0) \n\n# Разделим датасет на учебную и тестовую части\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y.astype(str), test_size = 0.25, random_state = 0)</code></pre><p>Конечно, луны – это метафора, выглядят эти пересекающиеся наборы точек примерно так:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/k-nearest-neighbors-sklearn-moons.jpg" class="kg-image" alt loading="lazy" width="491" height="365"></figure><p>Создадим и параметризируем двумерную сетку. Для этого определим минимальное и максимальное значение по осям x и y и добавим margin по краям, чтобы в зоне видимости оказались все наблюдения:</p><pre><code class="language-python"># X[:, 0].min() выберет из всех значений координат x минимальное значение\nx_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin\ny_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin\n\n# np.arange создаст вектор, где каждое следующее значение равноудалено \n# от предыдущего (шаг – mesh_size). Это ляжет в основу шкал на осях.\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\n\n# np.meshgrid() создаст координатную матрицу\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Создадим классификатор и классифицируем тренировочные данные\n# weights = 'uniform' означает, что у всех точек \n# одинаковое влияние на границы классификации\nclf = KNeighborsClassifier(15, weights = 'uniform')\nclf.fit(X, y)\n\n# Ось z – это непосредственно предсказания\n# np.c_ соединит последовательности координат в столбец\n# ravel() сделает из этого столбца одномерный массив \nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)</code></pre><p>Отобразим график – "карту уверенности" с исходными точками "лун":</p><pre><code class="language-python"># Обозначим учебные и тренировочные данные разными маркерами для наглядности\ntrace_specs = [\n    [X_train, y_train, '0', 'Train', 'square'],\n    [X_train, y_train, '1', 'Train', 'circle'],\n    [X_test, y_test, '0', 'Test', 'square-dot'],\n    [X_test, y_test, '1', 'Test', 'circle-dot']\n]\n\nfig = go.Figure(data = [\n    go.Scatter( # специальная обертка Plotly: наслоим для наглядности "луны"\n        x = X[y == label, 0], y = X[y == label, 1],\n        # Настроим легенду графика\n        name = f'{split} Split, Label {label}',\n        mode = 'markers', marker_symbol = marker\n    )\n    for X, y, label, split, marker in trace_specs\n])\n\n# Зададим размер маркера, толщину обводки и цвет\nfig.update_traces(\n    marker_size = 12, marker_line_width = 1.5,\n    marker_color = "white"\n)\n\nfig.add_trace(\n    go.Contour( # Теперь отобразим саму карту "уверенности"\n        x = xrange, # Шкалы размечаются с помощью равноудаленных точек\n        y = yrange,\n        z = Z, # Третье измерение – это цвет как степень уверенности\n        showscale = False,\n        colorscale = 'aggrnyl', # Предустановленная цветовая схема\n        opacity = 1,\n        name = 'Score',\n        hoverinfo = 'skip'\n    )\n)\nfig.show()</code></pre><p>Мы получим контур, характеризующий степени уверенности алгоритма в принадлежности той или иной точки к сгенерированным группам (точки каждой группы как бы расположились в форме т.н. полулуний). Обратите внимание на тестовые маркеры с точками в центре: модель не всегда классифицирует их верно; попадаются те, что со 100%-й уверенностью алгоритм отнес к неверной группе, поскольку располагаются они далеко, на самой темно-зеленой части графика:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/k-nearest-neighbors-contour-plot-1.png" class="kg-image" alt loading="lazy" width="2000" height="681" srcset="__GHOST_URL__/content/images/size/w600/2021/02/k-nearest-neighbors-contour-plot-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/k-nearest-neighbors-contour-plot-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/k-nearest-neighbors-contour-plot-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/02/k-nearest-neighbors-contour-plot-1.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1M3U-ByHbCTPoPA75R52nnqmbAWXf_Gdx?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@patschrei">@patschrei</a></p>		mietod-k-blizhaishikh-sosiedie	2021-02-05		
54	Стандартизованная оценка (Z-Score)		<p>Стандартизованная оценка (z) – метрика, характеризующая удаленность <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> от <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднего значения (Mean)</a> <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population)</a>. Иными словами, на сколько стандартных отклонений ниже или выше среднего находится наблюдение. Рассчитывается для каждого из них с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$z = \\frac{x_i - μ}{σ}, где$$<br>\n$$z\\space{–}\\space{cтандартизованная}\\space{оценка,}$$<br>\n$$x_i\\space{–}\\space{исходный}\\space{элемент}\\space{выборки,}$$<br>\n$$μ\\space{–}\\space{среднее}\\space{арифметическое,}$$<br>\n$$σ\\space{–}\\space{стандартное}\\space{отклонение}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Мы располагаем <a href="__GHOST_URL__/vyborka/">Выборкой (Sample)</a> из 10 наблюдений, где указано, какие оценки по литературе получил класс:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/z-score-1.png" class="kg-image" alt loading="lazy" width="120" height="539"></figure><p><a href="__GHOST_URL__/sriednieie-znachieniie/#-">Средним арифметическим значением (Mean)</a> выборки будет 3,6:</p><!--kg-card-begin: markdown--><p>$$\\bar{X} = (3 + 2 + 3 + 4 + 2 + 5 + 2 + 5 + 5 + 5) / 10 = 3,6$$</p>\n<!--kg-card-end: markdown--><p>Для вычисления z-оценок нам потребуется также <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>, которое рассчитывается с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$σ = \\sqrt{\\frac{Σ_{i=1}^n(x_i - \\bar{X})^2}{n}}, где$$<br>\n$$σ\\space(малая\\spaceсигма)\\space–\\spaceстандартное\\spaceотклонение$$<br>\n$$Σ\\space–\\spaceсумма$$<br>\n$$x\\space–\\space{i-й}\\spaceэлемент\\spaceвыборки$$<br>\n$$\\bar{X}\\space–\\spaceсреднее\\spaceзначение\\spaceвыборки$$<br>\n$$n\\space–\\spaceколичество\\spaceэлементов\\spaceв\\spaceвыборке$$</p>\n<!--kg-card-end: markdown--><p>Следуя формуле, вычислим квадрат разницы между i-м элементом выборки и средним значением. К примеру, для первого вхождения это будет:</p><!--kg-card-begin: markdown--><p>$$x_i – \\bar{X} = (3 - 3,6)^2 = 0,36$$</p>\n<!--kg-card-end: markdown--><p>Для каждой из наших оценок такой квадрат разницы будет равен числам справа:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/z-score-standard-deviation-2.png" class="kg-image" alt loading="lazy" width="229" height="494"></figure><p>Сумма значений правого столбца, разделенная на количество наблюдений, и даст нам значение коэффициента стандартного отклонения:</p><!--kg-card-begin: markdown--><p>$$σ = 16,4 / 10 = 1,64$$</p>\n<!--kg-card-end: markdown--><p>Теперь мы можем вычислить z-оценки для каждого наблюдения. К примеру, для первого из них она будет равна:</p><!--kg-card-begin: markdown--><p>$$σ = \\frac{3 - 3,6} / 1,64 ≈ -0,37$$</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/z-score-completed-2.png" class="kg-image" alt loading="lazy" width="229" height="494"></figure><h3 id="-">Применение</h3><p>Z-оценка – это способ сравнить результаты с «нормальной» частью совокупности. Результаты тестов или опросов имеют тысячи возможных результатов и единиц измерения, что затрудняет сравнение. Например, мы знаем, что вес человека составляет 150 фунтов, но сравнивать его с обширными табличными данными может быть трудозатратным (особенно если некоторые веса записаны в килограммах). Z-оценка может сказать вам, где вес этого человека находится на шкале "от дефицитного до избыточного".</p><p>Теперь, когда мы понимаем, что такое стандартное отклонение, не составит труда запомнить, что z-оценка – это лишь количество стандартных отклонений, на которые удалено наблюдение от среднего. Принято считать, что наблюдение выходит за пределы нормального, если абсолютное значение его z-оценки превышает 2 (то есть меньше -2 и больше 2).</p><h3 id="-z-">Таблица z-оценок</h3><p>Зная z-оценку, мы можем вычислить занимаемую площадь под кривой нормального распределения. В примере про оценки за литературу у оценки "двойка" коэффициент равен -0,37:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/z-score-2.png" class="kg-image" alt loading="lazy" width="700" height="327" srcset="__GHOST_URL__/content/images/size/w600/2021/02/z-score-2.png 600w, __GHOST_URL__/content/images/2021/02/z-score-2.png 700w"></figure><p>Чтобы уточнить площадь ярко-розовой части фигуры, используется таблица z-оценок, в данном случае, для отрицательных значений коэффициента. Для числа '-0,37' мы должны найти строку '0.3' в левом столбце z и столбец с названием '0,7':</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/z-score-lookup-table.png" class="kg-image" alt loading="lazy" width="725" height="1758" srcset="__GHOST_URL__/content/images/size/w600/2021/02/z-score-lookup-table.png 600w, __GHOST_URL__/content/images/2021/02/z-score-lookup-table.png 725w"></figure><p> На пересечении этих элементов таблицы находится число 0,35569, и это не только площадь фигуры, но и долю студентов за этой чертой успеваемости. Таким способом вычисляют самых способных студентов.</p><h3 id="-scipy">Стандартизированная оценка и SciPy</h3><p>Стандартизированную оценку можно вычислить с помощью метода SciPy:</p><pre><code class="language-python">from scipy import stats\n\na = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n               0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\nstats.zscore(a)</code></pre><p>Мы получим такие значения коэффициентов: </p><pre><code class="language-python">array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n       ​0.6748, -1.1488, -1.3324])</code></pre><p>Фото: <a href="https://unsplash.com/@mganeolsen">@mganeolsen</a></p>		standartizovannaia-otsienka	2021-02-07		
55	Контролируемое обучение\t(Supervised Learning)		<p>Контролируемое обучение (обучение с учителем) – это метод <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, при котором <a href="__GHOST_URL__/modiel/">Модель (Model)</a> обучается на Размеченных данных (Labeled Data). Например, исследовав опухоли, установив их размер, плотность и другие метрики, мы передаем эти данные модели с обязательной пометкой, какое <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a> к какому строению (доброкачественному или злокачественному) относится. Размеченные данные для контролируемого обучения изображены на верхней таблице, на нижней – неразмеченная клиентская база:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/02/unsupervised-learning-6.png" class="kg-image" alt loading="lazy" width="1484" height="915" srcset="__GHOST_URL__/content/images/size/w600/2021/02/unsupervised-learning-6.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/unsupervised-learning-6.png 1000w, __GHOST_URL__/content/images/2021/02/unsupervised-learning-6.png 1484w"></figure><h3 id="-">Категории обучения с учителем</h3><p>Алгоритмы контролируемого обучения подразделяются на следующие модели:</p><ul><li><strong>Классификация</strong>: используются для задач, в которых выходная переменная может быть категоризирована («Да» /  «Нет» и проч.).  Например, обнаружение спама, анализ эмоциональной окраски твита, прогнозирование сдачи экзамена.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/supervised-learning-decision-boundaries-1.png" class="kg-image" alt loading="lazy" width="700" height="515" srcset="__GHOST_URL__/content/images/size/w600/2021/02/supervised-learning-decision-boundaries-1.png 600w, __GHOST_URL__/content/images/2021/02/supervised-learning-decision-boundaries-1.png 700w"><figcaption>Цветы с известными размерами лепестков разделены по видам</figcaption></figure><ul><li><strong><a href="__GHOST_URL__/rieghriessiia/">Регрессия (Regression)</a></strong>: используются для задач, где выходной переменной является значение той или иной величины. Чаще всего используется для прогнозирования числовых значений на основе предыдущих наблюдений. Например, прогнозирование стоимости недвижимости, прогнозирование оценки на экзамене на основании часов, потраченных на обучение:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/supervised-learning-2.png" class="kg-image" alt loading="lazy" width="700" height="408" srcset="__GHOST_URL__/content/images/size/w600/2021/02/supervised-learning-2.png 600w, __GHOST_URL__/content/images/2021/02/supervised-learning-2.png 700w"><figcaption>Белая линия предсказывает цену дома</figcaption></figure><p>Наряду с этим выделяют еще три разновидности обучения:</p><ul><li><a href="__GHOST_URL__/niekontroliruiemoie-obuchieniie/">Обучение без учителя (Unsupervised Learning)</a></li><li><a href="__GHOST_URL__/obuchieniie-s-chastichnym-privliechieniiem-uchitielia/">Обучение с частичным привлечением учителя (Semi-Supervised Learning)</a></li><li><a href="__GHOST_URL__/obuchieniie-s-podkrieplieniiem/">Обучение с подкреплением (Reinforcement Learning)</a></li></ul><h3 id="-scikit-learn">Контролируемое обучение и Scikit-learn</h3><p>Посмотрим, как работает контролируемое обучение на примере <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regression)</a>. Мы попытаемся предсказать вес в фунтах в зависимости от роста в метрах. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport sklearn\nfrom sklearn.linear_model import LinearRegression</code></pre><p>Допустим, положение той или иной точки на оси y двумерной плоскости описывается с помощью следующего уравнения:</p><!--kg-card-begin: markdown--><p>$$y = 1 * x_0 + 2 * x_1 + 3$$</p>\n<!--kg-card-end: markdown--><p>В реальности задача <a href="__GHOST_URL__/data-saiientist/">Дата-сайентиста (Data Scientist)</a> – как раз найти такое уравнение, сейчас раскроем его сначала, чтобы упростить понимание. Мы располагаем скромными учебными данными – рядами, содержащими значения x_0 и x_1:</p><pre><code class="language-python">X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3</code></pre><p>Посмотрите, как выглядит на языке Python то же уравнение, описывающее взаимосвязь x_0, x_1 и у: метод <code>dot()</code> вычисляет скалярное произведение массивов <code>X</code> и <code>np.array([1, 2])</code>, в конце мы добавляем к уравнению 3. К примеру, для первого элемента ряда значением Y будет:</p><!--kg-card-begin: markdown--><p>$$y_1 = 1 * 1 + 2 * 1 + 3 = 6$$</p>\n<!--kg-card-end: markdown--><p>Инициализируем функцию линейной регрессии и передадим ей учебные данные с помощью метода <code>fit()</code>. Чтобы узнать, какова предсказательная способность модели, используем метод <code>score()</code>:</p><pre><code class="language-python">reg = LinearRegression().fit(X, y)\nreg.score(X, y)</code></pre><p><a href="__GHOST_URL__/post/">Скор (Score)</a> у нее пока совсем неправдоподобный – 100%, это происходит из-за того, что в проверочную часть y <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> входит и учебная X, но принцип, надеюсь, понятен:</p><pre><code class="language-python">1.0</code></pre><p>Напоследок попросим модель предсказать значение y для x_0 и x_1, равных 3 и 5 соответственно:</p><pre><code class="language-python">reg.predict(np.array([[3, 5]]))</code></pre><p>Модель считает, что координатой y точки, где x_0  и x_1 равны 3 и 5, будет число 16:</p><pre><code class="language-python">array([16.])</code></pre><p>Давайте проверим простой подстановкой:</p><!--kg-card-begin: markdown--><p>$$y_1 = 1 * 3 + 2 * 5 + 3 = 16$$</p>\n<!--kg-card-end: markdown--><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1ApdnVsQu2CpQtau1ddCxaU9Q8AUrMjHr?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@zmachacek">@zmachacek</a></p>		kontroliruiemoie-obuchieniie	2021-02-13		
56	Наблюдение\t(Observation)		<p>Наблюдение (в контексте также: пример, строка, запись, единица наблюдения, точка, сущность; обозначается X) – это ценные данные, собираемые во время исследования или эксперимента (рост человека, размер банковского счета в определенный момент времени, количество животных). Вместе с масштабом анализа определяет Совокупность (Population).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/dataset-matrix-1.png" class="kg-image" alt loading="lazy" width="360" height="246"><figcaption>Пять наблюдений</figcaption></figure><p>Например, предположим, что вы измеряете, как меняется размер сбережений за один год. Вы отслеживаете один параметр – баланс вашего банковского счета каждые три месяца, и за год получаете четыре наблюдения:</p><ul><li>Март = $564 </li><li>Июнь = $576</li><li>Сентябрь = $587</li><li>Декабрь =$599</li></ul><p>Обратите внимание: «наблюдение» не означает, что мы его <em>наблюдали</em>. Кто-то другой мог это записать. Это может быть даже информация, которую нашли в ходе раскопок. Многое зависит от того, что мы ищем. Допустим, наши находки с раскопок – записи лавочника, жившего в XII веке. В зависимости от цели исследования наблюдениями могут стать и продажи, и закупки, и доля распроданных товаров.</p><p>Эмпирические исследования – это практические эксперименты с результатами на основе реального опыта, а не теории или убеждений. Основополагающим принципом <a href="__GHOST_URL__/nauka-o-dannykh/">Науки о данных (Data Science)</a> стал приоритет наблюдения над предположением. Прежде чем создавать суждение, мы исследуем данные, но не наоборот. </p><p>Систематическая ошибка наблюдения возникает, когда информация собирается, интерпретируется или измеряется неточно в разных группах наблюдений. К примеру, в начале эксперимента лавочник записывал самые мелкие продажи, затем прекратил, и масштаб данных изменился.</p><h3 id="-">Типы наблюдений</h3><p>Измерения, содержащиеся в единице наблюдения, могут выражаться с помощью различных типов данных:</p><ul><li>Числовой тип: целые (Integer), вещественные (Real Number), а также числа с плавающей запятой (Float)) (например, количество фруктов: 10)</li><li>Булевый тип (Boolean Data; сдан ли экзамен: да / нет)</li><li><a href="__GHOST_URL__/katieghorialnaia-pieriemiennaia/" rel="noopener noreferrer">Категориальный (Categorical Variable</a>; жанры кино: комедия, ужасы, мелодрама)</li><li><a href="__GHOST_URL__/viektor/" rel="noopener noreferrer">Вектор (Vector</a>; пиксель как элемент изображения описывают с помощью RGB-кода: 255, 255, 0)</li></ul><p>Смысл наблюдения часто и в том, чтобы визулизировать данные; во многих случаях они превращаются в числа до построения графиков, к примеру, с помощью <a href="__GHOST_URL__/bystroie-kodirovaniie/">Быстрого кодирования (One-Hot Encoding)</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/observation------.png" class="kg-image" alt loading="lazy" width="700" height="527" srcset="__GHOST_URL__/content/images/size/w600/2021/02/observation------.png 600w, __GHOST_URL__/content/images/2021/02/observation------.png 700w"><figcaption>В одну точку, отображающую наблюдение, вкладывается много информации</figcaption></figure><p>Каждое наблюдение принято обозначать отдельной точкой на графике, причем форма, размер, цвет ее характеризуют сразу несколько признаков наблюдения. К примеру, на графике размера населения в разных странах размер точки характеризует величину популяции, цвет – расположение страны в той или иной части света (Европа, Азия и т.д.). Чем фиолетовее точка, тем плотнее население страны. За положение точки относительно осей координат отвечают параметры:</p><ul><li>Валовой внутренний продукт, приходящийся на человека в день (gdp_per_day – ось x)</li><li>Ожидаемая продолжительность жизни (life_expectancy – ось y)</li></ul><p>Фото: <a href="https://unsplash.com/@supernov">@supernov</a></p>		nabliudieniie	2021-02-13		
57	Предиктор (Predictor Variable)		<p>Предиктор (прогнозирующая переменная) –<a href="__GHOST_URL__/priznak/"> </a>переменная, используемая для прогнозирования <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>. В общих чертах это можно представить следующей формулой:</p><!--kg-card-begin: markdown--><p>$$y = f(x), где$$<br>\n$$y\\space{–}\\space{целевая}\\space{переменная}$$<br>\n$$x\\space{–}\\space{предиктор}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Мы хотим предсказать, уволится ли программист с текущей работы или нет. Нам необходимо проанализировать различные параметры, которые влияют на мышление сотрудника. Среди собранных в ходе исследования данных следующие <a href="__GHOST_URL__/priznak/">Признаки (Features)</a>:</p><ul><li>Удовлетворенность работой</li><li>Зарплата</li><li>Дистанция от дома до работы</li><li>Взаимоотношения с коллегами</li><li>Знание предметной области</li></ul><p>Исследователи установили, что первые три метрики влияют на вероятность увольнения больше всего. Если сотрудник доволен работой, зарплатой и быстро добирается до офиса, то будет держаться рабочего места в случае изменений условий труда, будь то изменение технологии, с которой ему предстоит работать, или экономического благополучия организации:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/predictor.png" class="kg-image" alt loading="lazy" width="2000" height="767" srcset="__GHOST_URL__/content/images/size/w600/2021/02/predictor.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/predictor.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/predictor.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/02/predictor.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><em>Функциональные типы переменных</em></figcaption></figure><p>Взаимоотношения с коллегами имеют условное влияние на взаимоотношения между целевой переменной и предикторами. Иными словами, такая <em>модерирующая переменная</em> повлияет на силу взаимосвязи между ними, но сама по себе не вынудит сотрудника искать новое место работы. Если разработчик удовлетворен проектами, которые реализует, зарплатой, соответствующей индустрии, и быстро добирается до работы, то появление токсичных коллег навряд ли вынудит его уволиться. Однако на <em>вероятность</em> уволиться все же повлияет.</p><p><em>Переменная-посредник</em> объясняет взаимосвязь между независимыми и зависимыми признаками. Допустим, по некоторым причинам работодателю потребовалось мигрировать с одного языка программирования на другой. Теперь, если проанализировать ситуацию, то разработчик:</p><ul><li>По-прежнему очень увлечен разработкой программного обеспечения</li><li>Доволен коллегами и компанией</li><li>Доволен доходом</li><li>Быстро добирается до офиса</li><li>Испытывает определенный стресс, поскольку вынужден работать с незнакомым языком программирования</li></ul><p>В этом случае существует вероятность увольнения. Но все же предикторы "набрали достаточно очков" удовлетворенности рабочим местом, и разработчик не уйдет в отставку. "Знания предметной области" – это промежуточная переменная, и именно она в такой стрессовой для разработчика ситуации будет влиять на его адаптационную способность и сохранение удовлетворенности основными условиями труда.</p><h3 id="-">Предиктор и Независимая переменная</h3><p>Все эксперименты имеют дело с той или иной формой переменных, которые измеряют и трансформируют, которыми манипулируют. Переменную-предиктор часто ошибочно принимают за Независимую переменную (Independent Variable), однако их определения немного различаются. Если независимая переменная может быть преобразована на протяжении всего эксперимента, то предиктор – нет. </p><p>Фото: <a href="https://unsplash.com/@katgo_83">@katgo_83</a></p>		priediktor	2021-02-16		
58	Медиана (Median)		<p>Медиана (x̃, M; Мера центральной тенденции) – это центральное значение <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a>.</p><p>В математике медиана также представляет собой тип <a href="__GHOST_URL__/sriednieie-znachieniie/">Среднего значения (Average)</a>, который используется для нахождения "центра". Поэтому ее еще называют мерой центральной тенденции.</p><h3 id="-">Нечетное количество элементов ряда</h3><p>Если в ряду нечетное количество элементов, то мы сортируем значения в возрастающем или убывающем порядке, а затем выбираем центральное.</p><p>Пример. Найдем медиану следующего ряда:</p><p><code>4, 17, 77, 25, 22, 23, 92, 82, 40, 24, 14, 12, 67, 23, 29</code></p><p>Расставив эти числа по порядку, мы получим:</p><p><code>4, 12, 14, 17, 22, 23, 23, 24, 25, 29, 40, 67, 77, 82, 92</code></p><p>Всего пятнадцать элементов, то есть 8-й будет центральным. Медианное значение этого набора чисел – 24.</p><h3 id="--1">Четное количество элементов ряда</h3><p>Если в ряду четное количество элементов, медиана рассчитывается с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$M = \\frac{n + 1}{2}, где$$<br>\n$$M\\space{–}\\space{медиана,}$$<br>\n$$n\\space{–}\\space{количество}\\space{элементов}\\space{в}\\space{выборке}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Найдем медиану следующего ряда:</p><p><code>1.79, 1.61, 2.09, 1.84, 1.96, 2.11</code></p><p>Выполнив подстановку, мы получим:</p><!--kg-card-begin: markdown--><p>$$M = \\frac{6 + 1}{2} = 3.5$$</p>\n<!--kg-card-end: markdown--><h3 id="--2">Центральная тенденция</h3><p>Помимо медианы, выделяют еще две другие меры центральной тенденции – <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее значение (Mean)</a> и <a href="__GHOST_URL__/moda/">Мода (Mode)</a>. Среднее – это частное от суммы всех <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> к их количеству. Мода – это наиболее часто повторяющееся значение выборки.</p><p>В <a href="__GHOST_URL__/nauka-o-dannykh/">Науке о данных (Data Science)</a> медиана иногда используется вместо среднего значения, когда в последовательности есть выбросы, которые могут исказить среднее. Выбросы меньше влияют на медианное значение, чем на среднее. Медиана отделяет верхнюю половину выборки, генеральной совокупности или Распределения вероятностей (Probability Distribution) от нижней. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/median-2.png" class="kg-image" alt loading="lazy" width="1602" height="662" srcset="__GHOST_URL__/content/images/size/w600/2021/02/median-2.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/median-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/median-2.png 1600w, __GHOST_URL__/content/images/2021/02/median-2.png 1602w" sizes="(min-width: 720px) 720px"><figcaption>Медиана распределения вероятностей</figcaption></figure><h3 id="-numpy">Медиана и NumPy</h3><p>Медиану можно вычислить с помощью NumPy. Для начала импортируем все необходимые библиотеки:</p><pre><code class="language-python">import numpy as np</code></pre><p>Создадим массив из 6 элементов и вызовем встроенный метод <code>median()</code>:</p><pre><code class="language-python">a = [10, 7, 4, 3, 2, 1]\nnp.median(a)</code></pre><p>NumPy определяет четность числа элементов массива (6) и применяет тот или иной метод расчета (согласно формуле):</p><pre><code class="language-python">3.5</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1a0k71DLSsUUkKE7f1Qu6-6MoWOodGMjU?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@garciasaldana_">@garciasaldana_</a></p>		miediana	2021-02-18		
59	Эксцесс (Kurtosis)		<p>Эксцесс (κ – "каппа") – это параметр распределения вероятностей (Probability Distribution), характеризующий его остроконечность. Эксцесс характеризует островершинность кривой и форму ее "хвостов":</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/02/kurtosis-1.png" class="kg-image" alt loading="lazy" width="700" height="408" srcset="__GHOST_URL__/content/images/size/w600/2021/02/kurtosis-1.png 600w, __GHOST_URL__/content/images/2021/02/kurtosis-1.png 700w"><figcaption>Типы эксцессов</figcaption></figure><p>Эксцесс рассчитывается с помощью формулы – частное 4-го Центрального момента и квадратичной дисперсии:</p><!--kg-card-begin: markdown--><p>$$κ = \\frac{3σ^4}{σ^2}, где$$<br>\n$$κ\\space{–}\\space{скошенность}$$<br>\n$$3σ^4\\space{–}\\space{4-й}\\space{центральный}\\space{момент,}$$<br>\n$$σ^2\\space{–}\\space{квадратичная}\\space{дисперсия}$$</p>\n<!--kg-card-end: markdown--><h3 id="-">Влияние на Машинное обучение</h3><p>В зависимости от значения коэффициента эксцесс делят на три вида:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/kurtosis-coefficients.png" class="kg-image" alt loading="lazy" width="605" height="179" srcset="__GHOST_URL__/content/images/size/w600/2021/02/kurtosis-coefficients.png 600w, __GHOST_URL__/content/images/2021/02/kurtosis-coefficients.png 605w"></figure><p><br>Многие годы статисты спорили, что сильнее влияет на значение эксцесса – островершинность пика кривой или же "жирные" хвосты. Последние победили:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/kurtosis_tails.png" class="kg-image" alt loading="lazy" width="754" height="287" srcset="__GHOST_URL__/content/images/size/w600/2021/02/kurtosis_tails.png 600w, __GHOST_URL__/content/images/2021/02/kurtosis_tails.png 754w" sizes="(min-width: 720px) 720px"></figure><p><br>Это значит также, что большое значение эксцесса приводит к массивным длинным хвостам длиной по 6-7 стандартных отклонений, а общепринятая норма здесь – 3-4. Это означает, помимо прочего, наличие выбросов, искажающих прогностическую силу Машинного обучения. </p><h3 id="-scipy">Эксцесс и SciPy</h3><p>Коэффициент эксцесса можно вычислить с помощью метода <code>kurtosis()</code> SciPy.</p><p>Пример. Исследователи собрали данные о урожае крыжовника в небольшом садоводстве. По одному килограмму собрали два садовода, по два – три и так далее. 10 килограммов удалось собрать лишь одному:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/kurtosis-berries-4.png" class="kg-image" alt loading="lazy" width="1116" height="671" srcset="__GHOST_URL__/content/images/size/w600/2021/02/kurtosis-berries-4.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/kurtosis-berries-4.png 1000w, __GHOST_URL__/content/images/2021/02/kurtosis-berries-4.png 1116w"></figure><p>Импортируем все необходимые библиотеки:</p><pre><code class="language-python">import scipy\nfrom scipy.stats import kurtosis</code></pre><p>Используем тот же набор наблюдений из задачи про крыжовник и рассчитаем значение коэффициента эксцесса для всей <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a>:</p><pre><code class="language-python">data = [2, 3, 4, 6, 7, 10, 11, 4, 2, 1]\nkurtosis(data, fisher = False)</code></pre><p>Эксцесс платикуртический, поскольку меньше 3:</p><pre><code class="language-python">2.1128515485938055</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1-eMh2q7p9erPWpDWfPUbtxfIdYk8lXsL?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@einarr05">@einarr05</a></p>		ekstsiess	2021-02-22		
120	Ядерный трюк (Kernel Trick)		<p>Ядерный трюк (Kernel Trick, Kernel Function, уловка с ядром) – способ классификации, позволяющий работать в исходном пространстве <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, не вычисляя координаты данных в пространстве более высокой размерности.</p><h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2">Метод опорных векторов</h3><p>Чтобы помочь вам понять, что такое ядерный трюк ​​и почему он важен, я сначала познакомлю вас с основами <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метода опорных векторов (SVM)</a>.</p><p>SVM – это <a href="__GHOST_URL__/alghoritm/">Алгоритм (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> с учителем, который в основном используется для <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>. Он учится разделять группы <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>, формируя границы принятия решений.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-20.png" class="kg-image" alt loading="lazy" width="1400" height="602" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-20.png 600w, __GHOST_URL__/content/images/size/w1000/2021/07/image-20.png 1000w, __GHOST_URL__/content/images/2021/07/image-20.png 1400w" sizes="(min-width: 720px) 720px"></figure><p>На графике выше мы замечаем, что существует два класса наблюдений: синие и сиреневые точки. Есть множество способов разделить эти два класса, как показано на графике слева. Однако мы хотим найти «лучшую» <a href="__GHOST_URL__/gipierploskost/">Гиперплоскость (Hyperplane)</a>, которая могла бы максимизировать расстояние между этими двумя классами, что означает, что расстояние между гиперплоскостью и ближайшими точками данных с каждой стороны является наибольшим. В зависимости от того, на какой стороне гиперплоскости находится новая точка данных, мы могли бы причислить ее к тому или иному классу.</p><p>В приведенном выше примере это звучит просто. Однако не все данные можно разделить линейно. Фактически, в реальном мире почти все данные распределены случайным образом, что затрудняет линейное разделение.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-21.png" class="kg-image" alt loading="lazy" width="838" height="334" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-21.png 600w, __GHOST_URL__/content/images/2021/07/image-21.png 838w" sizes="(min-width: 720px) 720px"></figure><p>Почему так важно использовать уловку с ядром? Как вы можете видеть на картинке выше, если мы найдем способ сопоставить данные из двухмерного пространства в трехмерном, то сможем найти способ принятия решений, который четко разделяет точки на классы. Моя первая мысль об этом процессе преобразования данных состоит в том, чтобы сопоставить все точки данных с более высоким измерением (в данном случае с третьим), найти границу и провести классификацию.</p><p>Однако, когда появляется все больше и больше измерений, вычисления в этом пространстве становятся все более <em>дорогими</em>. Вот тут-то и появляется уловка с ядром. Она позволяет нам работать в исходном пространстве функций, не вычисляя координаты данных в пространстве более высокой размерности.</p><p>Обучение классификатора линейных опорных векторов, как и почти любая проблема в машинном обучении и в жизни, является проблемой оптимизации. Мы максимизируем <a href="__GHOST_URL__/polie/">Поле (Margin)</a> – расстояние, разделяющее ближайшую пару точек данных, принадлежащих противоположным классам:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-22.png" class="kg-image" alt loading="lazy" width="651" height="611" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-22.png 600w, __GHOST_URL__/content/images/2021/07/image-22.png 651w"></figure><p>Эти точки называются опорными векторами, потому что они представляют собой данные наблюдений, которые «поддерживают» или определяют границу принятия решения. Чтобы обучить классификатор опорных векторов, мы находим гиперплоскость с максимальным запасом или оптимальную разделяющую гиперплоскость, которая разделяет два класса, чтобы сделать точные прогнозы классификации.</p><p>Опорные векторы – это точки на пунктирных линиях. Расстояние от пунктирной линии до сплошной линии – это поле, представленное стрелками.</p><p>Машины опорных векторов гораздо труднее интерпретировать в более высоких измерениях. Намного сложнее визуализировать, как данные могут быть линейно разделены и как будет выглядеть граница принятия решения. В трехмерном пространстве гиперплоскость – это обычная двумерная плоскость. </p><p>Классификация опорных векторов основана на этом понятии линейно разделяемых данных. Классификация с Мягким полем (Soft Margin) может компенсировать некоторые ошибки классификации обучающих данных в случае, когда данные не являются идеально линейно разделимыми. Однако на практике данные часто очень далеки от линейного разделения, и нам нужно преобразовать их в пространство более высокой размерности, чтобы соответствовать классификатору опорных векторов.</p><h3 id="%D0%BD%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B5-%D0%BF%D1%80%D0%B5%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F">Нелинейные преобразования</h3><p>Если данные нельзя линейно разделить в исходном или входном пространстве, мы применяем преобразования, которые конвертируют данные из исходного пространства в пространство признаков более высокого измерения. Цель состоит в том, чтобы после преобразования в пространство более высокой размерности классы теперь линейно разделялись в этом пространстве функций более высокой размерности. Затем мы можем установить границу решения, чтобы разделить классы и сделать прогнозы. Решение о границах будет гиперплоскостью.</p><h3 id="%D1%8F%D0%B4%D0%B5%D1%80%D0%BD%D1%8B%D0%B9-%D1%82%D1%80%D1%8E%D0%BA-%D0%B8-scikit-learn">Ядерный трюк и Scikit-learn</h3><p>Давайте посмотрим, как уловка реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport sklearn as sk\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_circles\nfrom sklearn import preprocessing</code></pre><p>Сгенерируем игрушечный <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> на тысячу наблюдений, причем точки обоих классов формируют эллипсы:</p><pre><code class="language-python">x, y = make_circles(n_samples = 1000, noise = 0.09)\nx = preprocessing.scale(x)\n\n# Разделение набора на тренировочную и тестовую части \nx_test = x[:500]\ny_test = y[:500]\nx = x[500:]\ny = y[500:]\n\n# Фильтруем точки по оси y – выбираем только равные [0, -1, 1]\ny = np.where(y == 0, -1, 1)\ny_test = np.where(y_test == 0, -1, 1)\n\nsns.scatterplot(x[:, 0], x[:, 1], hue = y.reshape(-1))</code></pre><p>Одно кольцо "опоясывает" другое, и реализован какой-никакой <a href="__GHOST_URL__/shum/">Шум (Noise)</a>, делающий картину чуть реалистичнее:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-23.png" class="kg-image" alt loading="lazy" width="385" height="248"></figure><p>Теперь нам предстоит создать собственный класс метода опорных векторов: научить модель использовать т.н. <em>Гауссово ядро</em>. Ядро для сглаживания определяет, как вычисляется среднее значение между соседними точками. Ядро Гаусса имеет форму кривой <a href="__GHOST_URL__/normalnoie-raspriedielieniie/">Нормального распределения (Normal Distribution)</a>. <code>sigma_sq</code> (sigma squared – сигма в квадрате) – квадрат распределения <a href="__GHOST_URL__/ostatok/">Остатков (Residual)</a>, который рассматривается как показатель <a href="__GHOST_URL__/dispiersiia/">Дисперсии (Variance)</a> распределения <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> y. Действительно, это распределение необходимо для метода максимального правдоподобия.</p><pre><code class="language-python">class support_vector_machine:\n    def __init__(self, C = 10, features = 2, sigma_sq = 0.1, kernel = "None"):\n        self.C = C\n        self.features = features\n        self.sigma_sq = sigma_sq\n        self.kernel = kernel\n        self.weights = np.zeros(features)\n        self.bias = 0.\n        \n    # Определяем меру схожести эллипсов\n    def __similarity(self, x, l):\n        # Вычисляем экспоненциальную функцию\n        return np.exp(-sum((x - l) ** 2) / (2 * self.sigma_sq))\n\n    # Усредняем границу классов\n    def gaussian_kernel(self, x1, x):\n        m = x.shape[0]\n        n = x1.shape[0]\n        op = [[self.__similarity(x1[x_index], x[l_index]) for l_index in range(m)] for x_index in range(n)]\n        return np.array(op)\n\n    # Вычисляем потери классификации\n    def loss_function(self, y, y_hat):\n        sum_terms = 1 - y * y_hat\n        sum_terms = np.where(sum_terms &lt; 0, 0, sum_terms)\n        return (self.C * np.sum(sum_terms) / len(y) + sum(self.weights ** 2) / 2)\n\n    # Обучаем модель\n    def fit(self, x_train, y_train, epochs = 1000, print_every_nth_epoch = 100, learning_rate = 0.01):\n        y = y_train.copy()\n        x = x_train.copy()\n        self.initial = x.copy()\n        \n        assert x.shape[0] == y.shape[0] , "Samples of x and y don't match."\n        assert x.shape[1] == self.features , "Number of Features don't match"\n        \n        if(self.kernel == "gaussian"):\n            x = self.gaussian_kernel(x, x)\n            m = x.shape[0]\n            self.weights = np.zeros(m)\n\n        n = x.shape[0]\n        \n        for epoch in range(epochs):\n        \t# np.dot – скалярное произведение двух массивов\n            y_hat = np.dot(x, self.weights) + self.bias \n            grad_weights = (-self.C * np.multiply(y, x.T).T + self.weights).T\n            \n            for weight in range(self.weights.shape[0]):\n                grad_weights[weight] = np.where(1 - y_hat &lt;= 0, self.weights[weight], grad_weights[weight])\n            \n            grad_weights = np.sum(grad_weights, axis = 1)\n            self.weights -= learning_rate * grad_weights / n\n            grad_bias = -y * self.bias\n            grad_bias = np.where(1 - y_hat &lt;= 0, 0, grad_bias)\n            grad_bias = sum(grad_bias)\n            self.bias -= grad_bias * learning_rate / n\n            if((epoch + 1) % print_every_nth_epoch == 0):\n                print("Эпоха {} --&gt; Потери = {}".format(epoch + 1, self.loss_function(y, y_hat)))\n    \n    # Оцениваем предсказание\n    def evaluate(self, x, y):\n        pred = self.predict(x)\n        pred = np.where(pred == -1, 0, 1)\n        diff = np.abs(np.where(y == -1, 0, 1) - pred) # np.abs вычисляет абсоюлтное значение (модуль числа)\n        return((len(diff) - sum(diff)) / len(diff))\n\n    # Предсказываем класс для новых наблюдений\n    def predict(self, x):\n        if(self.kernel == "gaussian"):\n            x = self.gaussian_kernel(x, self.initial)\n        return np.where(np.dot(x, self.weights) + self.bias &gt; 0, 1, -1)</code></pre><p>Ну что же, самая громоздкая ячейка пройдена, дело за малым – создадим функцию визуализации результата классификации:</p><pre><code class="language-python">def visualize(model, title):\n    print("Тестовая точность = {}".format(model.evaluate(x_test, y_test)))\n    # Для наглядности точки заливаются в двумерный график, где x находится в пределах от -5 до 6, а y – от -5 до 4\n    x1 = np.arange(-5, 6, 0.3) # np.arange возвращает равномерно раскиданные значения с шагом 0.3 в интервале от -5 до 6\n    x2 = np.arange(-5, 4, 0.3)\n    \n    for i in range(len(x1)):\n        for j in range(len(x2)):\n        \t# Генерируем предсказания классов точек\n            pred = model.predict(np.array([np.array(np.array([x1[i], x2[j]]))]))[0]\n            if(pred &gt; 0.5):\n                plt.scatter(x1[i], x2[j], c = "r")\n            else:\n                plt.scatter(x1[i], x2[j], c = "b")\n    plt.title(title)\n    plt.show()</code></pre><p>Создадим модель – экземпляр класса <code>support_vector_machine</code> и обучим модель в 20 эпох:</p><pre><code class="language-python">model = support_vector_machine(C = 10, kernel = "gaussian", sigma_sq = 0.01)\nmodel.fit(x, y, epochs = 20, print_every_nth_epoch = 2, learning_rate = 0.01)\nprint("Точность обучения = {}".format(model.evaluate(x, y)))</code></pre><p>Само по себе значение <a href="__GHOST_URL__/funktsiia-potieri/">Функции потерь (Loss Function)</a> по-настоящему начинает "играть", когда рассматривается в сравнении с предыдущими итерациями обучения. В нашем случае, к двадцатой эпохе потери значительно снижаются:</p><pre><code class="language-python">Эпоха 2 --&gt; Потери = 9.975194749508915\nЭпоха 4 --&gt; Потери = 9.9268022983303\nЭпоха 6 --&gt; Потери = 9.880306978905201\nЭпоха 8 --&gt; Потери = 9.835634230178764\nЭпоха 10 --&gt; Потери = 9.792712425248418\nЭпоха 12 --&gt; Потери = 9.751472755823215\nЭпоха 14 --&gt; Потери = 9.711849121234396\nЭпоха 16 --&gt; Потери = 9.673778021817862\nЭпоха 18 --&gt; Потери = 9.637198456496362\nЭпоха 20 --&gt; Потери = 9.602051824395948\nТочность обучения = 0.876</code></pre><p>Классифицирующее предсказание готово. Визуализируем результат собственной функцией <code>visualize()</code>:</p><pre><code class="language-python">visualize(model, "Метод опорных векторов с гауссовым ядром")</code></pre><p>На графике ниже не упорядоченные вдруг случайные точки, а их плотность их распределения относительно друг друга. В результате усреднения границ зона малого эллипсоида была как бы ужата. На стадии   </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-24.png" class="kg-image" alt loading="lazy" width="370" height="264"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1wrAS0gEKyVftT5_WwcY-SJCkxqMRGQn4?usp=sharing">здесь</a>.</p><p>Авторы оригинальных статей: <a href="https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f">Drew Wilimitis</a>, <a href="https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d">Grace Zhang</a>, <a href="https://www.kaggle.com/prathameshbhalekar/svm-with-kernel-trick-from-scratch">Prathamesh Bhalekar</a></p>		iadiernyi-mietod	2021-07-18		
121	StandardScaler		<p>StandardScaler – класс Scikit-learn, подвергающий передаваемый объект <a href="__GHOST_URL__/standartizatsiia/">Стандартизации (Standartization)</a>. Каждый стандартизованный элемент <a href="__GHOST_URL__/priznak/">Признака (Feature)</a> вычисляется следующим образом:</p><!--kg-card-begin: markdown--><p>$$x_{i,станд.} = \\frac{x_i - μ}{σ}, где$$<br>\n$$x_{i,станд.}\\space{–}\\space{стандартизованный}\\space{элемент}\\space{признака,}$$<br>\n$$x_i\\space{–}\\space{исходный}\\space{элемент,}$$<br>\n$$μ\\space{–}\\space{среднее}\\space{арифметическое,}$$<br>\n$$σ\\space{–}\\space{стандартное}\\space{отклонение}$$</p>\n<!--kg-card-end: markdown--><p>Вычисление <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднего арифметического (Mean)</a> и <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартного отклонения (Standard Deviation)</a> происходят независимо для каждого признака. Затем эти <a href="__GHOST_URL__/statistika/">Статистики (Statistics)</a> сохраняются для последующего использования с помощью преобразования.</p><p>Стандартизация набора данных является общим требованием для многих <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>: они могут "вести себя плохо", если отдельные признаки не выглядят как нормально распределенные данные (например, по Гауссу с нулевым средним и единичной дисперсией).</p><p>Например, многие элементы, используемые в <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> алгоритма, такие как ядро ​​<a href="__GHOST_URL__/mietod-opornykh-viektorov/">Машина опорных векторов<sub> </sub>(SVM)</a>, предполагают, что все функции сосредоточены вокруг нуля ​​и имеют дисперсию того же порядка. Если характеристика имеет <a href="__GHOST_URL__/dispiersiia/">Дисперсию (Variance)</a>, которая на порядок больше, чем у других, она может доминировать над целевой переменной и сделать <a href="__GHOST_URL__/modiel/">Модель (Model)</a> неспособной учиться на других.</p><h3 id="sklearnpreprocessingstandardscaler">sklearn.preprocessing.StandardScaler</h3><p>Давайте посмотрим, как работает класс StandardScaler. Импортируем его из модуля <code>preprocessing</code>:</p><pre><code class="language-python">import sklearn\nfrom sklearn.preprocessing import StandardScaler</code></pre><p>Создадим небольшой датасет, состоящий из пар значений – нулей и единиц, инициируем инструмент стандартизации и передадим ему данные:</p><pre><code class="language-python">data = [[0, 0], [0, 0], [1, 1], [1, 1]]\nscaler = StandardScaler()\nprint(scaler.fit(data))</code></pre><p>Система отображает стандартные настройки стандартизатора: например, в целях резервирования объект <code>data</code> копируется:</p><pre><code class="language-python">StandardScaler(copy=True, with_mean=True, with_std=True)</code></pre><p>Посмотрим, чему равно среднее:</p><pre><code class="language-python">print(scaler.mean_)</code></pre><p>Статистика равноудалена от крайних значений:</p><pre><code class="language-python">[0.5 0.5]</code></pre><p>Вызовем функцию <code>transform</code>, которая и выполнит трансформацию:</p><pre><code class="language-python">print(scaler.transform(data))</code></pre><p>Повторим: <code>StandardScaler</code> преобразует значения таким образом, что из каждого <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> каждого признака вычитается среднее арифметическое и результат делится на стандартное отклонение этого признака. Выведем наш список:</p><pre><code class="language-python">[[-1. -1.]\n [-1. -1.]\n [ 1.  1.]\n [ 1.  1.]]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1g1LpchdN8cjO0G6NQRG02_SjofMvCU80?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">scikit-learn</a></p>		standardscaler	2021-07-21		
60	Дисперсия (Variance)		<p>Дисперсия случайной величины (σ<sup>2</sup>, s<sup>2</sup>, Var(x)) – мера удаленности того или иного значения <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a> от <a href="__GHOST_URL__/sriednieie-znachieniie/">Среднего значения (Mean)</a>. Рассчитывается с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$σ^2 = \\frac{Σ{(x_i - x̅)^2}}{N}, где$$<br>\n$$σ^2\\space{–}\\space{Дисперсия}\\space{случайной}\\space{величины,}$$<br>\n$$x_i\\space{–}\\space{i-й}\\space{элемент}\\space{выборки,}$$<br>\n$$x̅\\space{–}\\space{среднее}\\space{арифметическое,}$$<br>\n$$N\\space{–}\\space{количество}\\space{элементов}\\space{выборки}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Друзья измерили рост своих собак разных пород и хотят выяснить, у скольких собак слишком большой и слишком маленький рост.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/variance-1.png" class="kg-image" alt loading="lazy" width="1895" height="821" srcset="__GHOST_URL__/content/images/size/w600/2021/02/variance-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/variance-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/variance-1.png 1600w, __GHOST_URL__/content/images/2021/02/variance-1.png 1895w" sizes="(min-width: 1200px) 1200px"></figure><p>Для начала найдем среднее арифметическое:</p><!--kg-card-begin: markdown--><p>$$σ^2 = \\frac{600 + 470 + 170 + 430 + 300}{5} = \\frac{1970}{5} = 394$$</p>\n<!--kg-card-end: markdown--><p>Теперь, с добавлением среднего, восприятие точек немного изменится:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/----------2.png" class="kg-image" alt loading="lazy" width="1714" height="809" srcset="__GHOST_URL__/content/images/size/w600/2021/02/----------2.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/----------2.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/----------2.png 1600w, __GHOST_URL__/content/images/2021/02/----------2.png 1714w" sizes="(min-width: 1200px) 1200px"></figure><p>Чтобы вычислить дисперсию, выясним разность между каждым элементом выборки и средним значением:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/variance-mean-difference.png" class="kg-image" alt loading="lazy" width="2000" height="751" srcset="__GHOST_URL__/content/images/size/w600/2021/02/variance-mean-difference.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/variance-mean-difference.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/variance-mean-difference.png 1600w, __GHOST_URL__/content/images/2021/02/variance-mean-difference.png 2238w" sizes="(min-width: 1200px) 1200px"></figure><p>Выполнив подстановку, мы получим значение дисперсии – 21 тысячу:</p><!--kg-card-begin: markdown--><p>$$σ^2 = \\frac{206^2 + 76^2 + (-224)^2 + 36^2 + (-94)^2}{5} = \\frac{108520}{5} = 21704$$</p>\n<!--kg-card-end: markdown--><p>Чтобы решить основную задачу, извлечем из этой цифры квадратный корень и получим тем самым <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>:</p><!--kg-card-begin: markdown--><p>$$σ = \\sqrt{21704} ≈ 147,32,\\space{где}$$<br>\n$$σ\\space{–}\\space{стандартное}\\space{отклонение}$$</p>\n<!--kg-card-end: markdown--><p>Теперь мы знаем, какое отклонение от среднего значения является нормой, а какое – нет:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/variance-standard-deviation.png" class="kg-image" alt loading="lazy" width="1715" height="809" srcset="__GHOST_URL__/content/images/size/w600/2021/02/variance-standard-deviation.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/variance-standard-deviation.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/02/variance-standard-deviation.png 1600w, __GHOST_URL__/content/images/2021/02/variance-standard-deviation.png 1715w" sizes="(min-width: 1200px) 1200px"></figure><p>Выходит, что из всей выборки очень высоким и очень низким ростом обладают две собаки.</p><p>Фото: <a href="https://unsplash.com/@aaronphs">@aaronphs</a></p>		dispiersiia	2021-02-22		
61	Обучение с частичным привлечением учителя\t(Semi-Supervised Learning)	Обучение с частичным привлечением учителя (полуавтоматическое обучение, частичное обучение) – алгоритм Машинного обучения (Machine Learning), который использует как размеченные, так и неразмеченные данные.	<p>Обучение с частичным привлечением учителя (полуавтоматическое обучение, частичное обучение) &ndash; алгоритм <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (Machine Learning)</a>, который использует как размеченные, так и неразмеченные данные. Например, исследовав опухоли, установив их размер, плотность и другие метрики, мы передаем эти данные модели с обязательной пометкой, какое <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a> к какому строению (доброкачественному или злокачественному) относится. Размеченные медданные изображены на верхней таблице, на нижней &ndash; неразмеченная клиентская база:</p>\r\n<figure class="kg-card kg-image-card kg-width-full"><img class="kg-image" src="__GHOST_URL__/content/images/2021/02/unsupervised-learning-8.png" srcset="__GHOST_URL__/content/images/size/w600/2021/02/unsupervised-learning-8.png 600w, __GHOST_URL__/content/images/size/w1000/2021/02/unsupervised-learning-8.png 1000w, __GHOST_URL__/content/images/2021/02/unsupervised-learning-8.png 1484w" alt="" width="1484" height="915" loading="lazy" /></figure>\r\n<p>Полуавтоматическое обучение отличается от контролируемого, где используются только размеченные данные. Популярный подход здесь &ndash; это создание <a href="__GHOST_URL__/graf/">Графа (Graph)</a>, который группирует наблюдения в <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a> и присваивает соответствующие <a href="__GHOST_URL__/iarlyk/">Ярлыки (Label)</a> тем них, кто находится поблизости:</p>\r\n<figure class="kg-card kg-image-card kg-width-full"><img class="kg-image" src="__GHOST_URL__/content/images/2021/02/semi-supervised-learning-1.png" srcset="__GHOST_URL__/content/images/size/w600/2021/02/semi-supervised-learning-1.png 600w, __GHOST_URL__/content/images/2021/02/semi-supervised-learning-1.png 1000w" alt="" width="1000" height="644" loading="lazy" /></figure>\r\n<p>Алгоритм был предложен Сяоджином Чжу и Зубином Гахрамани в 2002 году в работе &laquo;Изучение данных с ярлыками и без с помощью распространения ярлыков&raquo;. Сгенерированные графы группируют все наблюдения в <a href="__GHOST_URL__/dataset/">Датасете (Dataset)</a> на основе их расстояния:</p>\r\n<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img class="kg-image" src="__GHOST_URL__/content/images/2021/02/semi-supervised-learning_grap-based-semi-supervised-learning-2.png" srcset="__GHOST_URL__/content/images/size/w600/2021/02/semi-supervised-learning_grap-based-semi-supervised-learning-2.png 600w, __GHOST_URL__/content/images/2021/02/semi-supervised-learning_grap-based-semi-supervised-learning-2.png 700w" alt="" width="700" height="400" loading="lazy" />\r\n<figcaption>Наблюдения с массивной белой обводкой с высокой вероятностью "принадлежат" кластерам</figcaption>\r\n</figure>\r\n<p>Распространение ярлыков (Label Propagation) подразумевает, что узлам графа присваиваются ярлыки, которые распространяются на неразмеченные элементы. Процесс повторяется фиксированное количество раз, чтобы поляризовать вероятность того или иного ярлыка, вплоть до <a href="__GHOST_URL__/skhodimost-convergence/" rel="noopener noreferrer">Сходимости (Convergence)</a>, то есть сокращения ошибок.</p>\r\n<h2 id="-scikit-learn">Контролируемое обучение и Scikit-learn</h2>\r\n<p>Продемонстрируем частичное обучение в сравнении с контролируемым с помощью Scikit-learn. Для начала импортируем необходимые библиотеки:</p>\r\n<pre><code class="language-python">from numpy import concatenate\r\n\r\nimport sklearn\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.semi_supervised import LabelPropagation</code></pre>\r\n<h3 id="-">Простая логистическая регрессия</h3>\r\n<p>Мы сгенерируем набор данных и установим для него базовый уровень производительности. С первой подзадачей нам поможет встроенная функция <code>make_classification()</code>. Создадим датасет из 1000 примеров с двумя классами и двумя признаками (Feature).</p>\r\n<pre><code class="language-python"># Сгенерируем данные\r\nX, y = make_classification(n_samples = 1000, n_features = 2, n_informative = 2, n_redundant = 0, random_state = 1)\r\n\r\n# Разделим датасет на тренировочную и тестовую части\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.50, random_state = 1, stratify = y)\r\n\r\n# Разделим тренировочную часть на размеченную и неразмеченную подгруппы\r\nX_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size = 0.50, random_state = 1, stratify = y_train)\r\n\r\n# Отобразим краткую сводку по тренировочной части датасета\r\nprint('Labeled Train Set:', X_train_lab.shape, y_train_lab.shape)\r\nprint('Unlabeled Train Set:', X_test_unlab.shape, y_test_unlab.shape)\r\n\r\n# Отобразим краткую сводку по тестовой части датасета\r\nprint('Test Set:', X_test.shape, y_test.shape)</code></pre>\r\n<p>Вывод ячейки подтверждает, что у нас есть маркированные и немаркированные наборы тренировочных данных по 250 наблюдений каждый, а также тестовая часть длиной в 500 строк:</p>\r\n<pre><code class="language-python">Размеченный тренировочный набор: (250, 2) (250,)\r\nНеразмеченный тренировочный набор: (250, 2) (250,)\r\nТестовый набор: (500, 2) (500,)</code></pre>\r\n<p>У контролируемого обучения (Supervised Learning) было бы всего 250 наблюдений, у полуконтролируемого &ndash; 250 размеченных и еще столько же неразмеченных.</p>\r\n<p>Используя алгоритм обучения с учителем, мы определим базовый уровень производительности для частичного обучения, и применим его к размеченной части данных. Это важно, потому что мы ожидаем, что алгоритм частичного обучения превзойдет алгоритм обучения с учителем. Если этого не случатся, тогда первый из них плохо настроен.</p>\r\n<p>Для первого кейса мы будем использовать алгоритм <a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/">Логистической регрессии (Logistic Regression)</a>, подходящий для размеченной части обучающего набора.</p>\r\n<pre><code class="language-python"># Инициализируем модель логистической регрессии\r\nmodel = LogisticRegression()\r\n\r\n# Загрузим в модель размеченные данные\r\nmodel.fit(X_train_lab, y_train_lab)</code></pre>\r\n<p>Модель продемонстрирует свои гиперпараметры, которые определяют стиль Штрафования (Penalty), веса классов Несбалансированного датасета (Imbalanced Dataset), количество задействованных ядер CPU (n_jobs) и т.д.:</p>\r\n<pre><code class="language-python">LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\r\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\r\n                   multi_class='auto', n_jobs=None, penalty='l2',\r\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\r\n                   warm_start=False)</code></pre>\r\n<p>Такую модель можно впоследствии использовать для прогнозирования неразделенной тестовой части датасета и сравнивать с использованием <a href="__GHOST_URL__/tochnost-izmierienii/">Точности измерений (Accuracy)</a>.</p>\r\n<pre><code class="language-python"># Выполним тестовые предсказания\r\nyhat = model.predict(X_test)\r\n\r\n# Вычислим скор для тестового набора данных\r\nscore = accuracy_score(y_test, yhat)\r\nprint('Точность измерений: %.3f' % (score * 100))</code></pre>\r\n<p>Для такого синтетического датасета точность вполне удовлетворительная:</p>\r\n<pre><code class="language-python">Точность измерений: 84.800</code></pre>\r\n<p>Результаты каждой попытки могут отличаться друг от друга: это происходит из-за стохастической (случайной) природы алгоритма и процедуры оценки. Для этого <a href="__GHOST_URL__/data-saiientist/">Дата-сайентисты (Data Scientist)</a> запускают обучение несколько раз.</p>\r\n<p>Теперь давайте посмотрим, как применить алгоритм распространения меток.</p>\r\n<h3 id="--1">Логистическая регрессия и распространение меток</h3>\r\n<p>Алгоритм распространения меток прекрасно реализован в классе <code>LabelPropagation</code>. Инициализируем модель распространения меток. Важно отметить, что тренировочная часть данных, передаваемая функцией <code>fit()</code>, должна включать и размеченные, и неразмеченные примеры; последние с ярлыками "-1":</p>\r\n<pre><code class="language-python"># Сгенерируем датасет\r\nX, y = make_classification(n_samples = 1000, n_features = 2, n_informative = 2, n_redundant = 0, random_state = 1)\r\n\r\n# Разделим датасет на тренировочную и тестовую части\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.50, random_state = 1, stratify = y)\r\n\r\n# Разделим тренировочную часть на размеченную и неразмеченную подгруппы\r\nX_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size = 0.50, random_state = 1, stratify = y_train)\r\n\r\n# create the training dataset input\r\nX_train_mixed = concatenate((X_train_lab, X_test_unlab))\r\n\r\n# Присвоим "-1" (отсутствие ярлыка) неразмеченной части датасета\r\nnolabel = [-1 for _ in range(len(y_test_unlab))]\r\n\r\n# Перекомбинируем ярлыки тренировочной части датасета \r\ny_train_mixed = concatenate((y_train_lab, nolabel))</code></pre>\r\n<p>Модель получает данные тем же образом &ndash; вызовом методов <code>fit()</code> и <code>predict()</code>:</p>\r\n<pre><code class="language-python"># Инициализируем модель распространения меток\r\nmodel = LabelPropagation()\r\n\r\n# Загрузим в модель тренировочные данные\r\nmodel.fit(X_train_mixed, y_train_mixed)\r\n\r\n# Выполним тестовые предсказания\r\nyhat = model.predict(X_test)</code></pre>\r\n<p>Точность измерений приятно подросла:</p>\r\n<pre><code class="language-python">Точность измерений: 85.600</code></pre>\r\n<p>Теперь, когда мы знаем, как использовать алгоритм распространения меток, давайте применим его к полуконтролируемому обучению.</p>\r\n<h1 id="--2">Обучение с частичным привлечением учителя</h1>\r\n<p>Мы имеем доступ к оценкам с помощью класса <code>transduction_</code> (англ, "передача"). Мы используем эти ярлыки вместе со всеми входными данными для обучения и оценки контролируемого алгоритма обучения. Еще раз сгенерируем данные:</p>\r\n<pre><code class="language-python"># Сгенерируем датасет\r\nX, y = make_classification(n_samples = 1000, n_features = 2, n_informative = 2, n_redundant = 0, random_state = 1)\r\n\r\n# Разделим датасет на тренировочную и тестовую части\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.50, random_state = 1, stratify = y)\r\n\r\n# Разделим тренировочную часть на размеченную и неразмеченную подгруппы\r\nX_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size = 0.50, random_state = 1, stratify = y_train)\r\n\r\n# create the training dataset input\r\nX_train_mixed = concatenate((X_train_lab, X_test_unlab))\r\n\r\n# Присвоим "-1" (отсутствие ярлыка) неразмеченной части датасета\r\nnolabel = [-1 for _ in range(len(y_test_unlab))]\r\n\r\n# Перекомбинируем ярлыки тренировочной части датасета \r\ny_train_mixed = concatenate((y_train_lab, nolabel))</code></pre>\r\n<p>Алгоритм обучает "полууправляемую" модель на целостном датасете и пользуется оценками <code>transduction_</code>:</p>\r\n<pre><code class="language-python"># Инициализируем модель распространения меток\r\nmodel = LabelPropagation()\r\n\r\n# Загрузим в модель тренировочные данные\r\nmodel.fit(X_train_mixed, y_train_mixed)\r\n\r\n# get labels for entire training dataset data\r\ntran_labels = model.transduction_\r\n\r\n# Инициализируем модель контролируемого обучения логистической регрессии\r\nmodel2 = LogisticRegression()\r\n\r\n# Загрузим в модель контролируемого обучения весь датасет\r\nmodel2.fit(X_train_mixed, tran_labels)\r\n\r\n# Выполним тестовые предсказания\r\nyhat = model2.predict(X_test)</code></pre>\r\n<p>Вычислим <a href="__GHOST_URL__/post/">Скор (Score)</a> для тестового набора данных:</p>\r\n<pre><code class="language-python">score = accuracy_score(y_test, yhat)\r\nprint('Точность измерений: %.3f' % (score * 100))</code></pre>\r\n<p>Такой иерархический подход обеспечивает наивысшую результативность предсказаний:</p>\r\n<pre><code class="language-python">Точность измерений: 86.200</code></pre>\r\n<p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1itqzvLYoXo_t-jBJq7c5gspbuy_V1lgL?usp=sharing">здесь</a>.</p>\r\n<p>Фото: <a href="https://unsplash.com/@sebastian_unrau">@sebastian_unrau</a></p>		obuchieniie-s-chastichnym-privliechieniiem-uchitielia	2021-02-25	Основы	https://plus.unsplash.com/premium_photo-1661893957671-2a88b1eb3b45?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3574&q=80
62	Как прекрасному полу осваивать Data Science?		<p>Наука о данных – это обширная область. Если взглянуть свысока, то дисциплин внутри Data Science немало: статистика, линейная алгебра, программирование, английский язык, теория вероятностей. Что же такое Наука о данных простыми словами? Это процесс постановки вопросов, и только потом использование того или иного инструмента. Процесс в общих чертах выглядит так:</p><ul><li>Ставим задачу</li><li>Получаем данные</li><li>Подготавливаем и исследуем их</li><li>Создаем модель Машинного обучения</li><li>Преподносим открытия-инсайты</li></ul><h3 id="-"><strong>Компоненты обучения</strong></h3><p>Компонент первый. <strong>Программирование</strong>. Парочка Python и R заняла свое прочное положение в Data Science, причем R популярен в академических кругах, а Python – в бизнесе. Моя рекомендация – выбрать один язык (лучше Python) и осваивать его с помощью курсов, хотя бы на 200 часов.Если вы ищете ресурсы, которые помогут справиться с обучением, вот список. Каждый непонятный термин прямо так и гуглится, с помощью специального запроса: 'site: <a href="http://machinelearningmastery.com/" rel="noopener noreferrer">machinelearningmastery.com</a> knn’.</p><figure class="kg-card kg-image-card"><img src="https://avatars.mds.yandex.net/get-zen_doc/4756594/pub_603a2350732f3c7f622034ba_603a24a649b4e72890696836/orig" class="kg-image" alt loading="lazy"></figure><p>Компонент второй. Изучите <strong>анализ, манипулирование данными и их визуализацию</strong> с помощью NumPy, Pandas и Scikit-learn. Это одни из самых популярных библиотек, и неспроста: они обеспечивают ясные и последовательные интерфейсы для множества различных моделей.[Пример синтаксиса]Библиотеки предлагают множество параметров настройки, и также выбирают разумные значения по умолчанию. Документация исключительная, и на <a href="http://stackoverflow.com/" rel="noopener noreferrer">stackoverflow.com </a>предостаточно ответов, чтобы реализовать самый тонкий нюанс.Как только вы почувствуете свободу с этими составляющими, то потихоньку сможете перейти к более широкому инструментарию: теории вероятностей, вычислительной статистике, глубинному обучению. Вот прекрасный способ освоить эти забористые дисциплины: когда поставленную задачу очень хочется решить, освоение высокоуровневой концепции пройдет легче, обещаю.Компонент третий. На всем этом пути важно помнить про <strong>психогигиену</strong>, это наш нитроускоритель. Талантливый психолог <a href="https://vk.com/akpsyh" rel="noopener noreferrer">Анастасия Калашникова</a> утверждает, что айтишники подвержены определенным профессиональным перекосам самовосприятия: «синдрому самозванца», «синдрому отличницы» и так далее. Это совет не психолога, но программиста, нашедшего свой отдушину в психологии – прочитайте одну-две работы об устройстве вашей психики, вроде книг Карен Хорни и Ирвина Ялома, и справляться с волнением перед выступлением и прочими неприятностями станет основательно легче.</p><figure class="kg-card kg-image-card"><img src="https://avatars.mds.yandex.net/get-zen_doc/1895194/pub_603a2350732f3c7f622034ba_603a251449b4e728906a3a93/orig" class="kg-image" alt loading="lazy"></figure><p>Компонент четвертый. <strong>Культура</strong>. Однако на пути дата-сайентистки есть дополнительные малоизученные препятствия. Статистика демонстрирует интересные цифры для женщин-дата-сайентисток: только 22% из них получают работу в сфере и удерживаются в ней:</p><figure class="kg-card kg-image-card"><img src="https://avatars.mds.yandex.net/get-zen_doc/1348874/pub_603a2350732f3c7f622034ba_603a258b2d4e6e7971e44b52/orig" class="kg-image" alt loading="lazy"></figure><p>На мой взгляд, не образование играет решающую роль в становлении женщины-профессионала, а среда, в которую она попадает. Статистику числа собеседований оглашать не любит никто, однако прежде чем случится трудоустройство, может пройти десять и более собеседований. Просто помните: вам не нужно все осваивать, чтобы начать карьеру в области науки о данных, просто нужно начать!</p><figure class="kg-card kg-image-card"><img src="https://avatars.mds.yandex.net/get-zen_doc/1593402/pub_603a2350732f3c7f622034ba_603a25eb82fc21754d5123b4/orig" class="kg-image" alt loading="lazy"></figure><p>Фото: Unsplash<a href="https://unsplash.com/@karsten116">@karsten116</a></p>		kak-osvaivat-data-science	2021-02-27		
63	Генеральная совокупность (Population)		<p>Генеральная совокупность (совокупность) – это все данные, из которых формируется статистическая <a href="__GHOST_URL__/vyborka/">Выборка (Sample)</a>. Она может относиться к группе людей, событий, явлений и т.д. Иными словами, это коллекция сгруппированных по общему признаку <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> за объектами:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/population.png" class="kg-image" alt loading="lazy" width="623" height="451" srcset="__GHOST_URL__/content/images/size/w600/2021/02/population.png 600w, __GHOST_URL__/content/images/2021/02/population.png 623w"></figure><p>Когда из совокупности производится выборка, появляется так называемая Стандартная ошибка (Standard Error), связанная с непроизвольной субъективностью статиста или <a href="__GHOST_URL__/data-saiientist/">Дата-сайентиста (Data Scientist)</a> при выполнении Сэмплирования (Sampling). К примеру, алгоритм случайного выбора <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочной (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовой (Test Data)</a> частей данных создан людьми и является <em>псевдослучайным</em>: </p><pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1)</code></pre><p>Когда мы работаем с генеральной совокупностью, то в отличие от выборки, не измеряем такую <a href="__GHOST_URL__/oshibka/">Ошибку (Error)</a> вообще.</p><p>Совокупность определяет количество <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, которые используют, чтобы делать выводы о предметах исследования. Например, совокупность данных о количестве новорожденных в СНГ, общее количество технологических стартапов в России, средний рост всех кандидатов в мастера спорта РФ и так далее.</p><p>Совокупность можно определить более конкретно, например, количество новорожденных в СНГ с карими глазами, количество стартапов в России, которые потерпели неудачу за последние три года, средний рост всех женщин-кандидаток в мастера спорта РФ.</p><p>В большинстве случаев исследователи хотят знать характеристики каждого наблюдения, чтобы сделать наиболее точный вывод. Однако в большинстве случаев это невозможно или непрактично, поскольку совокупность обычно довольно велика. Например, если компания хочет узнать, удовлетворен ли каждый из ее 50 000 клиентов, обслуживаемых в течение года, звонить каждому из клиентов по телефону для проведения опроса может быть сложно, дорого и непрактично. Поскольку характеристики каждого наблюдения в совокупности невозможно измерить из-за ограничений по времени и ресурсам, генерируется выборка.</p><h3 id="-">Параметры совокупности</h3><p>Статистические данные совокупности, такие как <a href="__GHOST_URL__/sriednieie-znachieniie/">Среднее значение (Average)</a> и <a href="__GHOST_URL__/standartnoie-otklonieniie/">Cтандартное отклонение (Standard Deviation)</a>, называются параметрами совокупности. </p><p>Фото: <a href="https://unsplash.com/@amandabereckonedwith">@amandabereckonedwith</a></p>		gienieralnaia-sovokupnost	2021-02-28		
64	Дата-сайентист (Data Scientist)		<p>Дата-сайентист – это специалист по обработке данных, собирающий и анализирующий большие наборы структурированных и неструктурированных данных. Эти специалисты анализируют, обрабатывают и моделируют данные, а затем интерпретируют результаты для создания действенных планов для компаний и других организаций.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/data-scientist-andrew-ng.jpg" class="kg-image" alt loading="lazy" width="1920" height="1080" srcset="__GHOST_URL__/content/images/size/w600/2021/03/data-scientist-andrew-ng.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/03/data-scientist-andrew-ng.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/data-scientist-andrew-ng.jpg 1600w, __GHOST_URL__/content/images/2021/03/data-scientist-andrew-ng.jpg 1920w" sizes="(min-width: 1200px) 1200px"><figcaption>Дата-сайентист, сооснователь Coursera Эндрю Ын</figcaption></figure><p>Это аналитики, которые используют свои навыки как в области технологий, так и в социальных науках, чтобы находить неочевидные взаимосвязи и управлять данными. Они используют отраслевые знания и даже скептицизм по отношению к существующим предположениям, чтобы найти решения бизнес-задач.</p><p>Однако технические навыки – не единственное, что имеет значение. Специалисты по обработке данных часто работают в бизнес-среде и отвечают за передачу сложных идей и принятие организационных решений на основе данных. В результате для них очень важно быть эффективными коммуникаторами, лидерами и членами команды, а также аналитическими мыслителями высокого уровня.</p><p>Перед опытными специалистами по обработке данных и администраторами данных стоит задача разработать передовые методы компании, от очистки до обработки и хранения данных. Они работают кросс-функционально с другими командами в рамках своей организации, такими как маркетинг, логистика и т.д. Специалисты в этой области очень востребованы в сегодняшней экономике, связанной с данными и высокими технологиями.</p><h3 id="-">Навыки</h3><p>Вот составляющие специальности "Специалист по данным":</p><ul><li>Программирование</li><li><a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a></li><li><a href="__GHOST_URL__/vizualizatsiia-dannykh/">Визуализация данных (Dataviz)</a></li><li>Анализ рисков</li><li><a href="__GHOST_URL__/statistika/">Статистика (Statistics)</a></li><li>Линейная алгебра (Linear Algebra)</li><li>Коммуникативные навыки</li><li>Программное обеспечение <a href="__GHOST_URL__/bolshiie-dannyie/">Больших данных (Big Data)</a></li><li>Облачные вычисления (Cloud Computing)</li></ul><h3 id="--1">Разновидности дата-сайентистов</h3><p><br>Специалисты по обработке данных могут специализироваться в определенной отрасли или развивать серьезные навыки в таких областях, как <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственный интеллект (AI)</a>, машинное обучение, исследования или управление базами данных. Согласно функциональной классификации выделяют три подвида специальностей:</p><ul><li>Дата-сайентист (Data Scientist): научные некоммерческие исследования</li><li>Дата-инженер (Data Engineer): создание программного обеспечения</li><li>Аналитик данных (Data Analyst): решение бизнес-задач</li></ul><p>Существует и другая классификация, основанная на сочетании навыков и выполняемых задач:</p><ul><li><em>Инженер по машинному обучению (Machine Learning Engineer – MLE)</em>. Работая на стыке программной инженерии и науки о данных, инженеры по машинному обучению владеют широким спектром программных инструментов и имеют большой опыт в предоставлении практических программных решений. Инженер по машинному обучению берет прототипную (теоретическую) модель, предложенную специалистом по данным, и делает ее пригодной для использования в производственной среде. MLE создают программы, управляющие устройствами, и разрабатывают алгоритмы, которые помогают машинам определять закономерности в своих данных, понимать команды и даже учиться принимать собственные решения.</li><li><em>Ученый в сфере Машинного обучения</em>. В отличие от инженеров машинного обучения, специализирующихся на создании инфраструктур машинного обучения, ученые, занимающиеся машинным обучением, сосредоточены на исследовании новых подходов и исследовании новых алгоритмов. Результатами работы специалиста по машинному обучению являются отчеты и технические документы.</li><li><em>Статистик</em>. Работает как в области теоретической, так и прикладной статистики с прицелом на достижение бизнес-целей. Используя математические методы, статистики анализируют, интерпретируют и сообщают статистическую информацию, а также делают важные для бизнеса выводы на основе этих данных.</li><li><em>Бизнес-аналитики</em>. Используя соответствующие инструменты или создавая пользовательские приложения для бизнес-аналитики, они создают стратегии, которые помогают бизнесу улучшить процесс принятия решений.</li><li><em>Архитектор данных</em>. Разрабатывая, создавая и поддерживая решения для архитектуры данных компании, архитекторы данных обеспечивают высокую доступность данных компании. Они проектируют, создают и поддерживают базы данных.</li><li><em>Архитектор облачной инфраструктуры</em>. Наблюдая за большими данными компании, облачными вычислениями или общей стратегией данных, архитектор инфраструктуры переводит бизнес-требования в конкретные системные приложения. Он или она следит за тем, чтобы бизнес-системы работали, отвечали необходимым системным требованиям и могли поддерживать новые технологии.</li><li><em>Архитектор приложений</em>. Эта роль включает разработку и создание новых приложений, а также мониторинг поведения существующих приложений в организации. Архитекторы приложений разрабатывают прототипы продуктов, проводят тесты, изучают, как их приложения взаимодействуют с пользователями, и создают руководства по разработке приложений.</li></ul><p> Фото: <a href="https://unsplash.com/@anniespratt">@anniespratt</a></p>		data-saiientist	2021-03-03		
65	Тренировочные данные (Train Data)		<p>Тренировочные данные (обучающие) – это часть <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, обучающая основа <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Является одной из составляющих разделенного набора данных наряду с <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовыми (Test Data)</a> и <a href="__GHOST_URL__/validatsionnyye-dannyye/">Валидационными (Validation Data)</a> данными.</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/train_data-1.png" class="kg-image" alt loading="lazy" width="1100" height="318" srcset="__GHOST_URL__/content/images/size/w600/2021/03/train_data-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/train_data-1.png 1000w, __GHOST_URL__/content/images/2021/03/train_data-1.png 1100w"><figcaption>Типы разделений датасета&nbsp;</figcaption></figure><p>Пример. Мы создаем модель, предсказывающую потребление электроэнергии в городе. Если на тренировочных данных она, подобно человеческому мозгу, учится видеть скачки потребления электричества, то на тестовой <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> проверяет качество обучения. Но зачем же нужна валидационная часть?</p><p>Когда модель пытается улучшиться, она подыскивает оптимальные значения <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a>. В этом случае существует вероятность <a href="__GHOST_URL__/pierieobuchieniie/">Переобучения (Overfitting)</a>: модель слишком подробно учитывает особенности информации, на которой обучилась, и при переходе на другие реальные данные будет малоэффективна. Чтобы решить эту проблему, мы можем создать дополнительную валидационную часть датасета, и проверять качество модели на любом подходящем этапе разработки.</p><p>Не существует четких правил относительно того, сколько данных нужно. Для тех случаев, когда модель должна быть высокоточной (например, для беспилотных автомобилей), потребуются огромные объемы данных, тогда как узкоспециализированный алгоритм распознавания текста требует гораздо меньший датасет. Зачастую исходное предположение о необходимом объеме тренировочных данных оказывается заниженным: для достижения приемлемой точности распознавания (70% и выше) простого <a href="__GHOST_URL__/chat-bot/">Чат-бота (Chat Bot)</a>, распознающего одно-два намерения пользователя, могут потребоваться десятки тысяч текстовых сообщений.</p><h3 id="-scikit-learn">Тестовые данные и Scikit-learn</h3><p>Библиотека Scikit-learn предлагает беспрецедентно минималистичный синтаксис встроенной функции <code>train_test_split()</code>, позволяющий разделить датасет на тренировочную и тестовую части за одну строку кода. Посмотрим, как это работает. </p><p>Стоит отметить также, что не во всех случаях простое случайное разделение датасета на части уместно: если речь идет о небольшом датасете, применяется так называемая Кросс-Валидация (Cross Validation): данные делят на k частей, и как бы попеременно используют каждую из них то в тестовых, то в валидационных целях, то как часть тренировочного компонента.</p><p>Для начала импортируем необходимые библиотеки</p><pre><code class="language-python">import numpy as np\nimport sklearn \nfrom sklearn.model_selection import train_test_split</code></pre><p>Создадим датасет-лилипут с помощью <code>numpy.arange()</code> – метод сгенерирует <a href="__GHOST_URL__/priediktor/">Переменные-предикторы (Predictor Variable)</a>, ряд из десяти упорядоченных чисел с одинаковым интервалом, в данном случае, от 0 до 9. Метод <code>reshape()</code> сделает из полученного ряда матрицу 5 х 2, а встроенная Python-функция <code>range()</code>, отвечающая за целевую переменную, сгенерирует пять ярлыков <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> y – числа от 0 до 4 включительно.</p><pre><code class="language-python">X, y = np.arange(10).reshape((5, 2)), range(5)\nX</code></pre><p>Посмотрим, что мы "положили" в объект <code>X</code>:</p><pre><code class="language-python">array([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])</code></pre><p>И в <code>y</code>, в свою очередь:</p><pre><code class="language-python">list(y)</code></pre><p>Элегантная функция <code>range()</code> позволила нам сгенерировать вот такой ряд максимально кратко и без циклов, и это прекрасно:</p><pre><code class="language-python">[0, 1, 2, 3, 4]</code></pre><p>Теперь разделим наш датасет на тренировочную и тестовую части. Сниппет ниже – это пример обращения ко встроенной функции библиотеки <code>train_test_split()</code>. Мы имеем дело с размеченным датасетом, поскольку указываем переменные-предикторы <code>X</code>, и целевую переменную <code>y</code>. Размер тестовой части – 33% от общего размера датасета. <code>random_state = 42</code> задает случайный характер разделения данных:</p><pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)</code></pre><p>В первую очередь, алгоритм функции выбирает тестовые 33% случайным образом:</p><pre><code class="language-python">X_test</code></pre><p>Треть данных – это всего две строки, когда речь идет о датасете 5 x 3:</p><pre><code class="language-python">array([[2, 3],\n       [8, 9]])</code></pre><p>Вызовем тестовую часть данных целевой переменной – это <a href="__GHOST_URL__/iarlyk/">Ярлыки (Label)</a> двух рядов <code>X_test</code>:</p><pre><code class="language-python">y_test</code></pre><p>Ряды <code>[2, 3], [8, 9]</code> принадлежат классам 1 и 4 соответственно:</p><pre><code class="language-python">[1, 4]</code></pre><p>Разделив датасет на четыре части, мы вызываем тренировочную часть предикторов:</p><pre><code class="language-python">X_train</code></pre><p>Этот компонент – остаток тренировочных данных после выборки 33% тестовых <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>:</p><pre><code class="language-python">array([[4, 5],\n       [0, 1],\n       [6, 7]])</code></pre><p>Тоже самое для тренировочной части целевой переменной:</p><pre><code class="language-python">y_train</code></pre><p><code>y_train</code> – это Ярлыки (Label), которые соответствуют порядковому номеру рядов X_train:</p><pre><code class="language-python">[2, 0, 3]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1xnYMZknbYufCrf-RY_G7DQDpo9ZFoaUv?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@liuchen37">@liuchen37</a></p>		trienirovochnyie-dannyie	2021-03-04		
66	Валидационные данные (Validation Data)		<p>Валидационные данные (Validation Data, Holdout Data – "удержанные" данные) – это часть <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, основа для проверки работоспособности <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Является одной из составляющих разделенного набора данных наряду с <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочными (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовыми (Test Data)</a> данными.</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/validation_data_split-types.png" class="kg-image" alt loading="lazy" width="1100" height="318" srcset="__GHOST_URL__/content/images/size/w600/2021/03/validation_data_split-types.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/validation_data_split-types.png 1000w, __GHOST_URL__/content/images/2021/03/validation_data_split-types.png 1100w"><figcaption>Типы разделений датасета&nbsp;</figcaption></figure><p>Пример. Мы создаем модель, предсказывающую потребление электроэнергии в городе. Если на тренировочных данных она, подобно человеческому мозгу, учится видеть скачки потребления электричества, то на тестовой <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> проверяет качество обучения. Но зачем же нужна валидационная часть?</p><p>Когда модель пытается улучшиться, она подыскивает оптимальные значения <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a>. В этом случае существует вероятность <a href="__GHOST_URL__/pierieobuchieniie/">Переобучения (Overfitting)</a>: модель слишком подробно учитывает особенности информации, на которой обучилась, и при переходе на другие реальные данные будет малоэффективна. Чтобы решить эту проблему, мы можем создать дополнительную валидационную часть датасета, и проверять качество модели на любом подходящем этапе разработки. </p><p>В основе всех методов проверки лежит разделение данных при обучении модели. Принцип прост: мы случайным образом разбиваем данные в пропорции 70 : 30, причем большая часть отводится тренировочным данным, а меньшая – тестовым. Но что если в одно подмножество тренировочных данных попали энергопотребители только определенного возраста или уровня дохода? Эта систематическая ошибка называется Смещением выборки (Sampling Bias). В результате неслучайной Выборки (Sampling) из <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population)</a> вероятность включения одних типов Наблюдений (Observation) в выборку ниже, чем у других, что приводит к искаженному восприятию реальности моделью.</p><h3 id="-">Виды валидации</h3><p>Выделяют следующие методы проверки:</p><ul><li>Разделение данных (Train / Test Split)</li><li>k-блочная кросс-валидация (k-Fold Cross Validation)</li><li><a href="__GHOST_URL__/poeliemientnaia/">Поэлементная кросс-валидация (LOOCV)</a></li><li>Кросс-валидация по отдельным группам (Leave-One-Group-Out Cross-Validation)</li><li>Кросс-валидация <a href="__GHOST_URL__/vriemiennoi-riad/">Временных рядов (Time Series)</a></li><li>Тест знаковых рангов Уилкоксона (Wilcoxon Signed-Rank Test)</li><li>Тест Макнемара (McNemar’s Test)</li><li>Парный T-тест 5x2CV (5x2CV Paired T-Test)</li><li>Комбинированный F-тест 5x2CV (5x2CV Combined F-Test)</li></ul><p>Посмотрим, как работает одна из этих техник.</p><h3 id="k-">k-блочная кросс-валидация</h3><p>Чтобы свести к минимуму систематическую ошибку выборки, мы изменим формат разделения данных Вместо того, чтобы делать одно разбиение на тренировочную и тестовую части, мы сделаем их много и проверим модель на каждой комбинации:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/validation_data.png" class="kg-image" alt loading="lazy" width="1100" height="426" srcset="__GHOST_URL__/content/images/size/w600/2021/03/validation_data.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/validation_data.png 1000w, __GHOST_URL__/content/images/2021/03/validation_data.png 1100w"></figure><p>Преимущество заключается в том, что все наблюдения используются как для обучения, так и для проверки, а каждое наблюдение используется один раз для проверки. Обычно мы разбиваем датасет на 5 или 10 частей: это обеспечивает баланс между вычислительной сложностью и точностью.</p><h3 id="k-scikit-learn">k-блочная кросс-валидация и Scikit-learn</h3><p>Продемонстрируем, как реализована такая валидация в библиотеке Scikit-learn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport sklearn\nfrom sklearn.model_selection import KFold</code></pre><p>Создадим игрушечный размеченный датасет, где <code>X</code> – матрица 4 х 2, состоящая из <a href="__GHOST_URL__/priediktor/">Переменных-предикторов (Predictor Variable)</a>, а <code>y</code> – <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевая переменная (Target Variable)</a>, классы, к которым принадлежит то или иное наблюдение:</p><pre><code class="language-python">X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4])</code></pre><p>Применим k-блочную кросс-валидацию в две итерации:</p><pre><code class="language-python">kf = KFold(n_splits = 2)</code></pre><p>Выведем индексы тренировочных и тестовых данных для каждой из двух итераций:</p><pre><code class="language-python">for train_index, test_index in kf.split(X):\n    print("Тренировочные данные:", train_index, "Тестовые данные:", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]</code></pre><p>Поскольку датасет скромный, то вариантов разбиений немного: то в тестовую часть попадают 2-й и 3-й наблюдения, то 0-й и 1-й.</p><pre><code class="language-python">Тренировочные данные: [2 3] Тестовые данные: [0 1]\nТренировочные данные: [0 1] Тестовые данные: [2 3]</code></pre><p>Использование полного объема данных – это хорошо для модели, потому такая валидация улучшает предсказательную способность модели.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1bKrq_lYo-LqD0lS4EWPTSkwts--l1LJD?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@shotbycerqueira">@shotbycerqueira</a></p>		validatsionnyye-dannyye	2021-03-05		
67	Тестовые данные (Test Data)		<p>Тестовые данные – это часть <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, проверяющая основа <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Является одной из составляющих разделенного набора данных наряду с <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочными (Train Data)</a> и <a href="__GHOST_URL__/validatsionnyye-dannyye/">Валидационными (Validation Data)</a> данными.</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/train_data-1.png" class="kg-image" alt loading="lazy" width="1100" height="318" srcset="__GHOST_URL__/content/images/size/w600/2021/03/train_data-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/train_data-1.png 1000w, __GHOST_URL__/content/images/2021/03/train_data-1.png 1100w"><figcaption>Типы разделений датасета&nbsp;</figcaption></figure><p>Пример. Мы создаем модель, предсказывающую потребление электроэнергии в городе. Если на тренировочных данных она, подобно человеческому мозгу, учится видеть скачки потребления электричества, то на тестовой <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> проверяет качество обучения. </p><p>Тестовую часть информации используют в качестве золотого стандарта для сравнения различных конкурирующих моделей (например, на соревнованиях Kaggle). Наивысший результат во время такого теста и определяет наиболее эффективную модель. Набор для тестирования обычно формируется случайным способом и содержит <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> различных классов, с которыми может столкнуться модель при использовании в реальном мире.</p><h3 id="-scikit-learn">Тестовые данные и Scikit-learn</h3><p>Библиотека Scikit-learn предлагает беспрецедентно минималистичный синтаксис встроенной функции <code>train_test_split()</code>, позволяющий разделить датасет на тренировочную и тестовую части за одну строку кода. Посмотрим, как это работает. </p><p>Стоит отметить также, что не во всех случаях простое случайное разделение датасета на части уместно: если речь идет о небольшом датасете, применяется так называемая Кросс-Валидация (Cross Validation): данные делят на k частей, и как бы попеременно используют каждую из них то в тестовых, то в валидационных целях, то как часть тренировочного компонента.</p><p>Для начала импортируем необходимые библиотеки</p><pre><code class="language-python">import numpy as np\nimport sklearn \nfrom sklearn.model_selection import train_test_split</code></pre><p>Создадим датасет-лилипут с помощью <code>numpy.arange()</code> – метод сгенерирует <a href="__GHOST_URL__/priediktor/">Переменные-предикторы (Predictor Variable)</a>, ряд из десяти упорядоченных чисел с одинаковым интервалом, в данном случае, от 0 до 9. Метод <code>reshape()</code> сделает из полученного ряда матрицу 5 х 2, а встроенная Python-функция <code>range()</code>, отвечающая за целевую переменную, сгенерирует пять ярлыков <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> y – числа от 0 до 4 включительно.</p><pre><code class="language-python">X, y = np.arange(10).reshape((5, 2)), range(5)\nX</code></pre><p>Посмотрим, что мы "положили" в объект <code>X</code>:</p><pre><code class="language-python">array([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])</code></pre><p>И в <code>y</code>, в свою очередь:</p><pre><code class="language-python">list(y)</code></pre><p>Элегантная функция <code>range()</code> позволила нам сгенерировать вот такой ряд максимально кратко и без циклов, и это прекрасно:</p><pre><code class="language-python">[0, 1, 2, 3, 4]</code></pre><p>Теперь разделим наш датасет на тренировочную и тестовую части. Сниппет ниже – это пример обращения ко встроенной функции библиотеки <code>train_test_split()</code>. Мы имеем дело с размеченным датасетом, поскольку указываем переменные-предикторы <code>X</code>, и целевую переменную <code>y</code>. Размер тестовой части – 33% от общего размера датасета. <code>random_state = 42</code> задает случайный характер разделения данных:</p><pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)</code></pre><p>В первую очередь, алгоритм функции выбирает тестовые 33% случайным образом:</p><pre><code class="language-python">X_test</code></pre><p>Треть данных – это всего две строки, когда речь идет о датасете 5 x 3:</p><pre><code class="language-python">array([[2, 3],\n       [8, 9]])</code></pre><p>Вызовем тестовую часть данных целевой переменной – это Ярлыки (Label) двух рядов <code>X_test</code>:</p><pre><code class="language-python">y_test</code></pre><p>Ряды <code>[2, 3], [8, 9]</code> принадлежат классам 1 и 4 соответственно:</p><pre><code class="language-python">[1, 4]</code></pre><p>Разделив датасет на четыре части, мы вызываем тренировочную часть предикторов:</p><pre><code class="language-python">X_train</code></pre><p>Этот компонент – остаток тренировочных данных после выборки 33% тестовых <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>:</p><pre><code class="language-python">array([[4, 5],\n       [0, 1],\n       [6, 7]])</code></pre><p>Тоже самое для тренировочной части целевой переменной:</p><pre><code class="language-python">y_train</code></pre><p><code>y_train</code> – это Ярлыки (Label), которые соответствуют порядковому номеру рядов X_train:</p><pre><code class="language-python">[2, 0, 3]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1xnYMZknbYufCrf-RY_G7DQDpo9ZFoaUv?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@editholic7">@editholic7</a></p>		tiestovyie-dannyie	2021-03-08		
68	Точечная диаграмма (Scatterplot)		<p>Точечная диаграмма – это график, на котором каждое значение представлено точкой в n-мерном пространстве:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/scatterplot.png" class="kg-image" alt loading="lazy" width="720" height="512" srcset="__GHOST_URL__/content/images/size/w600/2021/03/scatterplot.png 600w, __GHOST_URL__/content/images/2021/03/scatterplot.png 720w" sizes="(min-width: 720px) 720px"><figcaption>Точечная диаграмма в двумерном пространстве</figcaption></figure><p>Точечная диаграмма активно используется в <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a> в самых разных визуализационных целях:</p><ul><li>Для оценки качества Кластеризации (Clustering)</li><li>Для оценки скученности <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a></li><li>Для оценки корреляции переменных</li><li>Для обнаружения <a href="__GHOST_URL__/vybros/">Выбросов (Outlier)</a></li><li>Для обнаружения ошибок в данных и проч.</li></ul><h3 id="-matplotlib">Точечная диаграмма и Matplotlib</h3><p>Точечную диаграмму легко создать с помощью Matplotlib. Для начала импортируем интерфейс Pyplot:</p><pre><code class="language-python">import matplotlib.pyplot as plt\n</code></pre><p>Используем случайные данные в качестве координат точек. Каждому n-му значению координаты x соответствует n-е значение y:</p><pre><code class="language-python">x = [16, 7, 28, 11, 9, 29, 22, 7, 29, 13, 15, 25, 17]\ny = [20, 13, 26, 22, 13, 24, 4, 25, 26, 24, 12, 17, 2]</code></pre><p>Зададим базовые характеристики – размер полотна графика (<code>["figure.figsize'] = (25, 10)</code>), размер точки (<code>s = 60</code>):</p><pre><code class="language-python">plt.scatter(x, y, s = 60)\nplt.rcParams["figure.figsize"] = (25, 10)\nplt.show()</code></pre><p>Мы получаем классическую точечную диаграмму без корреляций:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/scatterplot-matplotlib-1.png" class="kg-image" alt loading="lazy" width="1629" height="775" srcset="__GHOST_URL__/content/images/size/w600/2021/03/scatterplot-matplotlib-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/scatterplot-matplotlib-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/scatterplot-matplotlib-1.png 1600w, __GHOST_URL__/content/images/2021/03/scatterplot-matplotlib-1.png 1629w" sizes="(min-width: 1200px) 1200px"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1UJ5-ng_zj2EMiq8j9vvkMIrS1nVi6eiF?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@clemono">@clemono</a></p>		tochechnaya-diagramma	2021-03-12		
69	Временной ряд (Time Series)		<p>Временной ряд – совокупность <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>, собранных за определенный временной интервал. Этот тип данных используется для поиска долгосрочного тренда, прогнозирования будущего и прочих видов анализа. В отличие от <a href="__GHOST_URL__/dataset/">Датасетов (Dataset)</a> без временных рядов в качестве <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, наборы с временными рядами не выполняют основное требование линейной регрессии о независимости наблюдений.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/time-series.png" class="kg-image" alt loading="lazy" width="2000" height="770" srcset="__GHOST_URL__/content/images/size/w600/2021/03/time-series.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/time-series.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/time-series.png 1600w, __GHOST_URL__/content/images/2021/03/time-series.png 2042w" sizes="(min-width: 1200px) 1200px"><figcaption>Временной ряд – стоимость акции компании LG на моменты открытия и закрытия биржи</figcaption></figure><p>Наряду с тенденцией увеличиваться или уменьшаться, большинство датасетов с временными рядами демонстрируют сезонные тенденции. Например, изучая сбыт пуховиков на протяжении пяти лет, мы увидим более высокий их уровень продаж в холодное время года каждый год.</p><p>Такие свойства временного ряда коренным образом меняют характер взаимодействия при создании <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Прежде чем использовать такие данные для обучения <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сети (Neural Network)</a>, стоит добиться так называемой Стационарности (Stationarity).</p><h3 id="-">Стационарность</h3><p>Стационарность – это свойство временного ряда, постоянство его статистических свойств: <a href="__GHOST_URL__/sriednieie-znachieniie/">Cреднего значения (Average)</a> и <a href="__GHOST_URL__/dispiersiia/">Дисперсии (Variance)</a>. Это важно, поскольку большинство моделей работают, исходя из предположения о стационарности. Предполагается, что, если временной ряд ведет себя определенным образом, очень высока вероятность того, что он повторит те же паттерны в будущем. Стационарные ряды к тому же более просты в обработке.</p><h3 id="-statsmodels">Временные ряды и statsmodels</h3><p>Поработаем с временным рядом – хронологией стоимости акции компании LG. Для начала протестируем данные на стационарность, то есть пригодность к использованию в качестве обучающих данных модели машинного обучения и предскажем стоимость акции с помощью <a href="__GHOST_URL__/modiel-boksa-dzhienkinsa/">Модели Бокса — Дженкинса (ARIMA)</a>. Для начала импортируем все необходимые библиотеки:</p><pre><code class="language-python">import matplotlib.pyplot as plt\nfrom numpy import log\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima_model import ARIMA</code></pre><p>Помимо классических библиотек NumPy и Pandas нам понадобятся также классы Среднеквадратической ошибки (Mean Squared Error), теста Дики-Фуллера и непосредственно модели ARIMA. Теперь загрузим в ноутбук хронологию стоимости акции за текущий торговый год, то есть за период 01.01.2021 по 13.03.2021:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/j04e6thkqmk02z1/LPL.csv?dl=1')\ndf.head()</code></pre><p>Мы получили такой игрушечный датасет с помощью сервиса Yahoo. Finance, и набор отображаемых признаков обусловлен политикой сервиса:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/time-series-lg-yahoo-finance.png" class="kg-image" alt loading="lazy" width="549" height="228"></figure><p>Отобразим с помощью Линейной диаграммы (Line Plot) цену акции на момент открытия биржи:</p><pre><code class="language-python">plt.plot(df['Open'])\nplt.figure(figsize = (25, 10)) # Размер графика в дюймах\nplt.show()</code></pre><p>За два с небольшим месяца стоимость акции колебалась в пределах 8.5 – 11.5 долларов за штуку:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/time-series-lg-open-2021-1.png" class="kg-image" alt loading="lazy" width="1638" height="775" srcset="__GHOST_URL__/content/images/size/w600/2021/03/time-series-lg-open-2021-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/time-series-lg-open-2021-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/time-series-lg-open-2021-1.png 1600w, __GHOST_URL__/content/images/2021/03/time-series-lg-open-2021-1.png 1638w" sizes="(min-width: 1200px) 1200px"></figure><p>Именно с такими данными мы и будем создавать модель машинного обучения. Прежде чем приступить к построению модели, стоит выяснить, являются ли такие данные стационарными? В этом поможет тест Дики-Фуллера. Мы выберем целевой признак (Target Feature), значение которого впоследствии и будем предсказывать, – это цена открытия (<code>Open</code>) и сравним коэффициент такого теста с критическими значениями уровней Статистической значимости (Statistical Significance). Чтобы нейтрализовать восходящий тренд, мы логарифмизируем данные с помощью <code>np.log</code>, тем самым приведем более высокие поздние значения к масштабу более ранних:</p><pre><code class="language-python">Y = df['Open'].values # Выделим целевой признак \nY = log(Y)\nresult = adfuller(Y)\nprint('Коэффициент расширенного теста Дики-Фуллера: %f' % result[0])\nprint('Критические значения: %f' % result[1])\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))</code></pre><p>Итак, метрика теста равна 0,220931, и само по себе это значение не очень показательно. Оно работает в сравнении с критическими значениями Статистической значимости. Итак, <a href="__GHOST_URL__/statistika/">Статистика (Statistics)</a> теста равна 0.220931, и это значение больше всех трех значениями уровней статистической значимости, что означает стационарность.</p><pre><code class="language-python">Коэффициент расширенного теста Дики-Фуллера: -2.160400\nКритические значения: 0.220931\n\t1%: -3.578\n\t5%: -2.925\n\t10%: -2.601</code></pre><p>Настало время самой задорной части: предскажем стоимость акции. Для этого разделим датасет на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочную (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовую части (Test Data)</a> в пропорции 80 на 20. Для наглядности построим график, где эти части будут окрашены синим и красным соответственно:</p><pre><code class="language-python">train_data, test_data = df[0:int(len(df) * 0.8)], df[int(len(df) * 0.8):]\nplt.figure(figsize = (25, 10))\nplt.title('Стоимость акций LG') # Название графика\nplt.xlabel('Даты') # Название оси X\nplt.ylabel('Цены') # Название оси Y\nplt.plot(df['Open'], 'blue', label = 'Тренировочные данные') # Левая синяя часть линейного графика\nplt.plot(test_data['Open'], 'red', label = 'Тестовые данные') # Правая красная часть линейного графика\nplt.legend()</code></pre><p>Мы получили вот такое разделение:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/time-series-lg-train-test.png" class="kg-image" alt loading="lazy" width="1652" height="805" srcset="__GHOST_URL__/content/images/size/w600/2021/03/time-series-lg-train-test.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/time-series-lg-train-test.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/time-series-lg-train-test.png 1600w, __GHOST_URL__/content/images/2021/03/time-series-lg-train-test.png 1652w" sizes="(min-width: 1200px) 1200px"></figure><p>Определим функцию <code>smape_kun()</code>, которая поможет в дальнейшем охарактеризовать эффективность предсказаний модели – Симметричная средняя абсолютная ошибка в процентах (SMAPE):</p><pre><code class="language-python">def smape_kun(y_true, y_pred):\n    return np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))))</code></pre><p>Создадим пустые списки <code>predictions</code> и <code>history</code>, которые будем пополнять предсказаниями, и запустим цикл длительностью в тестовую часть датасета:</p><pre><code class="language-python">train_ar = train_data['Open'].values\ntest_ar = test_data['Open'].values\n\nhistory = [x for x in train_ar]\npredictions = list()\nfor t in range(len(test_ar)):\n    model = ARIMA(history, order = (5, 1, 0)) # Параметризуем модель\n    model_fit = model.fit(disp = 0) # Передадим обучающие данные\n    output = model_fit.forecast() # Сгенерируем предсказания\n    yhat = output[0] # Сделаем первое предсказание отдельной переменной\n    predictions.append(yhat) # Добавим в список предсказаний первое значение\n    obs = test_ar[t] # Создадим служебную переменную obs (observation)\n    history.append(obs) # Пополним служебной переменной список history \n\n# Определим эффективность модели    \nerror = mean_squared_error(test_ar, predictions)\nprint('Тестовая среднеквадратическая ошибка: %.3f' % error)\nerror2 = smape_kun(test_ar, predictions)\nprint('Симметричная средняя абсолютная ошибка в процентах: %.3f' % error2)</code></pre><p>Данных было немного, однако ошибка небольшая (измеряется в процентах):</p><pre><code class="language-python">Тестовая среднеквадратическая ошибка: 0.030\nСимметричная средняя абсолютная ошибка в процентах: 2.269</code></pre><p>Изучим сгенерированный список предсказаний, вызвав наполненный список <code>predictions</code>:</p><pre><code class="language-python">predictions</code></pre><p>Мир неидеален, потому наш список наполнился предсказаниями – массивами в один элемент, что может сгененировать новичку дополнительную работу в будущем. Но, к счастью, не в нашем случае:</p><pre><code class="language-python">[array([9.93664905]),\n array([10.25150669]),\n array([10.31789186]),\n array([10.13737506]),\n array([10.07325307]),\n array([9.96248479]),\n array([9.79555676]),\n array([9.83781138]),\n array([9.63763933]),\n array([9.89237211])]\n</code></pre><p>Для пущей наглядности визуализируем тестовые (реальные) значения стоимости акции и предсказанные:</p><pre><code class="language-python">plt.figure(figsize = (25, 10))\nplt.plot(df['Open'], 'green', color = 'blue', label = 'Тренировочные данные') # Обучающие данные\nplt.plot(test_data.index, predictions, color = 'green', marker = 'o', \n         linestyle = 'dashed', label = 'Предказанная цена') # Предсказания модели\nplt.plot(test_data.index, test_data['Open'], color = 'red', label = 'Реальная цена') # Реальные тестовые данные\nplt.title('Предсказанные цены акции LG') \nplt.xlabel('Даты')\nplt.ylabel('Цены')\nplt.xticks(np.arange(0, 60, 10), df['Date'][0:60:10]) # Зададим даты как подписи оси X\nplt.legend() # Отобразим легенду диаграммы</code></pre><p>Сейчас гораздо лучше видно, что модель не очень точна, вероятно, из-за малого объема тренировочных данных, но сейчас это не главное. Мы научились готовить данные с временными рядами к загрузке в модель.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/time-series-lg-predictions-1.png" class="kg-image" alt loading="lazy" width="1652" height="805" srcset="__GHOST_URL__/content/images/size/w600/2021/03/time-series-lg-predictions-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/time-series-lg-predictions-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/time-series-lg-predictions-1.png 1600w, __GHOST_URL__/content/images/2021/03/time-series-lg-predictions-1.png 1652w" sizes="(min-width: 1200px) 1200px"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1W-lSkt-j1Mujkp2qiA77SAvqHhTb32fn?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@diegojimenez">@diegojimenez</a></p>		vriemiennoi-riad	2021-03-14		
70	P-значение (P-Value)		<p>P-значение (значение вероятности) – это вероятность появления экстремального <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> при условии истинности <a href="__GHOST_URL__/nulievaia-ghipotieza/">Нулевой гипотезы (Null Hypothesis)</a>:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/p-value.png" class="kg-image" alt loading="lazy" width="1274" height="510" srcset="__GHOST_URL__/content/images/size/w600/2021/03/p-value.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/p-value.png 1000w, __GHOST_URL__/content/images/2021/03/p-value.png 1274w" sizes="(min-width: 1200px) 1200px"><figcaption>P-значение характеризует площадь розовой фигуры (в процентах) от кривой распределения</figcaption></figure><p>P-значение – это вероятность появления экстремального значения при пополнении исследуемой <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a> или <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population)</a>. P-значение по умолчанию считают равным 0,05 или 0,01.</p><p>P-значения используются при проверке истинности нулевых гипотез: чем оно меньше, тем больше вероятность, что вы отклоните нулевую гипотезу.</p><h3 id="%D0%BD%D1%83%D0%BB%D0%B5%D0%B2%D0%B0%D1%8F-%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7%D0%B0">Нулевая гипотеза</h3><p>Нулевая гипотеза – предположение, которое предстоит тестировать на истинность в рамках статистического анализа. </p><p>Пример. Мы хотим знать, есть ли разница в продолжительности жизни между двумя группами мышей, питавшихся по диетам A и B. </p><p><em>Нулевая гипотеза</em>: диета не имеет влияния; нет разницы в продолжительности жизни между двумя группами.</p><p><em>Альтернативная гипотеза</em>: диета имеет значение; между двумя группами существует разница в продолжительности жизни.</p><p>В нашем сравнении типов мышиного питания мы обнаружили, что средняя продолжительность жизни равна:</p><ul><li>2,1 годам, <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a> – 0,12 (A)</li><li>2,6 года, стандартное отклонение равно 0,1 (B)</li></ul><p>Более того, среди мышей, питающихся по типу A, нашлось менее 1% таких, что жили меньше 1,848 или больше 2,352 лет (то есть 2,1 ± 12%). То же верно и для группы B: в ней менее 1% примеров, когда мышь прожила менее 2,34 и более 2,86 года (то есть 2,6 ± 10%).</p><p>Наше сравнение двух диет для мышей дает p-значение менее 0,01, что ниже нашего порога по умолчанию – 0,05. Таким образом, мы определяем, что статистически значимая разница между двумя диетами <em>существует</em>.</p><h3 id="%D1%81%D1%82%D0%B0%D0%BD%D0%B4%D0%B0%D1%80%D1%82%D0%B8%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%B0%D1%8F-%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B0">Стандартизованная оценка</h3><p>Зная <a href="__GHOST_URL__/standartizovannaia-otsienka/" rel="noopener noreferrer">Стандартизованную оценку (Z-score)</a>, мы можем вычислить занимаемую площадь ярко-розовой фигуры (p-значение). Например, z-оценка равна -0,37 (подробнее в статье о стандартизованной оценке):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/02/z-score-2.png" class="kg-image" alt loading="lazy" width="700" height="327" srcset="__GHOST_URL__/content/images/size/w600/2021/02/z-score-2.png 600w, __GHOST_URL__/content/images/2021/02/z-score-2.png 700w"></figure><p>Чтобы уточнить площадь ярко-розовой части фигуры, используется таблица z-оценок, в данном случае, для отрицательных значений коэффициента. Для числа '-0,37' мы должны найти строку '0.3' в левом столбце z и столбец с названием '0,7':</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/02/z-score-lookup-table.png" class="kg-image" alt loading="lazy" width="725" height="1758" srcset="__GHOST_URL__/content/images/size/w600/2021/02/z-score-lookup-table.png 600w, __GHOST_URL__/content/images/2021/02/z-score-lookup-table.png 725w"></figure><p>На пересечении ряда '-0,3' и столбца '0.07' находится число 0,35569, и это не только площадь фигуры, но и, к примеру, долю студентов <em>за</em> этой чертой успеваемости. Таким способом вычисляют самых способных.</p><p>Фото: <a href="https://unsplash.com/@jeremybishop">@jeremybishop</a></p>		p-znachieniie	2021-03-19		
71	Нулевая гипотеза (Null Hypothesis)		<p>Нулевая гипотеза (H<sub>0</sub>) – предположение, которое предстоит тестировать на истинность в рамках статистического анализа. Для наглядности ее, как правило, иллюстрируют примером в сочетании с <a href="__GHOST_URL__/altiernativnaia-ghipotieza/">Альтернативной гипотезой (Alternative Hypothesis)</a>.</p><p>Пример. Производитель мыла утверждает, что ее продукт убивает в среднем 99% микробов. Чтобы проверить заявление этой компании, мы сформулируем нулевую и альтернативную гипотезы.</p><p>Нулевая: Среднестатистическое количество уничтоженных мылом микробов равно 99%.</p><p>Альтернативная: Мыло в среднем уничтожает менее 99% процентов микробов.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/null-hypothesis_definitive_definitive.png" class="kg-image" alt loading="lazy" width="1000" height="588" srcset="__GHOST_URL__/content/images/size/w600/2021/03/null-hypothesis_definitive_definitive.png 600w, __GHOST_URL__/content/images/2021/03/null-hypothesis_definitive_definitive.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>Мы доверяем нулевой гипотезе до тех пор, пока в <a href="__GHOST_URL__/vyborka/">Выборке (Sample)</a> достаточно доказательств, подтверждающих ее истинность. В ином случае мы отвергаем нулевую гипотезу и поддерживаем альтернативную. Если выборка не может предоставить достаточных доказательств для того, чтобы отвергнуть нулевую гипотезу, нам придется изучить несколько других выборок или всю <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральную совокупность (Population)</a>.</p><h3 id="%D0%BF%D1%80%D0%BE%D1%81%D1%82%D0%B0%D1%8F-%D0%B8-%D1%81%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%BD%D0%B0%D1%8F-%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7%D1%8B">Простая и составная гипотезы</h3><p>Когда гипотеза указывает точное значение параметра, а не диапазон значений, это простая гипотеза. Если речь идет об интервале значений, то мы говорим о составной. В примере с мылом речь идет о сложной гипотезе (диапазон 99-100%). А вот в другой иллюстрации: "Модель мотоцикла способна проехать 100 км на одном литре топлива", речь идет о простой гипотезе.</p><h3 id="%D0%BE%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D0%BE%D1%80%D0%BE%D0%BD%D0%BD%D1%8F%D1%8F-%D0%B8-%D0%B4%D0%B2%D1%83%D1%81%D1%82%D0%BE%D1%80%D0%BE%D0%BD%D0%BD%D1%8F%D1%8F-%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0-%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7">Односторонняя и двусторонняя проверка гипотез<br></h3><p>Если альтернативная гипотеза указывает на возможные значения в обоих направлениях (меньше m и больше n одновременно), указанного в нулевой гипотезе, это называется <a href="__GHOST_URL__/dvustoronnii-tiest/">Двусторонним тестом (TTT)</a>:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/null-hypothesis_two-tail-1.png" class="kg-image" alt loading="lazy" width="1000" height="588" srcset="__GHOST_URL__/content/images/size/w600/2021/03/null-hypothesis_two-tail-1.png 600w, __GHOST_URL__/content/images/2021/03/null-hypothesis_two-tail-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>Пример. Горнодобывающая компания утверждает, что в следующем году добыча железной руды составит не менее 100 и не более 2000 тонн.</p><p>Если альтернативная гипотеза указывает на однокомпонентный диапазон значений (например, только меньше m), это называется Левосторонним (Left-Tailed Test) или Правосторонним тестом (Right-Tailed Test). Это относится к первому примеру про эффективность мыла:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/null-hypothesis_one-tailed-test.png" class="kg-image" alt loading="lazy" width="1000" height="588" srcset="__GHOST_URL__/content/images/size/w600/2021/03/null-hypothesis_one-tailed-test.png 600w, __GHOST_URL__/content/images/2021/03/null-hypothesis_one-tailed-test.png 1000w" sizes="(min-width: 720px) 720px"><figcaption>Левосторонний тест нулевой гипотезы</figcaption></figure><h3 id="%D0%BD%D1%83%D0%BB%D0%B5%D0%B2%D0%B0%D1%8F-%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7%D0%B0-%D0%B8-%D1%82%D0%B8%D0%BF%D1%8B-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA">Нулевая гипотеза и типы ошибок</h3><p>Случается, что ошибочная гипотеза принимается за истинную, а истинная отвергается. Такие случаи условно обозначают ошибками I и II типа:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/null-hypothesis_error-types.png" class="kg-image" alt loading="lazy" width="1000" height="588" srcset="__GHOST_URL__/content/images/size/w600/2021/03/null-hypothesis_error-types.png 600w, __GHOST_URL__/content/images/2021/03/null-hypothesis_error-types.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>Пример. Человек арестован по обвинению в краже со взломом. Жюри присяжных должно решить, виновен он или нет.</p><p>H<sub>0</sub>: Человек невиновен (презумпция невиновности)</p><p>H<sub>1</sub>: Человек виновен</p><p>Ошибка типа I: жюри признало человека виновным (отклонило нулевую гипотезу), хотя человек был невиновен (нулевая гипотеза верна).</p><p>Ошибка типа II: жюри освободило человека [не отклонило H<sub>0</sub>], хотя человек виновен [H<sub>1</sub> верна].</p><p>Фото: <a href="https://unsplash.com/@chantalkemp">@chantalkemp</a></p><p>Автор статьи на английском: <a href="https://www.analyticsvidhya.com/blog/author/guest-blog/">Ritika Singh</a></p>		nulievaia-ghipotieza	2021-03-18		
72	Искусственный интеллект\t(AI)		<p>Искусственный интеллект (Artificial Intelligence, ИИ) – моделирование человеческого интеллекта с помощью машин, которые запрограммированы думать, как люди, и имитировать их действия. </p><p>Основной характеристикой искусственного интеллекта является его способность рационализировать и предпринимать действия для достижения конкретной цели. </p><h3 id="%D1%80%D0%B0%D0%B7%D0%BD%D0%BE%D0%B2%D0%B8%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D0%B8%D0%B8">Разновидности ИИ</h3><p>Стоит понимать, что Искусственный интеллект состоит из быстрорастущих дисциплин, активно взаимодействующих между собой. Если <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a> использует <a href="__GHOST_URL__/alghoritm/">Алгоритмы (Algorithm)</a> для анализа данных, их изучения и принятия обоснованных решений на основе полученных знаний, то <a href="earning">Глубокое обучение (DL)</a> структурирует алгоритмы по слоям для создания искусственной <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сети (Neural Network)</a>, которая может учиться и принимать интеллектуальные решения самостоятельно. Глубинное обучение – подвид Машинного. <a href="Стоит понимать, что Искусственный интеллект состоит из быстрорастущих дисциплин, активно взаимодействующих между собой. Если Машинное обучение (Machine Learning) использует Алгоритмы (Algorithm) для анализа данных, их изучения и принятия обоснованных решений на основе полученных знаний, то Глубокое обучение (DL) структурирует алгоритмы по слоям для создания искусственной Нейронной сети (Neural Network), которая может учиться и принимать интеллектуальные решения самостоятельно. Глубинное обучение – подвид Машинного. Обработка естественного языка (NLP), в свою очередь, анализирует речь и позволяет синтезировать ее. Компьютерное зрение (Computer Vision), как можно догадаться, позволяет компьютерам обнаруживать, отслеживать и классифицировать всевозможные объекты. Эти дисциплины, взаимодействуя друг с другом, позволяют создавать невероятно сложные программы.">Обработка естественного языка (NLP)</a>, в свою очередь, анализирует речь и позволяет синтезировать ее. <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерное зрение (CV)</a>, как можно догадаться, позволяет компьютерам обнаруживать, отслеживать и классифицировать всевозможные объекты. Эти дисциплины, взаимодействуя друг с другом, позволяют создавать невероятно сложные программы.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/artificial-intelligence.png" class="kg-image" alt loading="lazy" width="1920" height="1080" srcset="__GHOST_URL__/content/images/size/w600/2021/03/artificial-intelligence.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/artificial-intelligence.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/03/artificial-intelligence.png 1600w, __GHOST_URL__/content/images/2021/03/artificial-intelligence.png 1920w" sizes="(min-width: 1200px) 1200px"></figure><p>Согласно концепции Машинного обучения, компьютерные программы могут автоматически учиться и адаптироваться к новым данным без помощи человека. Методы Глубинного обучения обеспечивают автоматическое обучение за счет поглощения огромных объемов неструктурированных данных, таких как текст, изображения или видео.</p><p>Цель искусственного интеллекта – имитация когнитивной (познавательной) деятельности человека. <a href="__GHOST_URL__/data-saiientist/">Дата-сайентисты (Data Scientist)</a> делают удивительно быстрые успехи в имитации таких действий, как обучение, рассуждение и восприятие. Некоторые считают, что новаторы вскоре смогут разработать системы, которые превзойдут возможности людей. Но другие остаются скептически настроенными, потому что вся познавательная деятельность пронизана оценочными суждениями, которые зависят от человеческого опыта.</p><p>По мере развития технологий предыдущие тесты, которые определяли искусственный интеллект, устаревают. Например, компьютеры, которые распознают текст с помощью технологии Оптического распознавания символов (OCR), больше не считаются воплощением искусственного интеллекта, поскольку эта функция теперь воспринимается как должное, как неотъемлемая функция компьютера.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE-%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D0%B0">Применение искусственного интеллекта</h3><p>ИИ постоянно развивается, принося пользу во многих отраслях: математика, информатика, лингвистика, психология и многие другие. Например, ИИ тестируется и используется в сфере здравоохранения для дозирования лекарств и различных видов лечения пациентов, а также для хирургических процедур в операционной.</p><p>Другие примеры машин с искусственным интеллектом включают компьютеры, которые играют в шахматы, а также беспилотные автомобили. Каждая из этих машин должна взвесить последствия любого своего действия, поскольку оно повлияет на конечный результат. В шахматах конечный результат – победа. В случае беспилотных автомобилей система должна учитывать все внешние данные и вычислять их, чтобы предотвратить столкновение и достигнуть пункта назначения.</p><p>Искусственный интеллект также применяется в финансовой индустрии, где он используется для <a href="__GHOST_URL__/obnaruzhieniie-moshiennichieskikh-opieratsii/">Обнаружения мошеннических операций (Fraud Detection)</a>. </p><h3 id="%D0%BA%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D0%B8-%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE-%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D0%B0">Категории искусственного интеллекта</h3><p>Искусственный интеллект можно разделить на две разные категории: слабый и сильный. <a href="__GHOST_URL__/slabyi-iskusstviennyi-intielliekt/">Слабый искусственный интеллект (Weak AI)</a> представляет собой систему, предназначенную для выполнения одной конкретной работы. Слабые системы искусственного интеллекта включают видеоигры, такие как пример шахмат сверху, и личных помощников, таких как Amazon Alexa и Apple Siri. Вы задаете помощнику вопрос, он отвечает.</p><p><a href="__GHOST_URL__/silnyi-iskusstviennyi-intielliekt/">Сильные системы искусственного интеллекта (Strong AI)</a> – это системы, которые выполняют "человеческими" задачи. Они запрограммированы так, чтобы справляться с ситуациями, в которых от них может потребоваться решение проблемы без вмешательства человека. Такие системы можно найти в беспилотных автомобилях или в операционных.</p><h3 id="%D0%BE%D1%81%D0%BE%D0%B1%D1%8B%D0%B5-%D1%81%D0%BE%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F">Особые соображения</h3><p>С самого начала AI стал объектом пристального внимания как ученых, так и общественности. Одна из распространенных тем – идея, что машины станут настолько высокоразвитыми, что люди не смогут за ними поспевать, и первые будут эволюционировать сами, причем с экспоненциальной скоростью.</p><p>Во-вторых, машины могут нарушать право на личную жизнь человека и даже использоваться в качестве оружия. Другие спорят об этичности искусственного интеллекта и о том, следует ли относиться к интеллектуальным системам, таким как роботы, как к людям.</p><p>Еще одна спорная проблема, с которой сталкиваются многие люди, – влияние на уровень занятости населения. Поскольку многие отрасли стремятся автоматизировать рабочие места с помощью интеллектуального оборудования, есть опасения, что люди будут некомпетентны в качестве рабочей силы. Беспилотные автомобили могут устранить необходимость в такси и программах каршэринга, в то время как производители могут заменить человеческий труд машинным, понизив ценность навыков людей.</p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/contributors/82577/">Jake Frankenfield</a></p><p>Фото: <a href="https://unsplash.com/@martinkatler">@martinkatler</a></p>		iskusstviennyi-intielliekt	2021-03-21		
73	Скор (Score)		<p>Скор – одна из метрик производительности <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Выделяют несколько видов скора:</p><ul><li><a href="__GHOST_URL__/tochnost-izmierienii/">Точность классификации (Accuracy)</a></li><li><a href="__GHOST_URL__/logharifmichieskaia-potieria-log-loss/">Логарифмическая потеря (Logarithmic Loss)</a></li><li><a href="__GHOST_URL__/matritsa-oshibok/">Матрица ошибок (Confusion Matrix)</a></li><li>Площадь под кривой (Area under Curve)</li><li>Критерий F1 (F1 Score)</li><li><a href="__GHOST_URL__/sriedniaia-absoliutnaia-oshibka/">Средняя абсолютная ошибка (MAE)</a></li><li><a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратическая ошибка (MSE)</a></li></ul><p>Точность классификации – это то, что имеют в виду по умолчанию, когда используют термин «скор». Это отношение правильно спрогнозированных <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> к общему их количеству.</p><p>Автор оригинальной статьи: <a href="https://medium.com/@adi_myth?source=post_page-----f10ba6e38234--------------------------------">@adi_myth</a></p><p>Фото: <a href="https://unsplash.com/@tetrakiss">@tetrakiss</a></p>		post	2021-03-31		
74	Логарифмическая потеря (Log Loss)		<p>Логарифмическая потеря – метрика оценки эффективности <a href="__GHOST_URL__/modiel/">Модели (Model)</a> Бинарной классификации (Binary Classification).</p><h3 id="%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D0%BA%D0%B0%D0%B7%D0%B0%D0%BD%D0%B8%D1%8F">Вероятность предсказания</h3><p>Для того, чтобы разобраться в понятии, обратимся к концепции бинарной классификации. Такой алгоритм сначала предсказывает вероятность того, что <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a> будет отнесено к классу 1, а затем причисляет его к одному из двух классов (1 или 0) на основе того, пересекла ли вероятность пороговое значение, которое устанавливается по умолчанию равным 0,5:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/log-loss.png" class="kg-image" alt loading="lazy" width="852" height="196" srcset="__GHOST_URL__/content/images/size/w600/2021/03/log-loss.png 600w, __GHOST_URL__/content/images/2021/03/log-loss.png 852w"><figcaption>Результаты классификации email (1 – "спам")</figcaption></figure><p>Итак, прежде чем предсказывать класс записи, модель должна спрогнозировать вероятность того, что запись будет отнесена к классу 1. Помните, что именно от этой вероятности предсказания записи данных зависит значение логарифмической потери.</p><p>Логарифмическая потеря указывает, насколько близка вероятность предсказания к соответствующему истинному значению (0 или 1 в случае двоичной классификации). Чем больше прогнозируемая вероятность отклоняется от фактического значения, тем выше значение логарифма потерь. Формула расчета Log-Loss будет приведена чуть позже.</p><p>Рассмотрим задачу классификации электронных писем. Давайте представим спам как класс 1, а класс "нормальных" писем как 0. Давайте изучим настоящее спам-письмо (фактическое значение равно 1) и статистическую модель, которая классифицирует это письмо как спам с вероятностью 1. Поскольку вероятность предсказания равна почти 1, то и разность между предсказанной вероятностью и фактическим классом равна почти 0. Нулю равен, следовательно, и логарифм этой разности. На самом деле, значение логарифма потерь достаточно ничтожно, чтобы его можно было рассматривать как 0:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/log-loss-3.png" class="kg-image" alt loading="lazy" width="1001" height="126" srcset="__GHOST_URL__/content/images/size/w600/2021/03/log-loss-3.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/log-loss-3.png 1000w, __GHOST_URL__/content/images/2021/03/log-loss-3.png 1001w"></figure><p>Рассмотрим еще одно спам-письмо, классифицированное как спам с вероятностью 0,9. Вероятность прогноза модели на 0,1 отличается от фактического значения 1, и, следовательно, значение логарифмической потери больше нуля (равно 0,105):</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/log-loss-0-9.png" class="kg-image" alt loading="lazy" width="1001" height="126" srcset="__GHOST_URL__/content/images/size/w600/2021/03/log-loss-0-9.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/log-loss-0-9.png 1000w, __GHOST_URL__/content/images/2021/03/log-loss-0-9.png 1001w"></figure><p>А теперь давайте посмотрим на обычное электронное письмо. Модель классифицирует его как спам с вероятностью 0,2, то есть считает нормальным письмом (при условии, что порог по умолчанию равен 0,5). Абсолютная разница между вероятностью предсказания и фактическим значением, равным 0 (так как это нормально), составляет 0,2, что больше, чем то, что мы наблюдали в предыдущих двух наблюдениях. Значение логарифма потерь, связанное с прогнозом, составляет 0,223:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/03/log-loss-0-2-1.png" class="kg-image" alt loading="lazy" width="983" height="101" srcset="__GHOST_URL__/content/images/size/w600/2021/03/log-loss-0-2-1.png 600w, __GHOST_URL__/content/images/2021/03/log-loss-0-2-1.png 983w"></figure><p>Обратите внимание, как теперь значение Log-Loss более плохого прогноза (дальше от фактического значения) выше, чем у лучшего прогноза (ближе к фактическому значению).</p><p>Теперь предположим, что существует набор из 5 различных спам-писем, прогнозируемых с широким диапазоном вероятностей 1.0, 0.7, 0.3, 0.009 и 0.0001. Обученная статистическая модель неидеальна и, следовательно, выполняет (действительно) плохую работу по последним трем наблюдениям (классифицирует их как нормальные, поскольку значения вероятности ближе к 0, чем к 1). На графике видно, что значение логарифмических потерь экспоненциально возрастает по мере того, как растет разность между реальным классом и предсказанной вероятностью:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/log-loss-5.png" class="kg-image" alt loading="lazy" width="899" height="641" srcset="__GHOST_URL__/content/images/size/w600/2021/03/log-loss-5.png 600w, __GHOST_URL__/content/images/2021/03/log-loss-5.png 899w" sizes="(min-width: 720px) 720px"></figure><p>Если мы построим график логарифмических потерь для перечня спам-писем со всеми возможными видами вероятностей, график будет выглядеть следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/log-loss-curve.png" class="kg-image" alt loading="lazy" width="899" height="641" srcset="__GHOST_URL__/content/images/size/w600/2021/03/log-loss-curve.png 600w, __GHOST_URL__/content/images/2021/03/log-loss-curve.png 899w" sizes="(min-width: 720px) 720px"></figure><p>В случае с нормальными письмами график будет зеркальным отображением приведенного выше:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/log-loss-inverted-curve.png" class="kg-image" alt loading="lazy" width="899" height="641" srcset="__GHOST_URL__/content/images/size/w600/2021/03/log-loss-inverted-curve.png 600w, __GHOST_URL__/content/images/2021/03/log-loss-inverted-curve.png 899w" sizes="(min-width: 720px) 720px"></figure><p>Подводя итог, можно сказать, что чем дальше вероятность предсказания от фактического значения, тем выше значение логарифмических потерь. При обучении модели классификации мы хотели бы, чтобы наблюдение предсказывалось с вероятностью, максимально приближенной к фактическому значению (0 или 1). Следовательно, Log-Loss – хороший выбор в качестве <a href="__GHOST_URL__/funktsiia-potieri/">Функции потери (Loss Function)</a> для обучения и оптимизации. Чем дальше вероятность предсказания от ее истинного значения, тем выше штраф.</p><h3 id="%D1%84%D0%BE%D1%80%D0%BC%D1%83%D0%BB%D0%B0">Формула</h3><p>Теперь, когда мы понимаем логику, лежащую в основе концепции, мы можем обсудить формулу:</p><!--kg-card-begin: markdown--><p>$$Log-Loss = -[y_i × \\ln{}(p_i) + (1 - y_i) × ln(1 - p_i)]$$<br>\n$$y_i{–}\\space{истинный}\\space{класс}\\space{наблюдения,}$$<br>\n$$p_i\\space{–}\\space{предсказанная}\\space{вероятность}$$</p>\n<!--kg-card-end: markdown--><p>Мы используем натуральный логарифм, где основанием является число e (2,71...) и теперь, рассмотрев один из случаев в примере про письма, получим:</p><!--kg-card-begin: markdown--><p>$$Log-Loss = -[0 × \\ln{}(0,2) + (1 - 0) × ln(1 - 0,2)] = -[0 + ln0,8] = 0,223$$</p>\n<!--kg-card-end: markdown--><p>Чтобы оценить модель в целом, вычисляется <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее арифметическое (Mean)</a> логарифмических потерь всех наблюдений. Модель с совершенными навыками имеет логарифм потерь, равным нулю. Другими словами, идеальная модель предсказывает вероятность каждого наблюдения как фактическое значение.</p><p>Логарифмическая потеря для бинарной классификации – тоже, что <br><a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратическая ошибка (MSE)</a> для регрессии. Обе метрики показывают, насколько хороши или плохи результаты прогнозов, указывая на дистанцию между прогнозом и фактическим значением.</p><h3 id="log-loss-%D0%B8-scikit-learn">Log-Loss и Scikit-learn</h3><p>Логарифмические потери можно рассчитать с помощью SkLearn. Для начала импортируем функцию:</p><pre><code class="language-python">import matplotlib.pyplot as plt\n</code></pre><p>Применим функцию "на бегу", передав аргументы-списки:</p><pre><code class="language-python">log_loss(["спам", "нормальное письмо", "нормальное письмо", "спам"],\n[[.1, .9], [.9, .1], [.8, .2], [.35, .65]])</code></pre><p>Система здесь уже вынесла свой вердикт, и во втором списке находятся, как можно догадаться, пары значений, описывающих вероятности "спама" и "нормального письма". Функция сама определяет, что первый элемент внутреннего списка – число 0,1, описывает вероятность письма быть нормальным письмом, а второй – соответственно, спамом, и применив такой паттерн ко всем остальным парам значений, вычисляет разность между реальным классом и предсказанной вероятностью. Следуя формуле, описанной выше, она находит значение Log-Loss для каждого наблюдения и усредняет полученный результат.</p><pre><code class="language-python">0.21616...</code></pre><p>Фото: <a href="https://unsplash.com/@usefulcollective">@usefulcollective</a></p><p>Автор оригинальной статьи: @<a href="https://gaurav-dembla.medium.com/?source=post_page-----4e0c9979680a--------------------------------">gaurav-dembla</a></p>		logharifmichieskaia-potieria	2021-04-02		
75	Среднеквадратическая ошибка (MSE)		<p>Среднеквадратичная ошибка (Mean Squared Error) – <a href="__GHOST_URL__/sriednieie-znachieniie/#-" rel="noopener noreferrer">Среднее арифметическое (Mean)</a> квадратов разностей между предсказанными и реальными значениями <a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/" rel="noopener noreferrer">Машинного обучения (ML)</a>:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/03/mse-1.png" class="kg-image" alt loading="lazy" width="1580" height="756" srcset="__GHOST_URL__/content/images/size/w600/2021/03/mse-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/03/mse-1.png 1000w, __GHOST_URL__/content/images/2021/03/mse-1.png 1580w" sizes="(min-width: 1200px) 1200px"><figcaption>MSE как среднее дистанций между предсказаниями и реальными наблюдениями</figcaption></figure><p>Рассчитывается с помощью формулы, которая будет пояснена в примере ниже:</p><!--kg-card-begin: markdown--><p>$$MSE = \\frac{1}{n} × \\sum_{i=1}^n (y_i - \\widetilde{y}_i)^2$$<br>\n$$MSE\\space{}{–}\\space{Среднеквадратическая}\\space{ошибка,}$$<br>\n$$n\\space{}{–}\\space{количество}\\space{наблюдений,}$$<br>\n$$y_i\\space{}{–}\\space{фактическая}\\space{координата}\\space{наблюдения,}$$<br>\n$$\\widetilde{y}_i\\space{}{–}\\space{предсказанная}\\space{координата}\\space{наблюдения,}$$</p>\n<!--kg-card-end: markdown--><p>MSE практически никогда не равен нулю, и происходит это из-за элемента случайности в данных или неучитывания Оценочной функцией (Estimator) всех факторов, которые могли бы улучшить предсказательную способность.</p><p>Пример. Исследуем линейную регрессию, изображенную на графике выше, и установим величину среднеквадратической <a href="__GHOST_URL__/oshibka/">Ошибки (Error)</a>. Фактические координаты точек-<a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> выглядят следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/mse-data.png" class="kg-image" alt loading="lazy" width="142" height="378"></figure><p>Мы имеем дело с <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессией (Linear Regression)</a>, потому уравнение, предсказывающее положение записей, можно представить с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$y = M * x + b$$<br>\n$$y\\space{–}\\space{значение}\\space{координаты}\\space{оси}\\space{y,}$$<br>\n$$M\\space{–}\\space{уклон}\\space{прямой}$$<br>\n$$x\\space{–}\\space{значение}\\space{координаты}\\space{оси}\\space{x,}$$<br>\n$$b\\space{–}\\space{смещение}\\space{прямой}\\space{относительно}\\space{начала}\\space{координат}$$</p>\n<!--kg-card-end: markdown--><p>Параметры M и b уравнения нам, к счастью, известны в данном обучающем примере, и потому уравнение выглядит следующим образом:</p><!--kg-card-begin: markdown--><p>$$y = 0,5252 * x + 17,306$$</p>\n<!--kg-card-end: markdown--><p>Зная координаты реальных записей и уравнение линейной регрессии, мы можем восстановить полные координаты предсказанных наблюдений, обозначенных серыми точками на графике выше. Простой подстановкой значения координаты x в уравнение мы рассчитаем значение координаты ỹ:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/mse-y-tilde.png" class="kg-image" alt loading="lazy" width="195" height="378"></figure><p>Рассчитаем квадрат разницы между Y и Ỹ:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/mse-difference------.png" class="kg-image" alt loading="lazy" width="267" height="416"></figure><p>Сумма таких квадратов равна 4 445. Осталось только разделить это число на количество наблюдений (9):</p><!--kg-card-begin: markdown--><p>$$MSE = \\frac{1}{9} × 4445 = 493$$</p>\n<!--kg-card-end: markdown--><p>Само по себе число в такой ситуации становится показательным, когда <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> предпринимает попытки улучшить предсказательную способность модели и сравнивает MSE каждой итерации, выбирая такое уравнение, что сгенерирует наименьшую погрешность в предсказаниях. </p><h3 id="mse-%D0%B8-scikit-learn">MSE и Scikit-learn</h3><p>Среднеквадратическую ошибку можно вычислить с помощью SkLearn. Для начала импортируем функцию:</p><pre><code class="language-python">import sklearn\nfrom sklearn.metrics import mean_squared_error</code></pre><p>Инициализируем крошечные списки, содержащие реальные и предсказанные координаты y:</p><pre><code class="language-python">y_true = [5, 41, 70, 77, 134, 68, 138, 101, 131]\ny_pred = [23, 35, 55, 90, 93, 103, 118, 121, 129]</code></pre><p>Инициируем функцию <code>mean_squared_error()</code>, которая рассчитает MSE тем же способом, что и формула выше: </p><pre><code class="language-python">mean_squared_error(y_true, y_pred)\n</code></pre><p>Интересно, что конечный результат на 3 отличается от расчетов с помощью Apple Numbers:</p><pre><code class="language-python">496.0</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1fYtVbamX9gxicgdpsCPk_HlECw-O_TJs?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://medium.com/@mmoshikoo">@mmoshikoo</a></p><p>Фото: <a href="https://unsplash.com/@tobyelliott">@tobyelliott</a></p>		sriedniekvadratichieskaia-oshibka	2021-04-03		
76	Анализ главных компонент (PCA)		<p>Анализ главных компонент – это метод понижения размерности <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, который преобразует больший набор переменных в меньший с минимальными потерями информативности.</p><p>Уменьшение количества переменных в наборе данных происходит в ущерб точности, но хитрость здесь заключается в том, чтобы потерять немного в  точности, но обрести простоту. Поскольку меньшие наборы данных легче исследовать и визуализировать, анализ данных становится намного проще и быстрее для <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.</p><p>Идея PCA проста: уменьшить количество переменных в наборе данных, сохранив при этом как можно больше информации.</p><h3 id="%D1%88%D0%B0%D0%B3-%D0%BF%D0%B5%D1%80%D0%B2%D1%8B%D0%B9-%D1%81%D1%82%D0%B0%D0%BD%D0%B4%D0%B0%D1%80%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F">Шаг первый. Стандартизация</h3><p>Мы осуществляем Стандартизацию (Standartization) исходных переменных, чтобы каждая из них вносила равный вклад в анализ. Почему так важно выполнить стандартизацию до PCA? Метод очень чувствителен к <a href="__GHOST_URL__/dispiersiia/">Дисперсиям (Variance)</a> исходных <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>. Если есть больши́е различия между диапазонами исходных переменных, те переменные с бо́льшими диапазонами будут преобладать над остальными (например, переменная, которая находится в диапазоне от 0 до 100, будет преобладать над переменной, которая находится в диапазоне от 0 до 1), что приведет к необъективным результатам. Преобразование данных в сопоставимые масштабы может предотвратить эту ситуацию.</p><p>Математически это можно сделать путем вычитания <a href="__GHOST_URL__/sriednieie-znachieniie/">Среднего значения (Mean)</a> из каждого значения и деления полученной разности на <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>. После стандартизации все переменные будут преобразованы в исходные значения.</p><h3 id="%D1%88%D0%B0%D0%B3-%D0%B2%D1%82%D0%BE%D1%80%D0%BE%D0%B9-%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0-%D0%BA%D0%BE%D0%B2%D0%B0%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D0%B8">Шаг второй. Матрица ковариации</h3><p>Цель этого шага – понять, как переменные отличаются от среднего по отношению друг к другу, или, другими словами, увидеть, есть ли между ними какая-либо связь. Порой переменные сильно коррелированы и содержат избыточную информацию, и чтобы идентифицировать эти взаимосвязи, мы вычисляем Ковариационную матрицу (Covariance Matrix).</p><p>Ковариационная матрица представляет собой симметричную матрицу размера p × p (где p – количество измерений), где в качестве ячеек пребывают коэффициенты ковариации, связанные со всеми возможными парами исходных переменных. Например, для трехмерного набора данных с 3 переменными x, y и z ковариационная матрица представляет собой следующее:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/pca.png" class="kg-image" alt loading="lazy" width="281" height="271"><figcaption>Окрашенные голубым треугольники симметрично равны друг другу</figcaption></figure><p>Поскольку ковариация переменной с самой собой – это ее дисперсия, на главной диагонали (от верхней левой ячейки к нижней правой), у нас фактически есть дисперсии каждой исходной переменной. А поскольку ковариация коммутативна (в ячейке XY значение равно YX), элементы матрицы симметричны относительно главной диагонали. </p><p>Что коэффициенты ковариации говорят нам о корреляциях между переменными? На самом деле, имеет значение знак ковариации. Если коэффициент – это:</p><ul><li>положительное число, то две переменные прямо пропорциональны, то есть второй увеличивается или уменьшается вместе с первым.</li><li>отрицательное число, то переменные обратно пропорциональны, то есть второй увеличивается, когда первый уменьшается, и наоборот.</li></ul><p>Теперь, когда мы знаем, что ковариационная матрица – это не более чем таблица, которая отображает корреляции между всеми возможными парами переменных, давайте перейдем к следующему шагу.</p><h3 id="%D1%88%D0%B0%D0%B3-%D1%82%D1%80%D0%B5%D1%82%D0%B8%D0%B9-%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D1%85-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2">Шаг третий. Вычисление собственных векторов</h3><p>Собственные векторы (Eigenvector) и Собственные значения (Eigenvalues) – это понятия из области Линейной алгебры (Linear Algebra), которые нам нужно экстраполировать из ковариационной матрицы, чтобы определить так называемые главные компоненты данных. Давайте сначала поймем, что мы подразумеваем под этим термином.</p><p>Главная компонента – это новая переменная, смесь исходных. Эти комбинации выполняются таким образом, что новые переменные (то есть главные компоненты) не коррелированы, и большая часть информации в исходных переменных помещается в первых компонентах. Итак, идея состоит в том, что 10-мерный датасет дает нам 10 главных компонент, но PCA пытается поместить максимум возможной информации в первый, затем максимум оставшейся информации во второй и так далее, пока не появится что-то вроде того, что показано на графике ниже:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/pca-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/pca-1.png 600w, __GHOST_URL__/content/images/2021/04/pca-1.png 1000w"><figcaption>Объясненная вариация</figcaption></figure><p>Такая организация информации в главных компонентах позволит нам уменьшить размерность без потери большого количества информации за счет отбрасывания компонент с низкой информативностью.</p><p>Здесь важно понимать, что главные компоненты менее интерпретируемы и не имеют никакого реального значения, поскольку они построены как линейные комбинации исходных переменных.</p><p>С геометрической точки зрения, главные компоненты представляют собой <a href="__GHOST_URL__/viektor/">Векторы (Vector)</a> данных, которые объясняют максимальное количество отклонений. Главные компоненты – новые оси, которые обеспечивают лучший угол для оценки данных, чтобы различия между наблюдениями были лучше видны.</p><p>Поскольку существует столько главных компонент, сколько переменных в наборе, главные компоненты строятся таким образом, что первый из них учитывает наибольшую возможную дисперсию в наборе данных. Например, предположим, что диаграмма рассеяния нашего набора данных выглядит так:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/pca.gif" class="kg-image" alt loading="lazy" width="1000" height="400"><figcaption><em>Подбор собственного вектора</em></figcaption></figure><p>Можем ли мы проецировать первый главный компонент? Да, это линия, которая соответствует фиолетовым отметкам, потому что она проходит через начало координат, и проекции точек на компонент наиболее короткие. Говоря математически, это линия, которая максимизирует дисперсию (среднее квадратов расстояний от проецируемых красных точек до начала координат).</p><p>Второй главный компонент рассчитывается таким же образом, при условии, что он не коррелирован (т.е. перпендикулярен) первому главному компоненту и учитывает следующую по величине дисперсию. Это продолжается до тех пор, пока не будет вычислено p главных компонент, равное исходному количеству переменных.</p><p>Теперь, когда мы поняли, что подразумевается под главными компонентами, давайте вернемся к собственным векторам и собственным значениям. Прежде всего, нам нужно знать, что они всегда "ходят парами", то есть каждый собственный вектор имеет собственное значение. И их количество равно количеству измерений данных. Например, для 3-мерного набора данных есть 3 переменных, следовательно, есть 3 собственных вектора с 3 соответствующими собственными значениями.</p><p>За всей магией, описанной выше, стоят собственные векторы и собственные значения, потому что собственные векторы матрицы ковариации на самом деле являются направлениями осей, где наблюдается наибольшая дисперсия (большая часть информации) и которые мы называем главными компонентами. А собственные значения – это просто коэффициенты, прикрепленные к собственным векторам, которые дают величину дисперсии, переносимую в каждом основном компоненте.</p><p>Ранжируя собственные векторы в порядке от наибольшего к наименьшему, мы получаем главные компоненты в порядке значимости.</p><h3 id="%D1%88%D0%B0%D0%B3-%D1%87%D0%B5%D1%82%D0%B2%D0%B5%D1%80%D1%82%D1%8B%D0%B9-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80-%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%B0">Шаг четвертый. Вектор признака</h3><p>Как мы видели на предыдущем шаге, вычисляя собственные векторы и упорядочивая их по собственным значениям в в порядке убывания, мы можем ранжировать основные компоненты в порядке значимости. На этом этапе мы выбираем, оставить ли все эти компоненты или отбросить те, которые имеют меньшее значение, и сформировать с оставшимися матрицу векторов, которую мы называем Вектором признака (Feature Vector<em>)</em>.</p><p>Итак, вектор признаков – это просто матрица, в столбцах которой есть собственные векторы компонент, которые мы решили оставить. Это первый шаг к уменьшению размерности, потому что, если мы решим оставить только p собственных векторов (компонент) из n, окончательный набор данных будет иметь только p измерений.</p><h3 id="%D1%88%D0%B0%D0%B3-5-%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%BF%D0%BE-%D0%BE%D1%81%D1%8F%D0%BC-%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85-%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82">Шаг 5. Трансформирование данных по осям главных компонент</h3><p>На предыдущих шагах, помимо стандартизации, мы не вносили никаких изменений в данные, а просто выбирали основные компоненты и формировали вектор признаков, но исходной набор данных всегда остается.</p><p>На этом последнем этапе цель состоит в переориентации данных с исходных осей на оси, представленные главными компонентами (отсюда и название «Анализ главных компонент»). Это можно сделать, перемножив транспонированный исходный набор данных на транспонированный вектор признаков.</p><h3 id="pca-%D0%B8-scikit-learn">PCA и Scikit-learn</h3><p>PCA можно реализовать с помощью SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport pandas as pd\n\nimport sklearn\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler</code></pre><p>Мы будем использовать датасет банка, автоматизирующего выдачу кредитных продуктов своим клиентам:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/9t04t1haanbdvvt/bank-data-for-pca.csv?dl=1')\ndf</code></pre><p>Создадим список признаков, подлежащих уменьшению. Это макроэкономические показатели с невысоким уровнем важности, которые почти не попали в список выше. Выберем сокращаемые и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменные (Target Variable)</a>:</p><pre><code class="language-python">X = df[['Возраст', 'Длительность', 'Кампания', 'День недели', 'Предыдущий контакт', 'Индекс потребительских цен', 'Европейская межбанковская ставка', 'Количество сотрудников в компании']]\ny = df.iloc[:, -1]</code></pre><p>Выберем cамые важные признаки с помощью функции SelectKBest, которая использует критерий Хи-квадрат (Chi Square):</p><pre><code class="language-python">bestfeatures = SelectKBest(score_func = chi2, k = 'all')\nfit = bestfeatures.fit(X, y)</code></pre><p>Создадим объект <code>dfscores</code>, куда отправим, соответственно, очки важности всех признаков датасета:</p><pre><code class="language-python">dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)</code></pre><p>Создадим для коэффициентов отдельный объект, соединив названия столбцов и очки, и отобразим пять признаков, набравших наибольшее количество очков:</p><pre><code class="language-python">featureScores = pd.concat([dfcolumns, dfscores], axis = 1)\nfeatureScores.columns = ['Specs', 'Score']\nprint(featureScores.nlargest(5, 'Score'))</code></pre><p>Неожиданно, но самыми важными признаками оказались количество сотрудников в компании и порядковый номер рекламной кампании, в которой участвует клиент:</p><pre><code class="language-python">                               Specs         Score\n1                       Длительность  1.760568e+06\n7  Количество сотрудников в компании  5.251380e+03\n6   Европейская межбанковская ставка  3.239336e+03\n4                 Предыдущий контакт  3.089714e+03\n2                           Кампания  5.419261e+02</code></pre><p>Создадим список признаков, подлежащих понижению. Это макроэкономические показатели с невысоким уровнем важности, которые почти не попали в список выше:</p><pre><code class="language-python">features = ['Колебание уровня безработицы', 'Индекс потребительских цен', 'Индекс потребительской уверенности', 'Европейская межбанковская ставка']\nx = df.loc[:, features].values</code></pre><p>Выполним стандартизацию объекта X. <code>StandardScaler()</code><strong> </strong>на месте заменяет данные на их стандартизированную версию, и мы получаем признаки, где все значения как бы центрированы относительно нуля. Такое преобразование необходимо, чтобы правильно объединить признаки между собой.</p><pre><code class="language-python">x = StandardScaler().fit_transform(x)\npd.DataFrame(data = x, columns = features).head()</code></pre><p>Результат выглядит следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/pca-standartized-data.png" class="kg-image" alt loading="lazy" width="632" height="442" srcset="__GHOST_URL__/content/images/size/w600/2021/04/pca-standartized-data.png 600w, __GHOST_URL__/content/images/2021/04/pca-standartized-data.png 632w"></figure><p>Мы хотим получить один главный компонент. Передадим функции обучающие данные:</p><pre><code class="language-python">pca = PCA(n_components = 1)\n\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1'])\n\nprincipalDf.head()</code></pre><p>Мы получили вот такой главный компонент:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/pca-component.png" class="kg-image" alt loading="lazy" width="168" height="425"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1vjR5YkzUChriPpDJxL0SEpj54_9P_TH1?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis">Zakaria Jaadi</a></p><p>Фото: <a href="https://unsplash.com/@codyrs">@codyrs</a></p>		mietod-ghlavnykh-komponient	2021-04-07		
78	Датафрейм (Dataframe)		<p>Датафрейм – это двумерная структура данных со столбцами и строками. Это специальный аналог таблицы Excel или SQL – наборе Серий (Series) и наиболее часто используемый объект библиотеки Pandas:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/sample-gamers.png" class="kg-image" alt loading="lazy" width="404" height="431"></figure><p>Наряду с данными вы можете дополнительно передать индекс – столбец с уникальными значениями, однозначно идентифицирующими каждое <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a>. </p><p>Многие знают датафрейм как способ хранения данных в прямоугольных сетках, которые можно легко просмотреть. Каждая строка этой сетоки соответствует отдельному наблюдению, а каждый столбец – это <a href="__GHOST_URL__/priznak/">Признак (Feature)</a>. Cтроки датафрейма могут содержать значения разных типов: они могут быть числовыми, символьными, Булевыми (Boolean Data Type) и так далее. Можно сказать, что датафрейм состоит из трех основных компонентов: данных, индекса и столбцов.</p><h3 id="%D0%B8%D0%BD%D0%B8%D1%86%D0%B8%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%B4%D0%B0%D1%82%D0%B0%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B0">Инициализация датафрейма</h3><p>Первый из способов создания датафрейма – метод библиотеки Pandas <code>read_csv()</code>:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/9t04t1haanbdvvt/bank-data-for-pca.csv?dl=1')\ndf</code></pre><p>Еще один способ – создать его из нескольких серий:</p><pre><code class="language-python">d = {'one' : pd.Series([10, 20, 30, 40],\n                       index =['a', 'b', 'c', 'd']),\n      'two' : pd.Series([10, 20, 30, 40],\n                        index =['a', 'b', 'c', 'd'])}\n                        \ndf = pd.DataFrame(d)</code></pre><p>Способов инициализации великое множество – от передачи Списков (List) до создания Кортежей (Tuples) и превращения их в списки.</p><p>Фото: <a href="https://unsplash.com/@jadlimcaco">@jadlimcaco</a></p>		datafrieim	2021-04-10		
79	Функция потерь (Loss Function)		<p>Функция потерь (Loss Function, Cost Function, Error Function; J) – фрагмент программного кода, который используется для оптимизации <a href="__GHOST_URL__/alghoritm/">Алгоритма (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Значение, вычисленное такой функцией, называется «потерей».</p><p><a href="__GHOST_URL__/funktsiia/">Функция (Function)</a> потерь может дать бо́льшую практическую гибкость вашим <a href="__GHOST_URL__/nieironnaia-siet/">Нейронным сетям (Neural Network)</a> и будет определять, как именно выходные данные связаны с исходными.</p><p>Нейронные сети могут выполнять несколько задач: от прогнозирования непрерывных значений, таких как ежемесячные расходы, до Бинарной классификации (Binary Classification) на кошек и собак. Для каждой отдельной задачи потребуются разные типы функций, поскольку выходной формат индивидуален. </p><p>С очень упрощенной точки зрения Loss Function может быть определена как функция, которая принимает два параметра:</p><ul><li>Прогнозируемые выходные данные</li><li>Истинные выходные данные</li></ul><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/loss-function_general.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/loss-function_general.png 600w, __GHOST_URL__/content/images/2021/04/loss-function_general.png 1000w"><figcaption><em>Визуализация потерь нейронной сети</em></figcaption></figure><p>Эта функция, по сути, вычислит, насколько хорошо работает наша модель, сравнив то, что модель прогнозирует, с фактическим значением, которое она должна выдает. Если Y<sub>pred</sub> очень далеко от Yi, значение потерь будет очень высоким. Однако, если оба значения почти одинаковы, значение потерь будет очень низким. Следовательно, нам нужно сохранить функцию потерь, которая может эффективно наказывать модель, пока та обучается на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a>.</p><p>Этот сценарий в чем-то аналогичен подготовке к экзаменам. Если кто-то плохо сдает экзамен, мы можем сказать, что потеря очень высока, и этому человеку придется многое изменить внутри себя, чтобы в следующий раз получить лучшую оценку. Однако, если экзамен пройдет хорошо, студент может вести себя подобным образом и в следующий раз.</p><p>Теперь давайте рассмотрим классификацию как задачу и поймем, как в этом случае работает функция потерь.</p><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5-%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D0%B8">Классификационные потери</h3><p>Когда нейронная сеть пытается предсказать дискретное значение, мы рассматриваем это как модель классификации. Это может быть сеть, пытающаяся предсказать, какое животное присутствует на изображении, или является ли электронное письмо спамом. Сначала давайте посмотрим, как представлены выходные данные классификационной нейронной сети.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/loss-function_classification-output.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/loss-function_classification-output.png 600w, __GHOST_URL__/content/images/2021/04/loss-function_classification-output.png 1000w"><figcaption>Выходной формат данных нейросети бинарной классификации</figcaption></figure><p><br>Количество узлов выходного слоя будет зависеть от количества классов, присутствующих в данных. Каждый узел будет представлять один класс. Значение каждого выходного узла по существу представляет вероятность того, что этот класс является правильным.</p><p>Как только мы получим вероятности всех различных классов, рассмотрим тот,  что имеет наибольшую вероятность. Посмотрим, как выполняется двоичная классификация.</p><h3 id="%D0%B1%D0%B8%D0%BD%D0%B0%D1%80%D0%BD%D0%B0%D1%8F-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">Бинарная классификация</h3><p>В двоичной классификации на выходном слое будет только один узел. Чтобы получить результат в формате вероятности, нам нужно применить <a href="__GHOST_URL__/funktsiia-aktivatsii/">Функцию активации (Activation Function)</a>. Поскольку для вероятности требуется значение от 0 до 1, мы будем использовать Сигмоид (Sigmoid), которая приведет любое реальное значение к диапазону значений от 0 до 1.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/loss-function_sigmoid.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/loss-function_sigmoid.png 600w, __GHOST_URL__/content/images/2021/04/loss-function_sigmoid.png 1000w"><figcaption><em>Визуализация преобразования значения сигмоидом</em></figcaption></figure><p>По мере того, как входные реальные данные становятся больше и стремятся к плюс бесконечности, выходные данные сигмоида будут стремиться к единице. А когда на входе значения становятся меньше и стремятся к отрицательной бесконечности, на выходе числа будут стремиться к нулю. Теперь мы гарантированно получаем значение от 0 до 1, и это именно то, что нам нужно, поскольку нам нужны вероятности.</p><p>Если выход выше 0,5 (вероятность 50%), мы будем считать, что он попадает в положительный класс, а если он ниже 0,5, мы будем считать, что он попадает в отрицательный класс. Например, если мы обучаем нейросеть для классификации кошек и собак, мы можем назначить собакам положительный класс, и выходное значение в наборе данных для собак будет равно 1, аналогично кошкам будет назначен отрицательный класс, а выходное значение для кошек будет быть 0.</p><p>Функция потерь, которую мы используем для двоичной классификации, называется Двоичной перекрестной энтропией (BCE). Эта функция эффективно наказывает нейронную сеть за <a href="__GHOST_URL__/oshibka/" rel="noopener noreferrer">Ошибки (Error)</a> двоичной классификации. Давайте посмотрим, как она выглядит.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/loss-function_bce-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/loss-function_bce-1.png 600w, __GHOST_URL__/content/images/2021/04/loss-function_bce-1.png 1000w"><figcaption>Графики потери бинарной кросс-энтропии</figcaption></figure><p>Как видите, есть две отдельные функции, по одной для каждого значения Y. Когда нам нужно предсказать положительный класс (Y = 1), мы будем использовать следующую формулу:</p><!--kg-card-begin: markdown--><p>$$Потеря = -\\log(Y_{pred})\\space{,}\\space{где}$$<br>\n$$J\\space{}{–}\\space{Потеря,}$$<br>\n$$Y_pred\\space{}{–}\\space{Предсказанные}\\space{значения}$$</p>\n<!--kg-card-end: markdown--><p>И когда нам нужно предсказать отрицательный класс (Y = 0), мы будем использовать немного трансформированный аналог:</p><!--kg-card-begin: markdown--><p>$$Потеря = -\\log(1 - Y_{pred})\\space{,}\\space{где}$$<br>\n$$J\\space{}{–}\\space{Потеря,}$$<br>\n$$Y_pred\\space{}{–}\\space{Предсказанные}\\space{значения}$$</p>\n<!--kg-card-end: markdown--><p>Для первой функции, когда Y<sub>pred</sub> равно 1, потеря равна 0, что имеет смысл, потому что Y<sub>pred</sub> точно такое же, как Y. Когда значение Y<sub>pred</sub> становится ближе к 0, мы можем наблюдать, как значение потери сильно увеличивается. Когда же Y<sub>pred</sub> становится равным 0, потеря стремится к бесконечности. Это происходит, потому что с точки зрения классификации, 0 и 1 – полярные противоположности: каждый из них представляет совершенно разные классы. Поэтому, когда Y<sub>pred</sub> равно 0, а Y равно 1, потери должны быть очень высокими, чтобы сеть могла более эффективно распознавать свои ошибки.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/loss-function-loss-comparisons.png" class="kg-image" alt loading="lazy" width="221" height="334"><figcaption>Сравнение потерь двоичной классификации</figcaption></figure><h3 id="%D0%BF%D0%BE%D0%BB%D0%B8%D0%BD%D0%BE%D0%BC%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">Полиномиальная классификация</h3><p>Полиномиальная классификация (Multiclass Classification) подходит, когда нам нужно, чтобы наша модель каждый раз предсказывала один возможный класс. Теперь, поскольку мы все еще имеем дело с вероятностями, имеет смысл просто применить сигмоид ко всем выходным узлам, чтобы мы получали значения от 0 до 1 для всех выходных значений, но здесь кроется проблема. Когда мы рассматриваем вероятности для нескольких классов, нам необходимо убедиться, что сумма всех индивидуальных вероятностей равна единице, поскольку именно так определяется вероятность. Применение сигмоида не гарантирует, что сумма всегда равна единице, поэтому нам нужно использовать другую функцию активации.</p><p>В данном случае мы используем <a href="__GHOST_URL__/softmaks/">функцию активации Softmax</a>. Эта функция гарантирует, что все выходные узлы имеют значения от 0 до 1, а сумма всех значений выходных узлов всегда равна 1. Вычисляется с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$Softmax(y_i) = \\frac{e^{y_i}}{\\sum_{i = 0}^n e^{y_i}}\\space{,}\\space{где}$$<br>\n$$y_i\\space{}{–}\\space{i-e}\\space{наблюдение}$$</p>\n<!--kg-card-end: markdown--><p>Пример:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/loss-function_softmax.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/loss-function_softmax.png 600w, __GHOST_URL__/content/images/2021/04/loss-function_softmax.png 1000w"></figure><p>Как видите, мы просто передаем все значения в экспоненциальную функцию. После этого, чтобы убедиться, что все они находятся в диапазоне от 0 до 1 и сумма всех выходных значений равна 1, мы просто делим каждую экспоненту на сумму экспонент.</p><p>Итак, почему мы должны передавать каждое значение через экспоненту перед их нормализацией? Почему мы не можем просто нормализовать сами значения? Это связано с тем, что цель Softmax – убедиться, что одно значение очень высокое (близко к 1), а все остальные значения очень низкие (близко к 0). Softmax использует экспоненту, чтобы убедиться, что это произойдет. А затем мы нормализуем результат, потому что нам нужны вероятности.</p><p>Теперь, когда наши выходные данные имеют правильный формат, давайте посмотрим, как мы настраиваем для этого функцию потерь. Хорошо то, что функция потерь по сути такая же, как у двоичной классификации. Мы просто применим <a href="__GHOST_URL__/logharifmichieskaia-potieria-log-loss/">Логарифмическую потерю (Log Loss)</a> к каждому выходному узлу по отношению к его соответствующему целевому значению, а затем найдем сумму этих значений по всем выходным узлам.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/loss-function_categorical-cross-entropy.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/loss-function_categorical-cross-entropy.png 600w, __GHOST_URL__/content/images/2021/04/loss-function_categorical-cross-entropy.png 1000w"><figcaption>Категориальная кросс-энтропия</figcaption></figure><p>Эта потеря называется категориальной <a href="__GHOST_URL__/kross-entropiia/">Кросс-энтропией (Cross Entropy)</a>. Теперь перейдем к частному случаю классификации, называемому многозначной классификацией.</p><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE-%D0%BD%D0%B5%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%B8%D0%BC-%D0%BC%D0%B5%D1%82%D0%BA%D0%B0%D0%BC">Классификация по нескольким меткам</h3><p>Классификация по нескольким меткам (MLC) выполняется, когда нашей модели необходимо предсказать несколько классов в качестве выходных данных. Например, мы тренируем нейронную сеть, чтобы предсказывать ингредиенты, присутствующие на изображении какой-то еды. Нам нужно будет предсказать несколько ингредиентов, поэтому в Y будет несколько единиц.</p><p>Для этого мы не можем использовать Softmax, потому что он всегда заставляет только один класс "становиться единицей", а другие классы приводит к нулю. Вместо этого мы можем просто сохранить сигмоид на всех значениях выходных узлов, поскольку пытаемся предсказать индивидуальную вероятность каждого класса.</p><p>Что касается потерь, мы можем напрямую использовать логарифмические потери на каждом узле и суммировать их, аналогично тому, что мы делали в мультиклассовой классификации.</p><p>Теперь, когда мы рассмотрели классификацию, перейдем к регрессии.</p><h3 id="%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8F-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%B8">Потеря регрессии</h3><p>В <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a> наша модель пытается предсказать непрерывное значение, например, цены на жилье или возраст человека. Наша нейронная сеть будет иметь один выходной узел для каждого непрерывного значения, которое мы пытаемся предсказать. Потери регрессии рассчитываются путем прямого сравнения выходного и истинного значения.</p><p>Самая популярная функция потерь, которую мы используем для регрессионных моделей, – это <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратическая ошибка (MSE)</a>. Здесь мы просто вычисляем квадрат разницы между Y и Y<sub>Pred</sub> и усредняем полученное значение. </p><p>Автор оригинальной статьи: <a href="https://deeplearningdemystified.com/article/fdl-3">deeplearningdemystified.com</a></p><p>Фото: <a href="https://unsplash.com/@leni_eleni">@leni_eleni</a></p>		funktsiia-potieri	2021-04-14		
80	Справедливость (Fairness)		<p>Справедливость – метрика, которая проверяет, дает ли классификатор тот же результат для одного человека, что и для другого, который идентичен первому, за исключением одного или нескольких важных атрибутов, вроде цвета кожи или пола. </p><p>Это гибкое и субъективное понятие, которое необходимо оценивать в свете <br>обстоятельств и целей <a href="Справедливость – метрика, которая проверяет, дает ли классификатор тот же результат для одного человека, что и для другого, который идентичен первому, за исключением одного или нескольких важных атрибутов, вроде цвета кожи или пола. ">Машинного обучения (Machine Learning)</a>. <br>Сообщество FAT / ML (Fairness, Accountability, and Transparency in Machine Learning – англ. "Справедливость, Подотчетность и Прозрачность в Машинном обучении) предлагает следующее описание принципа справедливости: «Убедитесь, что алгоритмические решения не создают дискриминационных или несправедливых последствий для разных демографических групп».</p><p>Сообщество FAT объясняет, что оно сделало этот термин намеренно «широко интерпретируемым», чтобы облегчить его распространение, потому что «правильное применение этих принципов должно включать их понимание в конкретном контексте». </p><p>Под справедливостью понимается стремление к справедливым и равноправным результатам. Другими словами, справедливость позволяет избежать предвзятости, которая увековечивает или усиливает существующие социальные, экономические, политические или культурные преимущества или недостатки. Например, если алгоритм с большей вероятностью лишит заявителей женщин права на получение ссуд для начала предпринимательской деятельности, независимо от кредитоспособности заявителей, этот алгоритм может быть признанным несправедливым в отношении женщин (или предвзятым против них). </p><p>Однако также возможно, что в погоне за справедливостью алгоритм может намеренно внести предвзятость как средство устранения существовавшего ранее неравенства. В примере ссуды малому бизнесу модель может быть разработана таким образом, чтобы рейтинг кредитоспособности женщин был выше, чем у мужчин с аналогичными полномочиями, чтобы помочь устранить давние препятствия для женщин-предпринимателей. Следовательно, хотя справедливость и предвзятость взаимосвязаны, эти понятия не всегда однозначны.</p><p>Справедливость можно понимать по-разному в разных контекстах; она требует принятия определенных этических конструкций и критериев системы ценностей, которые используются для оценки определенного набора результатов и могут зависеть от прав и обязанностей отдельных лиц, определенных местными властями, законами или обычаями. Например, разные типы налоговых систем по-разному относятся к справедливости.</p><p>Все ли должны платить одинаковую сумму налога? Или же каждый налогоплательщик будет облагаться согласно прогрессивной шкале (у богатых налоги больше, чем у бедных, как в абсолютном, так и в процентном отношении). Ответы зависят от ценностей и убеждений: люди рассматривают разные обстоятельства по-разному.</p><p>Поскольку разные люди и группы обладают уникальными статусами, способностями и проблемами, аналитик – будь то человек или компьютер – должен учитывать все эти факторы, чтобы чтобы прийти к «справедливому» результату. Например, чьи потребности следует учитывать в большей мере при проектировании дороги? Как сделать дороги одинаково безопасными и доступными для людей, идущих пешком, передвигающихся на велосипедах или на автомобилях? А как насчет людей с физическими недостатками, которым может потребоваться специфический прямой доступ к дорогам, и дети, которым приходится ходить через небезопасные районы, чтобы добраться до общественного транспорта, направляющегося в школу? План разрабатывается, чтобы обеспечить базовый уровень доступа к дороге как можно большему количеству людей. </p><p>Принятая особая интерпретация справедливости может зависеть от местной культуры, местного правительства или конкретной группы, которая находится у власти. Независимо от того, проводятся ли анализы людьми или компьютерами, нет идеальных ответов к сложным реальным проблемам. Одним из мотивов использования машинного обучения в международном развитии является надежда на то, что компьютерный алгоритм может быть объективным и беспристрастным. Однако беспристрастность может также означать нечувствительность к нюансам ситуации, историческому неравенству, которое привело к тому, что определенные группы оказались в невыгодном положении, а также потребностям разных людей.</p><h3 id="%D0%B8%D1%81%D1%87%D0%B5%D0%B7%D0%BD%D0%BE%D0%B2%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D0%BF%D1%80%D0%B0%D0%B2%D0%B5%D0%B4%D0%BB%D0%B8%D0%B2%D0%BE%D1%81%D1%82%D0%B8-%D1%81-%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC-%D1%82%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D0%B9">Исчезновение справедливости с распространением технологий</h3><p>Исследования человеческого поведения показывают, что есть причины для беспокойства. Организации, использующие Машинное обучение, могут потерять из виду влияние решений компьютерных программ на людей. Растущая зависимость в принятии решений от машинного обучения, а не от человека, может способствовать явлению, известному как «этическое увядание», когда отдельные люди или организации приостанавливают свои этические рассуждения и принимают решения на основе финансовых или практических соображений. С увеличением автоматизации влияние на людей становится гораздо менее заметным, а этические последствия решений с большей вероятностью будут упущены из виду.</p><p>Еще одна проблема – это вина, возникающая при использовании Машинного обучения и вменяемая исполняющему лицу, которое в конечном итоге не несет ответственности за принятие решения. Ученые ввели термин «зона моральной деформации» для обозначения результатов введенной двусмысленности автоматизации и распределенного управления. Подобно тому, как внутри автомобиля находится шасси, поглощающее энергию столкновения, люди, выполняющие роли разработчиков, несут ответственность, когда что-то идет не так в полуавтоматической системе Машинного обучения. Например, в кредитной программе, которая использует ML для оценки кредитоспособности, кредитный специалист находится в зоне моральной деформации, беря на себя вину за несправедливый отказ.</p><p>Зоны этического увядания и моральной деформации могут быть вместе, например, при использовании системы на основе машинного обучения для принятия решений о найме. Предположим, компания внедряет новую систему программного обеспечения, которая оценивает кандидатов. Этическое увядание может стать проблемой, если руководство компании некритически принимает рекомендации системы по найму сотрудников из определенной группы. Кроме того, если компания принимает некачественные решения о приеме на работу, обвиняют не программное обеспечение, а скорее разработчиков. Сотрудник, составивший трудовой договор, также может заставить работодателя пребывать в зоне моральной деформации.</p><p>В международном развитии, хотя намерение состоит в том, чтобы помочь сообществам, более широкие цели организации (например, в области экономического или технологического развития), тем не менее, могут отдалиться от отдельных людей, затронутых проблемой, что может вызвать трудности или бедствие как для предполагаемых бенефициаров, так и для людей, которым поручено реализовать программу. Когда происходит упадок этических норм, люди просто не рассматривают этику как сферу своей ответственности, часто полагая, что любые этические соображения должным образом учтены технологией. В зонах моральной деформации ответственность по этическим соображениям неуместно возлагается на субъектов, которые имеют ограниченный контроль над решениями. Как может быть этическое увядание смягчено в контексте роста автоматизации? Как можно избежать зон моральной деформации? </p><p>Первый шаг – понять, что автоматизация решений с помощью методов Машинного обучения не обязательно улучшит равенство в кредитовании, жилье, найме, приеме в школу и других решения с потенциальными последствиями для изменения жизни. Напротив, чрезмерная зависимость от ML может закрепить предубеждения, существующие в обществе и выявленные на основе данных обучения, а также дистанцировать лиц, принимающих решения, от тех, на кого они влияют. Ответственность за результаты внедрения Машинного обучения должна быть распределены между разработчиками программного обеспечения, теми, кто собирает и готовит данные, теми кто вводит программное обеспечение в использование, и другими лицами, участвующими в процессе машинного обучения. Первый шаг состоит в том, чтобы признать вызов, который создает эта ситуация.</p><p>Автор оригинальной статьи: <a href="https://d-lab.mit.edu/sites/default/files/inline-files/Exploring_fairness_in_machine_learning_for_international_development_28022020_pages.pdf">MIT D-Lab</a></p><p>Фото: <a href="https://unsplash.com/@nicopic">@nicopic</a></p>		spraviedlivost	2021-04-16		
81	Дерево решений (Decision Tree)		<p>Дерево решений (решающее дерево) – это инструмент прогнозного моделирования, применяемого в ряде различных областей. Как правило, они строятся с помощью алгоритмического подхода, который определяет способы разделения набора данных на основе различных условий. Это один из наиболее широко используемых и практичных методов <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Контролируемого обучения (Supervised Learning)</a>. Деревья решений – это непараметрический метод обучения с учителем, используемый как для <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, так и для задач <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a>. Цель состоит в том, чтобы создать модель, которая предсказывает значение <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Feature)</a>, изучая простые правила принятия решений, выведенные из характеристик данных.</p><p>Правила принятия решений обычно имеют форму операторов if-then-else. Чем глубже дерево, тем сложнее правила и точнее модель. В дереве решений условиями называют узлы-разветвлений:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/04/image-2.png" class="kg-image" alt loading="lazy" width="730" height="265" srcset="__GHOST_URL__/content/images/size/w600/2022/04/image-2.png 600w, __GHOST_URL__/content/images/2022/04/image-2.png 730w" sizes="(min-width: 720px) 720px"><figcaption>7 условий-узлов дерева разного уровня</figcaption></figure><p>Прежде чем мы углубимся, давайте познакомимся с некоторыми терминами:</p><ul><li>Экземпляры – векторы <a href="__GHOST_URL__/priznak/" rel="noopener noreferrer">Признаков (Feature)</a>, атрибутов, которые определяют пространство ввода</li><li>Атрибут – количество, описывающее экземпляр</li><li>Концепция – функция, которая сопоставляет ввод с выводом</li><li>Целевая концепция – функция, которую мы пытаемся найти, то есть фактический ответ</li><li>Класс гипотез – набор всех возможных функций.</li><li><a href="__GHOST_URL__/vyborka/" rel="noopener noreferrer">Выборка (Sample)</a> – набор входных данных в паре с меткой, которая является правильным выходом. Также известна как <a href="__GHOST_URL__/trienirovochnyie-dannyie/" rel="noopener noreferrer">Тренировочные данные (Train Data)</a></li><li>Концепция кандидата – концепция, которая, по нашему мнению, является целевой</li><li><a href="__GHOST_URL__/tiestovyie-dannyie/" rel="noopener noreferrer">Тестовые данные (Test Data)</a> – аналогичен набору для обучения и используется для тестирования концепции кандидата и определения его производительности</li></ul><h3 id="%D0%B2%D1%81%D1%82%D1%83%D0%BF%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5">Вступление</h3><p>Дерево решений – это древовидный граф с узлами, представляющими место, где мы выбираем атрибут и задаем вопрос; ребра представляют собой ответы на вопрос; а листья представляют собой фактический результат или метку класса.</p><p>Деревья решений классифицируют примеры, сортируя их сверху вниз по от корня до листьев, причем листовой узел обеспечивает классификационное решение. Каждый узел в дереве соответствует одному из возможных ответов на вопрос. Этот процесс является рекурсивным по своей природе и повторяется для каждого поддерева.</p><p>Пример. Предположим, мы хотим поиграть в бадминтон в определенный день, скажем, в субботу. Как мы решим, играть или нет? Допустим, мы выходим на улицу и проверяем, жарко или холодно, проверяем скорость ветра и влажность, погоду: солнечно ли, облачно или дождливо. Мы принимаем во внимание все эти факторы.</p><p>Итак, мы рассчитываете все эти условия за последние десять дней и формируем справочную таблицу.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-combinations.png" class="kg-image" alt loading="lazy" width="536" height="403"></figure><p>Но что, если погода в субботу не соответствует ни одной из строк в таблице? Это может быть проблемой. Дерево решений было бы отличным способом представления таких данных, потому что оно учитывает все возможные пути, которые могут привести к окончательному решению, следуя древовидной структуре.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/decision-tree-1.png 600w, __GHOST_URL__/content/images/2021/04/decision-tree-1.png 1000w"><figcaption>Дерево решений для концепции "Игра в бадминтон"</figcaption></figure><p>Это обученное дерево решений. Каждый узел представляет атрибут, а ветвь каждого узла – его результат. Наконец, окончательное решение принимается на листьях. Если признаки являются Вещественными числами (Continuous Number), внутренние узлы проверят значение признака относительно порогового значения.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-continuious.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/decision-tree-continuious.png 600w, __GHOST_URL__/content/images/2021/04/decision-tree-continuious.png 1000w"></figure><p><br>Общий алгоритм для дерева решений можно описать следующим образом:</p><ul><li>Выберите лучший атрибут, который разделяет наблюдения на группы</li><li>Задайте соответствующий вопрос</li><li>Следуйте по путям ответов</li><li>Вернитесь к шагу 1</li></ul><p>Хороший сплит – это разделение двух разных этикеток на два варианта.</p><h3 id="%D0%B2%D1%8B%D1%80%D0%B0%D0%B7%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D0%B5%D0%B2-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9">Выразительность деревьев решений</h3><p>Деревья решений могут иметь в качестве узлов булевую пару значений "Да / Нет". Давайте воспользуемся ими для изучения трех логических концепций 'AND, OR и XOR'. Посмотрим, как строится дерево для первого оператора <em>"И" (AND). </em>Согласно логике оператора, любое значение False в столбцах A и B генерирует результат False, и только пара True генерирует положительный результат.  Конвертируя таблицу в деревья, мы рассматриваем два варианта развития событий: то A первично, то B, ибо такова иерархическая природа дерева решений. Посвятите одну-две минуты, чтобы проникнуться простой логикой дерева, дублирующей таблицу.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-gate-and-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/decision-tree-gate-and-1.png 600w, __GHOST_URL__/content/images/2021/04/decision-tree-gate-and-1.png 1000w"><figcaption>Дерево решений для оператора И (AND)</figcaption></figure><p>Есть две кандидат-концепции для создания дерева решений, которые подчиняются логике "И". Точно так же создается дерево решений, которое подчиняется логике "ИЛИ" (OR).</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-gate-or-2.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/decision-tree-gate-or-2.png 600w, __GHOST_URL__/content/images/2021/04/decision-tree-gate-or-2.png 1000w"><figcaption>Дерево решений для оператора ИЛИ (OR)</figcaption></figure><p>Давайте создадим дерево решений, выполняющее функцию XOR, используя 3 атрибута:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-gate-xor.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/decision-tree-gate-xor.png 600w, __GHOST_URL__/content/images/2021/04/decision-tree-gate-xor.png 1000w"><figcaption>Дерево решений для оператора «Исключительное ИЛИ» (XOR)</figcaption></figure><h3 id="%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%86%D1%8B-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%B0">Границы дерева</h3><p>Деревья решений делят пространство признаков на параллельные оси прямоугольники или гиперплоскости. Продемонстрируем это на примере. Давайте рассмотрим простую операцию "И" над двумя переменными. Предположим, что X и Y являются координатами по осям x и y соответственно, и нанесем возможные значения X и Y. На анимации изображено формирование границы принятия решения по мере принятия каждого решения:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-boundary.gif" class="kg-image" alt loading="lazy" width="600" height="234"></figure><p>Мы видим, что по мере принятия каждого решения пространство функций делится на меньшие прямоугольники, и больше точек данных классифицируются правильно.</p><h3 id="%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9-%D0%B8-scikit-learn">Дерево решений и Scikit-learn</h3><p>Дерево решений можно построить с помощью модуля SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree</code></pre><p>Мы будем использовать классический набор данных о подвидах цветка Ириса. </p><pre><code class="language-python">data = load_iris() # Загрузка данных в объект\nprint('Классы: ', data.target_names)</code></pre><p><a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> содержит информацию растениях в виде следующих признаков:</p><ul><li>Длина чашелистика</li><li>Ширина чашелистика</li><li>Длина лепестка</li><li>Ширина лепестка</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-iris.jpeg" class="kg-image" alt loading="lazy" width="500" height="410"></figure><p>Выделяют три класса растений:</p><pre><code class="language-python">Классы:  ['setosa' 'versicolor' 'virginica']</code></pre><ul><li>Ирис щетинистый (Iris Setosa)</li><li>Ирис разноцветный (Iris Versicolour)</li><li>Ирис виргинский (Iris Virginica)</li></ul><p>Задача – предсказать класс растения ириса по признакам. Извлечем данные так, чтобы наполнить столбцы данными и назвать их определенным образом:</p><pre><code class="language-python">X = data.data\ny = data.target\n\nprint('Количество наблюдений:', X.shape[0])</code></pre><p>Датасет совсем крошечный:</p><pre><code class="language-python">Количество наблюдений: 150</code></pre><p>Всегда полезно взглянуть на данные в исходном состоянии:</p><pre><code class="language-python">X[:4]</code></pre><p>В ячейке ниже показаны 4 атрибута первых четырех растений ириса:</p><pre><code class="language-python">array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2]])</code></pre><p>Теперь, когда мы извлекли атрибуты данных и соответствующие метки, мы разделим их, чтобы сформировать наборы данных для обучения и тестирования. Для этой цели мы будем использовать функцию <code>train_test_split()</code>, которая принимает атрибуты и метки в качестве входных данных и создает наборы <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовых данных (Test Data)</a>:</p><pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 47, test_size = 0.25)</code></pre><p>Мы установим критерий «энтропия», который параметризует способ разделения данных на тренировочную и учебную части:</p><pre><code class="language-python">clf = DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 50)</code></pre><p>Обучим классификатор:</p><pre><code class="language-python">clf.fit(X_train, y_train)</code></pre><p>Система отображает настройки классификатора по умолчанию:</p><pre><code class="language-python">DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,  min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best')</code></pre><p>Теперь используем обученный классификатор, чтобы предсказать подвиды ириса для тестового набора:</p><pre><code class="language-python">y_pred =  clf.predict(X_test)\n\nprint('Точность измерений (тренировочные данные): ', accuracy_score(y_true = y_train, y_pred = clf.predict(X_train)))\nprint('Точность измерений (тестовые данные): ', accuracy_score(y_true = y_test, y_pred = y_pred))</code></pre><p>В реальности такого высокого результата, конечно, приходится добиваться:</p><pre><code class="language-python">Точность измерений (тренировочные данные):  0.9553571428571429 \nТочность измерений (тестовые данные):  0.9736842105263158</code></pre><p>Чем больше значение <code>min_sample_split</code>, тем сглаженнее границы решения и маловероятнее <a href="__GHOST_URL__/pierieobuchieniie/">Переобучение (Overfitting)</a>. Посмотрим, как Модель (Model) представляет себе дерево решений:</p><pre><code class="language-python">tree.plot_tree(clf)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/decision-tree-iris.png" class="kg-image" alt loading="lazy" width="594" height="400"></figure><p>Чтобы создать первое разветвление, система предположила пограничное значение <code>X[3] ≤ 0.8</code> (ширина лепестка меньше 0,8 сантиметра). SkLearn, вероятно, указывает, как выглядит типичный представитель своего подвида в строках <code>value</code>.  Число наблюдений в каждом из двух разветвлений, как вы уже наверняка заметили, равно числу наблюдений родительской ветки.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/15VJ-OuTEjzdRmCuS1tPwePc9ZMfry208?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@cristina_gottardi">@cristina_gottardi</a></p><p>Автор оригинальной статьи: <a href="https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/">Shubham</a></p>		dierievo-rieshienii	2021-04-17		
82	Бэггинг (Bagging)		<p>Бэггинг (Бутстрэп-агрегирование) – это алгоритм, предназначенный для улучшения стабильности и точности алгоритмов <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, используемых для задач <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a> и <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a>.</p><p>Прежде чем мы перейдем к основному понятию статьи, давайте кратко рассмотрим важный базовый прием <a href="__GHOST_URL__/nauka-o-dannykh/">Науки о данных (Data Science)</a> – Бутстрап (Bootstrap).</p><p>Бутстрап – это мощный статистический метод оценки характеристик <a href="__GHOST_URL__/priznak/">Признака (Feature)</a> на основе <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a>. Мы поймем понятие, определив, является ли вычисленное <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее арифметическое (Mean)</a> или <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a> показательным с помощью этого метода.</p><p>Предположим, у нас есть выборка из массива данных на 100 значений, и мы хотим оценить, показательно ли среднее значение ее относительно <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Популяции (Population)</a>.</p><p>Мы можем вычислить среднее непосредственно классическим способом, разделив сумму значений на их количество:</p><!--kg-card-begin: markdown--><p>$$μ = \\frac{Σ_{i=1}^n a_i}{n}, где$$<br>\n$$μ\\space{–}\\space{среднее,}$$<br>\n$$Σ_{i=1}^n a_i\\space{–}\\space{сумма}\\space{всех}\\space{элементов}\\space{выборки},$$<br>\n$$n\\space{–}\\space{количество}\\space{наблюдений}$$</p>\n<!--kg-card-end: markdown--><p>Мы знаем, что наша выборка мала и среднее значение непоказательно. Улучшим оценку нашего среднего, используя бутстрэп:</p><ul><li>Создадим множество (например, 1000) случайных выборок нашего набора данных (мы можем выбрать одно и то же значение несколько раз)</li><li>Рассчитаем среднее значение каждой выборки</li><li>Вычислим среднее значение всех собранных нами средних значений и используем его в качестве наиболее показательного среднего.</li></ul><p>Например, мы сформировали три выборки из популяции и получили средние значения 2.3, 4.5 и 3.3. Взяв среднее от этой троицы, получим наиболее правдивое значение 3,367. Этот прием можно использовать для оценки других величин, таких как стандартное отклонение, и даже величин, используемых в алгоритмах Машинного обучения.</p><h2 id="%D0%B1%D1%83%D1%82%D1%81%D1%82%D1%80%D1%8D%D0%BF-%D0%B0%D0%B3%D1%80%D0%B5%D0%B3%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D0%B8%D0%BB%D0%B8-%D0%B1%D1%8D%D0%B3%D0%B3%D0%B8%D0%BD%D0%B3">Бутстрэп-агрегирование или бэггинг</h2><p>Бэггинг – это простой и очень мощный ансамблевый метод, который объединяет прогнозы из нескольких методов Машинного обучения вместе, чтобы предсказывать более точно, чем любая отдельная модель.</p><p>Бутстрэп-агрегирование – это процедура, которая используется для сокращения чрезмерной дисперсии (Variance) алгоритмов – Деревьев решений (Decision Tree), таких как Алгоритм классификации и регрессии (CART).</p><p>Деревья решений чувствительны к конкретным данным, на которых обучаются. Если <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные данные (Train Data)</a> изменены (например, обучение производится на подмножестве), результирующее дерево может быть совершенно другим, и прогнозы могут быть совершенно разными для каждого такого подмножества.</p><p>Предположим, у нас есть набор из 1000 экземпляров, и мы используем алгоритм CART. Тогда бэггинг будет работать следующим образом:</p><ul><li>Создадим множество (например, 100) случайных выборок нашего набора данных с возможностью переиспользования <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a></li><li>Обучим модель CART на каждом из сэмплов</li><li>Рассчитаем среднее значение для каждой модели.</li></ul><p>Например, если бы у нас было 5 деревьев решений, которые использовали следующую выборку: синий, синий, красный, синий и красный, мы бы предсказали наиболее распространенный класс “синий”.</p><p>При бэггинге с деревьями решений мы меньше беспокоимся о том, чтобы отдельные деревья достигали <a href="__GHOST_URL__/pierieobuchieniie/">Переобучения (Overfitting)</a> обучающими данными. По этой причине и для повышения эффективности отдельные деревья решений выращиваются глубоко (например, несколько обучающих выборок в каждом листовом узле дерева), и деревья не обрезаются. Эти деревья будут иметь как высокую дисперсию, так и низкую погрешность. Это важные характеристики моделей при объединении прогнозов.</p><p>Единственные параметры при объединении деревьев решений – это количество выборок и, следовательно, количество включаемых деревьев. До тех пор, пока точность не прекратит улучшаться, можно тренировать новые и новые деревья. Большое число моделей потребует времени, но переобучения не будет.</p><p>Как и сами деревья решений, Bagging можно использовать для задач классификации и регрессии.</p><h2 id="%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9-%D0%BB%D0%B5%D1%81">Случайный лес</h2><p><a href="__GHOST_URL__/sluchainyi-lies/">Случайные леса (Random Forest)</a> являются апгрейдом деревьев решений с бэггингом.</p><p>Проблема с деревьями решений, такими как CART, заключается в жадности: они выбирают, какую переменную использовать, используя “жадный” алгоритм, сводящий к минимуму ошибки. Таким образом, даже при использовании бэггинга деревья решений могут иметь много структурных сходств и, в свою очередь, иметь высокую корреляцию в своих прогнозах.</p><p>Объединение прогнозов из нескольких моделей в ансамбли способствует взрывному росту эффективности, если прогнозы из подмоделей некоррелированы или в лучшем случае слабо коррелированы.</p><p>Случайный лес изменяет алгоритм способа обучения поддеревьев, так что результирующие прогнозы из всех их имеют меньшую корреляцию.</p><p>В CART при выборе точки разделения алгоритму разрешено просматривать все переменные и их значения, чтобы выбрать наиболее оптимальную точку разделения. Алгоритм случайного леса изменяет эту процедуру таким образом, чтобы ограничиваться случайной выборкой функций, по которым выполняется поиск.</p><p>Количество признаков, которые могут быть найдены в каждой точке разделения, должно быть указано в качестве параметра алгоритма. Вы можете попробовать разные значения и настроить их с помощью <a href="__GHOST_URL__/kross-validatsiia/">Кросс-валидации (Cross Validation)</a>:</p><ul><li>Для классификации хорошее значение по умолчанию равно sqrt (p)</li><li>Для регрессии хорошее значение по умолчанию равно p / 3, где</li></ul><p>m – количество случайно выбранных объектов, которые можно искать в точке разделения,</p><p>p – количество входных переменных.</p><p>Например, если в наборе данных было 25 входных переменных для задачи классификации, то:</p><p>m = sqrt (25) = 5</p><h2 id="%D1%80%D0%B0%D1%81%D1%87%D0%B5%D1%82%D0%BD%D0%B0%D1%8F-%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Расчетная производительность</h2><p>Когда выборка сформирована, всегда найдутся наблюдения, не попавшие в нее. Эти записи называются Out-Of-Bag или OOB.</p><p>Производительность каждой модели на ее OOB-наблюдениях при усреднении может обеспечить некую точность бэггинг-моделей. Эту оценку производительности часто называют оценкой производительности OOB.</p><p>Эти показатели производительности представляют собой надежную оценку ошибки тестирования и хорошо коррелируют с оценками перекрестной проверки.</p><h2 id="%D0%B1%D1%8D%D0%B3%D0%B3%D0%B8%D0%BD%D0%B3-%D0%B8-scikit-learn">Бэггинг и Scikit-learn</h2><p>Бэггинг легко продемонстрировать с помощью соответствующей функции Scikit-learn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.datasets import make_classification</code></pre><p>Сгенерируем набор данных из 100 наблюдений и 4 признаков случайным образом:</p><pre><code class="language-python">X, y = make_classification(n_samples = 100, n_features = 4,\nn_informative = 2, n_redundant = 0,\nrandom_state = 0, shuffle = False)</code></pre><p>Применим Метод опорных векторов (SVM) в качестве метода оценки бэггинга:</p><pre><code class="language-python">clf = BaggingClassifier(base_estimator = SVC(),\nn_estimators = 10, random_state = 0).fit(X, y)</code></pre><p>Предскажем класс для наблюдения:</p><pre><code class="language-python">clf.predict([[0, 0, 0, 0]])</code></pre><p>Для записи, где каждый из четырех признаков равен нулю, класс будет равен единице:</p><pre><code class="language-python">array([1])</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать<a href="https://colab.research.google.com/drive/1ZkCKRq0VVFgJHTGN9FSW3X0zgj2a47sw?usp=sharing"> здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@wilstewart3">@wilstewart3</a></p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/">Jason Brownlee</a><br><br></p>		beghghingh	2021-04-21		
83	Логистическая регрессия (Logistic Regression)		<p>Логистическая регрессия (LogReg) – это алгоритм классификации, используемый для отнесения наблюдений к дискретному набору классов. В отличие от <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regression)</a>, которая выводит непрерывные числовые значения, LogReg преобразует свой вывод с помощью Сигмоида (Sigmoid), чтобы вернуть значение вероятности, которое затем может быть округлено в сторону одного из дискретных классов:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/--------------2021-04-21---10.17.38.png" class="kg-image" alt loading="lazy" width="2000" height="1100" srcset="__GHOST_URL__/content/images/size/w600/2021/04/--------------2021-04-21---10.17.38.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/--------------2021-04-21---10.17.38.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/04/--------------2021-04-21---10.17.38.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/04/--------------2021-04-21---10.17.38.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption>Сигмоидная функция (отмечена розовым) и дискретно классифицированные наблюдения (белые точки)</figcaption></figure><h3 id="%D0%BB%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F-%D0%B8-scikit-learn">Логистическая регрессия и Scikit-learn</h3><p>Натренируем <a href="__GHOST_URL__/nieironnaia-siet/">Нейронную сеть (Neural Network)</a>, предсказывающую, согласится ли клиент банка взять кредит. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB</code></pre><p>Загрузим <a href="__GHOST_URL__/dataset/" rel="noopener noreferrer">Датасет (Dataset)</a>:</p><pre><code class="language-python">scaled_data = pd.read_csv('https://www.dropbox.com/s/mlbceekfnkehd7p/scaled_bank_data.csv?dl=1')\nscaled_data.head()</code></pre><p>Подготовленный банковский набор данных выглядит следующим образом:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/04/logistic-regression.png" class="kg-image" alt loading="lazy" width="2000" height="243" srcset="__GHOST_URL__/content/images/size/w600/2021/04/logistic-regression.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/logistic-regression.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/04/logistic-regression.png 1600w, __GHOST_URL__/content/images/2021/04/logistic-regression.png 2362w"></figure><p>Все категориальные переменные, такие как «Работа», были закодированы, то есть каждому виду образования был присвоен числовой псевдоним. Для булевых признаков вроде «Ипотека» двум возможным значениям «Да» и «Нет» были присвоены значения 1 и 0. Числовые признаки, такие как «Возраст» были стандартизованы, дабы степень их воздействия на модель была адекватной.</p><p>Разделим данные на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочную (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовую (Test Data)</a> части. Чтобы оценить предсказательную способность модели, нам предстоит случайным образом отделить от основного массива двадцать процентов наблюдений на проверку. <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a> у нас предостаточно, потому опускаем те из них, что обладают наименьшей <a href="__GHOST_URL__/vazhnost-priznaka/">Важностью признака (Feature Importance)</a>:</p><pre><code class="language-python">X = scaled_data.drop(['День', 'Месяц', 'Индекс потребительских цен', 'Займ', 'Ипотека', 'Колебание уровня безработицы', 'y'], axis = 1)</code></pre><p><a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a> y мы тоже опустили в объекте X, потому что тут мы храним переменные, предсказывающие согласие клиента. Для y мы создадим отдельный одноименный объект, который будет представлять целевой признак:</p><pre><code class="language-python">y = scaled_data.y</code></pre><p>Библиотека SkLearn поможет нам максимально быстро разделить датасет случайным образом на тренировочную и тестовую части в стандартной пропорции 80 на 20:</p><pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y,train_size = 0.8, random_state = 1)</code></pre><p>Удостоверимся, что сплит прошел хорошо и уточним габариты полученных частей набора:</p><pre><code class="language-python">print("Разрешение тренировочного датасета на входе:", X_train.shape)\nprint("Разрешение тестового датасета на входе:", X_test.shape)\nprint("Разрешение тренировочного датасета на выходе:", y_train.shape)\nprint("Разрешение тестового датасета на выходе:", y_test.shape)</code></pre><p>Пропорция 20 на 80 выдержана (7.113 на 28.448):</p><pre><code class="language-python">Разрешение тренировочного датасета на входе:  (28448, 15)\nРазрешение тестового датасета на выходе:  (7113, 15)\nРазрешение тренировочного датасета на входе:  (28448,)\nРазрешение тестового датасета на выходе:  (7113,)</code></pre><p>Популярных типов <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> не так уж и много. Сейчас мы познакомимся с логистической регрессией – базовой разновидностью нейронной сети, которая прекрасно подходит для освоения азов. Инициируем ее, а вместе с этим еще и четыре других модели:</p><pre><code class="language-python">logreg_cv = LogisticRegression(random_state = 0)</code></pre><p>сv здесь – это <a href="__GHOST_URL__/kross-validatsiia/">Кросс-валидация (Cross Validation)</a>, то есть перекрестная проверка. Мы оценим с ее помощью эффективность каждой нейросети. Попробуем также <a href="__GHOST_URL__/dierievo-rieshienii/">Дерево решений (Decision Tree)</a>, <a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метод k-ближайших соседей (kNN)</a>, <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a> и Наивный байесовский классификатор (Naive Bayes):</p><pre><code class="language-python">dt_cv = DecisionTreeClassifier()\nknn_cv = KNeighborsClassifier()\nsvc_cv = SVC()\nnb_cv = BernoulliNB()</code></pre><p>В реальности дело зачастую обстоит так, что <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> не знает наверняка, какая модель лучше справится с задачей, потому обучает несколько таковых или использует так называемый ансамбль.</p><p>Создадим небольшой словарик со списком моделей, чтобы потом вывести точности каждой из них эффективно с помощью цикла:</p><pre><code class="language-python">cv_dict = {0: 'Логистическая регрессия', 1: 'Дерево решений', 2: 'Метод k ближайших соседей', 3: 'Метод опорных векторов', 4: 'Наивный байесовский классификатор’}</code></pre><p>С этой же целью создадим список моделей:</p><pre><code class="language-python">cv_models = [logreg_cv, dt_cv, knn_cv, svc_cv, nb_cv]</code></pre><p>И вот теперь мы отобразим с помощью цикла for, двойного счетчика <code>i, model</code> и функции <code>enumerate()</code> тестовые точности измерений каждой нейронной сети.</p><pre><code class="language-python">for i,model in enumerate(cv_models):\nprint("{} | Тестовая точность измерений: {}".format(cv_dict[i], cross_val_score(model, X, y, cv = 10, scoring = 'accuracy').mean()))</code></pre><p>Нам нужно как-то сравнивать модели между собой и в целом улучшать результаты, потому вводится термин <a href="__GHOST_URL__/tochnost-izmierienii/">«Точность измерений» (Accuracy)</a>.</p><p>Accuracy – это отношение правильно спрогнозированных <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> к общему их количеству. Для тех, кто готов погрузиться в понятие еще глубже, оставлю ссылку в ресурсах.</p><p>Давайте немного задержимся на этом моменте: мы запускаем обучение моделей таким необычным образом. Чтобы отобразить тестовую точность каждой из них, используем так называемую Интерполяцию (Interpolation) и метод <code>format().</code> На месте каждой пары фигурных скобок будет тип модели и соответствующая точность.</p><p>Передим для перекрестной проверки все необходимые аргументы – список признаков X, целевой признак y. Параметр cv, например, отвечает за способ формирования тестовых данных. Мы избираем <code>accuracy</code> в качестве метода сравнения. Метод <code>mean()</code> усредняет результат десяти проверок каждой из моделей на точность.</p><p>Вот таким невероятно кратким способом мы запустили пять моделей, провели по 10 проверок каждой и теперь можем узнать, какая нейросеть справилась лучше всего:</p><pre><code class="language-python">Логистическая регрессия | Тестовая точность измерений: 0.8781237696356271\nДерево решений | Тестовая точность измерений: 0.6430000192905322\nМетод k ближайших соседей | Тестовая точность измерений: 0.8746929484882704\nМетод опорных векторов | Тестовая точность измерений: 0.9188718011316903\nНаивный байесовский классификатор | Тестовая точность измерений: 0.8191811374646486</code></pre><p>Лучшая, как мы видим, – это Метод опорных векторов, которая добилась наивысшей точности в 91 с лишним процента. Это замечательный результат. Теперь мы можем передавать ей любую новую запись о клиенте, например, такую:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/--------------2021-04-21---10.13.52.png" class="kg-image" alt loading="lazy" width="2000" height="1100" srcset="__GHOST_URL__/content/images/size/w600/2021/04/--------------2021-04-21---10.13.52.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/--------------2021-04-21---10.13.52.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/04/--------------2021-04-21---10.13.52.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/04/--------------2021-04-21---10.13.52.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>И модель с высокой степенью точности предскажет, согласится ли человек взять кредит. Этот, кстати, согласится. Представьте, какие возможности это открывает для банков! По моим наблюдениям, крупные организации, вроде  Сбербанка, Тинькофф банка, используют Машинное обучение и другие характеристики клиентов для предварительных заявок на кредит.</p><p>К примеру, обнаружив обороты денежных средств (это тоже характеристика) на моей зарплатной карте, нейронная сеть Промсвязьбанка стала предлагать кредитный продукт определенного объема:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/--------------2021-04-21---10.13.48.png" class="kg-image" alt loading="lazy" width="2000" height="1100" srcset="__GHOST_URL__/content/images/size/w600/2021/04/--------------2021-04-21---10.13.48.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/--------------2021-04-21---10.13.48.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/04/--------------2021-04-21---10.13.48.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/04/--------------2021-04-21---10.13.48.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Так повышается эффективность рекламы, и холодных звонков совершать приходится меньше.</p><p>Ну что ж, теперь вы знаете больше о том, как бизнес совершенствует свои рекламные стратегии. Надеюсь, такие живые примеры подстегнут Вас исследовать и другие задачи Машинного обучения.</p><p>Мы уже достигли хорошей точности предсказаний, и последнее, что я хочу показать – это апгрейд существующей модели, его еще называют тюнингом <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a>. В предыдущей ячейке параметров нейросетей не было видно вообще, но они существуют, и порой за каждый больший процент точности приходится сражаться именно с ними.</p><p>Посмотрим, как выглядит тюнинг нашей первой LogReg:</p><pre><code class="language-python">param_grid = {'C': np.logspace(-4, 4, 50),\n'penalty': ['l1', ‘l2']}</code></pre><p><a href="__GHOST_URL__/gipierparamietr-c/">C (C Regularization Parameter)</a> – это параметр штрафования или неверной классификации. Оказывается, за каждую ошибку модель можно штрафовать (<code>penalty</code>), и это увеличит эффективность.</p><p>Создадим объект clf и с его помощью осуществим автопоиск лучших параметров модели. Делается это так:</p><pre><code class="language-python">clf = GridSearchCV(LogisticRegression(random_state = 0), param_grid, cv = 5, verbose = 0, n_jobs = -1)</code></pre><p><code>verbose</code>, равный 0, означает, что мы не хотим увидеть подробный отчет о поиске. <code>n_jobs</code> – это число одновременно запускаемых процессов поиска. -1 значит, что задействуется все процессоры. Теперь обучим нашу регрессионную модель с лучшими параметрами и определим новую точность:</p><pre><code class="language-python">best_model = clf.fit(X_train, y_train)\nprint(best_model.best_estimator_)\nprint("Средняя точность измерений:", best_model.score(X_test, y_test))</code></pre><p>Скор улучшился и составляет теперь 92 с лишним процента:</p><pre><code class="language-python">Средняя точность измерений:  0.9247856038239842</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1VEzhalQA6poEDLjahKs6knjJiXySY4bF?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@ninjason">@ninjason</a></p>		loghistichieskaia-rieghriessiia	2021-04-23		
84	Параметр регуляризации C (C Regularizaton Parameter)		<p>C (параметр регуляризации) – мера степени наказания <a href="__GHOST_URL__/modiel/">Модель (Model)</a> за каждую неверно классифицированную точку.</p><p>Метод опорных векторов – это такая разновидность <a href="__GHOST_URL__/alghoritm/">Алгоритма (Algorithm)</a> <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, которая пытается найти плоскости, отделяющие положительные точки от отрицательных:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/svm-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/svm-1.png 600w, __GHOST_URL__/content/images/2021/04/svm-1.png 1000w"></figure><p>Сплошная линия посередине представляет собой наилучшую возможную линию разделения положительных <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> от отрицательных. Точки в кружке – это Опорные векторы (Support Vector).</p><p>SVM также может находить разделительные кривые:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/svm-transformation-functions.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/svm-transformation-functions.png 600w, __GHOST_URL__/content/images/2021/04/svm-transformation-functions.png 1000w"></figure><p>Ядра – функции преобразования могут так трансформировать набор точек, что разделяющая гиперплоскость будет найдена.</p><h3 id="%D1%88%D1%83%D0%BC">Шум</h3><p>Реальные данные содержат так называемый шум, так что надежный классификатор SVM должен уметь игнорировать "зашумленные" <a href="__GHOST_URL__/vybros/">Выбросы (Outlier)</a>, чтобы обнаружить обобщаемую плоскость:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/c-hyperparameter-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/c-hyperparameter-1.png 600w, __GHOST_URL__/content/images/2021/04/c-hyperparameter-1.png 1000w"></figure><p>Соответственно, выделяют две разновидности зазоров (разделительных полей вдоль гиперплоскости) – жесткие и мягкие. Первые находят лучшую гиперплоскость, отделяющую положительные точки от отрицательных, так что ни один элемент не классифицируется неправильно. Второй же допускает неверную классификацию таких отстоящих наблюдений, и в этом случае разделяющая функция подвергается так называемому Наказанию (Penalizing) пропорционально степени ошибочной классификации. По умолчанию в большинстве библиотек реализованы мягкие зазоры.</p><p>Теперь понять значение гиперпараметра C будет легко: он контролирует, насколько мы хотим наказать модель за каждую неверно классифицированную точку. Большое значение C приводит к тому, что из всех возможных гиперплоскостей будет иметь приоритет тот, что совершил наименьшее количество классификационных ошибок. Низкое значение C, наоборот, выберет такую разделительную границу, что хорошо разделяет точки, допуская некоторую погрешность.</p><h3 id="%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80-%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8-c-%D0%B8-scikit-learn">Параметр регуляризации C и Scikit-learn</h3><p>Посмотрим, как влияет C на согласованность прогнозов. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport sklearn as sk\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import check_random_state\nfrom sklearn import datasets</code></pre><p>Сгенерируем датасет из 100 случайных наблюдений с 300 признаками у каждого:</p><pre><code class="language-python">rnd = check_random_state(1)\nn_samples = 100\nn_features = 300</code></pre><p>В SkLearn принято выделять два вида значений C: <code>l1</code> и <code>l2</code>.  Первая теория утверждает, что согласованность прогнозов невозможна из-за смещения l1. Однако согласованность модели с точки зрения нахождения правильного набора ненулевых параметров, а также их знаков, может быть достигнута путем <a href="__GHOST_URL__/normalizatsiia/">Нормализации (Normalization)</a> C.</p><pre><code class="language-python"># l1: только пять признаков считаются информативными\nX_1, y_1 = datasets.make_classification(n_samples = n_samples,\n                                        n_features = n_features, n_informative = 5,\n                                        random_state = 1)\n\n# l2: данные не разрежены, но признаков меньше\ny_2 = np.sign(.5 - rnd.rand(n_samples))\nX_2 = rnd.randn(n_samples, n_features // 5) + y_2[:, np.newaxis]\nX_2 += 5 * rnd.randn(n_samples, n_features // 5)</code></pre><p>Инициируем два алгоритма <code>LinearSVC</code> c двумя типами наказаний. Для классифицирующих моделей, особенно для метода опорных векторов, используют свой вид потерь – Потери Шарнира (Hinge Loss). Параметризуем цвета кривых обучения:</p><pre><code class="language-python">clf_sets = [(LinearSVC(penalty = 'l1', loss = 'squared_hinge', dual = False, tol = 1e - 3),\n             np.logspace(-2.3, -1.3, 10), X_1, y_1),\n            (LinearSVC(penalty = 'l2', loss = 'squared_hinge', dual = True),\n             np.logspace(-4.5, -2, 10), X_2, y_2)]\n\ncolors = ['navy', 'cyan', 'darkorange']\nlw = 2</code></pre><p>Настроим функцию графика, отображающего динамику параметра C для разных частей датасетов:</p><pre><code class="language-python">for clf, cs, X, y in clf_sets:\n    # Настроим график для каждой функции регрессии\n    fig, axes = plt.subplots(nrows = 2, sharey = True, figsize = (9, 10))\n\n    for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):\n        param_grid = dict(C = cs)\n        # Чтобы получить "хорошую" кривую, нам нужно большое число\n        # итераций и низкая Дисперсия случайной величины\n        grid = GridSearchCV(clf, refit = False, param_grid = param_grid,\n                            cv=ShuffleSplit(train_size = train_size,\n                                            test_size = .3,\n                                            n_splits = 250, random_state = 1))\n        grid.fit(X, y)\n        scores = grid.cv_results_['mean_test_score']\n\n        scales = [(1, 'No scaling'),\n                  ((n_samples * train_size), '1/n_samples'),\n                  ]\n\n        for ax, (scaler, name) in zip(axes, scales):\n            ax.set_xlabel('C')\n            ax.set_ylabel('CV Score')\n            grid_cs = cs * float(scaler)  # scale the C's\n            ax.semilogx(grid_cs, scores, label = "fraction %.2f" %\n                        train_size, color = colors[k], lw = lw)\n            ax.set_title('scaling = %s, penalty = %s, loss = %s' %\n                         (name, clf.penalty, clf.loss))\n\n    plt.legend(loc = "best")\nplt.show()</code></pre><p>На двух рисунках ниже показаны значения C на оси x и соответствующие баллы <a href="__GHOST_URL__/kross-validatsiia/">Кросс-валидации (Cross Validation)</a> на оси y для нескольких различных частей сгенерированного набора данных:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/c-regularization-parameter-l1.png" class="kg-image" alt loading="lazy" width="648" height="692" srcset="__GHOST_URL__/content/images/size/w600/2021/04/c-regularization-parameter-l1.png 600w, __GHOST_URL__/content/images/2021/04/c-regularization-parameter-l1.png 648w"><figcaption>В первом случае нет масштабирования, во втором – 1 / n_samples</figcaption></figure><p>Теория гласит, что для достижения согласованности прогнозирования параметр штрафа <code>l2</code> должен оставаться постоянным по мере роста количества выборок:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/04/c-regularization-parameter-l2-1.png" class="kg-image" alt loading="lazy" width="648" height="693" srcset="__GHOST_URL__/content/images/size/w600/2021/04/c-regularization-parameter-l2-1.png 600w, __GHOST_URL__/content/images/2021/04/c-regularization-parameter-l2-1.png 648w"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1ijUbYJHn9cpQ_UIAPPMdV3nyIoXR4T5C?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@sabeerdarr?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@sabeerdarr</a></p><p>Автор оригинальной статьи: <a href="https://queirozf.com/entries/choosing-c-hyperparameter-for-svm-classifiers-examples-with-scikit-learn">Felipe</a></p>		gipierparamietr-c	2021-04-24		
85	Кросс-валидация (Cross Validation)		<p>Кросс-валидация (перекрестная проверка) – это метод оценки <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> путем обучения нескольких из них на подмножествах доступных входных данных и их оценки на другом дополнительном подмножестве. Такая проверка используется для обнаружения <a href="__GHOST_URL__/pierieobuchieniie/">Переобучения (Overfitting)</a>, т.е. неспособности распознать паттерн.</p><p>Всегда необходимо проверять стабильность предсказывающего <a href="__GHOST_URL__/alghoritm/">Алгоритма (Algorithm)</a>: нам нужна уверенность в том, что модель имеет представление о большинстве шаблонов в данных, что ее эффективность не падает от шумных данных, или, другими словами, у нее низкий уровень <a href="__GHOST_URL__/smieshchieniie/">Смещения (Bias)</a> и <a href="__GHOST_URL__/dispiersiia/">Дисперсии (Variance)</a>.</p><h3 id="%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0">Проверка</h3><p>Валидация – это процесс принятия решения о том, приемлемы ли числовые результаты, определяющие предполагаемые взаимосвязи между переменными в качестве характеристики данных. Как правило, оценка <a href="__GHOST_URL__/oshibka/">Ошибки (Error)</a> для модели выполняется после обучения, более известного как оценка <a href="__GHOST_URL__/ostatok/">Остатков (Residuals)</a>. Мы выполняем численную оценка ошибкой обучения – разницы в предсказанных и истинных ответах. Однако это только дает нам представление о том, насколько хорошо модель работает с <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочными данными (Train Data)</a>. Однако вероятно, что модель склонна к переобучению или Недообучению (Underfitting). Проблема этого метода оценки заключается в том, что он не гарантирует приемлемый уровень обобщения новых неизвестных данных.</p><h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D1%83%D0%B4%D0%B5%D1%80%D0%B6%D0%B0%D0%BD%D0%B8%D1%8F-holdout-method">Метод удержания (Holdout Method)</h3><p>Простое решение вышеописанной проблемы заключается в удержании части обучающих данных и использовании ее для оценки предсказательной способности. Ошибка сообщает затем, как наша модель работает с новыми данными или валидационным набором. Хотя этот метод не требует дополнительных вычислительных затрат и лучше, чем традиционная проверка, он все же подвержен высокой дисперсии. Неизвестно, какие точки данных попадут в набор для проверки, и результат может быть совершенно разным для каждой случайной <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a>.</p><h3 id="%D0%BA%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE-k-%D0%B1%D0%BB%D0%BE%D0%BA%D0%B0%D0%BC">Кросс-валидация по K блокам</h3><p>Поскольку все возможные варианты загрузить в модель непросто, удержание части <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> для проверки создает проблему недообучения. Уменьшая объем обучающих данных, мы рискуем потерять важные закономерности и тенденции в наборе, что, в свою очередь, увеличивает ошибку, вызванную смещением. Итак, нам нужен метод, который предоставляет достаточно данных для обучения модели, а также оставляет достаточно данных для проверки. Кросс-валидация по K блокам (K-Fold Cross Vaidation) делает именно это.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/04/validation_data-k-fold_------------------1.png" class="kg-image" alt loading="lazy" width="1100" height="426" srcset="__GHOST_URL__/content/images/size/w600/2021/04/validation_data-k-fold_------------------1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/validation_data-k-fold_------------------1.png 1000w, __GHOST_URL__/content/images/2021/04/validation_data-k-fold_------------------1.png 1100w"></figure><p>При K-Fold проверке данные делятся на k подмножеств. Теперь удержание повторяется k раз, так что каждый из k подмножеств используется в качестве проверочного набора, а другие подмножества k-1 объединяются, чтобы сформировать обучающий набор. Ошибка усредняется по всем k испытаниям, чтобы получить обощенную эффективность нашей модели. Каждая точка данных попадает в набор для проверки ровно один раз и попадает в обучающий набор k-1 раз. Это значительно снижает смещение, поскольку мы используем большую часть данных для подгонки, а также значительно сокращаем дисперсию, поскольку большая часть данных также используется в наборе для проверки. Перестановка тренировочного и тестового наборов также повышает эффективность этого метода. По умолчанию K равен 5 или 10, но может принимать и любое другое значение.</p><h3 id="%D1%81%D1%82%D1%80%D0%B0%D1%82%D0%B8%D1%84%D0%B8%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%B0%D1%8F-%D0%BA%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE-k-%D0%B1%D0%BB%D0%BE%D0%BA%D0%B0%D0%BC">Стратифицированная кросс-валидация по K блокам</h3><p>В некоторых случаях может быть большой естественный дисбаланс. Например, в датасете о ценах на недвижимость может быть большое количество домов с высокой ценой. Или в случае <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, представителей одного класса может быть в несколько раз больше, чем другого. Для таких ситуаций и делается небольшое изменение в методике перекрестной проверки, так что каждый блок содержит примерно такую же пропорцию классов. Этот вариант проверки также известен как стратифицированная кросс-валидация по K блокам.</p><p>Вышеупомянутые методы называют неисчерпывающими методами перекрестной проверки. Они не вычисляют все способы разделения исходной выборки, т.е. вам просто нужно решить, сколько подмножеств необходимо сделать. Кроме того, это приближения метода, описанного ниже, также называемого исчерпывающим методом, который вычисляет все возможные способы разделения данных на обучающие и тестовые наборы.</p><h3 id="%D0%BF%D0%B5%D1%80%D0%B5%D0%BA%D1%80%D0%B5%D1%81%D1%82%D0%BD%D0%B0%D1%8F-%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0-%D0%B1%D0%B5%D0%B7-%D0%B8%D1%81%D0%BA%D0%BB%D1%8E%D1%87%D0%B5%D0%BD%D0%B8%D1%8F">Перекрестная проверка без исключения</h3><p>Leave-P-Out Cross Validation (LPO) оставляет P точек данных за пределами обучающего набора, т.е. если в исходной выборке имеется N точек данных, тогда N-P выборок используются для обучения модели, а P точек используются в качестве набора для проверки. Это повторяется для всех комбинаций, в которых исходный образец может быть отделен таким образом, а затем ошибка усредняется для всех испытаний, чтобы оценить общую эффективность.</p><p>Этот метод является исчерпывающим в том смысле, что он должен обучать и проверять модель для всех возможных комбинаций, а для умеренно больших P он может стать вычислительно невыполнимым. </p><p>Частным случаем этого метода является P = 1. Он известен как Поэлементная кросс-валидация (LOO). Этот метод предпочтительнее предыдущего, потому что не страдает от массивных вычислений, и количество возможных комбинаций равно количеству точек данных в исходной выборке (N).</p><p>Перекрестная проверка – очень полезный метод оценки эффективности вашей модели, особенно в тех случаях, когда вам нужно уменьшить переобучение. Это также полезно при определении <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a> при поиске наименьшей ошибки. </p><h3 id="%D0%BA%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F-%D0%B8-scikit-learn">Кросс-валидация и Scikit-learn</h3><p>Давайте посмотрим, как кросс-валидация реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import sklearn as sk\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC</code></pre><p>Используем датасет о пациентах, у которых диагностируют диабет. Отделим <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a> от <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>, создав объекты <code>X</code> и <code>y</code> соответственно. Для совершенствования модели используем так называемую Регуляризацию Лассо (L1 Regularization):</p><pre><code class="language-python">diabetes = datasets.load_diabetes()\nX = diabetes.data[:150]\ny = diabetes.target[:150]\nlasso = linear_model.Lasso()</code></pre><p>Создадим объект <code>cv_results</code>, в который загрузим результаты перекрестной проверки. Помимо переданных аргументов – типа регуляризации <code>lasso</code>, предикторов и целевой переменной, мы также определяем, насколько частей разделять датасет (<code>cv = 3</code>):</p><pre><code class="language-python">cv_results = cross_validate(lasso, X, y, cv = 3)\nsorted(cv_results.keys())\n\ncv_results['test_score']</code></pre><p>Отсортированный от большего к меньшему список точностей выглядит весьма плачевно: линейная модель справилась лишь с 33% наблюдений: </p><pre><code class="language-python">array([0.33150734, 0.08022311, 0.03531764])</code></pre><p>Перезапустим перекрестную проверку с другими настройками. Теперь мы используем стратегию оценки производительности <code>r2</code>, и отрицательное значение <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратической ошибки (MSE – Mean Squared Error)</a>:</p><pre><code class="language-python">scores = cross_validate(lasso, X, y, cv = 3,\n                        scoring = ('r2', 'neg_mean_squared_error'),\n                        return_train_score = True)\nprint(scores['test_neg_mean_squared_error'])\nprint(scores['train_r2'])</code></pre><p>Ну что ж, модель учебная, так что низкая предсказательная способность здесь не главное, хоть результат и улучшился до 39%. Процесс совершенствования модели здесь только начинается, и MSE каждой итерации – прекрасный способ сравнивать между собой качество последующих апгрейдов:</p><pre><code class="language-python">[-3635.51152303 -3573.34242148 -6114.78229547]\n[0.28010158 0.39088426 0.22784852]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/15IEP6h-eKxK83RfVhnKcnVgtEFXa9zAj?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@tricell1991?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@tricell1991</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f">@prashantgupta17</a></p>		kross-validatsiia	2021-04-28		
86	Квантовое машинное обучение (Quantum Machine Learning)		<p>Квантовая модель –алгоритм, способный представлять и обобщать данные квантово-механического происхождения. Для понимания термина необходимо ввести два вспомогательных понятия – квантовые данные и гибридные квантово-классические модели.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/shutterstock_1571766802.jpeg" class="kg-image" alt loading="lazy" width="1000" height="563" srcset="__GHOST_URL__/content/images/size/w600/2021/04/shutterstock_1571766802.jpeg 600w, __GHOST_URL__/content/images/2021/04/shutterstock_1571766802.jpeg 1000w"><figcaption>Квантовый компьютер</figcaption></figure><p>Квантовые данные демонстрируют суперпозицию и запутанность, что приводит к совместным распределениям вероятностей, для представления или хранения которых может потребоваться экспоненциальное количество классических вычислительных ресурсов.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/quantum-machine-learning-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/quantum-machine-learning-1.png 600w, __GHOST_URL__/content/images/2021/04/quantum-machine-learning-1.png 1000w"><figcaption><em>Квантовая суперпозиция</em></figcaption></figure><p>Квантовые данные, которые могут быть сгенерированы / смоделированы на квантовых процессорах / датчиках / сетях, включают моделирование химических веществ и квантовой материи, квантовое управление, квантовые коммуникационные сети, квантовую метрологию и многое другое.</p><p>Технический, но ключевой момент заключается в том, что квантовые данные, генерируемые процессорами NISQ (Noisy Intermediate-Scale Quantum), зашумлены и обычно запутываются непосредственно перед измерением. Однако применение квантового машинного обучения к зашумленным запутанным квантовым данным может максимизировать извлечение полезной классической информации. Вдохновленная этими методами, библиотека TensorFlow Quantum предоставляет примитивы для разработки моделей, которые распутывают и обобщают корреляции в квантовых данных, открывая возможности для улучшения существующих квантовых алгоритмов или открытия новых квантовых алгоритмов.</p><p>Вторая концепция, которую следует ввести, – это гибридные квантово-классические модели. Поскольку краткосрочные квантовые процессоры все еще довольно малы и шумны, квантовые модели не могут использовать только квантовые процессоры - процессоры NISQ должны будут работать вместе с классическими процессорами, чтобы стать эффективными. Поскольку TensorFlow уже поддерживает гетерогенные вычисления на процессорах, графических процессорах и TPU, это естественная платформа для экспериментов с гибридными квантово-классическими алгоритмами.</p><p><a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (Machine Learning)</a>, хотя оно не является точным моделированием систем в природе, способно изучать систему и прогнозировать ее поведение. За последние несколько лет классические модели показали себя многообещающими в решении сложных научных проблем, что привело к усовершенствованиям в обработке изображений для обнаружения рака, прогнозирования землетрясений, экстремальных погодных условий и обнаружения новых экзопланет. Благодаря недавнему прогрессу в развитии квантовых вычислений, разработка новых моделей квантового машинного обучения может оказать глубокое влияние на самые большие мировые проблемы, что приведет к прорывам в области медицины, материалов и связи. Однако на сегодняшний день не хватает исследовательских инструментов для открытия полезных моделей квантового машинного обучения, которые могут обрабатывать квантовые данные и выполняться на доступных сегодня квантовых компьютерах.</p><h3 id="%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D0%BE%D0%B5-%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">Программное обеспечение </h3><p>На сегодняшний день Google и IBM предлагают фреймворки для работы с квантовым ML:</p><ul><li><a href="https://www.tensorflow.org/quantum/overview">TensorFlow Quantum</a></li><li><a href="https://quantum-computing.ibm.com/docs/">IBM Quantum</a></li></ul><p>Фото: <a href="https://unsplash.com/@pueblovista?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@pueblovista</a></p><p>Автор оригинальной статьи: <a href="https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html">Alan Ho</a></p>		kvantovoie-mashinnoie-obuchieniie	2021-04-30		
87	Оптимизация (Optimization)		<p>Оптимизация – это процесс настройки <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a> для минимизации Целевой функции (Cost Function). Иными словами, это набор методов совершенствования <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.</p><p>Чтобы справляться со своей основной задачей, модель использует такую функцию, обладающую множеством аргументов. Значение каждого из этих аргументов меняется в ходе оптимизации. Выражаясь простыми словами, на анимации ниже можно увидеть, как целевая функция под воздействием того или иного оптимизатора стремится к локальному синему минимуму ошибки с разной "скоростью":</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/optimization.gif" class="kg-image" alt loading="lazy" width="1072" height="687" srcset="__GHOST_URL__/content/images/size/w600/2021/04/optimization.gif 600w, __GHOST_URL__/content/images/size/w1000/2021/04/optimization.gif 1000w, __GHOST_URL__/content/images/2021/04/optimization.gif 1072w"><figcaption>Популярные методы оптимизации</figcaption></figure><p>Важно минимизировать ошибку, поскольку она описывает несоответствие между истинным и предсказанным значениями параметра. Это сложная проблема, которая лежит в основе многих алгоритмов машинного обучения, от <a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/">Логистической регрессии (Logistic Regression)</a> до обучения искусственных <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a>. Методов оптимизации довольно много; на GIF выше изображено пять видов, включая оптимизатор <a href="__GHOST_URL__/impuls/">Импульса (Momentum)</a>, Среднеквадратичное распространение (RMSProp) и Адаптивную оценку момента (Adam).</p><h3 id="%D0%BA%D0%B0%D0%BA-%D0%B2%D1%8B%D0%B1%D1%80%D0%B0%D1%82%D1%8C-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC-%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8">Как выбрать алгоритм оптимизации</h3><p>Оптимизация относится к процедуре поиска аргументов функции, которые приводят к наилучшим (наибольшим или наименьшим) выходным значениям.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/perceptron.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/perceptron.png 600w, __GHOST_URL__/content/images/2021/04/perceptron.png 1000w"><figcaption>Оптимизаци – это и подбор весов w<sub>n</sub> узлов x<sub>n</sub> перцептрона</figcaption></figure><p>Наиболее распространенный тип проблем, встречающихся в машинном обучении, – это оптимизация непрерывной функции, где входными аргументами функции являются числа x<sub>n</sub> с плавающей запятой (Float Number). Выходные данные y в этом случае также представляют собой действительные числа.</p><p>Назовем проблемы этого типа оптимизацией непрерывных функций, чтобы отличать их от функций, которые принимают дискретные значения.</p><p>Существует множество различных алгоритмов оптимизации, которые можно использовать для решения задач оптимизации непрерывных функций, и, возможно, столько же способов их сгруппировать и обобщить. Как правило, чем больше информации о целевой функции доступно, тем проще ее оптимизировать.</p><p>Возможно, основное разделение в алгоритмах оптимизации – возможность дифференцировать целевую функцию в определенной точке:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/04/optimization.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/04/optimization.png 600w, __GHOST_URL__/content/images/2021/04/optimization.png 1000w"><figcaption>Градиентный спуск (Gradient Descent)</figcaption></figure><p>Может ли первая производная (градиент или наклон) функции быть вычислена для данного потенциального решения или нет? Это разделяет алгоритмы на те, которые могут использовать вычисленную информацию о градиенте, и те, которые этого не делают.</p><p>Дифференцируемая целевая функция – это алгоритмы:</p><ul><li>Использующие производные</li><li>Не использующие производные</li></ul><h3 id="%D0%B4%D0%B8%D1%84%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D0%B0%D1%8F-%D1%86%D0%B5%D0%BB%D0%B5%D0%B2%D0%B0%D1%8F-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F">Дифференцируемая целевая функция</h3><p>Дифференцируемая функция – это функция, в которой производная может быть вычислена для любой заданной точки во входном пространстве.</p><p>Производная функции для значения – это скорость или величина изменения функции в этой точке. Его часто называют Уклоном (Slope).</p><ul><li>Производная первого порядка (First-Order Derivative): наклон или скорость изменения целевой функции в заданной точке. Производная функции с более чем одной входной переменной (например, многомерные входные данные) обычно называется градиентом.</li><li>Градиент (Gradient): производная многомерной непрерывной целевой функции. Производная для многомерной целевой функции – это вектор, и каждый элемент в векторе называется частной производной или скоростью изменения данной переменной в точке, предполагающей, что все другие переменные остаются постоянными.</li><li>Частная производная (Partial Derivative): элемент производной многомерной целевой функции. Мы можем вычислить производную производной целевой функции, то есть скорость изменения скорости изменения целевой функции. Это называется второй производной.</li><li>Производная второго порядка (Second-Order Derivative): скорость, с которой изменяется производная целевой функции. Для функции, которая принимает несколько входных переменных, это матрица, называемая матрицей Гессе.</li><li>Матрица Гессе (Hessian Matrix): вторая производная функции с двумя или более входными переменными. Простые дифференцируемые функции можно оптимизировать аналитически с помощью исчисления. Как правило, интересующие нас целевые функции не могут быть решены аналитически.</li></ul><p>Оптимизация выглядит значительно проще, если можно вычислить градиент целевой функции, и поэтому было проведено гораздо больше исследований алгоритмов оптимизации, использующих производную, чем тех, которые этого не делают.</p><h3 id="%D0%B2%D0%B8%D0%B4%D1%8B-%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8">Виды оптимизации</h3><p>Выделяют десять основных видов Алгоритмов оптимизации (Solver), но список периодически пополняется:</p><ol><li><strong>Пакетный градиентный спуск</strong> (<strong>Batch Gradient Descent)</strong> вычисляет градиент, используя весь набор данных. Это отлично подходит для выпуклых или относительно гладких многообразий ошибок. В этом случае мы движемся несколько прямо к оптимальному решению, либо локальному, либо глобальному. Однако он не идеален для больших наборов данных, поскольку вычисление градиента на всем наборе данных может быть очень медленным.</li><li><strong>Стохастический градиентный спуск (SGD)</strong>: в отличие от пакетного градиентного спуска, SGD использует только одну выборку для выполнения каждого обновления. Образцы случайным образом перемешиваются и выбирается один для выполнения обновления.</li><li><strong>Мини-пакетный градиентный спуск</strong> <strong>(MBGD)</strong> – это комбинация пакетного градиентного спуска и SGD. Он использует мини-партию из «n» выборок для вычисления градиента на каждом шаге. Обычно это алгоритм выбора при обучении <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сети (NN)</a>.</li><li><a href="__GHOST_URL__/impuls/">Импульс (Momentum)</a> позволяет функции поиска создавать инерцию в пространстве, преодолевать колебания шумных градиентов и двигаться по ровным участкам пространства с целью точку Локального минимума (Local Minima).</li></ol><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-19.png" class="kg-image" alt loading="lazy" width="620" height="480" srcset="__GHOST_URL__/content/images/size/w600/2023/05/image-19.png 600w, __GHOST_URL__/content/images/2023/05/image-19.png 620w"></figure><ol><li><strong>Adagrad</strong>: этот алгоритм адаптивно масштабирует скорость обучения в зависимости от накопленного градиента на каждом шаге.</li><li><strong>RMSprop</strong> – улучшенная версия Adagrad, страдающего от снижения скорости обучения. RMSprop делит скорость обучения на экспоненциально затухающий средний квадрат градиентов.</li><li><strong>Адаптивная оценка момента (Adam)</strong> объединяет Momentum и RMSprop, вычисляет экспоненциальное скользящее среднее градиента и квадрат градиента, а параметры β<sub>n</sub> управляют скоростью затухания этих скользящих средних.</li><li><strong>Adadelta и Adamax</strong> являются вариантами алгоритма оптимизации Адама.</li><li><strong>Nadam</strong> <strong>(оценка адаптивного момента, ускоренная Нестеровым) –</strong> это вариант Адама с импульсом Нестерова.</li><li><strong>LBFGS (алгоритм Бройдена-Флетчера-Гольдфарба-Шанно с ограниченной памятью) </strong>аппроксимирует алгоритм Бройдена-Флетчера-Гольдфарба-Шанно (BFGS) с использованием ограниченного объема компьютерной памяти.</li></ol><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80-%D0%BA%D0%BE%D0%B4%D0%B0-%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8">Пример кода оптимизации</h3><p>Посмотрим, как работает простейший алгоритм. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np</code></pre><p>Зададим простейшую функцию линейной регрессии:</p><pre><code class="language-python">def linear_model(x, m, b):\n    return m * x + b</code></pre><p>Зададим функцию потерь – <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратическую ошибку (MSE)</a>:</p><pre><code class="language-python">def loss_function(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)</code></pre><p>Создадим функцию оптимизации (солвер) – градиентный спуск</p><pre><code class="language-python">def gradient_descent(x, y, m, b, learning_rate):\n    y_pred = linear_model(x, m, b)\n    error = y - y_pred\n    m_gradient = -2 * np.mean(x * error)\n    b_gradient = -2 * np.mean(error)\n    m -= learning_rate * m_gradient\n    b -= learning_rate * b_gradient\n    return m, b</code></pre><p>Сгенерируем игрушечные данные:</p><pre><code class="language-python">x = np.array([0, 1, 2, 3, 4])\ny = np.array([1, 3, 5, 7, 9])  # y = 2x + 1</code></pre><p>Зададим стартовые значения параметрам оптимизации m и b, а потом проведем обучение в 1000 итераций:</p><pre><code class="language-python">m = 0\nb = 0\n\nfor i in range(1000):\n    m, b = gradient_descent(x, y, m, b, learning_rate=0.01)\n    if i % 100 == 0:  # Print loss every 100 iterations\n        y_pred = linear_model(x, m, b)\n        print(f"Iteration {i}, Loss: {loss_function(y, y_pred)}")\n\nprint(f"Final parameters: m = {m}, b = {b}")</code></pre><p>Потери драматическим образом снизились в почти 6 раз, с ~24,75 до ~4,88</p><pre><code class="language-python">Iteration 0, Loss: 24.752399999999998\nIteration 100, Loss: 0.007062699700289552\nIteration 200, Loss: 0.0021329350719118114\nIteration 300, Loss: 0.0006441463206835073\nIteration 400, Loss: 0.00019453216739419578\nIteration 500, Loss: 5.8748708074470585e-05\nIteration 600, Loss: 1.7742107881958514e-05\nIteration 700, Loss: 5.358115989478818e-06\nIteration 800, Loss: 1.6181508503790829e-06\nIteration 900, Loss: 4.886815029243564e-07\nFinal parameters: m = 2.0002341674306385, b = 0.999332439924018</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1fAkkw1Qk-ar9hJVA2taXCTqT3cQBlbpU?usp=sharing">здесь</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/05/image-20.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure><p>Фото: <a href="https://unsplash.com/@villxsmil?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@villxsmil</a></p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/tour-of-optimization-algorithms/#:~:text=Optimization%20is%20the%20problem%20of,to%20training%20artificial%20neural%20networks.">Jason Brownlee</a></p>		optimization	2021-05-02		
88	Многослойный перцептрон (MLP)		<p>Многослойный персептрон (Multilayer Perceptron) – это <a href="__GHOST_URL__/nieironnaia-siet/">Нейронная сеть (Neural Network)</a>, область науки, в которой исследуется, как простые модели биологического мозга могут использоваться для решения сложных вычислительных задач, таких как прогнозирование в <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a>. </p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/multilayer-perceptron-neurons.jpg" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2021/05/multilayer-perceptron-neurons.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/05/multilayer-perceptron-neurons.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2021/05/multilayer-perceptron-neurons.jpg 1600w, __GHOST_URL__/content/images/2021/05/multilayer-perceptron-neurons.jpg 2000w" sizes="(min-width: 1200px) 1200px"><figcaption>Нейроны человеческого мозга. Фото: Массачусетский технологический институт</figcaption></figure><p>Персептрон – это модель отдельного нейрона, которая была предшественницей более крупных нейронных сетей.</p><p>Цель состоит не в создании реалистичных моделей мозга, а в разработке надежных <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a>, которые используются для моделирования сложных проблем.</p><p>Сила нейронных сетей заключается в их способности изучать паттерны в <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a> и как наилучшим образом связать его с <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>, которую мы хотим предсказать. Математически они способны создавать любую функцию и зарекомендовали себя как универсальный алгоритм аппроксимации, то есть замене одних объектов на другие, более упрощенные.</p><p>Прогностическая способность нейронных сетей обусловлена ​​иерархической или многоуровневой структурой сетей. Структура данных может выделять <a href="__GHOST_URL__/priznak/">Признаки (Feature)</a> в различных масштабах и объединять их в признаки более высокого порядка.</p><h3 id="%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D1%8B">Нейроны</h3><p>Строительными блоками нейронных сетей являются искусственные нейроны, простые вычислительные блоки, которые имеют взвешенные входные сигналы и вырабатывают выходной сигнал с помощью функции активации:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/mlp.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/mlp.png 600w, __GHOST_URL__/content/images/2021/05/mlp.png 1000w"><figcaption>Модель простого нейрона</figcaption></figure><h3 id="%D0%B2%D0%B5%D1%81-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%B0">Вес нейрона</h3><p>Возможно, вы знакомы с <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессией (Linear Regression)</a>, и в этом случае веса входных данных очень похожи на коэффициенты, используемые в уравнении регрессии.</p><p>Подобно линейной регрессии, каждый нейрон также имеет <a href="__GHOST_URL__/smieshchieniie/">Cмещение (Bias)</a>, которое можно рассматривать как входной вес, по умолчанию равный единице. Например, нейрон может иметь два источника входных данных, и в этом случае требуется три веса: один для каждого входного источника и один для весов, как на изображении выше.</p><p>Веса часто инициализируются небольшими случайными значениями, например значениями в диапазоне от 0 до 0,3, хотя могут использоваться более сложные схемы инициализации.</p><p>Как и в линейной регрессии, бо́льшие веса указывают на повышенную сложность <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. Желательно, чтобы веса в сети были небольшими, тогда применимы методы Регуляризации (Regularization).</p><h3 id="%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%B0%D1%86%D0%B8%D1%8F">Активация</h3><p>Взвешенные входные данные суммируются и передаются через Функцию активации (Activation Function), иногда называемую передаточной функцией. Это простое отображение суммированного взвешенного входа и выхода нейрона. Функция определяет порог, при котором нейрон <em>активируется</em>, и силу выходного сигнала.</p><p>Использовались исторически простые пошаговые функции активации: если суммарный вход был выше порога, например 0,5, то нейрон выводил бы значение 1,0, в противном случае - 0,0.</p><p>Традиционно используются нелинейные функции активации. Это позволяет сети более сложным образом комбинировать входные данные и, в свою очередь, расширять возможности функций, которые они могут моделировать. Обширно используются нелинейные функции, такие как Сигмоида (Sigmoid), которая выводит значение от 0 до 1 с s-образным распределением:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/loss-function_sigmoid.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/loss-function_sigmoid.png 600w, __GHOST_URL__/content/images/2021/05/loss-function_sigmoid.png 1000w"><figcaption>Преобразование выходных значений сигмоидой</figcaption></figure><p>Еще один пример – нелинейная функция гиперболического тангенса (tanh), которая выводит такое же распределение в диапазоне от -1 до +1.</p><h3 id="%D1%81%D0%B5%D1%82%D0%B8-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BE%D0%B2">Сети нейронов</h3><p>Нейроны организоваются в сети. Ряд нейронов называется слоем, а одна сеть может состоять из нескольких слоев. Архитектуру нейронов в сети часто называют топологией сети.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/mlp_netowrk.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/mlp_netowrk.png 600w, __GHOST_URL__/content/images/2021/05/mlp_netowrk.png 1000w"><figcaption>Модель простой нейросети</figcaption></figure><p>Нижний <em>входной Слой</em> (Layer), который принимает входные данные из нашего <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, называется видимым, потому что это открытая часть сети. Часто нейросеть рисуется с одним нейроном на входе: тогда это простое передаточное звено.</p><p>Слои после входного называются <em>скрытыми</em>, поскольку не подвергаются прямому воздействию. Простейшая сетевая структура заключается в том, чтобы иметь единственный нейрон в скрытом слое, который напрямую выводит значение.</p><p>При наличии вычислительной мощности и эффективных библиотек можно построить нейронные сети Глубокого обучения (Deep Learning), которое означает множество скрытых слоев. Сети глубокие, потому что ранее были бы невообразимо медленными, но с использованием современных методов и оборудования им требуются считаные секунды.</p><h3 id="%D0%B2%D1%8B%D1%85%D0%BE%D0%B4%D0%BD%D0%BE%D0%B9-%D1%81%D0%BB%D0%BE%D0%B9">Выходной слой</h3><p>Последний скрытый слой называется <em>выходным</em>, и он отвечает за вывод значений или их вектора в соответствующем формате.</p><p>Выбор функции активации в выходном слое сильно ограничен типом моделируемой проблемы. Например, </p><ul><li>Проблема регрессии может иметь единственный выходной нейрон, и этот нейрон может не иметь функции активации.</li><li>Задача Двоичной классификации (Binary Classification) может иметь один выходной нейрон и использовать сигмоиду  для вывода значения от 0 до 1, чтобы представить вероятный класс. С помощью порога, по умолчанию равного 0,5, эти вероятности превращаются в четкий класс.</li><li>Проблема Мультиклассовой классификации (Multi-Class Classification) может иметь несколько нейронов в выходном слое, по одному для каждого класса (например, три нейрона для трех видов цветов радужной оболочки глаза). В этом случае <a href="__GHOST_URL__/softmaks/">Функция активации Softmax</a> может использоваться для вывода вероятности предсказания сетью каждого из значений класса. Выбор результата с наибольшей вероятностью может быть использован для получения четкого значения класса.</li></ul><p>После настройки нейронную сеть необходимо обучить на вашем наборе данных.</p><h3 id="%D0%BF%D0%BE%D0%B4%D0%B3%D0%BE%D1%82%D0%BE%D0%B2%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Подготовка данных</h3><p>Сначала необходимо подготовить данные для обучения в нейронной сети. Они должны быть числами, например, Действительными (Float Number). Если у вас есть категориальные данные, такие как "Пол" со значениями «мужской» и «женский», мы можем произвести <a href="__GHOST_URL__/bystroie-kodirovaniie/">Горячее кодирование (One-Hot Encoding)</a>. Здесь для каждого значения класса добавляется один новый столбец (два столбца в случае пола мужчины и женщины), и в соответствующий столбец попадает единица или ноль в зависимости от пола.</p><p>Нейронные сети требуют согласованного масштабирования входных данных. Вы можете изменить масштаб до диапазона от 0 до 1, то есть произвести <a href="__GHOST_URL__/normalizatsiia/">Нормализацию (Normalization)</a>. Другой популярный метод - выполнить <a href="__GHOST_URL__/standartizatsiia/">Стандартизацию (Standartization)</a> так, чтобы распределение каждого столбца имело нулевое среднее значение и стандартное отклонение, равное единице.</p><p>Масштабирование также применяется к пиксельным данным изображения. Слова мы тоже можем преобразовать в числа, например, создав рейтинг его популярности в наборе данных.</p><h3 id="%D1%81%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9-%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9-%D1%81%D0%BF%D1%83%D1%81%D0%BA">Стохастический градиентный спуск</h3><p>Классический и все еще предпочитаемый алгоритм обучения нейронных сетей называется <a href="__GHOST_URL__/stokhastichieskii-ghradiientnyi-spusk/">Стохастическим градиентным спуском (SGD)</a>.</p><p>Здесь одна строка данных предоставляется сети в качестве входных данных. Сеть обрабатывает входные нейроны, активируя их по мере поступления, чтобы, наконец, сгенерировать выходные значения. Это называется Прямым проходом (Forward Pass) по сети, тип прохода, который также используется после обучения сети для прогнозирования новых данных.</p><p>Выходные данные сети сравниваются с ожидаемыми тестовыми значениями, и таким образом вычисляется <a href="__GHOST_URL__/oshibka/" rel="noopener noreferrer">Ошибка (Error)</a>. Она затем распространяется обратно по сети, по одному слою за раз, и веса обновляются. Этот математический элемент называется Методом обратного распространения ошибки (Backpropagation).</p><p>Этот процесс повторяется для всех <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> в тренировочных данных. Один раунд обновления сети для всего набора обучающих данных называется <a href="__GHOST_URL__/epokha/">Эпохой (Epoch)</a>. Сеть может быть обучена на протяжении десятков, сотен или миллионов эпох.</p><h3 id="%D0%BE%D0%B1%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B2%D0%B5%D1%81%D0%BE%D0%B2">Обновления весов</h3><p>Веса в сети могут быть обновлены на основе ошибок, рассчитанных для каждого обучающего примера, и это называется онлайн-обучением. Это может привести к быстрым, но также и хаотичным изменениям в сети.</p><p>В качестве альтернативы, ошибки могут быть сохранены во всех обучающих примерах, а сеть может быть обновлена ​​в конце. Это называется <a href="__GHOST_URL__/pakiet/">Пакетным (Batch)</a> обучением и часто бывает более стабильным.</p><p>Из-за величины набора данных и вычислительной эффективности, размер пакета, показываемого перед обновлением, часто сокращается до десятков или сотен примеров.</p><p>Количество обновляемых весов контролируется параметрами конфигурации, называемыми Скоростью обучения (Learning Rate). Он также называется размером шага и изменением веса сети для данной ошибки. Часто используются небольшие размеры веса, такие как 0,1 или 0,01. Уравнение обновления может быть дополнено дополнительными параметрами конфигурации:</p><ul><li><a href="__GHOST_URL__/impuls/">Импульс (Momentum)</a> – это термин, который включает свойства из предыдущего обновления, чтобы позволить весам изменяться в том же направлении, даже если ошибка уменьшается.</li><li>Снижение скорости обучения используется, чтобы сеть могла вносить бо́льшие изменения весов в начале и меньшие настройки под конец.</li></ul><h3 id="%D0%BF%D1%80%D0%BE%D0%B3%D0%BD%D0%BE%D0%B7">Прогноз</h3><p>После обучения нейронной сети ее можно использовать для прогнозирования. Мы можем делать прогнозы на основе <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовых данных (Test Data)</a>, чтобы оценить навыки модели на незнакомых ей данных. Вы также можете развернуть такую программу и использовать для непрерывного прогнозирования.</p><p>Топология сети и конечный набор весов – это все, что вам нужно сохранить для модели. Прогнозы выполняются путем предоставления входных данных в модель и выполнения прямого прохода, позволяющего генерировать выходные прогнозные данные.</p><p>Фото: <a href="https://unsplash.com/@manuelventurini">@manuelventurini</a></p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/neural-networks-crash-course/">Jason Brownlee</a></p>		mnoghosloinyi-piertsieptron	2021-05-05		
89	Метод опорных векторов (SVM)		<p>Метод опорных векторов (Support Vector Machine) – это алгоритм <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, который проецирует <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> в n-мерном пространстве <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a> с целью нахождения гиперплоскости, разделяющей наблюдения на классы:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svm-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svm-1.png 600w, __GHOST_URL__/content/images/2021/05/svm-1.png 1000w"></figure><p>Подход можно использовать как для <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, так и для задач <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a>. Чаще всего он используется в задачах классификации. Мы изображаем каждый элемент данных как точку в n-мерном пространстве, где n – количество признаков, причем значение каждой ячейки наблюдения является конкретной координатой в соответствующей плоскости. Затем мы выполняем классификацию, находя разделяющую гиперплоскость, которая разграничивает два класса.</p><p>Опорные векторы (обозначены розовым на изображении выше) формируются за счет пограничных наблюдений. Классификатор SVM – это граница, которая лучше всего разделяет два класса.</p><p>Как мы можем определить наилучшую гиперплоскость? Давайте разберемся:</p><ul><li>Сценарий 1: на картинке ниже у нас изображено три гиперплоскости (A, B и C). Легко заметить, что только гиперплоскость B правильно отделяет один класс записей от других. </li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/----------1svm-right-hyperplane-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/----------1svm-right-hyperplane-1.png 600w, __GHOST_URL__/content/images/2021/05/----------1svm-right-hyperplane-1.png 1000w"></figure><ul><li>Сценарий 2: на изображении теперь три гиперплоскости, и все они хорошо разделяют классы. Как мы определим правильную?</li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svmsvm-three-hyperplanes.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svmsvm-three-hyperplanes.png 600w, __GHOST_URL__/content/images/2021/05/svmsvm-three-hyperplanes.png 1000w"></figure><p>Плоскость с наибольшим расстоянием от крайних наблюдений обоих классов является наилучшей. Это расстояние называется <a href="__GHOST_URL__/polie/">Полем (Margin)</a>. </p><ul><li>Сценарий 3: если спроецировать крайние точки на каждую из разделяющих гиперплоскостей, то наилучшей окажется срединная:</li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svmmargin.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svmmargin.png 600w, __GHOST_URL__/content/images/2021/05/svmmargin.png 1000w"></figure><p>Важная причиной выбора гиперплоскости с более высоким запасом является надежность. Если мы выберем гиперплоскость с низким запасом, то высока вероятность ошибки в классификации.</p><ul><li>Сценарий 4: Несмотря на большие поля гиперплоскости B, наилучшей границей будет A, поскольку последняя не совершает ошибок классификации:</li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svmerror.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svmerror.png 600w, __GHOST_URL__/content/images/2021/05/svmerror.png 1000w"></figure><ul><li>Сценарий 5: разделить наблюдения на группы с помощью линейной гиперплоскости не получится, поскольку одна из записей серого цвета находится на территории кластера другого класса, и это <a href="__GHOST_URL__/vybros/">Выброс (Outlier)</a>:</li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svm-nonlinear.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svm-nonlinear.png 600w, __GHOST_URL__/content/images/2021/05/svm-nonlinear.png 1000w"></figure><p>Алгоритм SVM обладает Устойчивостью (Robustness) к выбросам и позволяет игнорировать выбросы и находить гиперплоскость с максимальным полем:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svm-outlier-handling.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svm-outlier-handling.png 600w, __GHOST_URL__/content/images/2021/05/svm-outlier-handling.png 1000w"></figure><ul><li>Сценарий 6: и снова линейного решения задаче не существует:</li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svm-circle-4.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svm-circle-4.png 600w, __GHOST_URL__/content/images/2021/05/svm-circle-4.png 1000w"></figure><p>SVM может решить и эту проблему без труда с помощью дополнительной функции <code>z = x<sup>2</sup> + y<sup>2</sup></code>. Благодаря введению оси z кластеры выглядят по-другому в паре осей x и z:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svm-z-axis.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svm-z-axis.png 600w, __GHOST_URL__/content/images/2021/05/svm-z-axis.png 1000w"></figure><p>На приведенном выше графике следует учитывать следующее:</p><ul><li> Все значения z всегда будут положительными, потому что это сумма квадратов x и y</li><li>На исходном графике белые точки появляются рядом с началом координат осей x и y, что снижает значение z. В то же время серые точки более удалены от начала координат, и это увеличивает z. </li></ul><p>В классификаторе SVM легко создать линейную гиперплоскость между этими двумя классами. Но возникает еще один животрепещущий вопрос: нужно ли добавлять эту функцию вручную, чтобы получить гиперплоскость? Нет, в алгоритме SVM есть метод, называемый Ядерным методом (Kernel Trick). Это функция, которая берет низкоразмерное входное пространство и преобразует его в более высокоразмерное, то есть преобразует неразрешимую проблему в разрешимую. Проще говоря, он выполняет несколько чрезвычайно сложных преобразований данных, а затем обнаруживает простой способ классификации.</p><p>Возвращаясь к исходным условиям сценария 6, мы получаем окружность как нелинейную гиперплоскость:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svm-back-to-x-y.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/svm-back-to-x-y.png 600w, __GHOST_URL__/content/images/2021/05/svm-back-to-x-y.png 1000w"></figure><h3 id="svm-%D0%B8-scikit-learn">SVM и Scikit-learn</h3><p>SVM прекрасно реализован в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport scipy\nfrom scipy import stats\n\nimport seaborn as sns; sns.set()\n\nimport sklearn as sk\n# сгенерируем игрушечный датасет\nfrom sklearn.datasets.samples_generator import make_blobs\n# используем функцию-классификатор SVC\nfrom sklearn.svm import SVC \n\n%matplotlib inline</code></pre><p>Сгенерируем датасет, где наблюдения сгруппированы в два кластера-класса по 25 наблюдений каждое. Стандартное отклонение внутри каждой группы равно 0.6. Построим <a href="__GHOST_URL__/tochechnaya-diagramma/">Точечную диаграмму (Scatterplot)</a> для нашего датасета:</p><pre><code class="language-python"># сгенерируем классифицированный набор данных\nX, y = make_blobs(n_samples = 50, centers = 2,\n                  random_state = 0, cluster_std = 0.60)\nplt.scatter(X[:, 0], X[:, 1], c = y, s = 50, cmap = 'autumn')</code></pre><p>Цветовая гамма "осень" окрасила кластеры в два цвета:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/svm-sklearn-initial.png" class="kg-image" alt loading="lazy" width="451" height="334"></figure><p>Наш классификатор попытается провести прямую линию, разделяющую два набора данных, и тем самым создать классифицирующую модель. Для двумерных данных задача зачастую легко выполнить вручную. Но сразу же мы видим проблему: существует несколько возможных разделительных линий, которые могут идеально различать два класса!</p><pre><code class="language-python"># создадим сетку. np.linspace() вернет равномерно расположенные числа в указанном интервале\nxfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c = y, s = 50, cmap = 'autumn')\n\n# выделим неверно классифицированные наблюдения крестиком\nplt.plot([0.6], [2.1], 'x', color = 'red', markeredgewidth = 2, markersize = 10)\n\n# построим сетку\nfor m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n    plt.plot(xfit, m * xfit + b, '-k')\n\n# зададим предельные значения оси x\nplt.xlim(-1, 3.5);</code></pre><p>Некоторые вариации гиперплоскостей ошибаются и в условиях задачи сразу будут признаваться непригодными. Однако среди сотен возможных границ есть множество тех, кто классфицирует записи корректно. Как выбрать наилучший вариант?</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/svm-sklearn-three-hyperplanes.png" class="kg-image" alt loading="lazy" width="451" height="334"></figure><p>Будущая гиперплоскость будет обладать полями, которые будут резервировать пространство на случай появления новых наблюдений ближе к разделительной черте. </p><pre><code class="language-python">xfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c = y, s = 50, cmap = 'autumn')\n\nfor m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n    yfit = m * xfit + b\n    plt.plot(xfit, yfit, '-k')\n    # отрисуем полупрозрачные поля разнойширины\n    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor = 'none',\n                     color = '#AAAAAA', alpha = 0.4)\n\nplt.xlim(-1, 3.5);</code></pre><p>Выбор между имеющимися многочисленными вариантами с появлением полей, к сожалению, не стал легче:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/svm-sklearn-three-hyperplanes-margin.png" class="kg-image" alt loading="lazy" width="451" height="334"></figure><p>На помощь здесь придет SVC (Support Vector Classification), который рассчитает оптимальную ширину поля и направление разделяющей гиперплоскости:</p><pre><code class="language-python">model = SVC(kernel = 'linear', C = 1E10) # выберем линейное решение\nmodel.fit(X, y) # обучим классификатор</code></pre><p>Настройки алгоритма по умолчанию выглядят следующим образом:</p><pre><code class="language-python">SVC(C=10000000000.0, break_ties=False, cache_size=200, class_weight=None,\n    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale',\n    kernel='linear', max_iter=-1, probability=False, random_state=None,\n    shrinking=True, tol=0.001, verbose=False)</code></pre><p>Нам предстоит инициализировать кастомную функцию <code>plot_svc_decision_function()</code>:</p><pre><code class="language-python">def plot_svc_decision_function(model, ax = None, plot_support = True):\n    # Отобразим разрешающую гиперплоскость в двумерном пространстве\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim() # определим максимальное значение по оси x\n    ylim = ax.get_ylim()\n    \n    # подготовим разметку для сетки\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    \n    Y, X = np.meshgrid(y, x) #  np.meshgrid() вернет матрицы координат\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n    \n    ax.contour(X, Y, P, colors = 'k', # нарисуем пунктирные границы полей  \n               levels = [-1, 0, 1], alpha = 0.5,\n               linestyles = ['--', '-', '--'])\n    \n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s = 300, linewidth = 1, facecolors = 'none');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)</code></pre><p>Несмотря на подробное описание функции, некоторые элементы (например, <code>support_vectors_</code>) работают неявно, что позволяет отнести SVM к <a href="__GHOST_URL__/chiernyi-iashchik/">Черным ящикам (Black Box)</a>.</p><pre><code class="language-python">plt.scatter(X[:, 0], X[:, 1], c = y, s = 50, cmap = 'autumn')\nplot_svc_decision_function(model);</code></pre><p>Наилучшее решение – гиперплоскость с максимальной шириной поля без классификационных ошибок выглядит так:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/svm-sklearn-best-hyperplane.png" class="kg-image" alt loading="lazy" width="451" height="334"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1a_tUs6AtVJxakcZ-z5xUxtEeIVGs9ZrU?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/?#">Sunil Ray</a>, <em><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html">Jake VanderPlas</a></em></p><p>Фото: <a href="https://unsplash.com/@utsmanmedia">@utsmanmedia</a></p>		mietod-opornykh-viektorov	2021-05-07		
90	Глубокое обучение (DL)		<p>Глубокое обучение (глубинное обучение) – это подраздел <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, связанный с алгоритмами, основанными на структуре и функциях мозга – искусственными <a href="__GHOST_URL__/nieironnaia-siet/">Нейронными сетями (Neural Network)</a>.</p><p>Если вы только начинаете заниматься глубоким обучением или некоторое обладаете некоторым опытом работы с нейронными сетями, то можете быть сбиты с толку. Лидеры и эксперты в этой области имеют представление о том, что такое глубокое обучение, и эти узкие точки зрения проливают свет на понятие.</p><h3 id="%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%E2%80%93-%D1%8D%D1%82%D0%BE-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D0%B5-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D0%B5%D1%82%D0%B8">Глубокое обучение – это большие нейронные сети</h3><p>Эндрю Ын, сооснователь Coursera и главный научный сотрудник Baidu Research, основал и Google Brain, что в конечном итоге привело к внедрению технологий глубокого обучения в сервисы Google. Он много говорил и писал о том, что такое глубокое обучение, и с этого можно начать.</p><p>В своих ранних докладах о глубоком обучении Эндрю описал глубокое обучение в контексте традиционных искусственных нейронных сетей. В своем выступлении 2013 года под названием «Глубокое обучение, самообучение и обучение без учителя» он описал идею глубокого обучения следующим образом:</p><p><em>"Используя симуляцию мозга, мы надеемся:</em></p><ul><li><em>Сделать алгоритмы обучения намного лучше и проще в использовании</em></li><li><em>Совершить революционные достижения в области Машинного обучения и Искусственного интеллекта (Artificial Intelligence).</em></li></ul><p><em>Я считаю, что это наш лучший шанс на пути к настоящему ИИ."</em></p><p>Позже его комментарии стали более тонкими. По словам Эндрю, суть глубокого обучения заключается в том, что теперь у нас достаточно быстрых компьютеров и достаточно данных для обучения больших нейронных сетей. Обсуждая, почему именно сейчас глубинное обучение набирает обороты на ExtractConf 2015 в докладе под названием «Что следует знать специалистам по данным о глубоком обучении», он прокомментировал:</p><p><em>"...очень большие нейронные сети, которые у нас есть, и ... огромные объемы данных, к которым у нас есть доступ..."</em></p><p>Он также прокомментировал важный момент: все дело в масштабе. По мере того, как мы создаем более крупные нейронные сети и обучаем их все большим количеством данных, их производительность продолжает расти. Это обычно отличается от других методов машинного обучения, которые достигают плато в производительности:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/deep-learning.png" class="kg-image" alt loading="lazy" width="1002" height="402" srcset="__GHOST_URL__/content/images/size/w600/2021/05/deep-learning.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/deep-learning.png 1000w, __GHOST_URL__/content/images/2021/05/deep-learning.png 1002w"></figure><p><em>"Для большинства алгоритмов обучения старых поколений ... производительность будет стабильной. … Глубокое обучение… – это первый класс алгоритмов… которые можно масштабировать. … Производительность становится все лучше по мере того, как вы предоставляете им больше данных".</em></p><p>Наконец, он ясно указывает на то, что преимущества глубокого обучения, которые мы наблюдаем на практике, исходят от обучения с учителем. Из выступления на ExtractConf в 2015 году он прокомментировал:</p><p><em>"Почти вся ценность глубокого обучения сегодня заключается в <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Контролируемом обучении (Supervised Learning)</a> или обучении на основе размеченных данных".</em></p><p>Эндрю часто упоминает, что мы увидим больше преимуществ, исходящих от неконтролируемого обучения, по мере того, как область глубокого обучения будет созревать, поскольку в реальности часто приходится иметь дело с обилием неразмеченных данных.</p><p>Джефф Дин (Jeff Dean) – старший научный сотрудник Google в группе по системам и инфраструктуре. Он принимал участие и, возможно, частично отвечал за масштабирование и внедрение глубокого обучения в Google. Джефф принимал участие в проекте Google Brain и разработке крупномасштабного программного обеспечения для глубокого обучения DistBelief, а затем и TensorFlow.</p><p>В своем выступлении 2016 года под названием «Глубокое обучение для построения интеллектуальных компьютерных систем» он сделал комментарий в том же ключе: глубокое обучение действительно связано с большими нейронными сетями:</p><p><em>"Когда вы слышите термин «глубокое обучение», просто представьте себе большую глубокую нейронную сеть. Глубокий обычно относится к количеству слоев, поэтому этот популярный термин используется в прессе. Я считаю их в целом глубокими нейронными сетями".</em></p><p>Он выступал с этим выступлением несколько раз и в модифицированном наборе слайдов для того же выступления он подчеркивает масштабируемость нейронных сетей, указывая на то, что результаты улучшаются с большим количеством данных и более крупными <a href="__GHOST_URL__/modiel/">Моделями (Model)</a>, которые, в свою очередь, требуют больше вычислительной мощности.</p><h3 id="%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%E2%80%93-%D0%B8%D0%B5%D1%80%D0%B0%D1%80%D1%85%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5-%D0%B8%D0%B7%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B9">Глубокое обучения – иерархическое изучение функций</h3><p>Помимо масштабируемости, еще одним часто упоминаемым преимуществом моделей глубокого обучения является их способность выполнять автоматическое извлечение <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a> из необработанных данных, также называемое освоение признаков</p><p>Йошуа Бенжио – еще один лидер в области глубокого обучения, хотя начинал с сильного интереса к автоматическому обучению функций, на которое способны большие нейронные сети.</p><p>Он описывает глубокое обучение с точки зрения способности алгоритмов обнаруживать и изучать хорошие представления с помощью функции обучения. В своей статье 2012 года под названием «Глубокое изучение представлений для неконтролируемого и трансфертного обучения» он прокомментировал:</p><p><em>"Алгоритмы глубокого обучения стремятся использовать неизвестную структуру входного распределения для обнаружения хороших представлений, часто на нескольких уровнях, с изученными функциями более высокого уровня, определенными в терминах функций более низкого уровня".</em></p><h3 id="%D0%BC%D0%B0%D1%81%D1%88%D1%82%D0%B0%D0%B1%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D0%BE%D0%B5-%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B2-%D1%80%D0%B0%D0%B7%D0%BD%D1%8B%D1%85-%D0%BE%D0%B1%D0%BB%D0%B0%D1%81%D1%82%D1%8F%D1%85">Масштабируемое глубокое обучение в разных областях</h3><p>Глубокое обучение лучше всего подходит для проблемных областей, где входы (и даже выходы) являются аналоговыми. Это означает, что это не несколько величин в табличном формате, а изображения пиксельных данных, документы текстовых данных или файлы аудиоданных.</p><p>Янн ЛеКун – директор Facebook Research и отец сетевой архитектуры, которая выделяется при распознавании объектов в данных изображения, называемой <a href="__GHOST_URL__/sviortochnaia-nieironnaia-siet/">Сверточной нейронной сетью (CNN)</a>. Этот метод пользуется большим успехом, потому что, как и многослойные нейронные сети с прямой связью Персептрона (Perceptron), метод масштабируется с учетом данных и размера модели и может быть обучен с помощью Обратного распространения (Back Propagation).</p><p>Это искажает его определение глубокого обучения как разработки очень больших CNN, которые добились большого успеха в распознавании объектов на фотографиях.</p><p>В своем выступлении в Ливерморской национальной лаборатории Лоуренса в 2016 году под названием «Ускорение понимания: глубокое обучение, интеллектуальные приложения и графические процессоры» он описал глубокое обучение в целом как изучение иерархических представлений и определил его как масштабируемый подход к созданию систем распознавания объектов:</p><p><em>"Глубокое обучение – набор модулей, каждый из которых можно обучить. … Обучение глубокое, потому что [имеет] несколько этапов в процессе распознавания объекта, и все эти этапы являются частью обучения".</em></p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/deep-learning_levels.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/deep-learning_levels.png 600w, __GHOST_URL__/content/images/2021/05/deep-learning_levels.png 1000w"><figcaption>Иерархическая презентация типов обучения. Фото: Unsplash@brondia</figcaption></figure><p>Юрген Шмидхубер является отцом другого популярного алгоритма, который, как Многоуровневый перцептрон (MLP) и CNN, также масштабируется в зависимости от размеров модели, набора данных и может быть обучен с помощью обратного распространения ошибки, но вместо этого адаптирован для данных последовательности обучения, называемой Долгая краткосрочная память (LSTM).</p><p>Мы действительно видим некоторую путаницу в формулировке понятия "Глубокое обучение". В своей статье 2014 года под названием «Глубокое обучение в нейронных сетях: обзор» он комментирует проблематичное именование области и различие между глубоким и поверхностным обучением. Он также интересно описывает глубину с точки зрения сложности проблемы, а не модели, используемой для решения проблемы:</p><p>"На какой глубине проблемы заканчивается поверхностное обучение и начинается глубокое? Обсуждения с экспертами пока не дали однозначного ответа на этот вопрос. […], Позвольте мне просто определить для целей этого обзора: проблемы глубины, превышающей 10 слоев, требуют глубокого обучения.</p><p>Демис Хассабис – основатель DeepMind, позже приобретенного Google. С Стартап в свое время совершил прорыв, объединив методы Deep Learning с <a href="__GHOST_URL__/obuchieniie-s-podkrieplieniiem/">Обучением с подкреплением (Reinforcement Learning)</a> для решения сложных задач обучения, таких как игра, что хорошо продемонстрировано в продуктах Atari и игре Alpha Go.</p><p>В соответствии с названием они назвали свою новую технику Deep Q-Network, сочетающую глубокое обучение с Q-Learning. Они также назвали эту более широкую область «Глубоким обучением с подкреплением».</p><p>В своем научном документе 2015 года под названием «Управление на уровне человека посредством глубокого обучения с подкреплением» они комментируют важную роль глубоких нейронных сетей в своем прорыве и подчеркивают необходимость иерархической абстракции:</p><p><em>"Для этого мы разработали новый агент, глубокую Q-сеть (DQN), которая способна сочетать обучение с подкреплением с классом искусственных нейронных сетей, известных как глубокие нейронные сети. Примечательно, что недавние достижения в области глубоких нейронных сетей, в которых несколько уровней узлов используются для построения все более абстрактных представлений данных, позволили искусственным нейронным сетям изучать такие концепции, как категории объектов, непосредственно из необработанных сенсорных данных".</em></p><p>Наконец, статью, которую можно считать определяющей в этой области, Янн ЛеКун, Йошуа Бенжио и Джеффри Хинтон опубликовали в журнале Nature под названием «Глубокое обучение». В ней они открываются с четкого определения глубокого обучения, подчеркивая многоуровневый подход.</p><p>Глубокое обучение позволяет вычислительным моделям, состоящим из нескольких уровней обработки, изучать представления данных с несколькими уровнями абстракции.</p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/what-is-deep-learning/">Jason Brownlee</a></p><p>Фото: <a href="https://unsplash.com/@redcharlie">@redcharlie</a></p>		glubokoye-obucheniye	2021-05-13		
91	Правило ассоциации (Association Rule)		<p>Правило ассоциации (ассоциация) – это метод определения взаимосвязи элементов друг с другом. В отличие от <a href="__GHOST_URL__/korrieliatsiia/">Корреляции (Correlation)</a>, где отыскиваются взаимосвязи между <a href="__GHOST_URL__/priznak/">Признаками (Feature)</a> <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, здесь идет речь о взаимосвязях внутри признака, например, дополняющие друг друга покупки в гипермаркете.</p><p>Ассоциацию можно измерить тремя распространенными способами. Для простоты адаптируем концепцию к тем же покупкам в гипермаркете.</p><ul><li>Поддержка (Support) – мера популярности набора товаров, если судить по доле транзакций, в которых он появляется. Он измеряется следующим образом:</li></ul><!--kg-card-begin: markdown--><p>$$S_M = \\frac{Число\\space{корзин}\\space{с}\\space{набором}\\space{M}}{Общее\\space{число}\\space{корзин}}\\space{, где}$$<br>\n$$S_M\\space{–}\\space{Поддержка}$$</p>\n<!--kg-card-end: markdown--><ul><li>Уверенность (Confidence) – степень уверенности в приобретении товара M<sub>2</sub> при покупке товара M<sub>1</sub>. Рассчитывается так:</li></ul><!--kg-card-begin: markdown--><p>$$С_{M_1 -&gt; M_2} = \\frac{Число\\space{корзин}\\space{с}\\space{наборами}\\space{M_1}\\space{и}\\space{M_2}}{Число\\space{корзин}\\space{с}\\space{набором}\\space{M_1}}\\space{, где}$$<br>\nC_M_1 -&gt; M_2\\space{–}\\space{Степень}\\space{уверенности}</p>\n<!--kg-card-end: markdown--><ul><li>Подъем (Lift) говорит о том, насколько вероятно, что предмет M<sub>2</sub> будет куплен, когда приобретен предмет M<sub>1</sub>, при этом контролируется, насколько популярным является предмет M<sub>2</sub>. Он измеряется следующим образом:</li></ul><!--kg-card-begin: markdown--><p>$$L_{M_1 -&gt; M_2} = \\frac{С_{M_1 -&gt; M_2}}{S_{M_2}}\\space{, где}$$<br>\n$$L_{M_1 -&gt; M_2}\\space{–}\\space{Степень}\\space{подъема}$$</p>\n<!--kg-card-end: markdown--><h3 id="%D0%B0%D1%81%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D1%86%D0%B8%D1%8F-%D0%B8-apriori">Ассоциация и Apriori</h3><p>Для демонстрации выделения ассоциаций мы используем готовый датасет покупок и специальную библиотеку Apriori. Для начала импортируем все необходимые компоненты:</p><pre><code class="language-python">!pip install apyori\n\nimport pandas as pd \nimport numpy as np\nimport apyori\nfrom apyori import apriori</code></pre><p>Мы используем готовый датасет с историей покупок, содержащий идентификатор покупателя, дату покупки и купленные товары. Посмотрим, какого размера наш набор:</p><pre><code class="language-python">groceries = pd.read_csv('https://www.dropbox.com/s/qoo8k66kxke52cw/Groceries_dataset.csv?dl=1')\ngroceries.shape</code></pre><p>Датасет состоит из 38+ тысяч записей с тремя признаками:</p><pre><code class="language-python">(38765, 3)</code></pre><p>Создадим несколько вспомогательных признаков. С помощью сочетания методов <code>groupby()</code> и <code>nunique()</code> мы сможем сгруппировать покупки по покупателям и подсчитать количество посещений гипермаркета:</p><pre><code class="language-python"># Разобъем записи на группы по дате и подсчитаем количество уникальных товаров\ngroceries_time = pd.DataFrame(groceries.groupby('Date')['itemDescription'].nunique().index)\n\n# Разобъем записи на группы по дате и подсчитаем количество уникальных посетителей\ngroceries_time['members_count'] = groceries.groupby('Date')['Member_number'].nunique().values\n\n# Разобъем записи на группы по дате и запишем все купленные отдельным посетителем товары в один список\ngroceries_time['items_count'] = groceries.groupby('Date')['itemDescription'].nunique().values\n\ngroceries_time['items'] = groceries.groupby('Date')['itemDescription'].unique().values\n\n# Зададим новый индекс\ngroceries_time.set_index('Date', inplace = True)\ngroceries_time.head()</code></pre><p>Теперь понятно, сколько за раз купил каждый пришедший:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/association.png" class="kg-image" alt loading="lazy" width="580" height="295"></figure><p>Нас интересует перечень покупок, потому мы выделим этот признак в отдельный список:</p><pre><code class="language-python">transactions = groceries_time['items'].tolist()</code></pre><p>Теперь применим функцию apriori. Зададим функции параметры по умолчанию – какой объект рассматривать (<code>transactions</code>), минимальный уровень поддержки и уверенности, форма выдачи результата. Ко всему прочему нам понадобится вспомогательная функция <code>inspect()</code>, которая сформирует <a href="__GHOST_URL__/datafrieim/">Датафрейм (DataFrame)</a> из обнаруженных ассоциативных пар:</p><pre><code class="language-python">rules = apriori(transactions = transactions, min_support = 0.00030, min_confidance = 0.01, min_lift = 3, min_length = 2, max_length = 2)\nresults = list(rules)\n\ndef inspect(results):\n    lhs = [tuple(result[2][0][0])[0] for result in results]\n    rhs = [tuple(result[2][0][1])[0] for result in results]\n    supports = [result[1] for result in results]\n    confidences = [result[2][0][2] for result in results]\n    lifts = [result[2][0][3] for result in results]\n    return list(zip(lhs, rhs, supports, confidences, lifts))\n\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Предмет #1', 'Предмет #2', 'Поддержка', 'Уверенность', 'Подъем'])\nresultsinDataFrame.head()</code></pre><p>Мы получили обширный список из 242 ассоциаций – настоящая кладезь для маркетолога! Посетитель гипермаркета и не поймет, почему в отделе с кухонными принадлежностями звучит ненавящевая реклама мужской косметики:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/association-most-associated-1.png" class="kg-image" alt loading="lazy" width="653" height="523" srcset="__GHOST_URL__/content/images/size/w600/2021/05/association-most-associated-1.png 600w, __GHOST_URL__/content/images/2021/05/association-most-associated-1.png 653w"></figure><p>Посмотрим, какие ассоциации самые ярковыраженные, отсортировав результирующий датафрейм <code>resultsDataFrame</code> от большего к меньшему:</p><pre><code class="language-python">resultsinDataFrame.nlargest(n = 10, columns = 'Lift')</code></pre><p>Наш Топ-10 выглядит весьма и весьма неожиданно:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/association-most-associated------.png" class="kg-image" alt loading="lazy" width="618" height="523" srcset="__GHOST_URL__/content/images/size/w600/2021/05/association-most-associated------.png 600w, __GHOST_URL__/content/images/2021/05/association-most-associated------.png 618w"></figure><p>Ликер и консервы! Вот это сочетание. Кухонные принадлежности и просекко – весьма вероятно и обыденно, впрочем, на перебор таких "очевидных" сочетаний может уйти много времени и рекламного бюджета, не так ли? В топ-3 попала пара "Кухонные принадлежности и Мужская косметика". Да, удивительное рядом. Так реклама становится гораздо точнее и эффективнее, а ведь это сокращает и баннерную нагрузку на конечных пользователей ритейла, что само по себе замечательно.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1dT_NQunTfszDyPOiBNZgWLX479rdXKn1?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.kaggle.com/gauravchopracg/introduction-to-association-rule-mining">Gaurav Chopra</a></p><p>Фото: <a href="https://unsplash.com/@montylov">@montylov</a></p>		pravilo-assotsiatsii	2021-05-15		
92	Алгоритм\t(Algorithm)		<p>Алгоритм – 1. В Машинное обучении это базис <a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модели (Model)</a>, такой набор функций, переменных и прочих объектов, который позволяет принять <a href="__GHOST_URL__/priediktor/" rel="noopener noreferrer">Предикторы (Predictor Variable)</a>, чтобы спрогнозировать с их помощью <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/" rel="noopener noreferrer">Целевую переменную (Target Variable)</a>. 2. В программировании это последовательность команд, предназначенная программе, в результате исполнения которой последняя решает определенную задачу.</p><p>Алгоритмы ML принято разделять на три основные группы:</p><h3 id="%D0%BA%D0%BE%D0%BD%D1%82%D1%80%D0%BE%D0%BB%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">Контролируемое обучение</h3><p><a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/" rel="noopener noreferrer">Контролируемое обучение (Supervised Learning)</a> требует переменных-предикторов и предсказываемого <a href="__GHOST_URL__/priznak/" rel="noopener noreferrer">Признака (Feature)</a>. Используя такие размеченные данные, мы генерируем аппроксимирующую функцию, то есть такую, что находит паттерны и тем самым позволяет предсказать значение выходной переменной на неизведанных данных в дальнейшем.</p><h3 id="%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B1%D0%B5%D0%B7-%D1%83%D1%87%D0%B8%D1%82%D0%B5%D0%BB%D1%8F">Обучение без учителя</h3><p><a href="__GHOST_URL__/niekontroliruiemoie-obuchieniie/" rel="noopener noreferrer">Неконтролируемое обучение (Unsupervised Learning)</a> подразумевает, что данные не размечены, то есть Целевую переменную не выделили. В таком случае задачей алгоритма становится, например, кластеризация Наблюдений (Observation) в значимые группы.</p><h3 id="%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81-%D1%87%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D1%8B%D0%BC-%D0%BF%D1%80%D0%B8%D0%B2%D0%BB%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC-%D1%83%D1%87%D0%B8%D1%82%D0%B5%D0%BB%D1%8F">Обучение с частичным привлечением учителя</h3><p>Полуавтоматическое обучение (Semi-Supervised Learning) отличается от контролируемого, где используются только размеченные данные. Популярный подход здесь – это создание <a href="__GHOST_URL__/graf/">Графа (Graph)</a>, который группирует наблюдения в <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a> и присваивает соответствующие <a href="__GHOST_URL__/iarlyk/">Ярлыки (Label)</a> тем из них, что находятся поблизости.</p><h3 id="%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81-%D0%BF%D0%BE%D0%B4%D0%BA%D1%80%D0%B5%D0%BF%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC">Обучение с подкреплением</h3><p><a href="__GHOST_URL__/obuchieniie-s-podkrieplieniiem/">Обучение с подкреплением (Reinforcement Learning)</a> подразумевает, что модель погружается в обучающую среду, где методом проб и ошибок обучается предсказывать значение выходной переменной. Такие алгоритмы активно используют свой прошлый опыт, чтобы увеличить собственную эффективность.</p><h3 id="%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B-ml">Распространенные алгоритмы ML</h3><p>Список существующих алгоритмов не ограничивается 11, но эти используют чаще всего:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/algorithm.png" class="kg-image" alt loading="lazy" width="650" height="572" srcset="__GHOST_URL__/content/images/size/w600/2021/05/algorithm.png 600w, __GHOST_URL__/content/images/2021/05/algorithm.png 650w"></figure><p>Фото:<a href="https://unsplash.com/@stephenmont"> @stephenmont</a></p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/">Sunil Ray</a></p>		alghoritm	2021-05-16		
93	Регрессия (Regression)		<p>Регрессия – группа <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Контролируемого обучения (Supervised Learning)</a>, используемых для прогнозирования непрерывных значений, таких как цены на недвижимость с учетом их характеристик (размер, цена и т.д.). </p><p>Выделяют следующие типы регрессионного анализа:</p><ul><li><a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейная регрессия (Linear Regression)</a></li><li>Полиномиальная регрессия (Polynomial Regression)</li><li>Регрессия опорных векторов (SVR)</li><li>Регрессия <a href="__GHOST_URL__/dierievo-rieshienii/">Дерева решений (Decision Tree)</a></li><li>Регрессия <a href="__GHOST_URL__/sluchainyi-lies/">Случайного леса (Random Forest)</a></li></ul><h3 id="%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F">Линейная регрессия</h3><p>Это одна из наиболее распространенных и доступных техник предсказания. Здесь мы прогнозируем <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a> Y на основе <a href="__GHOST_URL__/priediktor/">Предиктора (Predictor Variable)</a> X. Между первой и второй должна существовать линейная связь, и поэтому метод получил такое название.</p><p>Рассмотрим прогнозирование заработной платы сотрудника в зависимости от его возраста. Допустим, что существует корреляция между возрастом сотрудника и заработной платой (чем больше возраст, тем больше заработная плата). Гипотеза линейной регрессии такова:</p><!--kg-card-begin: markdown--><p>$$Y = a + bx\\space{,}\\space{где}$$<br>\n$$Y\\space{}{–}\\space{целевая}\\space{переменная,}$$<br>\n$$a\\space{,}\\space{b}\\space{–}\\space{коэффициенты}\\space{уравнения}$$</p>\n<!--kg-card-end: markdown--><p>Итак, чтобы предсказать Y (зарплату) с учетом X (возраста), нам нужно знать значения a и b (коэффициенты модели):</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/linear-regression_no-outlier-1.png" class="kg-image" alt loading="lazy" width="1000" height="401" srcset="__GHOST_URL__/content/images/size/w600/2021/05/linear-regression_no-outlier-1.png 600w, __GHOST_URL__/content/images/2021/05/linear-regression_no-outlier-1.png 1000w"></figure><p>Во время обучения регрессионной модели именно эти коэффициенты изучаются и подгоняются к обучающим данным. Цель тренировки – найти наиболее подходящую линию, минимизирующую <a href="__GHOST_URL__/funktsiia-potieri/">Функцию потерь (Loss Function)</a>. Последняя помогает измерить ошибку между фактическими и прогнозируемыми значениями.</p><p>На рисунке розовые точки – это реальные <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> – пары координат "Возраст – Зарплата", а белая линия – прогнозируемые значения оклада в зависимости от возраста. Чтобы сравнить реальное и прогнозируемое значения, точки фактических данных проецируются на линию.</p><p>Наша цель – найти такие значения коэффициентов, которые минимизируют функцию стоимости. Наиболее распространенная функция стоимости – это <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратичная ошибка (MSE)</a>, которая равна среднему квадрату разницы между фактическими и прогнозируемыми значениями наблюдения:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/mse.png" class="kg-image" alt loading="lazy" width="1580" height="756" srcset="__GHOST_URL__/content/images/size/w600/2021/05/mse.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/mse.png 1000w, __GHOST_URL__/content/images/2021/05/mse.png 1580w" sizes="(min-width: 720px) 720px"><figcaption>Квадраты дистанций между предсказанными и реальными значениями</figcaption></figure><p>Значения коэффициентов могут быть рассчитаны с использованием подхода Градиентного спуска (Gradient Descent). В градиентном спуске мы начинаем с некоторых случайных значений коэффициентов, вычисляем градиент функции потерь по этим значениям, обновляем коэффициенты и снова вычисляем функцию стоимости. Этот процесс повторяется до тех пор, пока мы не найдем минимальное значение функции стоимости.</p><h3 id="%D0%BF%D0%BE%D0%BB%D0%B8%D0%BD%D0%BE%D0%BC%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F">Полиномиальная регрессия</h3><p>В полиномиальной регрессии мы преобразуем исходные <a href="__GHOST_URL__/priznak/">Признаки (Feature)</a> в полиномиальные заданной степени, а затем применяем к ним линейную регрессию. Рассмотрим преобразованную линейную модель <code>Y = a + bX</code>:</p><!--kg-card-begin: markdown--><p>$$Y = a + bx + сx^2\\space{,}\\space{где}$$<br>\n$$Y\\space{–}\\space{целевая}\\space{переменная,}$$<br>\n$$a\\space{,}\\space{b,}\\space{с}\\space{–}\\space{коэффициенты}\\space{уравнения}$$</p>\n<!--kg-card-end: markdown--><p>Это все еще линейная модель, но кривая теперь квадратичная, а не прямая:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/regression.png" class="kg-image" alt loading="lazy" width="1001" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/regression.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/regression.png 1000w, __GHOST_URL__/content/images/2021/05/regression.png 1001w"></figure><p>Если мы увеличим степень до очень высокого значения, до достигнем <a href="__GHOST_URL__/pierieobuchieniie/">Переобучения (Overfitting)</a>, поскольку модель также "загребает" и <a href="__GHOST_URL__/shum/">Шум (Noise)</a>.</p><h3 id="%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F-%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2">Регрессия опорных векторов</h3><p>В SVR мы идентифицируем гиперплоскость с максимальным запасом, так что максимальное количество точек данных находится в пределах этого поля. SVR почти аналогична <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Методу опорных векторов (SVM)</a>:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/svm-sklearn-best-hyperplane-1.png" class="kg-image" alt loading="lazy" width="451" height="334"></figure><p>Вместо того, чтобы минимизировать частоту ошибок, как в простой линейной регрессии, мы пытаемся уместить ошибку в пределах определенного порога. Наша цель в SVR состоит в том, чтобы в основном учитывать моменты, которые находятся в пределах допуска. Наша лучшая линия – это гиперплоскость с максимальным количеством точек:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/regression-random-line.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/regression-random-line.png 600w, __GHOST_URL__/content/images/2021/05/regression-random-line.png 1000w"><figcaption>Наблюдения внутри "зоны охвата" полей</figcaption></figure><h3 id="%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%B0-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9">Регрессия дерева решений</h3><p>Деревья решений могут использоваться как для <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, так и для регрессии. В деревьях решений на каждом уровне нам нужно идентифицировать атрибут разделения. </p><p>Дерево решений строится путем разделения данных на подмножества, содержащие экземпляры с однородными значениями. <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a> используется для расчета однородности числовой <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a>. Если числовая выборка полностью однородна, ее стандартное отклонение равно нулю.</p><p>Шаги по поиску узла расщепления кратко описаны ниже:</p><ol><li>Рассчитайте стандартное отклонение целевой переменной</li><li>Разделите набор данных на разные атрибуты и вычислите стандартное отклонение для каждой ветви (стандартное отклонение для целевой переменной и предиктора). Это значение вычитается из стандартного отклонения перед разделением. Результатом является уменьшение стандартного отклонения.</li><li>В качестве узла разделения выбирается атрибут с наибольшим уменьшением стандартного отклонения.</li><li>Набор данных делится на основе значений выбранного атрибута. Этот процесс выполняется рекурсивно.</li></ol><p>Чтобы избежать переобучения, используется коэффициент отклонения, который решает, когда прекратить ветвление. Наконец, среднее значение каждой ветви присваивается соответствующему конечному узлу (при регрессии берется среднее значение).</p><h3 id="%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F-%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D0%BE%D0%B3%D0%BE-%D0%BB%D0%B5%D1%81%D0%B0">Регрессия Случайного леса</h3><p>Случайный лес – это Ансамблевый (Ensemble) подход, в котором мы учитываем прогнозы нескольких деревьев регрессии:</p><ol><li>Выберите K случайных точек</li><li>Определите n – количество создаваемых регрессоров дерева решений. Повторите шаги 1 и 2, чтобы создать несколько деревьев регрессии.</li><li>Среднее значение каждой ветви назначается конечному узлу в каждом дереве решений.</li><li>Чтобы предсказать результат для переменной, учитывается среднее значение всех прогнозов всех деревьев решений.</li></ol><p>Случайный лес предотвращает переобучение (что является обычным для деревьев решений) путем создания случайных подмножеств признаков и построения меньших деревьев с использованием этих подмножеств.</p><p>Автор оригинальной статьи: <a href="https://medium.datadriveninvestor.com/regression-in-machine-learning-296caae933ec">Apoorva Dave</a></p><p>Фото: <a href="https://unsplash.com/@kalenemsley">@kalenemsley</a></p>		rieghriessiia	2021-05-19		
94	Свёрточная нейронная сеть (CNN)		<p>Сверточная нейронная сеть (Convolutional Neural Networks, ConvNet) – класс Нейронных сетей (Neural Network), который специализируется на обработке данных, имеющих топологию в виде сетки, например изображений. Цифровое изображение – это двоичное представление визуальных данных. Он содержит серию пикселей, расположенных в виде сетки, где каждая ячейка содержит визуальные данные: яркость и цвет.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/cnn-pixel-grid.png" class="kg-image" alt loading="lazy" width="437" height="256"><figcaption>Изображение как сетка пикселей</figcaption></figure><p><br>Человеческий мозг обрабатывает огромное количество информации при просмотре изображения. Каждый нейрон работает в своем собственном рецептивном поле и связан с другими нейронами таким образом, что они покрывают все поле зрения. Подобно тому, как каждый нейрон реагирует на стимулы только в ограниченной области поля зрения, называемой рецептивным полем в системе биологического зрения, каждый нейрон в CNN также обрабатывает данные только в своем рецептивном поле. Слои расположены таким образом, что сначала они обнаруживают более простые узоры (линии, кривые и т. Д.), А затем более сложные узоры (лица, объекты и т. Д.). Используя CNN, можно обеспечить <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерное зрение (CV)</a>.</p><h3 id="%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0">Архитектура</h3><p>CNN обычно имеет три уровня: сверточный слой, слой объединения и полностью связанный слой:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/cnn-architecture------.png" class="kg-image" alt loading="lazy" width="772" height="344" srcset="__GHOST_URL__/content/images/size/w600/2021/05/cnn-architecture------.png 600w, __GHOST_URL__/content/images/2021/05/cnn-architecture------.png 772w"><figcaption>Архитектура CNN</figcaption></figure><h3 id="%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D0%B9-%D1%81%D0%BB%D0%BE%D0%B9"><br>Сверточный слой</h3><p>Сверточный слой является основным строительным блоком CNN. Он несет основную часть вычислительной нагрузки сети.</p><p>Этот уровень выполняет скалярное произведение между двумя матрицами, где одна матрица представляет собой набор обучаемых параметров, иначе называемых ядром, а другая матрица – ограниченной частью воспринимающего поля. Ядро пространственно меньше изображения, но имеет бо́льшую глубину. Это означает, что если изображение состоит из трех RGB-каналов, высота и ширина ядра будут пространственно малы, но глубина распространяется на все три канала:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/cnn-convolution.gif" class="kg-image" alt loading="lazy" width="1170" height="849" srcset="__GHOST_URL__/content/images/size/w600/2021/05/cnn-convolution.gif 600w, __GHOST_URL__/content/images/size/w1000/2021/05/cnn-convolution.gif 1000w, __GHOST_URL__/content/images/2021/05/cnn-convolution.gif 1170w" sizes="(min-width: 720px) 720px"><figcaption>Конволюция (свертывание)</figcaption></figure><p>Во время прямого прохода ядро ​​скользит по высоте и ширине изображения, создавая представление изображения этой рецептивной области. Это создает двумерное представление изображения, известное как карта Активации (Activation), которая дает реакцию ядра в каждой пространственной позиции изображения. Скользящий размер ядра называется шагом.<br>Если у нас есть входные данные размером W x W x D и количество ядер с пространственным размером F с шагом S и количеством отступов P, то размер выходного слоя можно определить по следующей формуле:</p><!--kg-card-begin: markdown--><p>$$W_{out} = \\frac{W - F + 2P}{S},\\space{где}$$<br>\n$$W_{out}\\space{–}\\space{размер}\\space{выходного}\\space{слоя,}$$<br>\n$$W\\space{–}\\space{размер}\\space{входного}\\space{слоя,}$$<br>\n$$F\\space{–}\\space{пространственный}\\space{размер,}$$<br>\n$$P\\space{–}\\space{число}\\space{отступов,}$$<br>\n$$S\\space{–}\\space{шаг}$$</p>\n<!--kg-card-end: markdown--><p>Мы получим выходное разрешение W<sub>out</sub> x W<sub>out</sub> x D<sub>out</sub>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/cnn-convolution-operation.png" class="kg-image" alt loading="lazy" width="452" height="824"><figcaption>Операция свертки</figcaption></figure><h3 id="%D0%BF%D1%80%D0%B8%D1%87%D0%B8%D0%BD%D1%8B-%D1%81%D0%B2%D0%BE%D1%80%D0%B0%D1%87%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F">Причины сворачивания</h3><p>Свертка использует три важные идеи, которые мотивировали исследователей компьютерного зрения: разреженное взаимодействие, совместное использование параметров и эквивариантное (равноценное) представление. Опишем подробно каждую из них.</p><p>Тривиальные слои нейронной сети используют результат перемножения матрицы на матрицу параметров, описывающих взаимодействие между входом и выходом. Это означает, что каждый блок вывода взаимодействует с каждым блоком ввода. Однако свёрточные нейронные сети взаимодействуют разреженно. Это достигается за счет уменьшения размера ядра по сравнению с входными данными, например, изображение может иметь миллионы или тысячи пикселей, но при его обработке с использованием ядра мы можем обнаружить значимую информацию, состоящую из десятков или сотен пикселей. Это означает, что нам нужно хранить меньше параметров, что не только снижает потребность модели в памяти, но и повышает ее статистическую эффективность.</p><p>Если вычисление одного объекта в пространственной точке (x<sub>1</sub>, y<sub>1</sub>) полезно, то оно также должно быть полезно в какой-то другой пространственной точке, например (x<sub>2</sub>, y<sub>2</sub>). Это означает, что для одного двумерного среза, то есть для создания одной карты активации, нейроны вынуждены использовать один и тот же набор весов. В традиционной нейронной сети каждый элемент матрицы весов используется один раз, а затем никогда не пересматривается, в то время как сеть свертки имеет общие параметры, то веса, применяемые к одному входу, такие же, как веса, применяемый в другом.</p><p>Благодаря совместному использованию параметров слои сверточной нейронной сети будут иметь свойство эквивалентности трансляции: если мы каким-то образом изменили входные данные, выходные также изменятся.</p><h3 id="%D1%81%D0%BB%D0%BE%D0%B9-%D0%BF%D1%83%D0%BB%D0%B8%D0%BD%D0%B3%D0%B0">Слой пулинга </h3><p>Слой пулинга (объединения) заменяет выходные данные сети в определенных местах, получая сводную статистику ближайших выходов. Это помогает уменьшить пространственный размер представления, что уменьшает необходимое количество вычислений и весов. Операция объединения обрабатывается отдельно для каждого фрагмента представления.</p><p>Существует несколько функций объединения, таких как среднее значение прямоугольной окрестности, норма L2 для прямоугольной окрестности и средневзвешенное значение, основанное на расстоянии от центрального пикселя. Однако самый популярный процесс – это максимальный пулинг, которое сообщает о максимальном выходе из окружения.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/cnn-convolution-operation_pooling-1.png" class="kg-image" alt loading="lazy" width="1000" height="401" srcset="__GHOST_URL__/content/images/size/w600/2021/05/cnn-convolution-operation_pooling-1.png 600w, __GHOST_URL__/content/images/2021/05/cnn-convolution-operation_pooling-1.png 1000w"></figure><p>Если у нас есть карта активации размером W x W x D, объединяющее ядро ​​пространственного размера F и шаг S, то размер выходного объема можно определить по следующей формуле:</p><!--kg-card-begin: markdown--><p>$$W_{out} = \\frac{W - F}{S} + 1,\\space{где}$$<br>\n$$W_{out}\\space{–}\\space{размер}\\space{выходного}\\space{слоя,}$$<br>\n$$W\\space{–}\\space{размер}\\space{входного}\\space{слоя,}$$<br>\n$$F\\space{–}\\space{пространственный}\\space{размер,}$$<br>\n$$S\\space{–}\\space{шаг}$$</p>\n<!--kg-card-end: markdown--><p>Это позволит вычислить выходной объем размером W<sub>out</sub> x W<sub>out</sub> x D.</p><p>Во всех случаях объединение обеспечивает некоторую инвариантность трансляции, что означает, что объект будет узнаваемым независимо от того, где он появляется в кадре.</p><h3 id="%D0%BF%D0%BE%D0%BB%D0%BD%D0%BE%D1%81%D1%82%D1%8C%D1%8E-%D1%81%D0%B2%D1%8F%D0%B7%D0%B0%D0%BD%D0%BD%D1%8B%D0%B9-%D1%81%D0%BB%D0%BE%D0%B9">Полностью связанный слой</h3><p>Нейроны в этом слое имеют полную связь со всеми нейронами предыдущего и последующего слоя, как это видно в обычном FCNN (Fully-Connected Convolutional Neural Network). Вот почему его можно вычислить как обычно, умножением матрицы с последующим эффектом смещения.</p><p>Полностью связанный cлой помогает сопоставить представление между данными на входе и выходе.</p><h3 id="%D0%BD%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B5-%D1%81%D0%BB%D0%BE%D0%B8">Нелинейные слои</h3><p>Поскольку свертка – это линейная операция, а изображения далеки от линейности, нелинейные слои часто размещаются непосредственно после сверточного слоя, чтобы внести нелинейность в карту активации.<br>Существует несколько типов нелинейных операций, самые популярные из которых:</p><ol><li><strong>Сигмоида</strong>. Сигмоидальная нелинейность берет Действительное число (Float Number) и «сжимает» его до диапазона от 0 до 1. Нежелательным свойством сигмоида является обнуление градиента, что когда активация происходит на любом из хвостов. Если локальный градиент становится очень маленьким, то при Обратном распространении (Back Propagation) он эффективно «убивает» градиент. Кроме того, если данные, поступающие в нейрон, всегда положительны, то на выходе сигмоида будут либо все положительные, либо все отрицательные значения, что приведет к зигзагообразной динамике обновления градиента для веса.</li><li><strong><strong>Tanh</strong></strong><br>Tanh (гиперболический тангенс числа) сжимает действительное число до диапазона [-1, 1]. Как и с cигмоидой, активация "насыщается", но в отличие от сигмоидных нейронов, ее выход центрирован относительно нуля.</li><li><strong>Функция активации выпрямителя (ReLU)</strong> стала очень популярной в последние несколько лет. У активации просто нулевой порог. По сравнению с сигмоидом и tanh, ReLU более надежен и ускоряет сходимость в шесть раз. К сожалению, минус в том, что ReLU может быть хрупким во время тренировки. Большой градиент, протекающий через него, может обновить его таким образом, что нейрон никогда не будет обновляться дальше. Однако мы можем работать с этим, установив надлежащую скорость обучения.</li></ol><h3 id="c%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F-%D1%81%D0%B5%D1%82%D1%8C-%D0%B8-tensorflow">Cверточная нейронная сеть и TensorFlow</h3><p>CNN удобно продемонстрировать с помощью фреймворка Google TensorFlow и датасета с изображениями животных и транспорта (CIFAR-10). Нам понадобится библиотека TensorFlow Keras – это  модуль для построения моделей глубинного обучения, matplotlib – это известнейшая библиотека для визуализации двумерной графики. Мы будем с вами использовать такие методы как figure(), show() и imshow(). Мы будем создавать области построения графика, отображать графики и преобразовывать набор пикселей в изображения.</p><pre><code class="language-python">import tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\n\nimport matplotlib\nimport matplotlib.pyplot as plt</code></pre><p>Мы импортируем два набора датасетов – это тренировочный, на котором будет производиться непосредственно обучение и проверочный, валидационный. Грубо говоря, мы используем одну и ту же базу данных, разобьем ее на 2 части. Все изображения, которые мы будем использовать, любезно промаркированы для нейросети, которую мы будем обучать, поэтому мы используем train_labels.</p><pre><code class="language-python">(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n\n# Нормализация значений цвета пикселей (приведем к диапазону от 0 до 1)\ntrain_images, test_images = train_images / 255.0, test_images / 255.0</code></pre><p>Что же такое CIFAR-10? Канадский институт перспективных исследований любезно подготовил обширную базу данных из 60 тысяч изображений объектов 10 видов. Мы сможем натренировать нашу нейронную сеть с помощью нескольких тысяч изображений лошадей, грузовиков, оленей, птиц и так далее. На самом деле, изображения размечены числами, а не словами. Это сделано для того, чтобы проще было создавать языковые версии нейронных сетей. К примеру, все изображения категории "олень" могут быть обозначены цифрой 7. Наши изображения будут описаны в цветовой модели RGB, состоящей из трех чисел в диапазоне от 0 до 255. Все пиксели изображения и мы будем описывать цветовой моделью RGB, которая обозначает цвета с помощью сочетания 3 цифр в диапазоне от 0 до 255 включительно. Для тренировки нейронной сети мы нормализуем эти числа и разделим каждый из трех значений этой цветовой модели каждого пикселя на 255. В результате мы получим набор значений от нуля до единицы. Это называется <a href="__GHOST_URL__/normalizatsiia/">Нормализация (Normalization)</a>. С такими данными и нейронной сети будет проще взаимодействовать. Обратите внимание: знак равенства один, а операций присвоения производится две.</p><pre><code class="language-python">Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 2s 0us/step</code></pre><p>Как я и говорила, категории у нас обозначены числами, но мы для удобства собственного восприятия обозначим их снова словами. Зададим холст и с помощью параметра <code>figsize</code> обозначим его размер. Для начала отобразим двадцать пять картинок из этого датасета. Нейросеть будет действовать как фильтр. Изучив все тренировочные данные, она сможет категоризировать все последующие изображения. Создадим цикл for и отобразим двадцать пять картинок. Каждое из изображение будет "подграфиком", то есть частью большого холста графика. Цифры 5 и 5 обозначают количество строк и столбцов на графике для подграфиков. Мы отключим отображение координат x и y, чтобы избежать перегрузки на глаза, иначе каждое из изображений будет размечено размерами. Мы также отключим сетку с помощью метода imshow() отобразим первые 25 тренировочных изображений.Мы задали псевдонимы для всех численных обозначений категорий изображений и теперь используем их для подписи на графике. Построим график.</p><pre><code class="language-python">class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    # The CIFAR labels happen to be arrays, \n    # which is why you need the extra index\n    plt.xlabel(class_names[train_labels[i][0]])\nplt.show()</code></pre><p>Даже в таком скромном качестве вы легко распознаете птицу, лошадь, грузовик, автомобиль, лягушку. Представьте, что кто-то никогда не виделэти объекты. Как он узнает, что есть что икак он отличит, например, оленя от коня?Ведь у них у обоих бывают каурый окрас, есть копыта. Мы-то с вами их можем различить друг от друга, потому что многоповидали на своем веку. Так что загрузим в компьютер большую базу изображений, чтобы он тоже научился отличать эти предметы друг от друга. Эти изображения мы будем раскладывать на многомерные тензоры. Каждое из этих изображений мы представимкак последовательностьпикселей, описываемых с помощью нормализованной модели RGB.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/cnn-cifar-10------.png" class="kg-image" alt loading="lazy" width="656" height="658" srcset="__GHOST_URL__/content/images/size/w600/2021/05/cnn-cifar-10------.png 600w, __GHOST_URL__/content/images/2021/05/cnn-cifar-10------.png 656w"></figure><p>Поскольку наше изображение размером 32 на 32 пикселя, то это будет 1024 пикселя 60 тысяч изображений. Для таких длинных последовательностей тензоров нам понадобится Keras. Этот высокоуровневый интерфейс как нельзя лучше подходит для подобных последовательностей. Мы будем использовать объект <code>Sequential()</code> этого API, который позволит нам привести данные к формату Keras. Создадим модель. используя метод <code>Sequential()</code>,не принимающий аргументов, добавим модель слой свёрточной нейронной двумерной сети. Смотрите, как происходит добавление слоя.  модель мы добавляем функции-фильтры, на самом деле их 128 в данном примере, которые будут вычислять собственные параметры автоматически на базе тренировочных изображений. Эти функции в конечном итоге будут производить категоризацию валидационных изображений, чтобы очистить тензоры от нецветовых значений, которые не удалось нормализовать, то есть привести к размеру от 0 до 255 и используется параметр reLU – это метод активации, который просто отсечёт часть тендзора, которая не является числом в диапазоне от нуля до единицы. Так мы получим набор только цветовых обозначений пикселов и с помощью параметра <code>input_shape</code> мы определяем, какого размера наше изображение, с помощью какой цветовой модели она обозначается. Для уменьшения объемов выходных данных мы будем использовать пулинг и метод <code>MaxPooling2D()</code>: так последовательно мы добавляем фильтр, на сей раз их 64; прочие параметры остаются теми же. И последний этап добавления функции фильтров, на сей раз их 128 с такими же параметрами активации.</p><pre><code class="language-python">model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))</code></pre><p>Запросим сводку модели:</p><pre><code class="language-python">model.summary()</code></pre><p>Пока числа не очень информативны; в ходе дальнейших преобразований мы увидим, как меняется число параметров каждого их 6 слоев.</p><pre><code class="language-python">Model: "sequential"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 30, 30, 32)        896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n=================================================================\nTotal params: 56,320\nTrainable params: 56,320\nNon-trainable params: 0</code></pre><p>Теперь преобразуем набор пикселей изображения размером 32 на 32 в двумерный массив, выстроим их как бы в ряд. В этом нам поможет метод <code>Flatten()</code>, не принимающий аргументов. К тому моменту как мы выстроили все пиксели изображения в одномерный массив, нейросеть содержит два полносвязных слоя. Каждый узел содержит оценку, указывающую вероятность принадлежностей изображений к одному из 10 классов.</p><pre><code class="language-python">model.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10))</code></pre><p>Мы добавили полносвязные слои с таким же методом активации, снова изучим сводку.</p><pre><code class="language-python">model.summary()</code></pre><p>Слои, добавленные на предыдущем этапе, остались неизменными; добавились уплощенный и плотные слои.</p><pre><code class="language-python">Model: "sequential"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 30, 30, 32)        896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n_________________________________________________________________\nflatten (Flatten)            (None, 1024)              0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                65600     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                650       \n=================================================================\nTotal params: 122,570\nTrainable params: 122,570\nNon-trainable params: 0\n_________________________________________________________________</code></pre><p>Количество параметров увеличилось. Соберем модель, для этого нам понадобится функция-оптимизатор и функция потери. Первое: поскольку нейросеть инициализируется со случайными значениями, то функция-оптимизатор переопределяет параметры тех самых 128 функций-фильтраторов, и с помощью функции потери измеряется точность распознавания тестовых изображений. Эй-ди-эй-эм (Adam) – это такой метод оптимизации, погружаться не будем в него. Параметр <code>metrics</code> позволит нам определить как часто нейронная сеть правильно категоризирует изображение относительно их настоящих <a href="__GHOST_URL__/iarlyk/">Ярлыков (Label)</a>. Cоздадим объект history и скормим нейронной сети датасет учебных изображений в 10 этапов, чтобы улучшить точность. Мы условно делим этот процесс на 10 этапов, чтобы улучшить качество обучения. </p><pre><code class="language-python">model.compile(optimizer = 'adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n              metrics = ['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs = 10, \n                    validation_data = (test_images, test_labels))</code></pre><p>Придется немного подождать:</p><pre><code class="language-python">Epoch 1/10\n1563/1563 [==============================] - 68s 43ms/step - loss: 1.7836 - accuracy: 0.3429 - val_loss: 1.3267 - val_accuracy: 0.5215\nEpoch 2/10\n1563/1563 [==============================] - 67s 43ms/step - loss: 1.2056 - accuracy: 0.5758 - val_loss: 1.1239 - val_accuracy: 0.6041\nEpoch 3/10\n1563/1563 [==============================] - 66s 42ms/step - loss: 1.0347 - accuracy: 0.6321 - val_loss: 1.0364 - val_accuracy: 0.6314\nEpoch 4/10\n1563/1563 [==============================] - 65s 42ms/step - loss: 0.9365 - accuracy: 0.6696 - val_loss: 0.9778 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 66s 42ms/step - loss: 0.8500 - accuracy: 0.7005 - val_loss: 0.8954 - val_accuracy: 0.6932\nEpoch 6/10\n1563/1563 [==============================] - 65s 41ms/step - loss: 0.7883 - accuracy: 0.7243 - val_loss: 0.8674 - val_accuracy: 0.7006\nEpoch 7/10\n1563/1563 [==============================] - 65s 41ms/step - loss: 0.7330 - accuracy: 0.7440 - val_loss: 0.8990 - val_accuracy: 0.6945\nEpoch 8/10\n1563/1563 [==============================] - 65s 41ms/step - loss: 0.6901 - accuracy: 0.7577 - val_loss: 0.8990 - val_accuracy: 0.6973\nEpoch 9/10\n1563/1563 [==============================] - 65s 41ms/step - loss: 0.6476 - accuracy: 0.7754 - val_loss: 0.8501 - val_accuracy: 0.7172\nEpoch 10/10\n1563/1563 [==============================] - 65s 42ms/step - loss: 0.6048 - accuracy: 0.7870 - val_loss: 0.9103 - val_accuracy: 0.7037</code></pre><p>Точность совсем низкая для начала, но она потом подрастет. Ура! Как вы видите, если параметры <code>loss</code> и <code>accuracy</code> характеризуют процесс обучения, то <code>validation_loss</code> и <code>validaion_accuracy</code> – потери и точность при проверке. Конечный результат – это 60 процентов точности, нормальный результат для первой попытки. Отобразим график, характеризующий скорость и качество обучения. Для  этого мы метрику <code>accuracy</code> и вводили. Зададим название осей x и y, пограничные значения осей координат, чтобы график был более наглядным; легенду мы отправим вправо вниз. В топ-3 самых распространенных картинок, которая помогает понять нейронной сети, входит эта диаграмма со слоями.</p><pre><code class="language-python">plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\n\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)</code></pre><p>Практика как всегда отличается от теории: точность при валидации существенно ниже:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/cnn-accuracy------.png" class="kg-image" alt loading="lazy" width="474" height="353"></figure><p>Посмотрим, как изменилась точность классификации:</p><pre><code class="language-python">print(test_acc)</code></pre><p>К счастью, эффективность модели подросла на целых 10%:</p><pre><code class="language-python">0.7037000060081482</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1yIYXjfgoKcbA9t5JxnQI21KEZGkQh3mT?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@davidmarcu">@davidmarcu</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939#:~:text=A%20Convolutional%20Neural%20Network%2C%20also,topology%2C%20such%20as%20an%20image.&amp;text=Each%20neuron%20works%20in%20its,cover%20the%20entire%20visual%20field.">Mayank Mishra</a></p>		sviortochnaia-nieironnaia-siet	2021-05-22		
95	Метод k-средних (K-Means)		<p>Метод k-средних (k-Means Clustering) – это очень известный и мощный алгоритм <a href="__GHOST_URL__/niekontroliruiemoie-obuchieniie/">Обучения без учителя (Unsupervised Learning)</a>, который группирует похожие элементы в k кластеров. Он используется для решения многих сложных задач <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. </p><p>Пример. Предположим, мы пошли в магазин за овощами и увидели, что они будут расположены на полках по типу. Вся морковь хранится в одном месте, картошка – в другом. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/k-means-borders-1.png" class="kg-image" alt loading="lazy" width="490" height="375"></figure><p>До применения кластеризации (появления окрашенных зон и обозначения записей разными иконками) перепутать категорию довольно легко. Неопытные мерчендайзеры до сих пор кладут арбузы в отдел ягод, хоть и правы с научной точки зрения. </p><p>Метод k-средних пытается сгруппировать похожие элементы в три этапа:</p><ol><li>Выберем значение k</li><li>Инициализируем центроиды (разделительные линии)</li><li>Выберем группу и найдем среднее значение расстояния между точками.</li></ol><p>Давайте разберемся в вышеуказанных шагах с помощью иллюстраций. Допустим, мы на глаз кластеризовали наблюдения, причислив половину к белой категории, оставшуюся часть – к розовой. </p><p>Шаг 1. Мы случайным образом выбираем значение K, равное 2:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stagesselect-2-points.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stagesselect-2-points.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stagesselect-2-points.png 1000w"></figure><p>Существуют различные методы, с помощью которых мы можем выбрать правильные значения параметра k. Об этом позже.</p><p>Шаг 2. Соединим две выбранные максимально удаленные точки, обозначенные белой полупрозрачной обводкой. Теперь, чтобы определить центроид, мы построим перпендикуляр к этой линии:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stagescentroid.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stagescentroid.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stagescentroid.png 1000w"></figure><p>Если вы заметили, одна белая точка попала в группу розовых, и теперь относится к другой группе, чем предположено изначально.</p><p>Шаг 3. Мы соединим две другие удаленные точки, проведем к ним перпендикулярную линию и найдем центроид. Теперь некоторые белые точки преобразуются в розовые:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stages-centroid-2.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stages-centroid-2.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stages-centroid-2.png 1000w"></figure><p><br>Этот процесс будет продолжаться до тех пор, пока мы не переберем все возможные сочетания пар дистанцированных точек и не уточним границы кластеров. Стабильность центроидов определяется путем сравнения абсолютного значения изменения среднего <a href="__GHOST_URL__/ievklidovo-rasstoianiie/">Евклидова расстояния (Euclidian Distance)</a> между наблюдениями и их соответствующими центроидами с пороговым значением.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stages-result.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stages-result.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stages-result.png 1000w"></figure><h3 id="%D0%BA%D0%B0%D0%BA-%D0%B2%D1%8B%D0%B1%D1%80%D0%B0%D1%82%D1%8C-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-k">Как выбрать значение k?</h3><p>Одна из самых сложных задач в этом алгоритме кластеризации – выбрать правильные значения k. Существует два метода.</p><h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%BB%D0%BE%D0%BA%D1%82%D1%8F">Метод локтя</h3><p><a href="__GHOST_URL__/mietod-loktia/">Метод локтя (Elbow Rule)</a> – один из самых известных методов, с помощью которого вы можете выбрать правильное значение k и повысить производительность <a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модели (Model)</a>. Этот эмпирический метод вычисляет сумму квадратов расстояний между точками и вычисляет <a href="__GHOST_URL__/sriednieie-znachieniie/#-" rel="noopener noreferrer">Среднее значение (Mean)</a>.</p><p>Когда значение k равно 1, сумма квадрата внутри кластера будет большой. По мере увеличения значения k сумма квадратов расстояний внутри кластера будет уменьшаться.</p><p>Наконец, мы построим график между значениями k и суммой квадрата внутри кластера, чтобы получить значение k. Мы внимательно рассмотрим график. В какой-то момент значение по оси x резко уменьшится. Эта точка будет считаться оптимальным значением k:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/k-means-elbow.png" class="kg-image" alt loading="lazy" width="1001" height="408" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-elbow.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/k-means-elbow.png 1000w, __GHOST_URL__/content/images/2021/05/k-means-elbow.png 1001w"><figcaption>Ось x – количество кластеров k, y – сумма квадрат расстояний между точками</figcaption></figure><h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0">Метод силуэта</h3><p><a href="__GHOST_URL__/silhouette-method/">Метод силуэта (Silhouette Method)</a> вычисляет среднее расстояние между точками в своем кластере a<sub>i</sub> и среднее расстояние от точек до следующего ближайшего кластера, называемого b<sub>i</sub>. </p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/k-means-silhouette.jpeg" class="kg-image" alt loading="lazy" width="1072" height="381" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-silhouette.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2021/05/k-means-silhouette.jpeg 1000w, __GHOST_URL__/content/images/2021/05/k-means-silhouette.jpeg 1072w"><figcaption>Чем меньше коэффициент силуэта (длина фигуры справа), тем оптимальнее выбран k</figcaption></figure><p>Теперь мы можем вычислить коэффициент силуэта всех точек в кластерах и построить график. Последний также поможет в обнаружении <a href="__GHOST_URL__/vybros/">Выбросов (Outlier)</a>. Значение метрики силуэта находится в диапазоне от -1 до 1. Обратите внимание, что коэффициент силуэта, равный –1 – это наихудший сценарий. Для картинки выше система вычислила расстояния между всеми точками при различных допущениях о числе кластеров и построила соответствующие горизонтальные гистограммы. Мы выбираем k, равный 3, потому что зеленая гистограмма меньше, хотя стоит, возможно, проверить и бо́льшие значения.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%B8%D0%BC%D1%83%D1%89%D0%B5%D1%81%D1%82%D0%B2%D0%B0-k-means">Преимущества K-Means</h3><ul><li>Простота реализации</li><li>Масштабируемость до огромных наборов данных</li><li>Метод очень быстро обучается на новых примерах</li><li>Поддержка сложных форм и размеров.</li></ul><h3 id="%D0%BD%D0%B5%D0%B4%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BA%D0%B8-k-means">Недостатки K-Means</h3><ul><li>Чувствительность к выбросам</li><li>Трудоемкость выбора k</li><li>Уменьшение масштабируемости.</li></ul><h3 id="k-means-%D0%B8-scipy">K-Means и <strong>SciPy</strong></h3><p>Давайте посмотрим, как метод реализован в SciPy. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nfrom numpy import array, random\n\nimport scipy\nfrom scipy.cluster.vq import vq, kmeans, whiten\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure</code></pre><p>Создадим набор из 50 точек с парой координат a и b. Метод <code>multivariate_normal()</code> преобразует случайные значения, сгенерированные <code>random()</code>, в многомерную нормальную случайную величину. Объект <code>features</code> объединяет координаты попарно.</p><pre><code class="language-python"># Создадим 50 наблюдений в кластерах a и b\npts = 50\na = np.random.multivariate_normal([0, 0], [[4, 1], [1, 4]], size = pts)\nb = np.random.multivariate_normal([30, 10],\n                                  [[10, 2], [2, 1]],\n                                  size = pts)\nfeatures = np.concatenate((a, b))</code></pre><p>Применим <a href="__GHOST_URL__/normalizatsiia/">Нормализацию (Normalization)</a> – "отбелим" (whiten) данные, прежде чем отправлять в кластеризующую модель. <code>codebook</code> – массив из k центроидов, подбираемых системой автоматически, и вместе с объектом <code>distortion</code> отправим эти данные в K-Means. Искажение (Distortion) здесь –среднее неквадратичное Евклидово расстояние между пройденными наблюдениями и сгенерированными центроидами:</p><pre><code class="language-python">whitened = whiten(features)\n# Найдем два кластера в данных\ncodebook, distortion = kmeans(whitened, 2)</code></pre><p>Отрисуем нормализрованные данные и отметим центры кластера красными точками:</p><pre><code class="language-python">plt.scatter(whitened[:, 0], whitened[:, 1])\nplt.scatter(codebook[:, 0], codebook[:, 1], c = 'r')\nplt.show()</code></pre><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/05/k-means.png" class="kg-image" alt loading="lazy" width="1247" height="282" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/k-means.png 1000w, __GHOST_URL__/content/images/2021/05/k-means.png 1247w"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1yK2F8OFmocZllnew7OD2W1_cVArANUhq?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2020/10/a-simple-explanation-of-k-means-clustering/#:~:text=k%2Dmeans%20clustering%20tries%20to,groups%20them%20into%20the%20clusters.">ADITYA610</a></p><p>Фото: <a href="https://unsplash.com/@robertlukeman">@robertlukeman</a></p>		mietod-k-sriednikh	2021-05-23		
96	Нейронная сеть (Neural Network)		<p>Нейронная сеть – это алгоритмы, которые пытаются распознать лежащие в основе взаимосвязи в <a href="__GHOST_URL__/dataset/">Датасете (Dataset)</a> посредством процесса, имитирующего работу человеческого мозга. В этом смысле нейронные сети относятся к системам нейронов органического или искусственного происхождения. Нейросети могут адаптироваться к изменению вводных даных и генерируют, таким образом, наилучший возможный результат без необходимости изменения критериев вывода. Концепция нейронных сетей, уходящая корнями в <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственный интеллект (AI)</a>, быстро набирает популярность при разработке всевозможных систем.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/neural-network-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/neural-network-1.png 600w, __GHOST_URL__/content/images/2021/05/neural-network-1.png 1000w"></figure><h3 id="%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D1%8B">Основы</h3><p>Нейронные сети в мире финансов, к примеру, помогают в развитии таких процессов, как прогнозирование <a href="__GHOST_URL__/vriemiennoi-riad/">Временных рядов (Time Series)</a>, алгоритмическая торговля, классификация ценных бумаг, моделирование кредитного риска и построение собственных индикаторов и производных цен.</p><p>Нейронная сеть работает аналогично человеческому мозгу. Нейрон здесь – это математическая функция, которая собирает и классифицирует информацию в соответствии с определенной архитектурой. Сеть очень похожа на статистические методы, такие как <a href="__GHOST_URL__/podghonka-krivykh/">Кривая аппроксимации (Curve Fitting)</a> и Регрессионный анализ (Regression Analysis).</p><p>Нейронная сеть содержит Слои (Layers) взаимосвязанных узлов. Каждый узел является <a href="__GHOST_URL__/piertsieptron/">Перцептроном (Perceptron)</a>, который подает сигнал в <a href="__GHOST_URL__/funktsiia-aktivatsii/">Функцию активации (Activation Function)</a>, которая может быть нелинейной.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/perceptron.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/perceptron.png 600w, __GHOST_URL__/content/images/2021/05/perceptron.png 1000w"><figcaption>Функция активации (обозначена розовым) получает взвешенный сигнал от входных узлов</figcaption></figure><p>Скрытые слои точно настраивают входные Веса (Weights) до тех пор, пока погрешность нейронной сети не станет минимальной. Предполагается, что скрытые слои экстраполируют характерные особенности входных данных, которые имеют предсказательную силу в отношении выходных данных. </p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D1%85-%D1%81%D0%B5%D1%82%D0%B5%D0%B9">Применение нейронных сетей</h3><p>Нейронные сети широко используются для финансовых операций, планирования, бизнес-аналитики и обслуживания продуктов. Нейронные сети также получили широкое распространение в бизнесе, среди задач – <a href="__GHOST_URL__/obnaruzhieniie-moshiennichieskikh-opieratsii/">Обнаружение мошеннических операций (Fraud Detection)</a>, прогнозирование спроса (Demand Prediction) и т.д.</p><p>В контексте инвестиций, нейронная сеть оценивает данные о ценах и открывает возможности для принятия торговых решений на основе анализа данных. Такие алгоритмы могут различать тонкие нелинейные взаимозависимости и закономерности, недоступные для других методов технического анализа. Согласно исследованиям, точность нейронных сетей при прогнозировании цен на акции различается. Некоторые модели предсказывают правильные цены на акции от 50 до 60 процентов времени, в то время как другие точны в 70 процентах всех случаев. Некоторые утверждают, что повышение эффективности на 10% – это достойный уровень прогресса для инвестора.</p><p>Важен не столько алгоритм, сколько хорошо подготовленные входные данные. В конечном итоге это и определяет успешность нейронной сети.</p><p>Фото: <a href="https://unsplash.com/@meijer45">@meijer45</a></p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/n/neuralnetwork.asp#:~:text=A%20neural%20network%20is%20a,way%20the%20human%20brain%20operates.&amp;text=Neural%20networks%20can%20adapt%20to,to%20redesign%20the%20output%20criteria.">James Chen</a></p>		nieironnaia-siet	2021-05-29		
97	TensorFlow		<p>TensorFlow – это библиотека с открытым исходным кодом для численных вычислений и крупномасштабного <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, созданная командой Google Brain. TensorFlow объединяет множество моделей и алгоритмов машинного и <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокого обучения (Deep Learning)</a>. Он использует Python и API для создания приложений, компилируясь на высокопроизводительном языке C++.</p><p>TensorFlow может обучать и запускать глубокие нейронные сети для <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a> рукописных цифр, Распознавания изображений (Image Recognition), <a href="__GHOST_URL__/riekurrientnaia-nieirosiet/">Рекуррентных нейронных сетей (RNN)</a>, <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a>. TensorFlow поддерживает масштабирование модели: алгоритм, обученный на 10 Мегабайтах данных прекрасно справится и с <a href="__GHOST_URL__/bolshiie-dannyie/">Большими данными (Big Data)</a>.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-tensorflow">Как работает TensorFlow</h3><p>Фреймворк позволяет разработчикам создавать <a href="__GHOST_URL__/graf/">Графы (Graph)</a> – абстрактную структуру данных, которая состоит из конечного набора вершин (также называемых узлами или точками) вместе с набором ребер. Вершины могут быть частью структуры графа или могут быть внешними объектами, представленными целочисленными индексами или ссылками:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/tensorflow-graph-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/tensorflow-graph-1.png 600w, __GHOST_URL__/content/images/2021/05/tensorflow-graph-1.png 1000w"></figure><p></p><p>Каждый узел в графе представляет собой математическую операцию, а каждое ребро – многомерный массив данных или <a href="__GHOST_URL__/tenzor/">Тензор (Tensor)</a>.</p><p>Все это TensorFlow предоставляет программисту на языке Python. С этим языком легко освоиться и работать., и он предоставляет удобные способы выражения взаимосвязи высокоуровневых абстракций. Узлы, тензоры и приложения в TensorFlow являются объектами Python.</p><p>Однако фактические математические операции в Python не выполняются. Библиотеки преобразований, доступные через TensorFlow – высокопроизводительные двоичные файлы C++. Python просто направляет трафик между компонентами и предоставляет абстракции программирования высокого уровня, чтобы связать их вместе.</p><p>Приложения TensorFlow можно запускать практически на любом девайсе: на локальном компьютере, кластере в облаке, устройствах iOS и Android, центральных или графических процессорах. Если вы используете собственное облако Google, то можете запустить TensorFlow на специализированном процессоре TensorFlow Processing Unit (TPU) для дальнейшего ускорения. Однако результирующие модели, созданные TensorFlow, можно развернуть практически на любом устройстве, где они будут использоваться.</p><p>TensorFlow 2.0, выпущенный в октябре 2019 года, во многом обновился на основе отзывов пользователей, чтобы упростить работу (например, с помощью относительно простого API Keras для обучения модели) и повысить производительность. Распределенное обучение (Distributed Learning) легче запускать благодаря новому API, а поддержка TensorFlow Lite позволяет развертывать модели на бо́льшем количестве платформ. Однако код, написанный для более ранних версий TensorFlow, должен быть переписан, чтобы максимально использовать преимущества новых функций TensorFlow 2.0.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%B8%D0%BC%D1%83%D1%89%D0%B5%D1%81%D1%82%D0%B2%D0%B0-tensorflow">Преимущества TensorFlow</h3><p>Самым большим преимуществом TensorFlow является абстракция. Вместо того, чтобы заниматься мельчайшими деталями реализации алгоритмов или выяснять правильные способы привязки вывода одной функции ко входу другой, разработчик может сосредоточиться на общей логике приложения. TensorFlow заботится о деталях за кулисами.</p><p>Продукт предлагает дополнительные удобства для разработчиков, которым необходимо отлаживать приложения. Режим активного выполнения позволяет нам оценивать и изменять каждую операцию графа поэлементно, вместо того, чтобы строить весь граф как один непрозрачный объект и оценивать его целиком. Пакет визуализации TensorBoard позволяет совершенствовать модель с помощью интерактивной веб-панели:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/tensorflow-tensorboard.gif" class="kg-image" alt loading="lazy" width="800" height="605"></figure><p>Google не только способствовал быстрому развитию этого проекта, но и создал множество важных предложений для TensorFlow, которые упрощают его развертывание и использование: вышеупомянутый TPU для повышения производительности в облаке Google; онлайн-центр для обмена моделями, созданными с помощью фреймворка; версии фреймворка для браузеров и мобильных устройств и многое другое.</p><p>Однако порой модель, обученная в одной системе, будет немного отличаться от модели, обученной в другой, даже если им будут переданы одни и те же данные. Причины этому – влияние случайных чисел, или случайное поведение графического процессора.</p><h3 id="tensorflow-%D0%BF%D1%80%D0%BE%D1%82%D0%B8%D0%B2-%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D0%B5%D0%BD%D1%82%D0%BE%D0%B2">TensorFlow против конкурентов</h3><p>Детище Джефа Дина (Jeff Dean) и Джеффри Хинтона (Geoffrey Hinton) конкурирует с множеством других фреймворков машинного обучения. PyTorch, CNTK и MXNet – это три основных фреймворка, которые удовлетворяют многие из тех же потребностей. Ниже я отметил, где они выделяются и уступают TensorFlow:</p><ul><li><strong>PyTorch</strong> помимо того, что построен на Python, имеет много других общих черт с TensorFlow: компоненты с аппаратным ускорением под капотом, высокоинтерактивная модель разработки, которая позволяет проектировать на ходу, и многие уже включенные полезные компоненты. Это лучший выбор для быстрой разработки проектов, но TensorFlow лучше всего подходит для более крупных проектов и сложных рабочих процессов.</li><li><strong>Microsoft Cognitive Toolkit (CNTK)</strong> использует структуру графа для описания потока данных, но больше всего фокусируется на создании нейронных сетей с глубоким обучением. CNTK быстрее выполняет многие задачи нейронной сети и имеет более широкий набор API (Python, C ++, C #, Java). Но CNTK в настоящее время не так легко изучить или развернуть, как TensorFlow.</li><li><strong>Apache MXNet</strong>, принятый Amazon в качестве ведущей платформы глубокого обучения на AWS, может практически линейно масштабироваться между несколькими графическими процессорами и несколькими машинами. Он также поддерживает широкий спектр языковых API: Python, C++, Scala, R, JavaScript, Julia, Perl, Go – хотя его собственные API не так приятны в работе, как TensorFlow.</li></ul><p>Фото: <a href="https://unsplash.com/@whale?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@whale</a></p><p>Автор оригинальной статьи: <a href="https://www.infoworld.com/article/3278008/what-is-tensorflow-the-machine-learning-library-explained.html#:~:text=Created%20by%20the%20Google%20Brain,way%20of%20a%20common%20metaphor.">Serdar Yegulalp</a></p>		tensorflow	2021-05-30		
98	Гомоскедастичность (Homoscedasticity)		<p>Гомоскедастичность – допущение линейной регрессии об "одинаковости" <a href="__GHOST_URL__/dispiersiia/">Дисперсии (Variance)</a>. Иными словами, разность между реальным Y<sub>pred</sub> и предсказанным Y<sub>actual</sub> значениями, скажем, <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regresion)</a> остается в определенном известном диапазоне, что позволяет в принципе использовать такую <a href="__GHOST_URL__/modiel/">Модель (Model)</a>. В случае такого единообразия ошибок <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> с большими значениями будут иметь то же влияние на предсказывающий <a href="__GHOST_URL__/alghoritm/">Алгоритм (Algorithm)</a>, что и наблюдения с меньшими значениями:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/variance-mean-difference.png" class="kg-image" alt loading="lazy" width="2000" height="751" srcset="__GHOST_URL__/content/images/size/w600/2021/05/variance-mean-difference.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/variance-mean-difference.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/05/variance-mean-difference.png 1600w, __GHOST_URL__/content/images/2021/05/variance-mean-difference.png 2238w" sizes="(min-width: 1200px) 1200px"></figure><p>Линейная регрессия базируется на предположении, что для всех случаев ошибки будут одинаковыми и с очень малой дисперсией.</p><p>Пример. У нас есть две переменные – высота дерева навскидку и реальный его рост. Естественно, по мере увеличения оценочной высоты реальные тоже растут. Итак, мы подбираем модель линейной регрессии и видим, что ошибки имеют одинаковую дисперсию:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/homoscedasticity-1.png" class="kg-image" alt loading="lazy" width="1001" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/homoscedasticity-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/homoscedasticity-1.png 1000w, __GHOST_URL__/content/images/2021/05/homoscedasticity-1.png 1001w"></figure><p>Прогнозы почти совпадают с линейной регрессией и имеют одинаковую <em>известную</em> дисперсию повсюду. Кроме того, если мы нанесем эти остатки на ось X, мы увидим их вдоль прямой линии, параллельной оси X. Это явный признак гомоскедастичности.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/image-7.png" class="kg-image" alt loading="lazy" width="850" height="377" srcset="__GHOST_URL__/content/images/size/w600/2021/05/image-7.png 600w, __GHOST_URL__/content/images/2021/05/image-7.png 850w"></figure><p>Когда это условие нарушается, в модели присутствует <a href="__GHOST_URL__/gietieroskiedastiichnost-heteroscedasticity/">Гетероскедастичность (Heteroscedasticity)</a>. Предположим, что для деревьев с меньшей приблизительной высотой разность между прогнозируемым и реальным значением меньше, чем для высоких представителей флоры. По мере увеличения высоты дисперсия в прогнозах увеличивается, что приводит к увеличению значения ошибки или <a href="__GHOST_URL__/ostatok/">Остатка (Residual)</a>. Когда мы снова построим график остатков, то увидим типичную коническую кривую, которая четко указывает на наличие гетероскедастичности в модели:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/image-8.png" class="kg-image" alt loading="lazy" width="901" height="402" srcset="__GHOST_URL__/content/images/size/w600/2021/05/image-8.png 600w, __GHOST_URL__/content/images/2021/05/image-8.png 901w" sizes="(min-width: 720px) 720px"></figure><p>Гетероскедастичность – это систематическое увеличение или уменьшение дисперсии остатков в диапазоне независимых переменных. Это проблема, потому нарушается базовое предположение о линейной регрессии: все ошибки должны иметь одинаковую дисперсию. </p><h3 id="%D0%BA%D0%B0%D0%BA-%D1%83%D0%B7%D0%BD%D0%B0%D1%82%D1%8C-%D0%BF%D1%80%D0%B8%D1%81%D1%83%D1%82%D1%81%D1%82%D0%B2%D1%83%D0%B5%D1%82-%D0%BB%D0%B8-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Как узнать, присутствует ли гетероскедастичность?</h3><p>Проще говоря, самый простой способ узнать, присутствует ли гетероскедастичность, – построить график остатков. Если вы видите какую-либо закономерность, значит, есть гетероскедастичность. Обычно значения увеличиваются, образуя конусообразную кривую.</p><h3 id="%D0%BF%D1%80%D0%B8%D1%87%D0%B8%D0%BD%D1%8B-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D0%B8">Причины гетероскедастичности</h3><ul><li>Есть большая разница в переменной. Другими словами, когда наименьшее и наибольшее значения переменной слишком экстремальны. Это также могут быть <a href="__GHOST_URL__/vybros/" rel="noopener noreferrer">Выбросы (Outlier)</a>.</li><li>Мы выбираем неправильную модель. Если вы подгоните модель линейной регрессии к нелинейным данным, это приведет к гетероскедастичности.</li><li>Когда масштаб значений в переменной некорректен (например, стоит рассматривать данные по сезонам, а не по дням).</li><li>Когда для регрессии используется неправильное преобразование данных.</li><li>Когда в данных присутствует <a href="__GHOST_URL__/skoshiennost/" rel="noopener noreferrer">Скошенность (Skewness)</a>.</li></ul><h3 id="%D1%87%D0%B8%D1%81%D1%82%D0%B0%D1%8F-%D0%B8-%D0%BD%D0%B5%D1%87%D0%B8%D1%81%D1%82%D0%B0%D1%8F-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D0%B8">Чистая и нечистая гетероскедастичности</h3><p>Когда мы подбираем правильную модель (линейную или нелинейную) и все же есть видимый образец в остатках, это называется чистой гетероскедастичностью.</p><p>Однако, если мы подбираем неправильную модель, а затем наблюдаем закономерность в остатках, то это случай нечистой гетероскедастичности. В зависимости от типа гетероскедастичности необходимо принять меры для ее преодоления. Это зависит и от сферы, в которой мы работаем.</p><h3 id="%D1%8D%D1%84%D1%84%D0%B5%D0%BA%D1%82%D1%8B-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D0%B2-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%BC-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B8">Эффекты гетероскедастичности в Машинном обучении</h3><p>Как мы обсуждали ранее, модель линейной регрессии делает предположение о наличии гомоскедастичности в данных. Если это предположение неверно, мы не сможем доверять полученным результатам.</p><p>Наличие гетероскедастичности делает коэффициенты менее точными, и, следовательно, правильные находятся дальше от значения <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population)</a>.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D0%BB%D0%B5%D1%87%D0%B8%D1%82%D1%8C-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Как лечить гетероскедастичность?</h3><p>Если мы обнаружили гетероскедастичность, есть несколько способов справиться с ней. Во-первых, давайте рассмотрим пример, в котором у нас есть две переменные: население города и количество заражений COVID-19.</p><p>В этом примере будет огромная разница в количестве заражений в крупных мегаполисах по сравнению с небольшими городами. Переменная «Количество инфекций» будет <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>, а «Население города» – <a href="__GHOST_URL__/priediktor/">Предиктором (Predictor Variable)</a>. Мы знаем, что в модели присутствует гетероскедастичность, и ее необходимо исправить.</p><p>В нашем случае, источник проблемы – это переменная с большой дисперсией (Население). Есть несколько способов справиться с подобным неоднообразием остатков, мы же рассмотрим три таких метода.</p><h3 id="%D1%83%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BF%D0%B5%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%BC%D0%B8">Управление переменными</h3><p>Мы можем внести некоторые изменения в имеющиеся переменные, чтобы уменьшить влияние этой большой дисперсии на прогнозы модели. Один из способов сделать это – осуществить <a href="__GHOST_URL__/normalizatsiia/">Нормализацию (Normalization)</a>, то есть привести значения <a href="__GHOST_URL__/priznak/">Признака (Feature)</a> к диапазону от 0 до 1. Это заставит признаки передавать немного другую информацию. От проблемы и данных будет зависеть, можно ли реализовать такой подход.</p><p>Этот метод требует минимальных модификаций и часто помогает решить проблему, а в некоторых случаях даже повысить производительность модели.</p><p>В нашем случае, мы изменим параметр «Количество инфекций» на «Скорость заражения». Это поможет уменьшить дисперсию, поскольку совершенно очевидно, что число инфекций в городах с большой численностью населения будет большим.</p><h3 id="%D0%B2%D0%B7%D0%B2%D0%B5%D1%88%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F">Взвешенная регрессия</h3><p>Взвешенная регрессия – это модификация нормальной регрессии, при которой точкам данных присваиваются определенные Веса (Weights) в соответствии с их дисперсией. Те, у которых есть бо́льшая дисперсия, получают небольшой вес, а те, у которых меньшая дисперсия, получают бо́льший вес.</p><p>Таким образом, когда веса возведены в квадрат, это позволяет снизить влияние остатков с большой дисперсией.</p><p>Когда используются правильные веса, гетероскедастичность заменяется гомоскедастичностью. Но как найти правильный вес? Один из быстрых способов – использовать инверсию этой переменной в качестве веса (население города превратится в дробь 1/n, где n – число жителей).</p><h3 id="%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%8F">Трансформация</h3><p>Преобразование данных – последнее средство, поскольку при этом вы теряете интерпретируемость функции. Это означает, что вы больше не сможете легко объяснить, что показывает признак. Один из способов – взятие логарифма. Воспринять новые значения высоты дерева (например, 16 метров превратятся в ≈2.772) будет сложнее.</p><p>Фото: <a href="https://unsplash.com/@sorasagano">@sorasagano</a></p><p>Автор оригинальной статьи: <a href="https://www.upgrad.com/blog/homoscedasticity-in-machine-learning/">Pavan Vadapalli</a></p>		gomoskiedastichnost	2021-05-30		
99	Функция активации\t(Activation Function)		<p>Функция активации – это фрагмент программного кода, добавляемый в искусственную <a href="__GHOST_URL__/nieironnaia-siet/">Нейронную сеть (Neural Network)</a>, чтобы помочь ей изучить сложные закономерности данных. В сравнении с нейронами нашего мозга, <a href="__GHOST_URL__/funktsiia/">Функция (Function)</a> активации решает, что должно быть запущено в следующий нейрон. Она принимает выходной сигнал из предыдущей ячейки и преобразует его в некоторую форму, которую можно использовать в качестве входных данных для следующей ячейки. На изображении ниже – устройство нейрона человеческого мозга (слева) и искусственного. Аксон – это отросток нейрона, по которому идут импульсы к иннервируемым органам и другим нервным клеткам:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/activation-function-neuron.jpg" class="kg-image" alt loading="lazy" width="889" height="321" srcset="__GHOST_URL__/content/images/size/w600/2021/05/activation-function-neuron.jpg 600w, __GHOST_URL__/content/images/2021/05/activation-function-neuron.jpg 889w"></figure><h3 id="%D0%B7%D0%B0%D1%87%D0%B5%D0%BC-%D1%8D%D1%82%D0%BE-%D0%BD%D1%83%D0%B6%D0%BD%D0%BE">Зачем это нужно?</h3><p>Существует несколько причин для использования нелинейных функций активации: </p><ul><li>Помимо биологического сходства, они также помогают поддерживать значение выходного сигнала нейрона, ограниченного определенным пределом в соответствии с нашими требованиями. Это важно, потому что входными данными в функцию активации являются:</li></ul><!--kg-card-begin: markdown--><p>$$W * x + b$$<br>\n$$W\\space{}{–}\\space{веса}\\space{ячейки,}$$<br>\n$$x\\space{}{–}\\space{входные}\\space{наблюдения,}$$<br>\n$$b\\space{}{–}\\space{смещение}$$</p>\n<!--kg-card-end: markdown--><p>Значение <a href="__GHOST_URL__/smieshchieniie/">Смещения (Bias)</a>, если не ограничиваться определенным пределом, может быть очень большим, особенно в случае глубоких нейронных сетей, которые имеют миллионы параметров. Это может привести к вычислительным проблемам. Существуют некоторые функции активации (например, <a href="__GHOST_URL__/softmaks/">Softmax</a>), которые выводят определенные значения для бинарных входных значений (0 или 1).</p><ul><li>Наиболее важной особенностью функции активации является ее способность добавлять нелинейность в нейронную сеть. Чтобы понять это, давайте рассмотрим многомерные данные, такие как показано на рисунке ниже:</li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/activation-function-multidimensional-data-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/activation-function-multidimensional-data-1.png 600w, __GHOST_URL__/content/images/2021/05/activation-function-multidimensional-data-1.png 1000w"></figure><p>Линейный классификатор, использующий три характеристики (вес, систолическое артериальное давление и возраст на рисунке выше), может дать нам линию в трехмерном пространстве, но он никогда не сможет точно изучить закономерность, которая делает человека курильщиком или некурящим, потому что шаблон, который определяет эту классификацию, просто не является линейным. Если мы используем Искусственную нейронную сеть (ANN) с одной ячейкой, но без функции активации, то "предсказывающее" уравнение равно W * x + b, что бесполезно, ибо W * x также имеет степень, равную 1 (x просто не возводится в степень), следовательно, это линейный классификатор.</p><p>Однако сложения слоев недостаточно, особенно для высокоуровневых проблем, например, <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерного зрения (CV)</a> или <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработкой естественного языка (NLP)</a>.</p><p>Чтобы модель получила возможность (или более высокую степень сложности) изучать нелинейные паттерны, между ними добавляются определенные нелинейные уровни – функции активации.</p><h3 id="%D0%B6%D0%B5%D0%BB%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BE%D1%81%D0%BE%D0%B1%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8-%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%B0%D1%86%D0%B8%D0%B8">Желательные особенности функции активации</h3><ul><li><strong>Проблема исчезающего градиента</strong>: нейронные сети обучаются с использованием процесса Градиентного спуска (Gradient Descent), который состоит из Обратного распространения ошибки (Backpropagation). Последний представляет собой цепное правило для получения изменения весов с целью уменьшения потерь после каждой <a href="__GHOST_URL__/epokha/">Эпохи (Epoch)</a>.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/image-10.png" class="kg-image" alt loading="lazy" width="600" height="494" srcset="__GHOST_URL__/content/images/2021/05/image-10.png 600w"></figure><ul><li><strong>С нулевым центром</strong>: выход функции активации должен быть симметричным относительно нуля, чтобы градиенты не смещались в определенном направлении.</li><li><strong>Вычислительные затраты</strong>: функции активации применяются после каждого уровня и должны рассчитываться миллионы раз в глубоких сетях. Следовательно, их вычисление должно быть недорогим в вычислительном отношении.</li><li><strong>Дифференцируемость</strong>: как уже упоминалось, нейронные сети обучаются с использованием процесса градиентного спуска, поэтому слои в модели должны быть дифференцируемыми. Это необходимое требование для того, чтобы функция работала как уровень функции активации.</li></ul><p>Сигмоид – одна из нелинейных функции активации:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/loss-function_sigmoid-1.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/loss-function_sigmoid-1.png 600w, __GHOST_URL__/content/images/2021/05/loss-function_sigmoid-1.png 1000w"></figure><p>Эта функция активации используется только по историческим причинам и никогда не используется в реальных <a href="__GHOST_URL__/modiel/">Моделях (Model)</a>. Это затратно с точки зрения вычислений, вызывает проблему исчезающего градиента и не имеет нулевого центра. Этот метод обычно используется для задач Двоичной классификации (Binary Classification).</p><p>Фото: <a href="https://unsplash.com/@tyssulpatel">@tyssulpatel</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253#:~:text=Simply%20put%2C%20an%20activation%20function,fired%20to%20the%20next%20neuron.">Vandit Jain</a></p>		funktsiia-aktivatsii	2021-06-02		
100	Рекуррентная нейросеть (RNN)		<p>Рекуррентная нейронная сеть – <a href="__GHOST_URL__/modiel/">Модель (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, работающая по принципу сохранения слоя и его переподачи. </p><h3 id="%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D0%B5%D1%82%D0%B8">Нейронные сети</h3><p><a href="__GHOST_URL__/nieironnaia-siet/">Нейронная сеть (Neural Network)</a> состоит из разных слоев, связанных друг с другом и работающих над функциями человеческого мозга. Последний учится на огромных объемах данных и использует сложные алгоритмы для обучения нейронной сети.</p><p>Вот пример того, как нейронные сети могут определять породу собаки по ее характеристикам:</p><ul><li>Пиксели изображения двух разных пород собак поступают на входной слой нейронной сети.</li><li>Пиксели изображения обрабатываются в скрытых слоях для выделения признаков.</li><li>Выходной слой дает результат, позволяющий определить, является ли это немецкой овчаркой или лабрадором.</li></ul><p>Такие сети не требуют запоминания прошлых результатов. Такие алгоритмы помогают решить разные бизнес-задачи. Давайте посмотрим на некоторые из них.</p><h3 id="%D0%BF%D0%BE%D0%BF%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D1%8B%D0%B5-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D0%B5%D1%82%D0%B8">Популярные нейронные сети</h3><ul><li><strong>Нейронная сеть с прямой связью (Feed-Forward Neural Network)</strong> используется для задач Регрессии (Regression) и <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>.</li><li><strong>Сверточная нейронная сеть (CNN)</strong> – для обнаружения объектов и классификации изображений.</li><li><strong>Сеть глубокого убеждения (Deep Belief Network)</strong> – в секторах здравоохранения для обнаружения рака.</li><li><strong>Рекуррентная нейронная сеть</strong> используется для распознавания речи, распознавания голоса, прогнозирования временных рядов и обработки естественного языка.</li></ul><p>Ниже показано, как преобразовать нейронную сеть с прямой связью в рекуррентную:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/06/rnn-simple.png" class="kg-image" alt loading="lazy" width="954" height="407" srcset="__GHOST_URL__/content/images/size/w600/2021/06/rnn-simple.png 600w, __GHOST_URL__/content/images/2021/06/rnn-simple.png 954w"></figure><p>Узлы на разных уровнях нейронной сети сжимаются, чтобы сформировать единый рекуррентный слой. A, B и C – параметры сети.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/rnn-fully-connected.gif" class="kg-image" alt loading="lazy" width="860" height="728"></figure><p>Здесь x – входной слой, h – скрытый слой, y – выходной слой. A, B и C используются для улучшения вывода модели. В любой момент времени t текущий вход представляет собой комбинацию входов в x(t) и x(t-1). Выходные данные в любой момент времени возвращаются в сеть для улучшения выходных данных.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/06/rnn-fully-connected-horizonta.gif" class="kg-image" alt loading="lazy" width="1736" height="678"></figure><h3 id="%D0%BF%D0%BE%D1%87%D0%B5%D0%BC%D1%83-rnn">Почему RNN?</h3><p>Рекуррентные нейронные сети были созданы, потому что в нейронной сети с прямой связью было несколько проблем:</p><ul><li>Не могут обрабатывать последовательные данные</li><li>Учитывают только текущую вводную информацию</li><li>Невозможно запомнить предыдущий ввод</li></ul><p>RNN же может работать последовательно, принимая текущие и ранее полученные входные данные, а также запоминать благодаря своей внутренней памяти.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F-%D1%80%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D1%85-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D1%85-%D1%81%D0%B5%D1%82%D0%B5%D0%B9">Приложения рекуррентных нейронных сетей</h3><p>RNN используются для:</p><ul><li>Распознавания предметов на изображениях:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://images.unsplash.com/photo-1568393691622-c7ba131d63b4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxkb2d8ZW58MHx8fHwxNjIyNTc2ODIw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" class="kg-image" alt loading="lazy" width="6000" height="3376" srcset="https://images.unsplash.com/photo-1568393691622-c7ba131d63b4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxkb2d8ZW58MHx8fHwxNjIyNTc2ODIw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=600 600w, https://images.unsplash.com/photo-1568393691622-c7ba131d63b4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxkb2d8ZW58MHx8fHwxNjIyNTc2ODIw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1000 1000w, https://images.unsplash.com/photo-1568393691622-c7ba131d63b4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxkb2d8ZW58MHx8fHwxNjIyNTc2ODIw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1600 1600w, https://images.unsplash.com/photo-1568393691622-c7ba131d63b4?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxkb2d8ZW58MHx8fHwxNjIyNTc2ODIw&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2400 2400w" sizes="(min-width: 720px) 720px"><figcaption>RNN узнает собаку с мячом. Фото: <a href="https://unsplash.com/@annadudkova?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Anna Dudkova</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption></figure><ul><li>Прогнозирования временных рядов: любую задачу временных рядов, такую ​​как прогнозирование цен акций в конкретном месяце, можно решить с помощью RNN.</li><li><a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработка естественного языка (NLP)</a>: эмоциональная оценка, анализ тональности.</li><li>Машинный перевод: текст с одного языка переводится на несколько из них.</li></ul><h3 id="rnn-torch">RNN: Torch</h3><p>Рекуррентная нейросеть вполне доступно реализована на Keras. Для начала импортируем все необходимые библиотеки:</p><pre><code class="language-python">import torch\nfrom torch import nn\n\nimport numpy as np</code></pre><p>Объединим все предложения и вычленим уникальные символы, затем создадим словарь из символов и их порядковых номеров и его инверсированную версию:</p><pre><code class="language-python">text = ['hey how are you','good i am fine','have a nice day']\nchars = set(''.join(text))\n\nint2char = dict(enumerate(chars))\n\nchar2int = {char: ind for ind, char in int2char.items()}</code></pre><p>Найдем наиболее длинное слово. Создадим цикл, который пройдет через список предложений и добавит столько пробелов, чтобы длина у всех предложений стала одинаковая:</p><pre><code class="language-python">maxlen = len(max(text, key = len))\n\nfor i in range(len(text)):\n  while len(text[i]) &lt; maxlen:\n      text[i] += ' '</code></pre><p>Создадим списки для входных и целевых последовательностей. В цикле удалим первые символ и букву первой последовательности</p><pre><code class="language-python">input_seq = []\ntarget_seq = []\n\nfor i in range(len(text)):\n  input_seq.append(text[i][:-1])\n    \n  target_seq.append(text[i][1:])\n  print("Входная последовательность: {}\\nЦелевая последовательность: {}".format(input_seq[i], target_seq[i]))</code></pre><p>Вот так странновато выглядят входная и целевая последовательности символов:</p><pre><code class="language-python">Входная последовательность: hey how are yo\nЦелевая последовательность: ey how are you\nВходная последовательность: good i am fine\nЦелевая последовательность: ood i am fine \nВходная последовательность: have a nice da\nЦелевая последовательность: ave a nice day</code></pre><p>Преобразуем с помощью цикла символы в целочисленные значения-псевдонимы:</p><pre><code class="language-python">for i in range(len(text)):\n    input_seq[i] = [char2int[character] for character in input_seq[i]]\n    target_seq[i] = [char2int[character] for character in target_seq[i]]</code></pre><p>Инициируем функцию <code>one_hot_encode</code> и создадим многомерный ряд нулей с желаемым разрешением. Создадим для каждой буквы по объекту, где заменим единицами вхождения этой буквы в предложении и нолями – другие буквы</p><pre><code class="language-python">dict_size = len(char2int)\nseq_len = maxlen - 1\nbatch_size = len(text)\n\ndef one_hot_encode(sequence, dict_size, seq_len, batch_size):\n    \n    features = np.zeros((batch_size, seq_len, dict_size), dtype = np.float32)\n    \n    for i in range(batch_size):\n        for u in range(seq_len):\n            features[i, u, sequence[i][u]] = 1\n    return features</code></pre><p>Входноsе разрешение -&gt; (Размер пакета, длина последовательности, масштаб <a href="__GHOST_URL__/bystroie-kodirovaniie/">Горячего кодирования (One-Hot Encoding)</a>):</p><pre><code class="language-python">input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)</code></pre><p>Инициируем объекты – входную и целевую последовательности:</p><pre><code class="language-python">input_seq = torch.from_numpy(input_seq)\ntarget_seq = torch.Tensor(target_seq)</code></pre><p><code>torch.cuda.is_available()</code> возвращает булевое True, если графический процессор доступен. Создадим переменную, обозначающую доступность графического процессора, чтобы в дальнейшем использовать ее.</p><pre><code class="language-python">is_cuda = torch.cuda.is_available()\n\nif is_cuda:\n    device = torch.device("cuda")\n    print('Графический процессор доступен')\nelse:\n    device = torch.device("cpu")\n    print("Графический процессор доступен, используем центральный")</code></pre><p><code>Cuda</code> недоступен:</p><pre><code class="language-python">Графический процессор недоступен, используем центральный</code></pre><p>Создадим класс модели. Определим некоторые параметры и определим слои RNN. Инициализируем скрытое состояние для первой порции входных данных используя метод ниже. Передаем входные данные и скрытое состояние в модель и получим вывод. Изменим разрешение выходных данных так, чтобы их можно было передать в полносвязный слой. Метод <code>init_hidden</code> генерирует первое скрытое состояние из нолей. Отправим тензор со скрытым состоянием в процессор:</p><pre><code class="language-python">class Model(nn.Module):\n    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n        super(Model, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n\n        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n        # Полносвязные слои\n        self.fc = nn.Linear(hidden_dim, output_size)\n    \n    def forward(self, x):\n        \n        batch_size = x.size(0)\n        hidden = self.init_hidden(batch_size)\n\n        out, hidden = self.rnn(x, hidden)\n        out = out.contiguous().view(-1, self.hidden_dim)\n        out = self.fc(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n        return hidden</code></pre><p>Создадим экземпляр модели с заданными гиперпараметрами. Подадим модель процессору (центральный по умолчанию). Определим гиперпараметры. Определим тип потерь, функцию оптимизации</p><pre><code class="language-python">model = Model(input_size = dict_size, output_size = dict_size, hidden_dim = 12, n_layers = 1)\nmodel.to(device)\n\nn_epochs = 100\nlr = 0.01\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = lr)</code></pre><p>Запустим тренировку:</p><pre><code class="language-python">for epoch in range(1, n_epochs + 1):\n    optimizer.zero_grad() # Очищает существующие градиенты от данных предыдущей эпохи\n    input_seq.to(device)\n    output, hidden = model(input_seq)\n    loss = criterion(output, target_seq.view(-1).long())\n    loss.backward() # Осуществляет обратное распространение ошибки и вычисляет градиенты\n    optimizer.step() # Обновляет веса последовательно\n    \n    if epoch%10 == 0:\n        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end = ' ')\n        print("Loss: {:.4f}".format(loss.item()))</code></pre><p>Потери становятся очень показательными, когда рассматриваются в сравнении. И вот теперь отчетливо заметна динамика снижения потерь:</p><pre><code class="language-python">Epoch: 10/100............. Loss: 2.4213\nEpoch: 20/100............. Loss: 2.1452\nEpoch: 30/100............. Loss: 1.7736\nEpoch: 40/100............. Loss: 1.3867\nEpoch: 50/100............. Loss: 1.0286\nEpoch: 60/100............. Loss: 0.7298\nEpoch: 70/100............. Loss: 0.5091\nEpoch: 80/100............. Loss: 0.3600\nEpoch: 90/100............. Loss: 0.2619\nEpoch: 100/100............. Loss: 0.1983</code></pre><p>Создадим функцию, которая принимает символы как аргументы и возвращает предсказание – следующий символ. Применим горячее кодирование ко входным данным для передачи в модель. Выбираем класс с наибольшей вероятностью:</p><pre><code class="language-python">def predict(model, character):\n    character = np.array([[char2int[c] for c in character]])\n    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n    character = torch.from_numpy(character)\n    character.to(device)\n    \n    out, hidden = model(character)\n\n    prob = nn.functional.softmax(out[-1], dim = 0).data\n    char_ind = torch.max(prob, dim = 0)[1].item()\n\n    return int2char[char_ind], hidden</code></pre><p>Эта функция принимает желаемую длину выходные данных и входные символы как аргументы, возвращая сгенерированное предложение:</p><p>Теперь посмотрим, какие символы модель ожидает увидеть после слова good:</p><pre><code class="language-python">sample(model, 15, 'good')</code></pre><p>Разобраться в таком игрушечном датасете просто, потому и ответ приходит сразу:</p><pre><code class="language-python">good i am fine </code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1G_dByrC4_AlAufrSZi0NBntfqrkIy1cK?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@grantritchie">@grantritchie</a></p><p>Автор оригинальной статьи: <a href="https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn">Avijeet Biswal</a></p>		riekurrientnaia-nieirosiet	2021-06-05		
101	Ярлык (Label)		<p>Ярлык – идентификатор необработанных данных, значимая метка для <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>, определение которой зачастую становится основной целью <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>:</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/unsupervised-learning.png" class="kg-image" alt loading="lazy" width="1484" height="915" srcset="__GHOST_URL__/content/images/size/w600/2021/06/unsupervised-learning.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/unsupervised-learning.png 1000w, __GHOST_URL__/content/images/2021/06/unsupervised-learning.png 1484w"><figcaption>Ярлык – это "доброкачественная" или "злокачественная" [опухоль] в верхней таблице</figcaption></figure><p>В машинном обучении Маркировка данных (Data Labeling) – это процесс идентификации необработанных данных (изображений, текстовых файлов, видео и т.д.) и добавления одной или нескольких значимых и информативных меток для обеспечения контекста, чтобы модель машинного обучения могла учиться на них. Например, метки могут указывать, есть ли на фотографии птица или машина, какие слова были произнесены в аудиозаписи. Маркировка данных требуется для <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерного зрения (CV)</a>, <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a> и Распознавания речи (Speech Recognition).</p><h3 id="%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-%D0%BC%D0%B0%D1%80%D0%BA%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Как работает маркировка данных?</h3><p>Сегодня в большинстве практических моделей машинного обучения используется <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Обучение с учителем (Supervised Learning)</a>, которое применяет алгоритм для сопоставления входных и выходных данных. Чтобы контролируемое обучение работало, нам нужен маркированный набор данных, на которых модель может учиться, чтобы принимать правильные решения. Маркировка данных обычно начинается с того, что <em>людей</em> просят вынести суждение о части немаркированных данных. Например, этикетировщика просят пометить все изображения в наборе данных, где есть птица. Маркировка может быть грубой, например, просто «да / нет», или же детальной (определение конкретных пикселей изображения, "затронутых" с птицей). Модель использует метки, предоставленные человеком, для изучения основных закономерностей в данных. Результатом является обученная модель, которую можно использовать для прогнозирования новых данных.</p><p>В машинном обучении правильно помеченный набор данных, который мы используем в качестве объективного стандарта для обучения и оценки модели, часто называют «наземной истиной» (Ground Truth). Точность обученной модели будет зависеть от точности фактов, потому очень важно потратить время и ресурсы на обеспечение высокоточной маркировки данных.</p><h3 id="%D0%BA%D0%B0%D0%BA%D0%B8%D0%B5-%D0%B1%D1%8B%D0%B2%D0%B0%D1%8E%D1%82-%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5-%D1%82%D0%B8%D0%BF%D1%8B-%D0%BC%D0%B0%D1%80%D0%BA%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B8-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Какие бывают распространенные типы маркировки данных?<br></h3><ul><li><strong>Компьютерное зрение</strong>: при создании такой системы нам сначала предстоит пометить изображения, пиксели или ключевые точки или создать границу (ограничивающую рамку), полностью охватывающую цифровое изображение, для создания набора <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a>. Например, мы можем классифицировать изображения по типу (например, изображения продукта или стиля интерьера) или по содержанию (что на самом деле на изображении).</li><li><strong>Обработка естественного языка</strong> требует, чтобы мы сначала вручную идентифицировали важные разделы текста или пометили текст определенными метками. Например, мы определяем эмоциональную окраску или намерение рекламного текста, определяем части речи, классифицируем имена собственные (места и люди), а также текст на изображениях, PDF- и других файлах. Для этого мы можем нарисовать ограничивающие рамки вокруг текста, а затем вручную расшифровать текст в своем наборе обучающих данных. Модели обработки естественного языка используются для <a href="__GHOST_URL__/analiz-tonalnosti-tieksta/">Анализа тональности текста (Sentiment Analysis)</a> и оптического распознавания символов (OCR).</li><li><strong>Обработка звука</strong> преобразует все виды аудиодорожек (речь, шумы дикой природы, "промышленные" звуки – бьющееся стекло, сигналы тревоги) и структурируют их, чтобы использовать при создании распознающего <a href="__GHOST_URL__/alghoritm/" rel="noopener noreferrer">Алгоритма (Algorithm)</a>. Обработка аудио часто требует, чтобы мы сначала вручную преобразовали его в письменный текст и оттуда получили более подробную информацию с помощью тегов.</li></ul><h3 id="%D0%BF%D0%B5%D1%80%D0%B5%D0%B4%D0%BE%D0%B2%D1%8B%D0%B5-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B-%D0%BC%D0%B0%D1%80%D0%BA%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B8-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Передовые методы маркировки данных</h3><p>Существует множество методов повышения эффективности и точности маркировки данных. Некоторые из этих методов включают:</p><ul><li><strong>Интуитивно понятные и оптимизированные интерфейсы</strong>, помогающие минимизировать когнитивную нагрузку на специалистов.</li><li><strong>Соглашение между специалистов по маркировке</strong>, чтобы помочь противодействовать ошибкам и предвзятости отдельных лиц.</li><li><strong>Активное обучение</strong> для повышения эффективности маркировки данных. С помощью машинного обучения определяются наиболее <em>полезные</em> данные к размечиванию.</li></ul><h3 id="%D0%BA%D0%B0%D0%BA-%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE-%D1%8D%D1%84%D1%84%D0%B5%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%BE-%D0%BC%D0%B0%D1%80%D0%BA%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D1%82%D1%8C-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D0%B5">Как можно эффективно маркировать данные?</h3><p>Успешные модели строятся на больших объемах высококачественных обучающих данных. Но процесс создания обучающих данных, необходимых для построения этих моделей, часто бывает дорогостоящим, сложным и требует много времени. Большинство современных моделей требуют, чтобы человек вручную пометил данные таким образом, чтобы модель могла научиться принимать правильные решения. Чтобы решить эту проблему, маркировку можно сделать более эффективной за счет использования маркирующей модели. Такой алгоритм сначала обучается на подмножестве необработанных данных, которые были помечены людьми. Если модель маркировки имеет высокую степень уверенности в своих результатах, то автоматически "навешивает" метки на необработанные данные. Если модель имеет меньшую уверенность в своих результатах, то передаст данные людям для маркировки. Созданные человеком метки затем передаются обратно в модель для переобучения и улучшения классифицирующей способности. Со временем модель может автоматически маркировать все больше и больше данных, что существенно ускоряет создание обучающих наборов.</p><p>Автор оригинальной статьи: <a href="https://aws.amazon.com/ru/sagemaker/groundtruth/what-is-data-labeling/#:~:text=In%20machine%20learning%2C%20data%20labeling,model%20can%20learn%20from%20it.">Amazon</a></p><p>Фото: <a href="https://unsplash.com/@albertorestifo">@albertorestifo</a></p>		iarlyk	2021-06-06		
102	Большие данные (Big Data)		<p>Большие данные – огромный объем данных, который невозможно сохранить и обработать с использованием традиционного вычислительного подхода в течение определенного периода времени.</p><p>Но насколько огромными должны быть эти данные, чтобы называться большими данными? Существует множество неправильных представлений о том, какой объем данных можно назвать большим.</p><p>Обычно данные в гигабайтах, терабайтах, петабайтах, эксабайтах или в любом другом формате, превышающем этот размер, считаются большими данными. И здесь возникает заблуждение. Даже небольшой объем данных может называться большим в зависимости от контекста. Чтобы прояснить это, позвольте использовать несколько примеров.</p><p>Например, если мы попытаемся прикрепить к электронному письму документ размером 100 мегабайт, то не сможем это сделать, поскольку система электронной почты не поддерживает вложения такого размера.<br>Таким образом, эти 100 мегабайт, относящиеся к электронной почте, можно назвать большими данными.</p><p>Допустим, у нас есть около 10 терабайт файлов изображений, которые необходимо обработать. Предположим, мы хотим изменить их размер и улучшить качество в течение заданного периода времени. Если воспользуемся настольным компьютером для выполнения этой задачи, то не сможем выполнить эту задачу в отведенные сроки, поскольку вычислительных ресурсов десктопного компьютера будет недостаточно для выполнения этой задачи. Для своевременного выполнения этой задачи нам потребуется мощный сервер с высокопроизводительными вычислительными ресурсами. Потому эти 10 терабайт файлов изображений можно назвать большими данными в контексте обработки на настольном компьютере. <br>Надеюсь, теперь Вам совершенно ясно, что подразумевается под большими данными.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D1%86%D0%B8%D1%80%D1%83%D1%8E%D1%82%D1%81%D1%8F-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D0%B5-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D0%B5">Как классифицируются большие данные?</h3><p>Большие данные подразделяются на 3 категории:</p><ul><li><strong>Структурированные </strong>данные – упорядоченная информация, например, базы данных, файлы CSV и электронные таблицы Excel</li><li><strong>Полуструктурированные</strong> данные –  электронные письма, лог-файлы и текстовые документы</li><li><strong>Неструктурированные</strong> данные – изображения, аудио- и видеофайлы</li></ul><h3 id="%D1%85%D0%B0%D1%80%D0%B0%D0%BA%D1%82%D0%B5%D1%80%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D1%85-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Характеристики больших данных</h3><p>Большие данные подразделяются на 3 важные характеристики:</p><ul><li>Объем – это размер собранных вместе данных.</li><li>Скорость – темп сбора или генерации информации</li><li>Разнообразие – количество типов создаваемых данных.</li></ul><h3 id="%D1%82%D1%80%D0%B0%D0%B4%D0%B8%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B9-%D0%BF%D0%BE%D0%B4%D1%85%D0%BE%D0%B4-%D0%BA-%D1%85%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8E-%D0%B8-%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B5-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D1%85-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Традиционный подход к хранению и обработке больших данных</h3><p>При традиционном подходе данные, генерируемые банками, фондовыми рынками или больницами, вводятся в систему ETL (извлечение, преобразование и загрузка). Система извлекает эти данные, преобразует их в правильный формат и, наконец, загружает в базу данных. После завершения этого процесса конечные пользователи смогут выполнять различные операции, такие как создание отчетов и аналитика.</p><p>Но по мере того, как данные увеличиваются в объеме, управление и обработка традиционными подходами становятся сложными задачами. Это одна из причин отказа от традиционного подхода к хранению и обработке больших данных.</p><p>Теперь давайте попробуем разобраться в некоторых основных недостатках, связанных с использованием традиционного подхода. Первый недостаток заключается в том, что это дорогая система и требует больших вложений для внедрения или обновления, поэтому малые и средние компании не смогут себе это позволить. </p><p>Второй недостаток – масштабируемость. По мере роста объемов данных расширение этой системы станет сложной задачей.</p><p>И последний недостаток – это трудоемкость. Обработка и извлечение ценной информации занимает много времени, поскольку вычислительные системы рассчитаны на меньшие объемы и скорости. </p><h3 id="%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D1%8B-%D1%81%D0%B2%D1%8F%D0%B7%D0%B0%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D0%BC%D0%B8-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D0%BC%D0%B8">Проблемы, связанные с большими данными</h3><p>С большими данными связаны две основные проблемы. Первая проблема заключается в том, как управлять таким огромным объемом данных аккуратно?</p><p>Вторая – как нам обрабатывать и извлекать ценную информацию из такого огромного объема данных в течение заданного периода времени? Это две основные проблемы, связанные с хранением и обработкой больших данных, которые привели к созданию фреймворка Hadoop.</p><p>Фото: <a href="https://unsplash.com/@redcharlie">@redcharlie</a></p><p>Автор оригинальной статьи: <a href="https://medium.com/swlh/big-data-explained-38656c70d15d">The Startup</a></p>		bolshiie-dannyie	2021-06-06		
103	Эпоха (Epoch)		<p>Эпоха – это Гиперпараметр (Hyperparameter) <a href="__GHOST_URL__/modiel/">Модели (Model)</a>, одна из повторяющихся стадий обработки <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочного набора (Train Data)</a>.</p><p>Для удобства восприятия термин стоит рассматривать в контексте определенного алгоритма, например, <a href="__GHOST_URL__/stokhastichieskii-ghradiientnyi-spusk/">Стохастического градиентного спуска (SGD)</a>.</p><p>Два гиперпараметра, которые часто путают новички, – это число <a href="__GHOST_URL__/pakiet/">Пакетов (Batch)</a> и количество эпох. Они оба являются целочисленными значениями и, похоже, делают одно и то же.</p><h3 id="%D0%B2-%D1%87%D0%B5%D0%BC-%D1%80%D0%B0%D0%B7%D0%BD%D0%B8%D1%86%D0%B0-%D0%BC%D0%B5%D0%B6%D0%B4%D1%83-%D0%BF%D0%B0%D0%BA%D0%B5%D1%82%D0%BE%D0%BC-%D0%B8-%D1%8D%D0%BF%D0%BE%D1%85%D0%BE%D0%B9">В чем разница между пакетом и эпохой?</h3><p>Стохастический градиентный спуск – это алгоритм <a href="__GHOST_URL__/optimizatsiia/">Оптимизации (Optimization)</a>, используемый для обучения Алгоритмов (Algorithm) <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, в первую очередь, искусственных <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a>, используемых в <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубоком обучении (Deep Learning)</a>.</p><p>Задача алгоритма – найти набор внутренних параметров модели, которые хорошо работают с показателями производительности, такими как <a href="__GHOST_URL__/logharifmichieskaia-potieria/">Логарифмические потери (Log Loss)</a> или <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратичная ошибка (MSE)</a>.</p><p>Оптимизация – это тип поискового процесса, и вы можете думать об этом как о обучении. Алгоритм оптимизации называется «градиентный спуск», где «градиент» относится к вычислению темпа снижения ошибки, а «спуск» относится к движению вниз по этому наклону к некоторому минимальному уровню ошибки.</p><p>Алгоритм итерационный. Это означает, что процесс поиска состоит из нескольких дискретных шагов, каждый из которых, мы надеемся, немного улучшает параметры модели.</p><p>Каждый шаг включает использование модели с текущим набором внутренних параметров для прогнозирования некоторых выборок, сравнение прогнозов с реальными ожидаемыми результатами, вычисление ошибки и использование ошибки для обновления внутренних параметров модели.</p><p>Эта процедура обновления различается для разных алгоритмов, но в случае искусственных нейронных сетей используется алгоритм обновления с Обратным распространением ошибки (Backpropagation).</p><p>Прежде чем мы углубимся в пакеты и эпохи, давайте посмотрим, что подразумевается под <a href="__GHOST_URL__/vyborka/">Выборкой (Sample)</a>.</p><p><a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a> – это одна строка данных. Оно содержит входные данные, которые вводятся в алгоритм, и выходные, которые используются для сравнения с прогнозом и вычисления ошибки. Набор обучающих данных состоит из множества наблюдений. Выборку также можно назвать входным вектором или вектором признаков.</p><p>Теперь, когда мы знаем, что такое образец, давайте определим партию. Что это такое? Размер пакета – это гиперпараметр, который определяет количество выборок, которые необходимо обработать перед обновлением внутренних параметров модели.</p><p>Подумайте о пакете как о цикле <code>for</code>, который выполняет итерацию по одной или нескольким выборкам и делает прогнозы. В конце пакета прогнозы сравниваются с ожидаемыми выходными переменными и вычисляется ошибка. Из-за этой ошибки алгоритм обновления используется для улучшения модели.</p><p>Набор обучающих данных можно разделить на один или несколько пакетов. Когда все обучающие записи используются для создания одного пакета, алгоритм обучения называется пакетным градиентным спуском. Когда размер пакета составляет одну выборку, алгоритм обучения называется стохастическим градиентным спуском. Когда размер пакета составляет более одной выборки и меньше размера обучающего набора данных, алгоритм обучения называется мини-пакетным градиентным спуском.</p><ul><li><strong>Пакетный градиентный спуск</strong>: Размер партии = Размер обучающего набора</li><li><strong>Стохастический градиентный спуск</strong>: Размер партии = 1</li><li><strong>Мини-пакетный градиентный спуск (Mini-Batch Gradient Descent)</strong>: 1 &lt; Размер партии &lt; Размер обучающего набора</li></ul><p>В случае мини-пакетного градиентного спуска популярные размеры пакета – 32, 64 и 128 образцов. Но что делать, если набор данных не делится равномерно по размеру пакета? Это может происходить и часто случается при обучении модели. Это просто означает, что в последней партии образцов меньше, чем в других партиях.</p><p>В качестве альтернативы вы можете удалить некоторые образцы из набора данных или изменить размер пакета так, чтобы количество образцов в наборе данных делилось равномерно на размер пакета.</p><p>Пакет обновляет модель с использованием записей. Теперь давайте изучим эпоху.</p><p>Количество эпох – это гиперпараметр, который определяет, сколько раз алгоритм обучения будет обрабатывать весь набор обучающих данных.</p><p>Одна эпоха означает, что каждая выборка в наборе обучающих данных имела возможность обновить внутренние параметры модели. Эпоха состоит из одной или нескольких пакетов-партий. Например, как указано выше, эпоха с одним пакетом называется алгоритмом обучения пакетного градиентного спуска.</p><p>Вы можете думать о цикле по количеству эпох, когда каждый цикл проходит по набору обучающих данных. Внутри этого цикла for находится еще один вложенный цикл <code>for</code>, который выполняет итерацию по каждому пакету выборок, где один пакет имеет заданное количество выборок «размер пакета».</p><p>Число эпох традиционно велико, часто сотни или тысячи, что позволяет алгоритму обучения работать до тех пор, пока ошибка модели не будет достаточно минимизирована. Вы можете увидеть примеры количества эпох в литературе и в учебных пособиях, установленных на 10, 100, 500, 1000 и больше.</p><p>Обычно создают линейные графики, которые показывают эпохи по оси x как время, а ошибки или навыки модели по оси y. Эти графики иногда называют Кривой обучения (Learning Curve). Эти графики помогут определить, переобучена ли модель, недостаточно обучена и подходит ли она для обучающего набора данных.</p><p>Если все еще непонятно, давайте посмотрим на различия между партиями и эпохами.</p><p>Размер пакета – это количество образцов, обработанных перед обновлением модели. Количество эпох – это количество полных проходов через обучающий набор данных.</p><p>Размер пакета должен быть больше или равен единице и меньше или равен количеству выборок в обучающем наборе данных.</p><p>Для количества эпох можно задать целочисленное значение от единицы до бесконечности. Вы можете запускать алгоритм столько, сколько захотите, и даже останавливать его, используя другие критерии, помимо фиксированного количества эпох, такие как изменение (или отсутствие изменений) ошибки модели с течением времени.</p><p>Оба они являются целочисленными значениями, и оба являются гиперпараметрами алгоритма обучения. Вы должны указать размер пакета и количество эпох для алгоритма обучения. </p><p>Не существует волшебных правил настройки этих параметров. Вы должны попробовать разные значения и посмотреть, что лучше всего подходит для вашей проблемы.</p><p>Пример. Предположим, у вас есть набор данных с 200 строками данных, и вы выбрали размер пакета из 5 и 1000 эпох. Это означает, что набор данных будет разделен на 40 пакетов по пять образцов в каждом. Вес модели будет обновляться после каждой партии из пяти образцов.</p><p>Это также означает, что одна эпоха будет включать 40 пакетов или 40 обновлений модели.</p><p>Для 1000 эпох модель будет представлена ​​или пройти через весь набор данных 1000 раз. Это всего 40 000 партий за весь тренировочный процесс.</p><p>Фото: <a href="https://unsplash.com/@andrewcoop?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@andrewcoop</a></p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/">Jason Brownlee</a></p>		epokha	2021-06-13		
104	Мода (Mode)		<p>Мода – это наиболее часто встречающееся в наборе данных значение. Признак (Feature) Датасета (Dataset) может иметь одну или более мод или вообще не иметь таковую. Другие популярные меры центральной тенденции включают <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее арифметическое (Mean)</a> и <a href="__GHOST_URL__/miediana/">Медианy (Median)</a>.</p><p>В статистике данные могут распределяться по-разному. Наиболее часто цитируемым распределением является классическое <a href="__GHOST_URL__/normalnoie-raspriedielieniie/">Нормальное распределение (Normal Distribution)</a>. В этом и некоторых других распределениях среднее значение и мода совпадают. </p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/06/normal-distribution.png" class="kg-image" alt loading="lazy" width="700" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/06/normal-distribution.png 600w, __GHOST_URL__/content/images/2021/06/normal-distribution.png 700w"></figure><p>Мода наиболее полезна в качестве меры центральной тенденции при изучении категориальных данных, таких как модели автомобилей или вкусы газированных напитков, для которых невозможно вычислить среднее математическое среднее значение.</p><p>Во многих случаях модальное значение будет отличаться от среднего значения в данных.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B">Примеры</h3><p>Например, в следующем списке чисел модой является 16, поскольку встречается чаще, чем любое другое число: </p><p><code>[3, 3, 6, 9, 16, 16, 16, 27, 27, 37, 48]</code><br>Набор чисел может иметь более одной моды (бимодальный ряд), если два значения встречаются с одинаковой частотой и чаще, чем другие.</p><p><code>[3, 3, 3, 9, 16, 16, 16, 27, 37, 48]</code><br>В приведенном выше примере и число 3, и число 16 являются модальными значениями, поскольку каждый из них встречается три раза, и никакое другое число не встречается чаще.</p><p><code>[3, 6, 9, 16, 27, 37, 48]</code></p><p>Если ни одно число в наборе чисел не встречается более одного раза, этот набор не имеет режима, как список выше:</p><p>Набор чисел с двумя модами является бимодальным, набор чисел с тремя режимами является тримодальным, а любой набор чисел с более чем одним режимом является мультимодальным.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%B8%D0%BC%D1%83%D1%89%D0%B5%D1%81%D1%82%D0%B2%D0%B0-%D0%B8-%D0%BD%D0%B5%D0%B4%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BA%D0%B8">Преимущества и недостатки</h3><p>Преимущества моды:</p><ul><li>Проста для понимания и расчета</li><li>Устойчива к влиянию экстремальных значений</li><li>Легко идентифицируется в наборе данных</li><li>Полезна для качественных данных</li><li>Легко вычисляется, даже если таблица пополняется на ходу</li><li>Легко найти на графике распределения визуально</li></ul><p>Недостатки:</p><ul><li>Не определяется, если в наборе данных нет повторов</li><li>Не дает целостного представления о наборе</li><li>Непоказательна, когда набор маленький</li><li>Вносит мультимодальность. Множество модальных значений мешают получить представление о самых популярных значениях</li></ul><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/m/mode.asp">Adam Hayes</a></p><p>Фото: <a href="https://unsplash.com/@milada_vigerova">@milada_vigerova</a></p>		moda	2021-06-11		
105	Смещение (Bias)		<p>Смещение (Предвзятость, Bias) – феномен наблюдения за результатами, систематически неверно описываемыми из-за ошибочных допущений. Например, <a href="__GHOST_URL__/nieironnaia-siet/">Нейронная сеть (Neural Network)</a>, принимающая решения о выдаче кредита малому предпринимательству на основании исторически смещенных в пользу белокожих мужчин данных, чаще отказывает в кредите афроамериканцам и женщинам. Термин тесно связан с понятием <a href="__GHOST_URL__/spraviedlivost/">Справедливости (Fairness)</a>.</p><p>Мирей Хильдебрандт, юрист и философ, работающая на стыке права и технологий, много писала и говорила о проблеме предвзятости и справедливости в <a href="__GHOST_URL__/alghoritm/">Алгоритмах (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. В своей статье об агностическом и свободном от предвзятости машинном обучении, (Hildebrandt, 2019) она утверждает, что свободного от смещения машинного обучения не существует и для того, чтобы алгоритм мог моделировать данные и делать релевантные прогнозы, необходимо корректировать предвзятое отношение. Три основных типа смещений, которые могут возникать в системе прогнозирования:</p><ul><li>Присущее любой системе восприятия, обусловленная несовершенством органов восприятия или аппаратуры</li><li>Несправедливое по отношению к определенным социальным группам</li><li>Дискриминирующее на правовых основаниях</li></ul><p>Производительность <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> машинного обучения достигается за счет минимизации Целевой функции затрат (Cost Function). Выбор функции стоимости и, следовательно, пространства возможных значений вносит в систему то, что мы называем производственной предвзятостью. Другие источники ее исходят из контекста, цели, адекватности <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовых данных (Test Data)</a>, используемого метода <a href="__GHOST_URL__/optimizatsiia/">Оптимизации (Optimization)</a>, а также из-за компромиссов между скоростью, точностью, <a href="__GHOST_URL__/pierieobuchieniie/">Переобучением (Overfitting)</a> и чрезмерным обобщением. Каждый выбор имеет соответствующую "стоимость" непредвзятости. <em>Машинное обучение по умолчанию является предвзятым</em>, поскольку таково фундаментальное свойство подобных систем.</p><p>Кроме того, данные обучения также обязательно имеют смещение, и задача <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">Разведочного анализа данных (EDA)</a> состоит в том, чтобы разделять дискриминационную предвзятость, которая как бы подгоняет <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> под одну гребенку, и предвзятость как простой вычислительный артефакт.</p><p>Без предположений, однако, алгоритм был бы менее производителен при решении задачи, чем при случайном выборе. Этот принцип, формализованный Вольпертом в 1996 году, называют "Теоремой о запрете бесплатного обеда" (No Free Lunch Theorem). Согласно этой теории, все классификаторы имеют одинаковую частоту ошибок при усреднении по всем возможным распределениям данных. Следовательно, классификатор <em>должен</em> иметь определенную предвзятость в отношении определенных распределений, чтобы лучше моделировать их. Выражаясь на языке кредитования малого бизнеса, система принимает свою предвзятость относительно большей успешности белокожих мужчин в сфере малого предпринимательства и <em>потому лучше классифицирует лиц</em>, способных осуществить возврат заимствованных средств, чем если бы исключила гендерную и расовую предвзятость.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/bias.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/06/bias.png 600w, __GHOST_URL__/content/images/2021/06/bias.png 1000w"><figcaption>Наилучшая производительность сопровождается потерей качества обобщения</figcaption></figure><p>Также всегда следует помнить, что данные, используемые для обучения алгоритма, являются <em>конечными</em> и поэтому не отражают реальность. Это также приводит к смещению, которое возникает из-за выбора обучающих и тестовых данных и их совокупности. Еще одно предположение: ограниченные обучающие данные могут моделировать и точно классифицировать тестовые.</p><p><em>«[…] Самое современное машинное обучение основывается на решающем предположении, что распределение обучающих примеров идентично распределению тестовых. Несмотря на то, что нам необходимо сделать это предположение для получения теоретических результатов, важно помнить, что это предположение часто нарушается на практике». © </em>Тим Митчелл</p><p>Даже если нам удастся избавить наши системы от смещения, все равно существует вероятность того, что с течением времени предвзятость будет накапливаться. После того, как алгоритмы обучены, налажены и запущены в производство, спустя пару лет легко заметить, как окружающий мир меняется относительно него.</p><p>Осознание различных предубеждений, присутствующих в этих системах, требует способности объяснять и интерпретировать их работу, повторяя тему прозрачности выступления Адриана Веллера: если вы не можете проверить это, вы не можете оспорить это. Нам необходимо четко понимать, насколько продуктивным является предвзятость, обеспечивающая функциональность этих систем, и выяснять остающуюся несправедливость либо в обучающем наборе, либо в алгоритмах. Это позволило бы нам сделать вывод о третьем и наиболее серьезном виде предвзятости, которое приводит к дискриминации на запрещенных правовых основаниях.</p><p>Кришна Гуммади, глава исследовательской группы сетевых систем в Институте программных систем Макса Планка, много работал над снижением дискриминационной предвзятости в классификаторах, его исследования в этой области касаются дискриминации с вычислительной точки зрения и разрабатывают алгоритмические методы для ее минимизации.</p><p>Поскольку системы машинного обучения становятся все более распространенными в автоматизированном принятии решений, очень важно, чтобы мы делали эти системы чувствительными к типу предвзятости, которая приводит к дискриминации, особенно дискриминации по незаконным основаниям. Машинное обучение уже используется для принятия или оказания помощи в принятии решений в следующих областях: набор персонала (отбор кандидатов на работу), банковское дело (кредитные рейтинги / одобрение ссуд), судебная система (оценка риска рецидивизма), благосостояние (социальное пособие, право на участие в жизни ребенка), журналистика (рекомендательные системы новостей) и т.д. Учитывая масштаб и влияние этих отраслей, крайне важно, чтобы мы принимали меры для предотвращения несправедливой дискриминации в них как с помощью юридических, так и технических средств.</p><p>Чтобы проиллюстрировать свои приемы, Кришна Гуммади сосредоточился на вызывающем споры инструменте прогнозирования рецидивов под названием COMPAS, разработанном Northpointe Inc., результаты которого используются судьями по всей территории Соединенных Штатов при предварительном рассмотрении дела и вынесении приговора. Алгоритм основан на ответах респондентов на 137 вопросов, включая темы семейной истории, жилого района, успеваемости в школе и т.д., чтобы предсказать риск совершения преступлений.</p><p>Результат алгоритма использовался судьями для определения продолжительности и типа приговора в режиме <a href="__GHOST_URL__/chiernyi-iashchik/">Черного ящика (Black Box)</a>. В то время как некоторые исследования показали, что COMPAS (Профилирование исправительных правонарушителей для альтернативных санкций) не лучше предсказывает риск рецидивизма, чем случайно набранные интернет-незнакомцы, другие исследования были больше сосредоточены на проверке алгоритмов обработки различных значимых социальных групп.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/bias-recidivated-people.png" class="kg-image" alt loading="lazy" width="1508" height="644" srcset="__GHOST_URL__/content/images/size/w600/2021/06/bias-recidivated-people.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/bias-recidivated-people.png 1000w, __GHOST_URL__/content/images/2021/06/bias-recidivated-people.png 1508w" sizes="(min-width: 1200px) 1200px"><figcaption>Алгоритм ошибочно классифицирует невиновных черно- и белокожих</figcaption></figure><p>Исследование, проведенное ProPublica (Джефф Ларсон, 2016), показало, что алгоритм в два раза чаще относил чернокожих защитников к группе повышенного риска, которые в конечном итоге не совершали повторных преступлений, по сравнению с белыми защитниками. Об этом свидетельствует показатель "Удержался от рецидива" чернокожих обвиняемых, который составляет 44,85 (то есть 44,85% черных обвиняемых, отнесенных к категории рецидивистов, не совершали повторных преступлений) по сравнению с 23,45 среди белых обвиняемых. Однако Нортпойнт выступил с опровержением: в соответствии с используемыми мерами, у черных и белых защитников была одинаковая степень ошибочной классификации.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/image.png" class="kg-image" alt loading="lazy" width="404" height="242"><figcaption>Оценка по 10-балльной шкале с аналогичными показателями</figcaption></figure><p>Оказывается, обе стороны правы, и это потому, что они используют разные меры справедливости. И не существует алгоритма, который мог бы одинаково хорошо работать со всеми показателями справедливости, если базовые показатели рецидивов различаются для черных и белых. Обе эти меры справедливости представляют собой неизбежные компромиссы. <br>Мы должны быть в состоянии понять компромиссы этих мер справедливости, чтобы принимать обоснованные решения в отношении их дискриминационной способности. Вот почему нам нужна вычислительная перспектива. Итак, давайте сначала разберемся, что такое дискриминация:</p><p><em>"неправомерное навязывание лицам относительно невыгодного положения на основании их принадлежности к какой-либо значимой социальной группе, например, по признаку расы или пола."</em></p><p>Как мы реализуем это в алгоритме? Вышеприведенное выражение содержит несколько нечетких понятий, требующих формализации, «неправомерное навязывание», «значимая социальная группа» и т.д. Если мы сосредоточимся только на компоненте «невыгодного положения», то определим первый тип дискриминации – Несопоставимое обращение.</p><p>Кришна использовал вышеупомянутую задачу Двоичной классификации (Binary Classification), чтобы лучше понять типы дискриминации. Проблема состоит в том, чтобы предсказать, будет ли выплачена ссуда, на основе m + 1 характеристик, из которых одна характеристика является чувствительной, например раса клиента.</p><p><strong>Несопоставимое обращение.</strong> Такой тип относительной дискриминации обнаруживается, если прогнозируемый пользователем результат изменяется при изменении чувствительного признака. В приведенном выше примере это будет означать, что алгоритм предсказывает успешный возврат ссуды для белого человека и неуспешный – для чернокожего человека, даже если все остальные характеристики людей одинаковы. Чтобы предотвратить любую зависимость от расы, нам нужно удалить чувствительные функции из набора данных.</p><p><strong>Несопоставимое воздействие</strong> – тип относительной дискриминации, который может быть обнаружен, если есть разница в доле положительных (отрицательных) результатов для разных чувствительных групп. В приведенном выше случае это произошло бы, если бы бо́льший процент чернокожих людей был отнесен к категории неплательщиков по сравнению с белыми.</p><p>Этот тип измеряет уровень косвенной дискриминации по отношению к группе и часто также проявляется в процессе принятия решений людьми. Даже если мы удалим чувствительный признак, дискриминация все равно может "проскочить" через другие коррелирующие характеристики, такие как почтовый индекс. Такая подчас непростая корректировка гарантирует большую непредвзятость. Многие критики считают, что некоторые наборы невозможно освободить от непредвзятости.</p><p><strong>Несоизмеримое жестокое обращение</strong>. Этот тип относительной дискриминации обнаруживается, когда мы измеряем разницу в долях точных результатов для разных чувствительных групп. Это тип дискриминации, который был обнаружен Propublica в алгоритме Northpointe, который ошибочно классифицировал невинных чернокожих защитников как повторно совершающих преступления в два раза чаще, чем белых людей. Мы можем исправить это плохое обращение, потребовав одинаковых пропорций точных результатов для всех вовлеченных чувствительных групп.</p><h3 id="%D0%BC%D0%B5%D1%85%D0%B0%D0%BD%D0%B8%D0%B7%D0%BC%D1%8B-%D0%BD%D0%B5%D0%B4%D0%B8%D1%81%D0%BA%D1%80%D0%B8%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F">Механизмы недискриминационного машинного обучения</h3><p>Прежде чем формализовать механизмы для исправления различения в алгоритмах, мы должны принять во внимание тот факт, что алгоритмы не беспристрастны, как эта статья. Они объективны по сравнению с людьми, но это не делает их справедливыми, просто объективно дискриминационными. Задача алгоритма – оптимизировать функцию затрат, чтобы достичь наилучшего приближения к фактической ситуации. Если лучшая функция классифицирует всех членов неблагополучной группы как повторно совершающих правонарушения или неспособных выплатить ссуду, то алгоритм будет продолжать генерировать подобные решения. Объективные решения могут быть несправедливыми и дискриминационными.</p><p>Проще говоря, алгоритм находит наиболее похожую функцию, которая принимает признаки в качестве входных данных. Он определяет наилучшие параметры для этой функции, основываясь на том, какие из них минимизируют разницу между выходными данными функции и фактическими результатами. Это называется проблемой оптимизации (Optimisation Problem). Мы можем добавить дополнительные ограничения к этой задаче, потребовав, чтобы приближенная функция также подчинялась одному или всем сформулированным выше требованиям, чтобы избежать различения. </p><p>Однако добавление ограничений приводит к компромиссу в области точности. Кришна применил это ограничение к алгоритму предсказания рецидивов и ценой небольшой точности сумел получить алгоритм, который имеет такой же уровень ошибок для черных и белых обвиняемых. Он использовал ограничения на коэффициент ложноположительных результатов (FPR), который представляет собой вероятность того, что невиновный обвиняемый будет классифицирован как рецидивист, и коэффициент ложноотрицательных результатов (FNR), который представляет собой вероятность того, что в будущем обвиняемый, совершивший повторное преступление, будет классифицирован как не совершающий повторное преступление. Как мы видим на рисунке ниже, разница в FPR и FNR черных и белых обвиняемых приближается к 0, поскольку ограничения ужесточаются с небольшой потерей точности (от ~ 66,7% до ~ 65,8%):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/image-1.png" class="kg-image" alt loading="lazy" width="451" height="235"></figure><p>Короче говоря, несмотря на опасения, что алгоритмы будут служить только для дальнейшего закрепления и распространения человеческих предубеждений, сообщество <a href="__GHOST_URL__/iskusstviennyi-intielliekt/" rel="noopener noreferrer">Искусственного интеллекта (AI)</a> прилагает значительные усилия, чтобы избежать и исправить дискриминационные предубеждения в алгоритмах, а также сделать их более прозрачными. Кроме того, появление Общего регламента по защите данных (GDPR) формализует и структурирует эти усилия, чтобы убедиться, что отрасли  стимулированы следовать лучшим практикам, когда дело доходит до хранения и обработки огромного количества личных данных граждан, и уделяют приоритетное внимание справедливости за счет некоторой точности. В конечном итоге алгоритмические системы будут отражением общества, которое они пытаются моделировать, и потребуются активные усилия со стороны правительства и частного сектора, чтобы убедиться, что это работает не только для закрепления и дальнейшего усугубления неравенства, присущего нашим структурам, но исправляет их, вводя строгие меры и "штрафующие" ограничения. Это позволяет нам представить себе общество, в котором процесс принятия решений может потенциально избавиться от субъективности человеческих предубеждений, заменив их объективными алгоритмическими решениями, которые осознают свои предубеждения, даже если не полностью свободны от них.</p><p>Фото: <a href="https://unsplash.com/@alex_marchenko">@alex_marchenko</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac">Jaspreet</a></p>		smieshchieniie	2021-06-12		
106	Обучение с подкреплением (Reinforcement Learning)		<p>Обучение с подкреплением (RL) – подход, который находится между <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Контролируемым обучением (Supervised Learning)</a> и <a href="__GHOST_URL__/niekontroliruiemoie-obuchieniie/">Обучением без учителя (Unsupervised Learning)</a>. Оно не контролируется строго, поскольку не полагается только на набор помеченных данных обучения, но и не является обучением без учителя, потому что есть максимизируемое вознаграждение. <a href="__GHOST_URL__/modiel/">Модели (Model)</a> предстоит найти «правильные» действия в различных ситуациях для достижения своей цели.</p><p>Обучение с подкреплением – это наука о принятии решений.<br> Здесь нет супервизора – живого человека, когда-то разметившего <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>, и модель использует только сигнал вознаграждения, чтобы определить, хорошо справляется или нет. Время является ключевым компонентом: процесс является последовательным с отложенной обратной связью. Каждое действие модели влияет на следующие данные, которые она получает.</p><h3 id="%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0-rl">Проблема RL</h3><p>До сих пор мы говорили, что агент должен найти «правильное» действие, которое зависит от награды. Вознаграждение Rₜ представляет собой скалярный сигнал обратной связи, который указывает, насколько хорошо <a href="__GHOST_URL__/alghoritm/">Алгоритм (Algorithm)</a> работает на шаге времени t.</p><p>В обучении с подкреплением нам нужно определить нашу проблему так, чтобы применить ее для удовлетворения нашей гипотезы вознаграждения. Примером может служить игра в шахматы, где алгоритм получает вознаграждение за победу в игре и наказание за проигрыш.</p><p>Гипотеза вознаграждения: все цели можно описать как максимизацию ожидаемого совокупного вознаграждения. Поскольку наш процесс включает в себя последовательные задачи по принятию решений, наши действия на раннем этапе могут иметь долгосрочные последствия для нашей общей цели. Иногда может быть лучше пожертвовать немедленным вознаграждением (вознаграждением на временном шаге Rₜ), чтобы получить более долгосрочное. Выражаясь на языке шахмат, мы можем пожертвовать пешкой сейчас для взятия ладьи на более позднем этапе.</p><p>При обучении с подкреплением агент принимает решения о том, какие действия предпринять на каждом временном шаге Aₜ. Агент принимает эти решения на основе получаемого скалярного вознаграждения Rₜ и наблюдаемой среды Oₜ.</p><p> Фото: <a href="https://unsplash.com/@heyeje?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@heyeje</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d">Ryan Wong</a></p>		obuchieniie-s-podkrieplieniiem	2021-06-19		
107	Правило большого пальца (Rule of Thumb)		<p>Правило большого пальца (эмпирическое правило диапазона) – это удобный метод оценки диапазона по <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартному отклонению (Standard Deviation)</a>. Это говорит нам о том, что диапазон обычно примерно в четыре раза превышает стандартное отклонение.</p><p>Итак, если ваше стандартное отклонение равно 2, вы можете предположить, что ваш диапазон составляет около восьми.</p><p>Используя эмпирическое правило диапазона, также можно оценить стандартное отклонение. Просто разделите диапазон на четыре.</p><p>Эмпирическое правило диапазона лучше всего работает для данных из <a href="__GHOST_URL__/normalnoie-raspriedielieniie/">Нормального распределения (Normal Distribution)</a>, где размер выборки близок к тридцати. Для выборок, которые либо намного меньше, либо намного больше тридцати, либо не соответствуют нормальному распределению, это может быть очень плохой догадкой.</p><p>Правило предполагает, что большинство значений находится в области, охватываемой четырьмя стандартными отклонениями, то есть в пределах двух стандартных отклонений выше или ниже среднего. Это позволяет нам определять необычные значения как те, которые не попадают в этот диапазон. Мы называем максимальное обычное значение средним значением плюс два стандартных отклонения, а минимальное обычное значение – средним минус два стандартных отклонения.</p><h3 id="%D0%BF%D1%80%D0%B0%D0%B2%D0%B8%D0%BB%D0%BE-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%BE%D0%B3%D0%BE-%D0%BF%D0%B0%D0%BB%D1%8C%D1%86%D0%B0-%D0%B2-%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%BC-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B8">Правило большого пальца в Глубоком обучении</h3><p>В контексте <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> правилами большого пальца называют параметры <a href="__GHOST_URL__/modiel/">Модели (Model)</a>, принятые по умолчанию:</p><ul><li><strong>Количество Слоев (Layer)</strong>: начните с двух скрытых слоев (исключая последний).</li><li><a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/"><u><strong>Количество узлов промежуточных слоев</strong></u></a>: число в геометрической прогрессии: 2, 4, 8, 16, 32 и т.д. Первый слой должен составлять примерно половину числа объектов входных данных. Размер следующего слоя – половина предыдущего.</li><li><strong>Количество узлов (размер) выходного слоя для классификации</strong>: при Двоичной классификации (Binary Classification) размер равен единице. Для Мультиклассовой классификации (Multi-Class Classification) размер – это количество классов.</li><li><strong>Размер выходного слоя для Регрессии (Regression)</strong>: если ответ один, то размер тоже. Для регрессии с несколькими ответами размер – это количество ответов.</li><li><strong>Активация для промежуточных слоев</strong>: используйте активацию <code>relu</code>.</li><li><strong>Активация для выходного слоя</strong>: используйте Сигмоид (Sigmoid) для двоичной классификации, <a href="__GHOST_URL__/softmaks/">Softmax</a> для мультиклассового классификатора и линейный для регрессии. </li><li><strong><a href="__GHOST_URL__/vypadaiushchii-sloi/">Выпадающие слои (Dropout Layer)</a></strong>: добавляйте выпадающие слои после каждого слоя, кроме входного (определяется отдельно). Установите коэффициент отсева на 0,5. Показатель отсева &gt; 0,5 контрпродуктивен. Если мы считаем, что коэффициент 0,5 регулирует слишком много узлов, увеличим размер слоя вместо того, чтобы уменьшать частоту выпадения до менее 0,5. Если вы чувствуете себя обязанным это сделать, установите коэффициент отсева &lt;0,2.</li><li><strong>Предварительная обработка данных</strong>: предполагается, что наши <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor)</a> X являются числовыми, и мы уже преобразовали любые категориальные столбцы в однократное <a href="__GHOST_URL__/bystroie-kodirovaniie/">Горячее кодирование (One-Hot Encoding)</a>. Перед использованием данных для обучения модели выполните масштабирование данных. UseMinMaxScaler из <code>sklearn.preprocessing</code>. Если это не работает, добавьте StandardScaler в ту же библиотеку. Масштабирование необходимо для y в регрессии.</li><li><strong>Разделение данные для обучения, проверки, валидации</strong>: используйте train_test_split из sklearn.model_selection. См. Пример ниже.</li><li><strong>Веса (Weights) классов</strong>: если у нас несбалансированные данные, установим веса классов, чтобы сбалансировать потери в вашей модели .fit. Для двоичного классификатора веса должны быть: {0: количество единиц / размер данных, 1: количество нулей / размер данных}. Для крайне несбалансированных данных (редкие события) вес класса может не работать. Будьте осторожны, добавляя его.</li><li><strong>Оптимизатор</strong>: используйте <code>adam</code> со скоростью обучения по умолчанию.</li><li><strong>Потеря классификации</strong>: для двоичной классификации используйте binary_crossentropy. Для мультиклассовой классификации используйте <code>categorical_crossentropy</code>, если метки имеют горячее кодирование, в противном случае используйте <code>sparse_categorical_crossentropy</code>, если метки являются целыми числами.</li><li><strong>Потеря регрессии</strong>: используйте <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратическая ошибку (MSE)</a>.</li><li><strong>Метрики для классификации</strong>: используйте точность, которая показывает процент правильных классификаций. Для несбалансированных данных также включайте <code>tf.keras.metrics.Recall()</code> и <code>tf.keras.metrics.FalsePositives()</code>. </li><li><strong>Метрика регрессии</strong>: используйте <code>tf.keras.metrics.RootMeanSquaredError()</code>.</li><li><a href="__GHOST_URL__/epokha/"><strong>Эпохи (Epoch)</strong></a>: начните с 20, чтобы увидеть, показывает ли обучение модели уменьшение потерь и повышение точности. Если нет минимального успеха с 20 эпохами, двигайтесь дальше. Если вы добились минимального успеха, сделайте число эпох, равное 100.</li><li><strong>Размер Пакета (Batch)</strong>: выберите размер пакета из геометрической прогрессии 2, 4, 8 и т.д. Для несбалансированных наборов данных используйте бо́льшее значение, например 128, в противном случае начните с 16.</li></ul><p>Фото: <a href="https://unsplash.com/@katya?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">katya</a>, <a href="https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af">Chitta Ranjan</a></p><p>Автор оригинальной статьи: <a href="https://www.statisticshowto.com/range-rule-of-thumb/#:~:text=The%20range%20rule%20of%20thumb%20suggests%20that%20most%20values%20would,t%20fall%20into%20this%20range.">Stephanie Glen</a></p>		pravilo-bolshogho-paltsa	2021-06-19		
108	TensorBoard		<p>TensorBoard – плагин, позволяющий отслеживать основные потери<br><a href="__GHOST_URL__/modiel/">Модели (Model)</a>, визуализировать <a href="__GHOST_URL__/graf/">Графы (Graph)</a>, контролировать различные времязависимые векторы.</p><p>Мы сконцентрируемся на метаданных процесса, поскольку это позволит Вам использовать ключевые концепции более эффективно. Вы наверняка заметили, что код фреймворка TensorFlow зачастую выглядит минималистично, и это вызывает вопросы вроде «А как кастомизировать<br>нейронную сеть под себя?».</p><p>Мы загружаем датасет, тренируем модель, настраиваем логгинг, и на этой стадии мы взглянем, что же у нашей модели под капотом. Загрузим расширение с помощью директивы «percent sign load extension»:</p><pre><code class="language-python">%load_ext tensorboard</code></pre><p>Импортируем TensorFlow под псевдонимом tf, а также библиотеку datetime<br>для автоматизации нейминга логов:</p><pre><code class="language-python">import tensorflow as tf\nimport datetime</code></pre><p>Очистим предыдущие логи в специальной директории, если таковые имеются:</p><pre><code class="language-python">!rm -rf ./logs/</code></pre><p>Используем модифицированный датасет Национального института стандартов и разделим загруженный датасет на тренировочную и валидационную части с помощью множественного присваивания.</p><pre><code class="language-python">mnist = tf.keras.datasets.mnist\n\n# Разделим загруженный датасет на тренировочную и валидационную части \n# с помощью множественного присваивания\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\n\n# Нормализуем данные, разделив значения на 255. Поскольку мы используем\n# изображения, то описываются они как последовательность пикселей\n# в цветовой модели RGB. Каждый пиксель описывается в три числа от 0 до 255,\n# потому мы и делим их все на 255, чтобы привести к диапазону от 0 до 1. \n# С таким форматом данных нейронная сеть и будет работать.\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Определим функцию–создателя модели\ndef create_model():\n  # Используем последовательную модель API Keras. Она подходит для \n  # простого стека слоев, где на входе и выходе по одному тензору.\n  return tf.keras.models.Sequential([\n    # Сделаем слои плоскими. Поскольку мы делим датасет на партии (batches)\n    # и не указываем разрешение канала, то Flatten() добавляет его и тем \n    # самым упорядочивает обучение. Размер изображений – 28 на 28.\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n\n    # Добавим полносвязный слой размерностью 512\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n\n    # Добавим второй полносвязный слой и используем метод активации softmax.\n    # Типов одежды у нас 10. Нейросеть будет работать как фильтр, принимая\n    # изображения размером 28 на 28 и делая преположения, к какой из 10 \n    # категорий относится то или иное изображение. \n    tf.keras.layers.Dense(10, activation='softmax')\n  ])</code></pre><p>Создадим модель и скомпилируем ее, используя метод оптимизации Adam.<br>Введем метрику точности, чтобы визуализировать ход обучения, зададим директорию для хранения логов – мини-отчетов обучения. С помощью метода now() мы зашьем в название каждого лога временной штамп.</p><pre><code class="language-python">model = create_model()\n\n# Скомпилируем модель, используя метод оптимизации Adam\nmodel.compile(optimizer='adam',\n              # и уменьшая потери с помощью встроенного алгоритма \n              # кросс-энтропии, на котором мы не будем останавливаться \n              # подробно.\n              loss='sparse_categorical_crossentropy',\n              # Введем метрику точности, чтобы визуализировать ход обучения\n              metrics=['accuracy'])\n\n# Зададим директорию для хранения логов – мини-отчетов обучения.\n# Используем метод now() библиотеки datetime, чтобы именовать каждый файл\n# лога временным штампом – год, месяц, день, час, минута, секунда.\nlog_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")\n\n# Процесс генерации логов будет идти параллельно обучению, потому мы используем\n# коллбэки.\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n# Начнем тренировку в с использованием учебных данных в 5 эпох с последующей \n# валидацией и коллбэком создания логов.\nmodel.fit(x=x_train, \n          y=y_train, \n          epochs=5, \n          validation_data=(x_test, y_test), \n          callbacks=[tensorboard_callback])\n</code></pre><p>Раздел «Скаляры» показывает изменение параметров потери и точности:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/--------------2021-06-19---20.04.30.png" class="kg-image" alt loading="lazy" width="2000" height="908" srcset="__GHOST_URL__/content/images/size/w600/2021/06/--------------2021-06-19---20.04.30.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/--------------2021-06-19---20.04.30.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/06/--------------2021-06-19---20.04.30.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/06/--------------2021-06-19---20.04.30.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Раздел «Графы» отображают процесс обработки данных наподобие UML-диаграммы, только синтаксис видоизменен:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/--------------2021-06-19---20.04.51.png" class="kg-image" alt loading="lazy" width="2000" height="907" srcset="__GHOST_URL__/content/images/size/w600/2021/06/--------------2021-06-19---20.04.51.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/--------------2021-06-19---20.04.51.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/06/--------------2021-06-19---20.04.51.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/06/--------------2021-06-19---20.04.51.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>В разделе «Распределения» можно увидеть, как тензор<br>распределяется по времени и смещение графа.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/--------------2021-06-19---20.08.09.png" class="kg-image" alt loading="lazy" width="2000" height="907" srcset="__GHOST_URL__/content/images/size/w600/2021/06/--------------2021-06-19---20.08.09.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/--------------2021-06-19---20.08.09.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/06/--------------2021-06-19---20.08.09.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/06/--------------2021-06-19---20.08.09.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Теперь вы знаете, как создать полносвязную нейронную сеть, где каждый узел слоя связан с окружающими его узлами, и вступили на путь тонкой настройки процесса <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1oPcAxLvv9mes1MX5yabollN9lYAzDvw7?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@luciandachman?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@luciandachman</a></p>		tensorboard	2021-06-20		
109	DialogFlow		<p>DialogFlow – популярная условно-бесплатная платформа для создания чат-ботов, приобретенная Google в 2016 году. Обладает обширным функционалом во фронтенд-части и позволяет создавать виртуальных ассистентов без навыков программирования; впрочем, перенос на бэкенд тоже возможен. </p><h3 id="%D0%BE%D1%81%D0%BE%D0%B1%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8">Особенности</h3><ul><li><strong>WYSIWIG-подобный интерфейс</strong>, позволяющий загружать и размечать тренировочные фразы:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/dialogflow.cloud.google.com_.png" class="kg-image" alt loading="lazy" width="1096" height="274" srcset="__GHOST_URL__/content/images/size/w600/2021/06/dialogflow.cloud.google.com_.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/dialogflow.cloud.google.com_.png 1000w, __GHOST_URL__/content/images/2021/06/dialogflow.cloud.google.com_.png 1096w" sizes="(min-width: 720px) 720px"><figcaption>Предустановленный объект @sys.location узнает город и на русском языке</figcaption></figure><p>Бот распознает намерение (Intent) пользователя. Например, в запросе, "нужен курьер срочно Санкт-Петербург" речь идет, очевидно, про доставку. Каждый член фразы-запроса дробится на так называемые Объекты (Entities), сродни подлежащему, сказуемому, определению, обстоятельству и дополнению в синтаксисе языков: </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/dialogflow.cloud.google.com_--1-.png" class="kg-image" alt loading="lazy" width="1004" height="1642" srcset="__GHOST_URL__/content/images/size/w600/2021/06/dialogflow.cloud.google.com_--1-.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/dialogflow.cloud.google.com_--1-.png 1000w, __GHOST_URL__/content/images/2021/06/dialogflow.cloud.google.com_--1-.png 1004w" sizes="(min-width: 720px) 720px"><figcaption>Объект "Курьер" и его семантические, стилевые синонимы</figcaption></figure><ul><li><strong>Предустановленные пакеты реплик</strong> позволяют повысить качество виртуального помощника с помощью диалогов о погоде, переводах слов и фразах Small Talk для поддержания беседы:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/dialogflow.cloud.google.com_--2-.png" class="kg-image" alt loading="lazy" width="1516" height="476" srcset="__GHOST_URL__/content/images/size/w600/2021/06/dialogflow.cloud.google.com_--2-.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/dialogflow.cloud.google.com_--2-.png 1000w, __GHOST_URL__/content/images/2021/06/dialogflow.cloud.google.com_--2-.png 1516w" sizes="(min-width: 720px) 720px"></figure><ul><li>Платформа поддерживает <strong>интеграцию с многочисленными мессенджерами, системами телефонии и даже социальными сетями</strong>, причем перечень партнеров постоянно меняется:</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/dialogflow-intergrations.png" class="kg-image" alt loading="lazy" width="1770" height="934" srcset="__GHOST_URL__/content/images/size/w600/2021/06/dialogflow-intergrations.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/dialogflow-intergrations.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/06/dialogflow-intergrations.png 1600w, __GHOST_URL__/content/images/2021/06/dialogflow-intergrations.png 1770w" sizes="(min-width: 720px) 720px"><figcaption>Поддерживаемые мессенджеры, июнь 2021 г.</figcaption></figure><ul><li>Помимо прочих, к полезным фичам можно причислить также <strong>возможность переноса логики на бэкенд</strong>, <strong>поддержку 7 языков</strong> (русский, английский, французский, немецкий, датский, испанский, финский), <strong>автоматическое исправление ошибок и опечаток</strong>, <strong>озвучивание реплик</strong>.</li></ul><p>Фото: <a href="https://unsplash.com/@trevmepix?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@trevmepix</a></p>		dialogflow	2021-06-23		
110	Обработка естественного языка (NLP)		<p>Обработка естественного языка (Natural Language Processsing, НЛП, ОЕЯ) – это область <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a>, которая дает машинам возможность читать, понимать и извлекать значение из человеческих языков.</p><p>Все, что мы выражаем устно или письменно, несет в себе огромное количество информации. Тема, которую мы выбираем, наш тон, выбор слов – все это добавляет какой-то тип информации, которую можно интерпретировать и извлечь из нее ценность. Теоретически мы можем понять и даже предсказать поведение человека, используя эту информацию.<br> Но есть проблема: один человек может генерировать сотни или тысячи слов за раз, причем каждое предложение имеет соответствующую сложность. Если вы хотите масштабировать и анализировать несколько сотен, тысяч или миллионов людей или реплик в заданной географии, тогда ситуация неуправляема.</p><p>Данные, полученные из разговоров, заявлений или даже твитов, являются примерами неструктурированных данных. Неструктурированные данные плохо вписываются в традиционную структуру строк и столбцов реляционных баз данных и представляют собой подавляющее большинство данных, доступных в реальном мире. Ими сложно манипулировать. Тем не менее, благодаря достижениям в таких дисциплинах, как <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a>, в этой сфере происходит революция. В настоящее время речь идет уже не о попытках интерпретировать текст или речь на основе  ключевых слов (старомодный механический способ), а о понимании значения этих слов (когнитивный способ). Таким образом можно обнаружить речевые образы: иронию, или даже провести анализ настроений.</p><p>ОЕЯ – это дисциплина, которая фокусируется на взаимодействии <a href="__GHOST_URL__/nauka-o-dannykh/">Науки о данных (DS)</a> и человеческого языка и распространяется на многие отрасли. Сегодня сфера переживает бум благодаря огромным улучшениям в доступе к данным и увеличению вычислительной мощности, которые позволяют практикам достигать значимых результатов в таких областях, как здравоохранение, СМИ, финансы, человеческие ресурсы и проч.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5">Применение</h3><p>Проще говоря, NLP представляет собой автоматическую обработку естественного человеческого языка, такого как речь или текст, и, хотя сама концепция увлекательна, реальная ценность этой технологии заключается в ее применении.</p><p>NLP может помочь вам с множеством задач, и области применения расширяются с каждым днем. Приведем несколько примеров. </p><ul><li>Наука позволяет распознавать и прогнозировать заболевания на основе электронных медицинских карт и собственной речи пациента. Эта способность исследуется в условиях здоровья, которые варьируются от сердечно-сосудистых заболеваний до депрессии и даже шизофрении. Например, Amazon Comprehend Medical – это сервис, который использует NLP для изучения болезненных состояний, лекарств и результатов лечения из записей пациентов, отчетов о клинических испытаниях и других электронных медицинских записей.</li><li>Организации могут определять, что клиенты говорят об услуге или продукте, идентифицируя и извлекая информацию из таких источников, как социальные сети. Этот анализ настроений может предоставить много информации о выборе клиентов и их факторах принятия решений.<br> Изобретатель IBM разработал когнитивного помощника, который работает как персонализированная поисковая машина, узнавая все о вас, а затем напоминая вам имя, песню или что-нибудь, что вы не можете вспомнить в тот момент, когда вам это нужно.</li><li>Такие компании, как Yahoo и Google, фильтруют и классифицируют ваши электронные письма с помощью NLP, анализируя текст в электронных письмах, проходящих через их серверы, и останавливая спам еще до того, как они попадут в почтовый ящик.</li><li>Чтобы помочь идентифицировать фальшивые новости, группа NLP в Массачусетском технологическом институте разработала новую систему для определения объективности или политической предвзятости источника новостей, а также подлинности публикаций.</li><li>Amazon Alexa и Apple Siri являются примерами интеллектуальных голосовых интерфейсов, которые используют NLP для ответа на реплики человека и выполняют широкий спектр действий: найти конкретный магазин, сообщить прогноз погоды, предложить лучший маршрут до офиса или включить свет дома.</li><li>Понимание того, что происходит и о чем говорят люди, может быть очень ценным для трейдеров. NLP используется для отслеживания новостей, отчетов, комментариев о возможных слияниях компаний, все может быть затем включено в торговый алгоритм для получения огромной прибыли. Помните: покупайте слухи, продавайте новости.</li><li>NLP также используется как на этапах поиска, так и на этапах отбора талантов, для определения навыков потенциальных сотрудников, а также для выявления потенциальных клиентов до того, как они станут активными на рынке труда.</li><li>На основе технологии IBM Watson NLP LegalMation разработала платформу для автоматизации рутинных судебных задач и помогает юридическим группам сэкономить время, сократить расходы и сместить стратегический фокус.</li></ul><p>NLP особенно быстро развивается в сфере здравоохранения. Эта технология улучшает оказание медицинской помощи, диагностику заболеваний и снижает расходы, в то время как медицинские организации все чаще используют электронные медицинские карты. Тот факт, что клиническая документация может быть улучшена означает, что пациентов можно лучше понять и получить пользу за счет лучшего здравоохранения. Цель должна заключаться в оптимизации их опыта, и несколько организаций уже работают над этим.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/image-4.png" class="kg-image" alt loading="lazy" width="640" height="426" srcset="__GHOST_URL__/content/images/size/w600/2021/06/image-4.png 600w, __GHOST_URL__/content/images/2021/06/image-4.png 640w"><figcaption>Количество публикаций, содержащих предложение «обработка естественного языка» в PubMed за период 1978–2018 гг. По состоянию на 2018 год PubMed содержит более 29 миллионов ссылок на биомедицинскую литературу.</figcaption></figure><p>Такие компании, как Winterlight Labs, значительно улучшают лечение болезни Альцгеймера, отслеживая когнитивные нарушения с помощью речи, а также могут поддерживать клинические испытания и исследования широкого спектра заболеваний центральной нервной системы. Следуя аналогичному подходу, Стэнфордский университет разработал Woebot, терапевта-чат-бота с целью помочь людям с тревогой и другими расстройствами.</p><p>Но по этому поводу ведутся серьезные разногласия. Пару лет назад Microsoft продемонстрировала, что, анализируя большие выборки запросов поисковых систем, они могут идентифицировать интернет-пользователей, страдающих от рака поджелудочной железы, еще до того, как им поставят диагноз. Как пользователи отреагируют на такой диагноз? А что было бы, если бы у вас был ложноположительный результат? (это означает, что у вас может быть диагностирована болезнь, даже если у вас ее нет). Это напоминает случай с Google Flu Trends, который в 2009 году был объявлен способным прогнозировать грипп, но позже исчез из-за его низкой точности и неспособности соответствовать прогнозируемым показателям.</p><p>NLP может стать ключом к эффективной клинической поддержке в будущем, но в краткосрочной перспективе еще предстоит решить множество проблем.</p><p>Основные недостатки, с которыми мы сталкиваемся в наши дни при использовании NLP, связаны с тем, что человеческий язык очень сложен. Процесс понимания языка и управления им чрезвычайно комплексный, и по этой причине принято использовать разные методы для решения различных задач, прежде чем связывать все вместе. Языки программирования, такие как Python или R, широко используются для реализации этих методов, но прежде чем углубляться в строки кода (это будет темой другой статьи), важно понять концепции, лежащие в основе них.</p><p>Давайте разъясним некоторые из наиболее часто используемых алгоритмов:</p><h3 id="%D0%BC%D0%B5%D1%88%D0%BE%D0%BA-%D1%81%D0%BB%D0%BE%D0%B2">Мешок слов</h3><p><a href="__GHOST_URL__/mieshok-slov/">Мешок слов (Bag of Words)</a> – это широко используемая модель, которая позволяет вам подсчитывать все слова в фрагменте текста. По сути, он создает матрицу вхождений для предложения или документа, игнорируя грамматику и порядок слов. Эти частоты ("встречаемости") слов затем используются в качестве признаков для обучения классификатора. В качестве краткого примера я взял первое предложение песни «Через Вселенную» из The Beatles:</p><p><em>Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe.</em></p><p>А теперь посчитаем слова:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/image-5.png" class="kg-image" alt loading="lazy" width="1060" height="220" srcset="__GHOST_URL__/content/images/size/w600/2021/06/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/image-5.png 1000w, __GHOST_URL__/content/images/2021/06/image-5.png 1060w" sizes="(min-width: 720px) 720px"></figure><p>Этот подход может обладает недостатками, такими как отсутствие семантического значения и контекста; служебные части речи (например, артикли «the», «a») добавляют шум в анализ. Более того, некоторые слова не имеют соответствующего веса («вселенная» весит меньше, чем слово «они»).</p><p>Чтобы решить эту проблему, изменяют частоту слов в зависимости от того, как часто они появляются во всех текстах (а не только в том, который мы анализируем), чтобы для часто встречающихся слов, таких как «the», применялся специальный "штраф". Такой подход к оценке называется "Мера оценки важности слова в контексте документа" (TF-IDF), и он улучшает набор слов с помощью "частотных" весов. Через TF-IDF часто встречающиеся в тексте термины «вознаграждаются» (например, слово «они» в нашем примере), но они также «наказываются», если эти термины часто встречаются в других текстах, которые мы также включаем в алгоритм. Этот метод выделяет и «награждает» уникальные или редкие термины с учетом всех текстов. Тем не менее, у этого подхода нет ни контекста, ни семантики.</p><h3 id="%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-tokenization">Токенизация (Tokenization)</h3><p>Это процесс разбиения текста на предложения и слова. По сути, это задача разрезать текст на части, называемые токенами, и в то же время отбросить определенные символы, например знаки препинания. Следуя нашему примеру, результатом токенизации будет:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/image-6.png" class="kg-image" alt loading="lazy" width="665" height="97" srcset="__GHOST_URL__/content/images/size/w600/2021/06/image-6.png 600w, __GHOST_URL__/content/images/2021/06/image-6.png 665w"></figure><p>Довольно просто, правда? Что ж, хотя в этом случае это может показаться довольно простым в таких языках, как английский, где слова разделяются пробелом (так называемые сегментированные языки), не все языки ведут себя одинаково, и, если подумать, одних пробелов недостаточно даже для английского, чтобы выполнить правильную токенизацию. Разделение на пробелы может разбить то, что следует рассматривать как один токен, как в случае некоторых имен собственных (например, Сан-Франциско или Нью-Йорк) или заимствованных иностранных фраз (например, laissez faire).</p><p>Токенизация также может удалить знаки препинания, облегчая путь к правильной сегментации слов, но также вызывая возможные осложнения. В случае точек, следующих за аббревиатурой (например, Dr.), Точка, следующая за этой аббревиатурой, должна рассматриваться как часть того же символа и не удаляться.</p><p>Процесс токенизации может быть особенно проблематичен при работе с биомедицинскими текстовыми доменами, которые содержат много дефисов, скобок и других знаков препинания.</p><h3 id="%D1%83%D0%B4%D0%B0%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D1%82%D0%BE%D0%BF-%D1%81%D0%BB%D0%BE%D0%B2-stop-words-removal">Удаление стоп-слов (Stop Words Removal)</h3><p>Включает в себя избавление от артиклей, местоимений и предлогов, таких как «the» или «to» на английском языке. В этом процессе некоторые очень распространенные слова, которые, по-видимому, не представляют никакой ценности для цели NLP, фильтруются и исключаются из обрабатываемого текста, таким образом удаляя распространенные и часто встречающиеся термины, которые не информативны для соответствующего текста.<br> Стоп-слова можно безопасно игнорировать, выполняя поиск в заранее определенном списке ключевых слов, освобождая место в базе данных и сокращая время обработки.</p><p>Универсального списка стоп-слов не существует. Их можно выбрать заранее или создать с нуля. Возможный подход – начать с принятия заранее определенных стоп-слов и добавить слова в список позже. В последнее время общая тенденция заключалась в том, чтобы перейти от использования больших стандартных списков стоп-слов к использованию вообще без списков.</p><p>Дело в том, что удаление стоп-слов может стереть релевантную информацию и изменить контекст в данном предложении. Например, если мы выполняем анализ настроений и удалим стоп-слово, например «не», мы можем сбить наш алгоритм с правильного пути. В этих условиях мы можем выбрать минимальный список стоп-слов и добавить дополнительные термины в зависимости от конкретной цели.</p><h3 id="%D1%81%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3-stemming">Стемминг (Stemming)</h3><p>Относится к процессу разрезания конца или начала слова с целью удаления аффиксов (лексических дополнений к корню слова). </p><p><em>Аффиксы, которые добавляются в начале слова, называются префиксами (например, «астро» в слове «астробиология»), а аффиксы, прикрепленные в конце слова, называются суффиксами (например, «н» в слове «полезный»).</em></p><p>Проблема в том, что аффиксы могут создавать или расширять новые формы одного и того же слова (так называемые флективные аффиксы) или даже сами создавать новые слова (так называемые словообразовательные аффиксы). В английском языке префиксы всегда являются производными (аффикс создает новое слово, как в примере с префиксом «эко» в слове «экосистема»), но суффиксы могут быть производными (аффикс создает новое слово, как в примере с суффикс «ист» в слове «гитарист») или словоизменительный (аффикс создает новую форму слова, как в примере с суффиксом «ее» в слове «быстрее»).</p><p>Итак, как мы можем определить разницу и нарезать нужный кусок?</p><p>Возможный подход состоит в том, чтобы рассмотреть список общих аффиксов и правил (языки Python и R имеют разные библиотеки, содержащие аффиксы и методы) и выполнить стемминг на их основе, но, конечно, этот подход имеет ограничения. Поскольку стеммеры используют алгоритмические подходы, результатом процесса стемминга может быть не настоящее слово или даже не изменение значения слова (и предложения). Чтобы компенсировать этот эффект, вы можете редактировать эти предопределенные методы, добавляя или удаляя аффиксы и правила, но вы должны учитывать, что вы можете улучшать производительность в одной области, производя ухудшение в другой. Всегда смотрите на картину целиком и проверяйте работоспособность своей модели.</p><p>Итак, если у стемминга есть серьезные ограничения, почему мы его используем? Прежде всего, его можно использовать для исправления орфографических ошибок токенов. Стеммеры просты в использовании и работают очень быстро (они выполняют простые операции со строкой), и если скорость и производительность важны в модели NLP, то стемминг – лучший вариант. Помните, мы используем его с целью повышения производительности, а не как упражнение по грамматике.</p><h3 id="%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F">Лемматизация</h3><p>Цель <a href="__GHOST_URL__/liemmatizatsiia/">Лемматизации (Lemmatization)</a> – привести слово к его основной форме и сгруппировать различные формы одного и того же слова. Например, глаголы в прошедшем времени заменяются на настоящее (например, «пошел» заменен на «пойти»), а синонимы унифицированы (например, «лучший» заменен на «хороший»), таким образом стандартизируются слова со схожим значением их корня. Хотя это кажется тесно связанным с процессом выделения корней, лемматизация использует другой подход для достижения корневых форм слов.</p><p>Лемматизация преобразует слова в их словарную форму (известную как лемма), для чего требуются подробные словари, в которых алгоритм может искать и связывать слова с соответствующими леммами.<br>Например, слова «бегая», «бежит» и «бегал» являются формами слова «бегать», поэтому «бегать» – это лемма всех предыдущих слов.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/image-7.png" class="kg-image" alt loading="lazy" width="563" height="85"></figure><p>Лемматизация также принимает во внимание контекст слова для решения других проблем, таких как устранение неоднозначности, что означает, что она может различать идентичные слова, которые имеют разные значения в зависимости от конкретного контекста. Подумайте о таких словах, как «bat» (что может соответствовать животному или бите, используемой в бейсболе) или «bank» (что соответствует финансовому учреждению или земле рядом с водоемом). Определяя параметр "часть речи" того или иного слова (будь то существительное, глагол и т.д.), можно определить его роль в предложении и устранить его двусмысленность.</p><p>Как вы уже могли представить, лемматизация – это гораздо более ресурсоемкая задача, чем стемминг. В то же время, поскольку для этого требуется больше знаний о структуре языка, чем при подходе к стеммингу, он требует большей вычислительной мощности, чем настройка или адаптация алгоритма стемминга.</p><h3 id="%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5">Тематическое моделирование</h3><p>Метод обнаружения скрытых структур в наборах текстов или документов. По сути, он группирует тексты, чтобы обнаруживать скрытые темы на основе их содержания, обрабатывая отдельные слова и присваивая им значения на основе их распределения. Этот метод основан на предположении, что каждый документ состоит из смеси тем и что каждая тема состоит из набора слов, а это означает, что если мы сможем обнаружить эти скрытые темы, мы сможем раскрыть смысл наших текстов.</p><p>Из вселенной методов тематического моделирования, вероятно, наиболее часто используется скрытое распределение Дирихле (LDA). Этот относительно новый алгоритм (изобретенный менее 20 лет назад) работает как метод <a href="__GHOST_URL__/niekontroliruiemoie-obuchieniie/">Обучения без учителя (Unsupervised Learning)</a>, который раскрывает различные темы, лежащие в основе набора документов. В таких методах обучения без учителя, как этот, нет выходной переменной, которая бы направляла процесс обучения, и данные исследуются с помощью алгоритмов для поиска закономерностей. Чтобы быть более конкретным, LDA находит группы связанных слов по:</p><ul><li>Назначению каждого слова случайной теме, где пользователь определяет количество тем, которые он хочет раскрыть. Вы не определяете сами темы (вы определяете только количество тем), и алгоритм будет сопоставлять все документы с темами таким образом, чтобы слова в каждом документе в основном захватывались этими воображаемыми темами.</li><li>Алгоритм итеративно перебирает каждое слово и переназначает слово теме, принимая во внимание вероятность того, что слово принадлежит теме, и вероятность того, что документ будет создан темой. Эти вероятности вычисляются многократно, до сходимости алгоритма.<br>В отличие от других алгоритмов кластеризации, таких как Метод K-средних (K-Means), которые выполняют жесткую кластеризацию (где темы не пересекаются), LDA назначает каждый документ смешанным темам, что означает, что каждый документ может быть описан одной или несколькими темами (например, Документ описывается 70% темы A, 20% темы B и 10% темы C) и отражают более реалистичные результаты.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/image-8.png" class="kg-image" alt loading="lazy" width="579" height="162"></figure><p>Тематическое моделирование чрезвычайно полезно для классификации текстов, построения рекомендательных систем (например, чтобы рекомендовать вам книги на основе прочитанных) или даже для выявления тенденций в онлайн-публикациях.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D0%B2%D1%8B%D0%B3%D0%BB%D1%8F%D0%B4%D0%B8%D1%82-%D0%B1%D1%83%D0%B4%D1%83%D1%89%D0%B5%D0%B5">Как выглядит будущее?</h3><p>В настоящее время NLP пытается обнаружить нюансы в значении языка, будь то из-за отсутствия контекста, орфографических ошибок или диалектных различий.</p><p>В марте 2016 года Microsoft запустила Тэя, чат-бота с искусственным интеллектом, выпущенного в Twitter в качестве эксперимента. Идея заключалась в том, что чем больше пользователей будет общаться с Тэем, тем умнее он станет. Однако через 16 часов его пришлось удалить из-за его расистских и оскорбительных комментариев:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/06/image-9.png" class="kg-image" alt loading="lazy" width="600" height="302" srcset="__GHOST_URL__/content/images/2021/06/image-9.png 600w"><figcaption>Британский комик Рики Джейрвейс усвоил тоталитаризм вслед за Гитлером, изобретателем атеизма</figcaption></figure><p>Microsoft извлекла урок и несколько месяцев спустя выпустила Zo, англоязычный чат-бота второго поколения, который не будет совершать те же ошибки, что его предшественник. Zo использует комбинацию инновационных подходов для распознавания намерения и поддержания разговора, а другие компании изучают возможности использования ботов, которые могут запоминать детали, характерные для отдельного разговора. Хотя будущее NLP выглядит чрезвычайно сложным и полным угроз, эта дисциплина развивается очень быстрыми темпами (вероятно, как никогда раньше), и мы, вероятно, достигнем уровня прогресса в ближайшие годы, который сделает сложные приложения возможными.</p><p>Фото: <a href="https://unsplash.com/@milada_vigerova?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@milada_vigerova</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1">Diego Lopez Yse</a></p>		obrabotka-iestiestviennogho-iazyka	2021-06-26		
111	Ошибка (Error)		<p>Ошибка – мера оценки эффективности <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.</p><p>Во времена огромного прогресса в области компьютерных наук и <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a> легко забыть, что по самой своей природе модели не идеальны. Одна из самых больших головных болей – это решить, насколько она точна.</p><p>Первый шаг – понять цену ошибок. Вы бы предпочли ошибиться, предположив истинность чего-либо, когда это не так, или не угадать вообще? Иногда может быть дешевле потерять клиента, чем потратить часы на его удержание. Такой вид анализа затрат и выгод будет информировать о методологии, используемой для определения вероятности ошибки.</p><h3 id="%D0%B2%D0%B0%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7%D1%8B">Важность гипотезы</h3><p>Самая важная часть моделирования начинается с содержательного вопроса. Инстинктивно мы формулируем вопрос как декларацию чего-то правдивого, например, «расстояние между креслами в самолетах сокращается». Однако лучшая статистическая практика – создать <a href="__GHOST_URL__/nulievaia-ghipotieza/">Нулевую гипотезу (Null Hypothesis)</a>, например, «расстояние между сиденьями осталось прежним», а затем попытаться доказать, что она неверна.</p><p>Это обеспечивает статистическую версию невиновности до тех пор, пока виновность не будет доказана, и может помочь устранить влияние случайности. Если нулевая гипотеза опровергается до статистически значимой степени, то рассматривается альтернативная гипотеза (Alternative Hypothesis), и тенденция авиакомпании к увеличению прибыльности теперь рассматривается как возможное, но не однозначное объяснение.</p><p>Важно отметить, что нулевая гипотеза НЕ является полной противоположностью альтернативной гипотезы, а скорее является подтверждением того, что причина наблюдений не просто случайность.</p><h3 id="%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%B8-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-p">Вероятность и значения P</h3><p>Вероятность предоставляет общий способ интерпретации статистической силы модели. Вызываемое <a href="__GHOST_URL__/p-znachieniie/">Значением P (P Value)</a>, оно может находиться в диапазоне от 0 до 1 и представляет, насколько вероятно получение результата, если нулевая гипотеза (H<sub>0</sub>) верна. Это означает, что чем ниже значение, тем лучше показатель того, что альтернативная гипотеза (H<sub>1</sub>) действительно верна.</p><p>Пороговое значение для P-Value называется уровнем значимости. Если вероятность равна или меньше 0,05 (хотя в зависимости от варианта использования, который может измениться), результат часто считается значительным. Проще говоря, мы, вероятно, получим подтверждение нулевой гипотезы 5 раз из 100 (или, наоборот, подтвердим альтернативную гипотезу 95 раз из 100). Чем выше значение p, тем ближе к случайному совпадению и тем более вероятна нулевая </p><h3 id="%D0%B8%D0%B7%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BF%D0%BE%D0%B3%D1%80%D0%B5%D1%88%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D0%B2-%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0%D1%85-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8">Измерение погрешности в задачах классификации</h3><p>Классификационные проблемы обычно представляют собой двоичную идентификацию. Из любой классификационной модели можно выделить четыре типа результатов:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/06/image-10.png" class="kg-image" alt loading="lazy" width="564" height="185"></figure><ul></ul><p>Давайте рассмотрим значения ячеек (истинно позитивные, ошибочно позитивные, ошибочно негативные, истинно негативные) с помощью "беременной" аналогии.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/confusion-matrix-error-types.png" class="kg-image" alt loading="lazy" width="512" height="512"></figure><p><br><strong>Истинно позитивное предсказание (True Positive, сокр. TP)</strong><br>Вы предсказали положительный результат, и женщина действительно беременна.</p><p><strong>Истинно отрицательное</strong> <strong>предсказание (True Negative, TN)</strong><br>Вы предсказали отрицательный результат, и мужчина действительно не беременен.</p><p><strong>Ошибочно положительное</strong> <strong>предсказание (ошибка типа I, False Positive, FN)</strong><br>Вы предсказали положительный результат (мужчина беременен), но на самом деле это не так.</p><p><strong>Ошибочно отрицательное</strong> <strong>предсказание (ошибка типа II, False Negative, FN)</strong><br>Вы предсказали, что женщина не беременна, но на самом деле она беременна.</p><p>Погрешность рассчитывается по различным соотношениям и формулам на основе этих четырех состояний. Легко видеть, что в зависимости от стоимости типа I или типа II способ измерения ошибки может быть скорректирован.</p><h3 id="%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B0-%D0%B8%D0%B7%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D1%8F">Ошибка измерения</h3><p>Чтобы понять, как работает модель, существует множество способов измерений. <a href="__GHOST_URL__/matritsa-oshibok/">Матрица ошибок (Confusion Matrix)</a> используется для представления нескольких типов измерений ошибок, чтобы специалист по данным может определить, хорошо работает модель или нет. Мы рассмотрим следующие типы ошибок:</p><ul><li>Доля истинно негативных классификаций (TNR)</li><li>Точность (PPV)</li><li>Доля истинно положительных классификаций (TPR)</li><li>Критерий F (F<sub>1</sub>, F<sub>0.5</sub>, F<sub>2</sub>)</li><li>Коэффициент корреляции Мэтью (MCC)</li><li><a href="__GHOST_URL__/roc-krivaia/">ROC-кривая (ROC AUC)</a></li><li><a href="__GHOST_URL__/dolia-lozhnykh-polozhitielnykh-klassifikatsii/">Доля ложных положительных классификаций (FPR)</a></li><li>Коэффициент детерминации (R Squared)</li><li><a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратичная ошибка (MSE)</a></li><li><a href="__GHOST_URL__/sriedniaia-absoliutnaia-oshibka/">Средняя абсолютная ошибка (MAE)</a></li></ul><p>Фото: <a href="https://unsplash.com/@redcharlie?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@redcharlie</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/machine-learning-an-error-by-any-other-name-a7760a702c4d">Kendall Fortney</a></p>		oshibka	2021-06-23		
112	Стохастический градиентный спуск\t(SGD)		<p>Стохастический градиентный спуск (Stochastic Gradient Descent) – это простой, но очень эффективный подход к подгонке линейных классификаторов и регрессоров под выпуклые <a href="__GHOST_URL__/funktsiia-potieri/" rel="noopener noreferrer">Функции потерь (Loss Function)</a>, такие как <a href="__GHOST_URL__/mietod-opornykh-viektorov/" rel="noopener noreferrer">Метод опорных векторов (SVM)</a> и <a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/" rel="noopener noreferrer">Логистическая регрессия (Logistic Regression)</a>. Несмотря на то, что SGD существует в сообществе <a href="__GHOST_URL__/mashinnoie-obuchieniie/" rel="noopener noreferrer">Машинного обучения (ML)</a> уже давно, совсем недавно он привлек значительное внимание в контексте крупномасштабного обучения.</p><p>SGD успешно применяется для решения крупномасштабных и разреженных задач машинного обучения, часто встречающихся при <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a> текста и <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/" rel="noopener noreferrer">Обработке естественного языка (NLP)</a>. Учитывая, что данные немногочисленны, классификаторы в этом модуле легко масштабируются для решения задач с более чем 10<sup>5</sup> обучающими примерами и более чем 10<sup>5</sup> <a href="__GHOST_URL__/priznak/" rel="noopener noreferrer">Признаками (Feature)</a>.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%B8%D0%BC%D1%83%D1%89%D0%B5%D1%81%D1%82%D0%B2%D0%B0-%D0%B8-%D0%BD%D0%B5%D0%B4%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BA%D0%B8">Преимущества и недостатки</h3><p>К достоинствам СГП можно причислить:</p><ul><li>Эффективность</li><li>Простоту реализации (множество возможностей для настройки кода)</li></ul><p>К недостаткам:</p><ul><li>Требовательность (SGD необходим ряд гиперпараметров, таких как параметр регуляризации и количество итераций)</li><li>Чувствительность к масштабированию признаков.</li></ul><h3 id="sgd-%D0%B8-scikit-learn">SGD и Scikit-learn</h3><p>Давайте посмотрим, как SGD реализован в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import sklearn\nfrom sklearn.linear_model import SGDClassifier</code></pre><p>Как и другие классификаторы, SGD должен быть "снабжен" двумя массивами: массив формы X (n_samples, n_features), содержащий обучающие образцы, и массив формы y (n_samples,), содержащий целевые значения (метки классов) для обучающих образцов:</p><pre><code class="language-python">X = [[0., 0.], [1., 1.]]\ny = [0, 1]\nclf = SGDClassifier(loss = "hinge", penalty = "l2", max_iter = 5)\nclf.fit(X, y)</code></pre><p>Система отражает полный спектр настроек модели, к примеру, перемешивание обучающих сэмплов (<code>shuffle = True</code>) или подробность отчета об обучении (<code>verbose = 0</code>):</p><pre><code class="language-python">/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  ConvergenceWarning)\nSGDClassifier(alpha=0.0001, average=False, class_weight=None,\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n              random_state=None, shuffle=True, tol=0.001,\n              validation_fraction=0.1, verbose=0, warm_start=False)</code></pre><p>После установки <a href="__GHOST_URL__/modiel/">Модель (Model)</a> может быть использована для прогнозирования новых значений:</p><pre><code class="language-python">clf.predict([[2., 2.]])</code></pre><p>Легко сопоставить, что паре "2, 2" будет соответствовать ряд "1":</p><pre><code class="language-python">array([1])</code></pre><p>SGD подгоняет линейную модель к обучающим данным. Атрибут <code>coef_</code> содержит параметры модели:</p><pre><code class="language-python">clf.coef_\narray([[9.91080278, 9.91080278]])</code></pre><p>Атрибут <code>intercept_</code> содержит перехват, также известный как <a href="__GHOST_URL__/smieshchieniie/">Смещение (Bias)</a>:</p><pre><code class="language-python">clf.intercept_\narray([-9.97004991])</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1mEeF8KFmQlw4suF00F61qLnENQrDmvCo?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@kirk7501">@kirk7501</a></p><p>Автор оригинальной статьи: <a href="https://scikit-learn.org/stable/modules/sgd.html">scikit-learn.org</a></p><p>Понравилась статья? Поддержите нас, поделившись статьей в социальных сетях и <a href="https://zen.yandex.ru/id/5fd12882382a85570c79c48c">подписавшись на канал</a>. И попробуйте курс «Введение в Машинное обучение» на <a href="https://bit.ly/3n31nxS">Udemy</a>.</p>		stokhastichieskii-ghradiientnyi-spusk	2021-06-30		
113	Эпсилон (Epsilon)		<p>Эпсилон (ε, epsilon) –<strong> </strong>1.<strong> </strong>(В статистике) Степень эффекта (Effect Size), используемая для сравнения. 2. (В Машинном обучении) порог некоторых классификаторов (например, sklearn.SGDClassifier), при котором становится менее важным получение точного прогноза.</p><h3 id="%D0%B2-%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B5">В статистике</h3><p>Например, лекарство A лучше, чем лекарство B, при лечении депрессии. Но насколько? Традиционная Проверка гипотез (Hypothesis Testing) не даст вам такого ответа. Лекарство B может быть в десять раз лучше, а может в два. Эта изменчивость (вдвое или в десять раз больше) и есть то, что называется величиной эффекта.<strong> </strong></p><p>В статистике эпсилон в квадрате является мерой величины эффекта. Это один из наименее распространенных способов определения метрики: омега в квадрате и эта в квадрате используются чаще. Формула оценки эффекта выглядит следующим образом:</p><!--kg-card-begin: markdown--><p>$$ε = \\frac{SS_M - df_M * MS_E}{SS_T}$$<br>\n$$SS_M\\space{}{–}\\space{сумма}\\space{квадратов,}$$<br>\n$$df_M\\space{}{–}\\space{степень}\\space{свободы,}$$<br>\n$$MS_E\\space{}{–}\\space{среднее}\\space{суммы}\\space{квадратов,}$$<br>\n$$SS_T\\space{}{–}\\space{общая}\\space{сумма}\\space{квадратов,}$$</p>\n<!--kg-card-end: markdown--><h3 id="%D0%B2-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%BC-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B8">В Машинном обучении</h3><p>При нечувствительности к эпсилону любые различия между текущим предсказанием и правильной меткой игнорируются, если они меньше этого порога. К примеру, <a href="__GHOST_URL__/stokhastichieskii-ghradiientnyi-spusk/">Стохастический градиентный спуск (SGD)</a>, реализованный на scikit-learn, устанавливает <code>epsilon</code> по умолчанию равным 0.1:</p><pre><code class="language-python">X = [[0., 0.], [1., 1.]]\ny = [0, 1]\nclf = SGDClassifier(loss = "hinge", penalty = "l2", max_iter = 5)\nclf.fit(X, y)</code></pre><p>Это позволяет неизвестным ранее парам значений (например, [0.95, 0.93]) не ухудшать своими ярлыками предсказательную способность модели: </p><pre><code class="language-python">/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  ConvergenceWarning)\nSGDClassifier(alpha=0.0001, average=False, class_weight=None,\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n              random_state=None, shuffle=True, tol=0.001,\n              validation_fraction=0.1, verbose=0, warm_start=False)</code></pre><p>Фото: <a href="https://unsplash.com/@betagamma">@betagamma</a></p><p>Автор оригинальной статьи: <a href="https://www.statisticshowto.com/epsilon-squared/">statisticshowto.com</a></p>		epsilon	2021-07-03		
114	Кросс-энтропия (Cross-Entropy)		<p>Кросс-энтропия (Перекрестная энтропия) – это <a href="__GHOST_URL__/funktsiia-potieri/">Функция потерь (Loss Function)</a>, которую можно использовать для количественной оценки разницы между двумя Распределениями вероятностей (Probability Distribution). </p><p>Лучше всего это можно объяснить на примере. Предположим, у нас есть две модели, A и B, и мы хотели выяснить, какая из них лучше:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image.png" class="kg-image" alt loading="lazy" width="875" height="418" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image.png 600w, __GHOST_URL__/content/images/2021/07/image.png 875w" sizes="(min-width: 720px) 720px"></figure><p>Примечание. Цифры рядом с точками данных представляют вероятность того, что <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a> принадлежит к соответствующему классу – цветовой зоне. Например, вероятность того, что красная точка в левой верхней части графика модели A принадлежит "красному" классу, равна 0,8.<br>Интуитивно мы знаем, что модель B лучше, поскольку красные точки находятся на красном распределении, а синие точки – в синем. Но как мы передадим модели эти знания?</p><p>Один из способов – взять вероятности каждой точки в модели A и перемножить их. Это даст полную вероятность модели, как мы знаем из общего правила умножения вероятностей. Мы можем сделать то же самое для модели B:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/1_M5aFTjlbwqL28rHQJMYKMQ--1---1-.png" class="kg-image" alt loading="lazy" width="875" height="439" srcset="__GHOST_URL__/content/images/size/w600/2021/07/1_M5aFTjlbwqL28rHQJMYKMQ--1---1-.png 600w, __GHOST_URL__/content/images/2021/07/1_M5aFTjlbwqL28rHQJMYKMQ--1---1-.png 875w" sizes="(min-width: 720px) 720px"></figure><p>Как видно из изображения выше, наилучшей моделью является B, поскольку вероятность выше. Так мы можем выяснить, какая модель лучше, используя вероятность.</p><p>С этой последовательностью, однако, есть некоторые проблемы. Как вы, возможно, догадались, чем больше наблюдений, тем меньше результирующая вероятность. Кроме того, если бы мы изменили одну точку данных, результирующая вероятность резко изменилась бы вслед.<br>Одним словом, использование прозведений – не лучшая идея. Как мы можем это исправить? Один из способов – использовать вместо этого суммы. Если мы вспомним логарифмы, есть способ связать произведение вероятностей с их суммой:</p><!--kg-card-begin: markdown--><p>$$ \\log_{a × b} = \\log_{a} + \\log_{b}, где$$</p>\n<!--kg-card-end: markdown--><p>Давайте применим это правило к нашим вероятностям:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-3.png" class="kg-image" alt loading="lazy" width="875" height="527" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-3.png 600w, __GHOST_URL__/content/images/2021/07/image-3.png 875w" sizes="(min-width: 720px) 720px"></figure><p>Это выглядит неплохо, но давайте избавимся от негативов, сделав записи в журналах отрицательными и посчитаем общее количество:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-3.png" class="kg-image" alt loading="lazy" width="875" height="121" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-3.png 600w, __GHOST_URL__/content/images/2021/07/image-3.png 875w" sizes="(min-width: 720px) 720px"></figure><p>Использование отрицательных логарифмов вероятностей – это так называемая кросс-энтропия, где большое число означает плохие модели, а маленькое число – хорошую.</p><p>Когда мы вычисляем логарифм для каждой точки данных, мы фактически получаем функцию ошибок для нее. Например, функция ошибок для точки 0,2 в модели A равна -ln (0,2), что равно 1,61. Обратите внимание, что неправильно классифицированные точки имеют большие значения, следовательно, имеют большие ошибки.</p><p>Итак, давайте еще немного разберемся с перекрестной энтропией. На самом деле она говорит о том, что если у нас есть события и вероятности, насколько вероятно, что события произойдут на основе вероятностей? Если это очень вероятно, у нас малая кросс-энтропия, а если маловероятно, у нас высокая кросс-энтропия. Мы увидим это подробнее на примере.</p><p>Например, если мы возьмем вероятность того, что за тремя дверями есть подарок, и у нас есть таблица, которая выглядит следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-4.png" class="kg-image" alt loading="lazy" width="1275" height="270" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2021/07/image-4.png 1000w, __GHOST_URL__/content/images/2021/07/image-4.png 1275w" sizes="(min-width: 720px) 720px"></figure><p>Здесь мы видим, что если кросс-энтропия велика, вероятность того, что событие произойдет, мала, и наоборот.</p><p>Предположим, мы возьмем первый случай, когда за дверью № 1 подарок с вероятностью 0,8; за дверью № 2 – 0,7, № 3 – 0,1:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-5.png" class="kg-image" alt loading="lazy" width="846" height="459" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-5.png 600w, __GHOST_URL__/content/images/2021/07/image-5.png 846w" sizes="(min-width: 720px) 720px"></figure><p>Обратите внимание, что мы описываем третью дверь как 1-p, что означает 1 минус вероятность подарка. Это даст нам вероятность того, что подарка нет. Также обратите внимание, что Y описывает, сколько подарков находится за дверью. Таким образом, кросс-энтропия может быть описана следующей формулой:</p><!--kg-card-begin: markdown--><p>$$ -Σp(x) logqx + (1 - p(x)) log (1 - q(x))$$<br>\n$$p(x)\\space{требуемая}\\space{вероятность,}$$<br>\n$$q(x)\\space{фактическая}\\space{вероятность,}$$</p>\n<!--kg-card-end: markdown--><p>Эта формула предназначена только для двоичной кросс-энтропии и описывает, насколько близко предсказанное распределение к истинному распределению.</p><p>Фото: <a href="https://unsplash.com/@hardsurface">@hardsurface</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616">Anjali Bhardwaj</a></p>		kross-entropiia	2021-07-04		
115	Случайный лес (Random Forest)		<p>Случайный лес – метод <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, использующий Ансамбль (Ensemble) <a href="__GHOST_URL__/dierievo-rieshienii/">Деревьев решений (Decision Tree)</a> для задач <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>. Каждое отдельное дерево в таком лесу дает предсказание класса, и набравший наибольшее количество голосов Класс (Class), становится предсказанием <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. Он использует <a href="__GHOST_URL__/beghghingh/">Бэггинг (Bagging)</a> и случайность признаков при построении каждого отдельного дерева, чтобы создать <em>некоррелированный</em> лес из деревьев, прогноз которого "комитетом" более точен, чем прогноз любого отдельного дерева.</p><p>Бóльшая часть Машинного обучения – это классификация: мы хотим знать, к какому типу (или группе) принадлежит <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a>. Возможность точно классифицировать наблюдения чрезвычайно важна для различных бизнес-приложений, таких как прогнозирование покупки, или вероятность неуплаты кредита.</p><p><a href="__GHOST_URL__/nauka-o-dannykh/">Наука о данных (Data Science)</a> предоставляет множество <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a> классификации, таких как <a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/">Логистическая регрессия (Logistic Regression)</a>, <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a>, Наивный байесовский классификатор (Naive Bayes) и деревья решений. Но в верхней части иерархии классификаторов находится классификатор случайного леса.</p><p>Мы рассмотрим, как работают базовые деревья решений, как отдельные деревья решений объединяются для создания случайного леса, и в конечном итоге выясним, почему случайные леса так хороши в том, что делают.</p><h3 id="%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D1%8F-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9">Деревья решений</h3><p>Давайте быстро освежим тему деревьев решений, поскольку они являются строительными блоками модели случайного леса. К счастью, они довольно интуитивны. Я готов поспорить, что большинство людей использовали дерево решений, сознательно или нет, в какой-то момент своей жизни.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/07/random-forest.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/07/random-forest.png 600w, __GHOST_URL__/content/images/2021/07/random-forest.png 1000w"></figure><p>Наверное, гораздо легче понять, как работает дерево решений на примере.<br>Представьте, что наш набор данных состоит из списка чисел в верхней части рисунка [1, 1, 0, 0, 0, 0, 0]. У нас есть две единицы и пять нулей (1 и 0 – наши классы), и мы хотим разделить их, используя характеристики цвета и "подчеркнутости". Итак, как мы можем это сделать?</p><p>Цвет кажется довольно очевидной особенностью для разделения, поскольку все нули, кроме одного, белые. Таким образом, мы можем использовать вопрос: «Это розовый?» чтобы разделить числа в первом узле. Вы можете представить себе узел как разветвление дерева, где ветки разделяются на две части "да" и "нет".</p><p>Ветвь «нет» (белая) теперь имеет нулевые значения, и на ней дальнейшее деление невозможно. Но нашу ветвь «да» все еще можно разделить. Теперь мы можем использовать вторую характеристику и спросить: «Ряд подчеркнутый?» чтобы сделать второй раскол – "сплит".</p><p>Две подчеркнутые единицы отправляются вниз по ответвлению "да", а ноль, который не подчеркнут, идет по правой ветви. Наше дерево решений использовало эти две функции-характеристики для идеального разделения данных. Победа!</p><p>Очевидно, что в реальной жизни наши данные не будут такими чистыми, но логика, которую использует дерево решений, остается прежней. На каждом узле он спросит: </p><p><em>Какая функция позволит мне разделить имеющиеся наблюдения таким образом, чтобы результирующие группы максимально отличались друг от друга (и члены каждой результирующей подгруппы были максимально похожи друг на друга)?</em></p><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D0%BE%D0%B3%D0%BE-%D0%BB%D0%B5%D1%81%D0%B0">Классификатор случайного леса</h3><p>Случайный лес, как следует из названия, состоит из большого количества отдельных деревьев решений.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/07/random-forest_trees.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/07/random-forest_trees.png 600w, __GHOST_URL__/content/images/2021/07/random-forest_trees.png 1000w"><figcaption>5 деревьев решили, что класс 1, одно – 0. "Демократический" результат – 1.</figcaption></figure><p>Фундаментальная концепция случайного леса проста, но действенна – мудрость толпы. Говоря языком науки о данных, причина того, что модель случайного леса так хорошо работает, заключается в следующем:<br>Большое количество относительно некоррелированных моделей (деревьев), работающих как комитет, превзойдут любую из отдельных составляющих моделей.</p><p>Ключевым моментом является низкая корреляция между моделями. Подобно тому, как инвестиции с низкой корреляцией (например, акции и облигации) объединяются, чтобы сформировать портфель, превышающий сумму его частей, некоррелированные модели могут давать ансамблевые прогнозы, которые более точны, чем любые индивидуальные. Причина этого замечательного эффекта в том, что деревья "защищают"друг друга от своих индивидуальных ошибок (до тех пор, пока все они не ошибаются постоянно в одном и том же направлении). В то время как некоторые деревья могут быть неправильными, многие другие будут верны, поэтому деревья могут двигаются в правильном направлении как группа. Итак, предпосылки для хорошей работы случайного леса:</p><ul><li>В наших функциях должен быть какой-то реальный сигнал, чтобы модели, построенные с использованием этих <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, работали лучше, чем случайное предположение.</li><li>Прогнозы (и, следовательно, ошибки), сделанные отдельными деревьями, должны иметь низкую корреляцию друг с другом.</li></ul><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80-%D1%82%D0%BE%D0%B3%D0%BE-%D0%BF%D0%BE%D1%87%D0%B5%D0%BC%D1%83-%D0%BD%D0%B5%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D1%8B%D0%B5-%D1%80%D0%B5%D0%B7%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D1%82%D1%8B-%D1%82%D0%B0%D0%BA-%D1%85%D0%BE%D1%80%D0%BE%D1%88%D0%B8">Пример того, почему некоррелированные результаты так хороши</h3><p>Замечательные эффекты некоррелированных моделей – это настолько важная концепция, что я хочу показать вам пример, который поможет ей по-настоящему проникнуться. Представьте, что мы играем в следующую игру:<br>я использую генератор случайных чисел от 0 до 100 для получения числа. Если полученное мной число больше или равно 40, вы выигрываете (так что у вас есть шанс на победу 60%), и я плачу вам немного денег. Если оно ниже 40, я выигрываю, и вы платите мне ту же сумму.</p><p>Теперь я предлагаю вам следующие варианты. Мы можем сыграть:</p><ul><li>Игру 1: 100 генераций числа, ставка – $1</li><li>Игра 2: 10 – 10</li><li>Игра 3: 1 – 100</li></ul><p>Какой вариант Вы предпочитаете? Ожидаемая ценность каждой игры одинакова:</p><ul><li>1: (0,60 * 1 + 0,40 * -1) * 100 = 20</li><li>2: (0,60 * 10 + 0,40 * -10) * 10 = 20</li><li>3: 0,60 * 100 + 0,40 * -100 = 20</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/07/image-9.png" class="kg-image" alt loading="lazy" width="1000" height="750" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-9.png 600w, __GHOST_URL__/content/images/2021/07/image-9.png 1000w" sizes="(min-width: 720px) 720px"><figcaption>Распределение результатов для 10 000 симуляций для каждой игры</figcaption></figure><p>А что насчет распределений? Давайте визуализируем результаты с помощью моделирования Монте-Карло (мы запустим 10 000 имитаций каждого типа игры; например, мы будем моделировать 10 тысяч раз 100 партий по схеме № 1. Взгляните на таблицу: какую игру Вы бы выбрали? Несмотря на то, что ожидаемые значения одинаковы, распределения результатов сильно различаются: от положительных и узких (синего цвета) до двоичных (розового).</p><p>Игра 1 (в которой мы играем 100 раз) дает лучший шанс заработать немного денег – из 10 000 симуляций, которые я провел, вы зарабатываете деньги в 97% из них! Для игры 2 (в которой мы играем 10 раз) вы зарабатываете деньги в 63% симуляций, резкое снижение вознаграждения за риск (и резкое увеличение вероятности потерять деньги). И в игре 3, в которую мы играем только один раз, вы, как и ожидалось, зарабатываете деньги на 60% симуляций.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-10.png" class="kg-image" alt loading="lazy" width="651" height="440" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-10.png 600w, __GHOST_URL__/content/images/2021/07/image-10.png 651w"></figure><h3 id="%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%B7%D0%B0%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0-%D0%BD%D0%B0-%D0%BA%D0%B0%D0%B6%D0%B4%D0%BE%D0%B9-%D0%B8%D0%B3%D1%80%D0%B5">Вероятность заработка на каждой игре</h3><p>Таким образом, даже несмотря на то, что игры имеют одинаковую ожидаемую ценность, их распределение результатов совершенно разное. Чем больше мы разделим нашу ставку в 100 долларов на разные игры, тем больше у нас будет уверенности в том, что мы заработаем деньги. Как упоминалось ранее, это работает, потому что каждая игра не зависит от других.</p><p>Случайный лес такой же: каждое дерево похоже на одну игру в нашей предыдущей игре. Мы просто видели, как наши шансы заработать деньги увеличивались, чем больше раз мы играли. Точно так же с моделью случайного леса наши шансы сделать правильные прогнозы увеличиваются с увеличением количества некоррелированных деревьев.</p><h3 id="%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B4%D0%B8%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9-%D0%B4%D1%80%D1%83%D0%B3-%D0%B4%D1%80%D1%83%D0%B3%D0%BE%D0%BC">Обеспечение диверсификации моделей друг другом</h3><p>Итак, случайный лес гарантирует, что поведение каждого отдельного дерева не слишком коррелирует с поведением любого из других деревьев в модели. Он использует следующие два метода:</p><ul><li><strong>Бэггинг (Bagging)</strong>. Деревья решений очень чувствительны к данным, на которых обучаются: небольшие изменения в обучающем наборе могут привести к существенно разным древовидным структурам. Случайный лес использует это преимущество, позволяя каждому отдельному дереву случайным образом выбирать из набора данных, в результате чего получаются разные деревья. Этот процесс известен как бэггинг.</li></ul><p>Обратите внимание, что при бэггинге мы не разделяем обучающие данные на более мелкие фрагменты и не обучаем каждое дерево на разных фрагментах. Скорее, если у нас есть выборка размера N, мы по-прежнему скармливаем каждому дереву обучающий набор размера N (если не указано иное). Но вместо исходных обучающих данных мы берем <em>случайную</em> выборку размера N <em>с заменой</em>. Например, если наши обучающие данные были [1, 2, 3, 4, 5, 6], мы могли бы дать одному из наших деревьев следующий список [1, 2, 2, 3, 6, 6]. Обратите внимание, что оба списка имеют длину шесть и что «2» и «6» повторяются в случайно выбранных обучающих данных, которые мы передаем нашему дереву (потому что мы производим выборку с заменой).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/07/image-11.png" class="kg-image" alt loading="lazy" width="620" height="384" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-11.png 600w, __GHOST_URL__/content/images/2021/07/image-11.png 620w"><figcaption>Разделение узлов основано на случайности</figcaption></figure><ul><li><strong>Случайность признаков. </strong>В обычном дереве решений, когда приходит время разделения, мы рассматриваем все возможные признаки и выбираем тот, который дает наилучший сплит между наблюдениями в левом узле и наблюдениями в правом узле. Напротив, каждое дерево в случайном лесу может выбирать только из случайного подмножества функций-признаков. Это вызывает еще большее разнообразие деревьев в модели и в конечном итоге приводит к более низкой корреляции между деревьями и большей диверсификации.</li></ul><p>Давайте рассмотрим наглядный пример: на картинке выше традиционное дерево решений (выделено синим цветом) может выбирать из всех четырех функций при принятии решения о том, как разделить узел. Он решает использовать функцию 1 (черная подчеркнутая), поскольку она разделяет данные на группы наиболее четко.</p><p>Теперь давайте посмотрим на наш случайный лес. В этом примере мы рассмотрим только два дерева в нашем лесу. Когда мы проверяем случайное дерево леса 1, мы обнаруживаем, что оно может учитывать только функции 2 и 3 (выбранные случайным образом) для принятия решения о разделении узлов. Из нашего традиционного дерева решений (выделено синим цветом) мы знаем, что признак №1 – лучший способ разделения, но Дерево 1 не может видеть его, поэтому оно вынуждено использовать признак № 2 (черный и подчеркнутый). Дерево 2, с другой стороны, может видеть только характеристики 1 и 3, поэтому выбирает первый признак.</p><p><em>Таким образом, в нашем случайном лесу мы получаем деревья, которые не только обучаются на разных наборах данных (благодаря бэггингу – "пакетированию"), но также используют разные функции для принятия решений.</em></p><p>И это создает некоррелированные деревья, которые <em>буферизируют</em> и защищают друг друга от их ошибок.</p><p>Случайные леса – мой личный фаворит. Приходя из мира финансов и инвестиций, святым Граалем всегда было построение набора несогласованных моделей, каждая из которых имеет положительный ожидаемую доходность. Затем их объединяют в портфель, чтобы получить огромную рыночная доходность. Случайный лес – это эквивалент такого альфа-портфеля в науке о данных. </p><h3 id="%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9-%D0%BB%D0%B5%D1%81-%D0%B8-scikit-learn">Случайный лес и scikit-learn</h3><p>Давайте посмотрим, как метод реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification</code></pre><p>Сгенерируем тысячу случайных наблюдений, состоящие из четырех признаков. Параметр <code>n_redundant</code>, например, отвечает за корреляцию между признаками: если его значение равно нулю, то ни один из признаков не будет в зависимости от другого (такие пары признаков обычно "разбивают" удалением одного).</p><p>Сразу же запустим <code>RandomForestClassifier</code>, причем зададим глубину дерева, равную двум. В ином случае, ветвей у каждого узла может быть очень много, вплоть до числа наблюдений.</p><pre><code class="language-python">X, y = make_classification(n_samples = 1000, n_features = 4,\n                           n_informative = 2, n_redundant = 0,\n                           random_state = 0, shuffle = False)\nclf = RandomForestClassifier(max_depth = 2, random_state = 0)\nclf.fit(X, y)</code></pre><p>Система указывает стандартные настройки модели: мы не назначаем классам веса (<code>class_weight=None</code>), создаем 100 деревьев (<code>n_estimators=100</code>):</p><pre><code class="language-python">RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=2, max_features = 'auto', max_leaf_nodes = None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=0, verbose=0, warm_start=False)</code></pre><p>Модель готова к использованию, так что отправим ей запись, где значением каждого из четырех признаков равно нулю:</p><pre><code class="language-python">print(clf.predict([[0, 0, 0, 0]]))</code></pre><p>100 деревьев – это сила; модель относит это наблюдение к классу 1:</p><pre><code class="language-python">[1]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1b01hfNpnnHDTdh6yrOQ4A1O6AuX6zLm_?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2">Tony Yiu</a></p><p>Фото: <a href="https://unsplash.com/@lucabravo?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@lucabravo</a></p>		sluchainyi-lies	2021-07-08		
116	Переобучение (Overfitting)		<p>Переобучение – это случай, когда значение <a href="__GHOST_URL__/funktsiia-potieri/">Функции потери (Loss Function)</a> действительно малó, но <a href="__GHOST_URL__/modiel/">Модель (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> ненадежна. Это связано с тем, что модель «слишком много учится» на обучающем наборе данных.</p><p>Когда мы входим в сферу ML, появляются двусмысленные термины: Переобучение, Недообучение (Underfitting) и Дилемма смещения-дисперсии (Bias-Variance Trade-off). Эти концепции лежат в основе Машинного обучения в целом. Почему нам вообще должно быть до этого дело?</p><p>Возможно, модели машинного обучения преследуют одну единственную цель: хорошо <em>обобщать</em>. </p><p><em>Обобщение (генерализация) – это способность модели давать разумные предсказания на основе входных данных, которых она никогда раньше не видела.</em></p><p>Обычные программы не могут этого сделать, так как они могут выдавать выходные данные только алгоритмически, то есть на основании вручную определенных опций (например, если зарплата человека меньше определенного порога, банковский алгоритм не предлагает кредит в приложении). Производительность модели, а также ее полезность в целом во многом зависит от ее обобщающей способности. Если модель хорошо обобщает, она служит своей цели. Существует множество методов оценки такой производительности.</p><p>Основываясь на этой идее, пере- и недообучение относятся к недостаткам, от которых может пострадать предсказательная способность модели. «Насколько плохи» ее прогнозы – это степень близости ее к пере- или недообучению.</p><p><em>Модель, которая хорошо обобщает, не является ни переобученной, ни недообученной.</em></p><p>Возможно, это пока не имеет большого смысла, но мне нужно, чтобы Вы запомнили это предложение на протяжении всей статьи, так как это общая картина темы. </p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80">Пример</h3><p>Допустим, мы пытаемся построить модель машинного обучения для следующего набора данных:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-12.png" class="kg-image" alt loading="lazy" width="775" height="401" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-12.png 600w, __GHOST_URL__/content/images/2021/07/image-12.png 775w" sizes="(min-width: 720px) 720px"></figure><p>Ось X – это <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a> – например, площадь дома, а ось Y – <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевая переменная (Target Variable)</a> – стоимость дома. Если у Вас есть опыт обучения модели, Вы, вероятно, знаете, что есть несколько <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a>, однако для простоты в нашем примере выберем одномерную <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейную регрессию (Linear Regression)</a>. </p><h3 id="%D1%8D%D1%82%D0%B0%D0%BF-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F">Этап обучения</h3><p>Обучение модели линейной регрессии в нашем примере сводится к минимизации общего расстояния (т.е. стоимости) между линией, которую мы пытаемся проложить, и фактическими точками <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>. Проходит несколько итераций, пока мы не найдем оптимальную конфигурацию нашей линии. Это именно то место, где происходит пере- и недообучение. Мы хотим, чтобы наша модель следовала примерно такой линии:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-13.png" class="kg-image" alt loading="lazy" width="768" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-13.png 600w, __GHOST_URL__/content/images/2021/07/image-13.png 768w" sizes="(min-width: 720px) 720px"></figure><p>Несмотря на то, что общие потери не являются минимальными (т.е. существует лучшая конфигурация, в которой линия может давать меньшее расстояние до точек данных), линия выше очень хорошо вписывается в тенденцию, что делает модель надежной. Допустим, мы хотим знать значение Y при неизвестном доселе модели значении X (т. е. обобщить). Линия, изображенная на графике выше, может дать очень точный прогноз для нового X, поскольку с точки зрения машинного обучения ожидается, что результаты будут следовать тенденции, наблюдаемой в обучающем наборе.</p><h3 id="%D0%BF%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">Переобучение</h3><p>Когда мы запускаем обучение нашего алгоритма на <a href="__GHOST_URL__/dataset/">Датасете (Dataset)</a>, мы стремимся уменьшить потери (т.е. расстояния от каждой точки до линии) с увеличением количества итераций. Длительное выполнение этого обучающего алгоритма приводит к минимальным общим затратам. Однако это означает, что линия будет вписываться во все точки, включая <a href="__GHOST_URL__/shum/">Шум (Noise)</a>, улавливая вторичные закономерности, которые не требуются модели.</p><p>Возвращаясь к нашему примеру, если мы оставим алгоритм обучения запущенным на долгое время, он, в конце концов, подгонит строку следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-14.png" class="kg-image" alt loading="lazy" width="778" height="409" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-14.png 600w, __GHOST_URL__/content/images/2021/07/image-14.png 778w" sizes="(min-width: 720px) 720px"></figure><p>Выглядит хорошо, правда? Да, но насколько это надежно? Не совсем. <br>Суть такого алгоритма, как линейная регрессия, состоит в том, чтобы захватить доминирующий тренд и подогнать нашу линию к нему. На рисунке выше алгоритм уловил все тенденции, но не доминирующую. Если мы хотим протестировать модель на входных данных, которые выходят за пределы имеющихся у нас строк (т.е. обобщить), как бы эта линия выглядела? На самом деле нет возможности сказать. Следовательно, результаты ненадежны. Если модель не улавливает доминирующую тенденцию, которую мы все видим (в нашем случае с положительным увеличением), она не может предсказать вероятный результат для входных данных, которых никогда раньше не видела, что противоречит цели машинного обучения с самого начала!</p><p>Переобучение – это случай, когда общие потери-затраты действительно невелики, но обобщение модели ненадежно. Это связано с тем, что модель «слишком много учится» на обучающем наборе. Это может показаться абсурдным, но переобучение, или высокая <a href="__GHOST_URL__/dispiersiia/">Дисперсия (Variance)</a>, приводит к бóльшему количеству плохих, чем хороших результатов. Какая польза от модели, которая очень хорошо усвоила данные обучения, но все еще не может делать надежные прогнозы для новых входных данных?</p><h3 id="%D0%BD%D0%B5%D0%B4%D0%BE%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">Недообучение</h3><p>Мы хотим, чтобы модель совершенствовалась на обучающих данных, но не хотим, чтобы она училась слишком многому (то есть слишком много паттернов). Одним из решений может быть досрочное прекращение тренировки. Однако это может привести к тому, что модель не сможет найти достаточно шаблонов в обучающих данных и, возможно, даже не сможет уловить доминирующую тенденцию. Этот случай называется недообучением:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-15.png" class="kg-image" alt loading="lazy" width="782" height="427" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-15.png 600w, __GHOST_URL__/content/images/2021/07/image-15.png 782w" sizes="(min-width: 720px) 720px"></figure><p>Это случай, когда модель «недостаточно усвоила» обучающие данные, что приводит к низкому уровню обобщения и ненадежным прогнозам.<br> Как вы, вероятно, и ожидали, такое большое <a href="__GHOST_URL__/smieshchieniie/">Смещение (Bias)</a> так же плохо для модели, как и переобучение. При большом смещении модель может не обладать достаточной гибкостью с точки зрения подгонки линии, что приводит к чрезмерной упрощенности.</p><h3 id="%D0%B4%D0%B8%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0-%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B4%D0%B8%D1%81%D0%BF%D0%B5%D1%80%D1%81%D0%B8%D0%B8">Дилемма смещения-дисперсии</h3><p>Итак, какова правильная мера? Этот компромисс является наиболее важным аспектом обучения модели машинного обучения. Как мы уже говорили, модели выполняют свою задачу, если хорошо обобщают, а это связано с двумя нежелательными исходами – большим смещением и высокой дисперсией. Это и есть Дилемма смещения-дисперсии. Ответственность за определение того, страдает ли модель от одного из них, полностью лежит на разработчике модели.</p><h3 id="%D0%BF%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8">Переобучение графически</h3><p>Давайте посмотрим, как выглядит переобучение в сравнении с остальными сценариями развития. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import pandas as pd\nimport numpy  as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt</code></pre><p>Сгенерируем датасет – набор точек, "укладывающихся" в параболу:</p><pre><code class="language-python">x_parabola = 50 * np.random.default_rng(100).random((50,))\ny_parabola = ((x_parabola - 15) ** 2) + (np.random.default_rng(30).random((50,)) - 0.5) * 100</code></pre><p>Добавим <a href="__GHOST_URL__/vybros/">Выбросы (Outlier)</a>, чтобы сделать данные шумнее и реалистичнее, а демонстрацию концепции нагляднее:</p><pre><code class="language-python">x_outliers = 50 * np.random.default_rng(80).random((10,))\ny_outliers = ((x_outliers - 15) **2) + (np.random.default_rng(500).random((10,)) -0.5) * 750</code></pre><p>Усложним алгоритму задачу еще и сгруппируем записи в две группы:</p><pre><code class="language-python">x_points = np.concatenate((x_parabola, x_outliers), axis = 0)\ny_points = np.concatenate((y_parabola, y_outliers), axis = 0)</code></pre><p>Настал черед отобразить переобучение на графике. Построим три таких, первый – недообучение, второй – адекватное обучение, третий – переобучение. Обратите внимание на уравнения, описывающие положение точек: в первом случае, модель опишет набор линейным уравнением <code>underfit = (m * x + c)</code>, во втором – кубическим <code>goodfit = (a * x ** 2 + m * x + c)</code>, то есть подходящем параболе. В третьем случае мы попробуем описать точки кривой <code>overfit = np.poly1d(fit)</code>.</p><pre><code class="language-python"># График недообучения\nfig = plt.figure(figsize = (7, 8))\nax = fig.add_subplot(3, 1, 1)\nfig_1 = plt.scatter(x = x_points, y = y_points)\nplt.axis('off')\nx = np.linspace(0, 50, 400) \n\nax.text(10,450, 'Недообучение', fontsize = 18)\nfit = (np.polyfit(x_points, y_points, 1))\nm = fit[0]\nc = fit[1]\nunderfit = (m * x + c)\nfig_1 = plt.plot(x, underfit, color = 'orange', linewidth = 3)\nax.set(xlim = (0, 45), ylim = (-200, 850))\n\n# График адекватного обучения\nax = fig.add_subplot(3, 1, 2)\n#ax.set_title ("a good fit", fontsize=18)\nax.text(15, 350,'Адекватное обучение', fontsize = 18)\nfit = (np.polyfit(x_points, y_points, 2))\na = fit[0]\nc = fit[2]\nm = fit[1]\ngoodfit = (a * x ** 2 + m * x + c)\nfig_1 = plt.plot(x,goodfit, color = 'orange', linewidth = 3)\nfig_1 = plt.scatter(x = x_points, y = y_points)\nax.set(xlim = (0, 45), ylim = (-200, 850))\nplt.axis('off')\n\n# График переобучения\nax = fig.add_subplot(3, 1, 3)\n# overfit\n#ax.set_title ("overfitting", fontsize=18)\nax.text(15, 350, 'Переобучение', fontsize = 18)\nfit = (np.polyfit(x_points, y_points, 50))\noverfit = np.poly1d(fit)\nfig_1 = plt.plot(x,overfit(x), color = 'orange', linewidth = 3)\nfig_1 = plt.scatter(x = x_points, y = y_points)\nax.set(xlim = (0, 45), ylim = (-200, 850))\nplt.axis('off')\nplt.show()</code></pre><p>Теперь разница между сценариями наглядна:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/07/image-17.png" class="kg-image" alt loading="lazy" width="404" height="449"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1_DOCpEj7oqP76lodyZBA_JleCiiD5WuE?usp=sharing">здесь</a>.</p><p>Фото: <a href="https://unsplash.com/@kalenemsley?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">@kalenemsley</a></p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690">Anas Al-Masri</a>, <a href="https://www.kaggle.com/carlmcbrideellis/overfitting-and-underfitting-the-titanic">Carl McBride Ellis</a></p>		pierieobuchieniie	2021-07-10		
117	AdaBoost		<p>AdaBoost (Adaptive Boosting) – классифицирующая <a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модель (Model)</a>, которая объединяет несколько слабых классификаторов для повышения точности. Является  Ансамблем (Ensemble).</p><p>Алгоритм предложен Йоавом Фройндом (Yoav Freund) и Робертом Шапире (Robert Schapire) в 1996 году. Комбинируя несколько неэффективных классификаторов, мы получаем сильный классификатор высокой точности.</p><p>Основная концепция заключается в установке весов классификаторов и обучении выборки данных на каждой итерации, чтобы обеспечить точные предсказания необычных наблюдений. Любой алгоритм <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> может использоваться в качестве базового классификатора, если он принимает веса в обучающем наборе.</p><p>AdaBoost должен соответствовать двум условиям:</p><ul><li>Классификатор следует обучать интерактивно на различных взвешенных обучающих примерах.</li><li>На каждой итерации он пытается обеспечить отличное соответствие этим примерам за счет минимизации ошибки обучения.</li></ul><p>Чтобы построить классификатор AdaBoost, представьте, что в качестве первого базового классификатора мы обучаем алгоритм <a href="__GHOST_URL__/dierievo-rieshienii/">Дерева решений (Decision Tree)</a> для прогнозирования наших обучающих данных. Теперь, следуя методологии, вес неверно классифицированных обучающих экземпляров увеличен. Второй классификатор обучается, принимает обновленные веса и повторяет процедуру снова и снова. В конце каждого прогноза мы в конечном итоге увеличиваем веса неправильно классифицированных экземпляров, чтобы следующая модель лучше справлялась с ними, и так далее.</p><p>AdaBoost добавляет в ансамбль <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a>, постепенно улучшая его. Большим недостатком этого алгоритма является то, что модель не может быть распараллелена, поскольку каждый предиктор может быть обучен только после того, как предыдущий был обучен и оценен.</p><h3 id="adaboost-%D0%BD%D0%B0-scikit-learn">AdaBoost на scikit-learn</h3><p>Давайте посмотрим, как AdaBoost реализован в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import sklearn as sk\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n\nimport pandas as pd\nimport numpy as np</code></pre><p>Импортируем датасет о раке груди и разделим <a href="__GHOST_URL__/priznak/">Признаки (Feature)</a> на предикторы и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой признак (Target Variable)</a>:</p><pre><code class="language-python">breast_cancer = load_breast_cancer()\nX = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\ny = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)</code></pre><p>Закодируем целевые метки значениями от 0 до значения [число классов  – 2], то есть возможными значениями являются 0 и 1:</p><pre><code class="language-python">encoder = LabelEncoder()\nbinary_encoded_y = pd.Series(encoder.fit_transform(y))</code></pre><p>Разделим датасет на тренировочную и тестовую части функцией <code>train_test_split</code> и применим перемешивание (<code>random_state = 1</code>):</p><pre><code class="language-python">train_X, test_X, train_y, test_y = train_test_split(X, binary_encoded_y, random_state = 1)</code></pre><p>Создадим объект-классификатор и обучим его <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочными данными (Train Data)</a>:</p><pre><code class="language-python">classifier = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth = 1),\n    n_estimators = 200)\n\nclassifier.fit(train_X, train_y)</code></pre><p>Система демонстрирует параметры модели по умолчанию. К примеру, в качестве базового <a href="__GHOST_URL__/alghoritm/">Алгоритма (Algorithm)</a> будет использоваться <a href="__GHOST_URL__/dierievo-rieshienii/">Дерево решений (Decision Tree)</a> (<code>base_estimator=DecisionTreeClassifier</code>):</p><pre><code class="language-python">AdaBoostClassifier(algorithm='SAMME.R', base_estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=1, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best'),\nlearning_rate=1.0, n_estimators=200, random_state=None)</code></pre><p>Сгенерируем предсказания для <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовых данных (Test Data)</a>:</p><pre><code class="language-python">predictions = classifier.predict(test_X)</code></pre><p>Проверим модель с помощью <a href="__GHOST_URL__/matritsa-oshibok/">Матрицы ошибок (Confusion Matrix)</a>:</p><pre><code class="language-python">confusion_matrix(test_y, predictions)</code></pre><p>На главной диагонали – крупные числа (в сравнении с остальными элементами матрицы),  а это означает, что количество верно классифицированных истинных и ложных случаев значительно превышает неверные (96,5%):</p><pre><code class="language-python">array([[86,  2],\n       [ 3, 52]])</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1k2X_4GGfgz6catp7It9C3xcREbwK3p7I?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.kaggle.com/prashant111/adaboost-classifier-tutorial">Prashant Banerjee</a></p><p>Фото: <a href="https://unsplash.com/@davealmine">@davealmine</a></p>		adaboost	2021-07-11		
118	Пакет (Batch)		<p>Пакет (партия) – <a href="__GHOST_URL__/vyborka/">Выборка (Sample)</a>, которую необходимо обработать перед обновлением внутренних параметров <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. Размер партии может быть следующим:</p><ul><li>Пакетный (batch) режим: размер пакета равен общему набору данных, что делает значения итерации и эпохи эквивалентными</li><li>Мини-пакетный (mini-batch) режим: размер пакета больше единицы, но меньше общего размера набора данных. Обычно это число, которому кратен общий размер набора данных.</li><li>Стохастический (stochastic) режим: размер партии равен единице. Поэтому градиент и параметры нейронной сети обновляются после каждой выборки.</li></ul><p><a href="__GHOST_URL__/epokha/">Статья о различии между эпохой и пакетом</a></p>		pakiet	2021-07-17		
119	Чат-бот (Chatbot)		<p>Чат-бот – виртуальный ассистент, позволяющий автоматизировать исполнение рутинных задач и снизить загруженность живых специалистов.С точки зрения <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, это реализация <a href="__GHOST_URL__/moda/">Модели (Model)</a> <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, позволяющая автоматически определять намерения пользователей и передаваемые данные-переменные.</p><p>Наряду с остальными задачами, понимание текста является одной из главных задач в современном машинном обучении. </p><p>В то время как задачи с изображениями впечатляюще решаются одна за другой, текстовые немного отстают: действительно хороший разговорный навык, который требуется для правильного чат-бота, еще недостижим.</p><p>Итак, вы можете подумать: «Если у <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a> так много проблем, зачем кому-то инвестировать в чат-бота?» В какой-то степени вы правы. Даже такие гиганты, как Apple, Amazon и Google, часто терпят неудачу со своими диалоговыми интерфейсами. Похоже, что у машинного обучения еще нет ответа на подобные вопросы. Так зачем кому-то работать над чат-ботами по коммерческим причинам?</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/07/image-18.png" class="kg-image" alt loading="lazy" width="182" height="277"><figcaption>Siri путает просьбу о вызове скорой с изменением псевдонима пользователя</figcaption></figure><p>Причина в том, что хотя вы не можете сделать все идеально, но можете сделать что-то полезное. Например, вы не можете создать чат-бота, чтобы обсуждать смысл жизни, или бота, который помогал бы с некоторыми сложными проблемами, но вы определенно можете создать такого, что ответит на основные вопросы техподдержки интернет-провайдера. </p><h3 id="%D0%B2%D0%BE%D0%B7%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D0%B8-%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D1%8B-%D1%81-%D1%87%D0%B0%D1%82-%D0%B1%D0%BE%D1%82%D0%B0%D0%BC%D0%B8-%D0%BA%D1%83%D0%B4%D0%B0-%D1%86%D0%B5%D0%BB%D0%B8%D1%82%D1%8C%D1%81%D1%8F">Возможности и проблемы с чат-ботами: куда целиться?</h3><p>Как уже было сказано, до полного разговора с помощью машинного обучения нам еще далеко, однако боты по-прежнему полезны. Подумайте о Siri, Google Home и Alexa, которые являются очень популярными платформами для чат-ботов: хотя они и не идеальны, многие люди считают их очень хорошими продуктами. </p><p>Чат-боты также могут быть очень полезны для простых разговорных задач, таких как (базовая) поддержка клиентов, как более интеллектуальная поисковая система и многое другое.</p><p>А теперь вспомните, когда Вы в последний раз разговаривали с представителем службы поддержки, в тысячный раз объяснили ему свою проблему и получили ответ, который он повторял уже десять тысяч раз. <br>Есть много однообразных заданий, которые можно было бы заменить базовым навыком разговора с несколькими десятками / сотнями предписанных ответов.</p><p>Отсутствие способности решать проблему «разговорных навыков» требует от создателей чат-ботов творческого подхода и разработки цепочки задач, которые в сочетании с некоторыми бизнес-правилами и эвристикой поиска могут дать несколько полезных чат-ботов.</p><h3 id="%D0%BF%D0%B0%D1%80%D0%B0%D0%B4%D0%B8%D0%B3%D0%BC%D0%B0-%D0%BD%D0%B0%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D0%B9-%D0%B8-%D1%81%D1%83%D1%89%D0%BD%D0%BE%D1%81%D1%82%D0%B5%D0%B9">Парадигма намерений и сущностей</h3><p>Надеюсь, Вы сможете увидеть потенциал чат-ботов, несмотря на возможные недостатки. Теперь давайте обсудим, как его построить.</p><p>Хотя я не сказал об этом прямо, в чат-боте не обязательно есть NLP. Если на одном конце шкалы чат-бота находится «полноценный разговорный» бот, который обладает общими навыками, подобными человеческим, с другой стороны, есть детерминированный бот с предопределенным деревом разговоров (обычно глубиной в 3-4 уровня) на основе множества операторов.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/07/image-19.png" class="kg-image" alt loading="lazy" width="470" height="364"><figcaption>Пример дерева разговоров в детерминированном боте</figcaption></figure><p>Начиная с этого базового механизма, NLP может быть полезно в следующих частях:</p><ul><li>Пользовательский ввод / классификация вопросов</li><li>Лучшее распознавание слов / сущностей</li><li>Признание состояния (где мы находимся в дереве)</li><li>Генерация ответов</li></ul><p>Проведя небольшое исследование, я обнаружил, что хорошим вариантом является парадигма намерения и сущности, которая, как следует из ее названия, работает в два этапа: классификация намерений и распознавание сущностей.</p><p>Мы предполагаем, что знаем, где мы находимся в потоке разговора, и игнорируем состояние и генерируем соответствующие ответы. Эта парадигма облегчает проектирование и обучение ботов, но не технических специалистов, и используется в большинстве известных интерфейсов.</p><p>Давайте посмотрим на пример. Пример: бот для поиска ресторанов будет иметь следующие возможности:</p><ul><li>Поиск ресторана: пользователь ищет конкретный или список ресторанов.</li><li>Бронирование столика: пользователь хочет заказать столик в каком-нибудь ресторане.</li><li>Общий запрос: у пользователя есть конкретный запрос о ресторане, например, подходит ли он для веганов или является кошерным.</li></ul><p>Кроме того, мы хотели бы найти в запросе пользователя следующие объекты, если они были упомянуты:</p><ul><li>Кухня, например, итальянская, азиатская и т.д.</li><li>Расположение желаемого ресторана</li><li>Тип кухни, например, веганский, кошерный.</li></ul><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D0%B9">Классификация намерений</h3><p>В этой парадигме намерение означает общую цель пользовательского запроса, например, поиск компании или места, назначение встречи и т.д.<br> Бот должен классифицировать ваш запрос и действовать соответствующим образом (для поиска места, получения как можно более подробных сведений. Для настройки встречи, запроса сведений о встрече, сопровождающих и т.д.). Таким образом, легко увидеть, что это задача классификации простого текста.</p><p>Классификация текстов – это хорошо изученная задача машинного обучения, однако большая часть исследований проводится на мягких постановках задач, таких как анализ тональности. В реальных ситуациях у вас почти никогда не бывает менее 5 возможных намерений.</p><p>Точность такой модели зависит от различных параметров:</p><ul><li>Количество намерений – среднее количество для одного приложения должно составлять 5–10 намерений. Меньшее количество намерений будет упрощенным, в то время как большее количество намерений повредит точности.</li><li>Объем и качество данных – все мы знаем, что, как и в любой задаче машинного обучения, чем больше у нас данных и чем ближе они к логическим запросам, тем лучше результаты.</li><li>Возможность переноса обучения – использование предварительно обученной модели для аналогичной проблемы может быть очень полезным, если доступно.</li><li>Размер вводимых сообщений: пользователи не склонны кратко запрашивать нашего бота. поэтому специализированные инструменты помогут нашему приложению повысить точность.</li></ul><p>Если бы мы смогли «оптимизировать» вышеуказанные гиперпараметры, мы, как правило, смогли бы достичь около 80% точности без особых усилий и начнем стремиться к 90%, что является сложной задачей, но считается реальной. Результат ниже 80% приведет к разочарованию в продукте.</p><h3 id="%D1%81%D1%83%D1%89%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Сущность</h3><p>Сущность в тексте может быть бизнесом, местоположением, именем человека, временем и т.д. Объект, который имеет значение в запросе и будет иметь дополнительное значение в логике бота.</p><p>Распознавание сущностей само по себе также является хорошо известной проблемой NLP, и это одна из раздражающих проблем: она очень сильно зависит от наборов данных и эвристики (например, заглавных букв, вопросительных знаков).Есть много библиотек, которые решают эту задачу, например, мой любимый spacy.Сравнить 48 конструкторов между собой позволяет платформа chatimize.com (<a href="https://chatimize.com/chatbot-platform-comparison/" rel="noopener noreferrer">ссылка на сравнительную таблицу</a>):</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://avatars.mds.yandex.net/get-zen_doc/4032365/pub_60f2f7a76de099448424b752_60f2fa32a7eab75c27d4d621/scale_1200" class="kg-image" alt loading="lazy"></figure><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/chatbots-vs-reality-how-to-build-an-efficient-chatbot-with-wise-usage-of-nlp-77f41949bf08" rel="noopener noreferrer">Gidi Shperber</a></p>		chat-bot	2021-07-17		
122	Функция (Function)		<ol><li>В программировании: элемент кода, к которому можно обратиться из другого места программы.</li><li>В Машинном обучении: <a href="__GHOST_URL__/priznak/">Признак (Feature)</a>, характеристика <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> в наборе данных, столбец <a href="__GHOST_URL__/datafrieim/">Датафрейма (Dataframe)</a>:</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/07/dataset-matrix.png" class="kg-image" alt loading="lazy" width="360" height="246"><figcaption>Функции "Дистанция", "Загрузка" и "Толщина"</figcaption></figure>		funktsiia	2021-07-25		
123	Поле (Margin)		<p>Поле – расстояние, разделяющее ближайшую пару точек данных, принадлежащих противоположным классам. Термин используется в различных <a href="__GHOST_URL__/alghoritm/">Алгоритмах (Algorithm)</a> <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, включая <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/07/image-22.png" class="kg-image" alt loading="lazy" width="651" height="611" srcset="__GHOST_URL__/content/images/size/w600/2021/07/image-22.png 600w, __GHOST_URL__/content/images/2021/07/image-22.png 651w"><figcaption>Величина поля – длина черной стрелки</figcaption></figure><p>В ходе <a href="__GHOST_URL__/optimizatsiia/">Оптимизации (Optimization)</a> <a href="__GHOST_URL__/modiel/">Модели (Model)</a>, мы стремимся максимизировать поле, то есть найти наиболее четкую границу между <a href="Semi-Supervised Learning, ">Кластерами (Cluster)</a> <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>.</p><p>Две точки на графике, касающиеся границы поля, называются опорными векторами, поскольку представляют собой данные наблюдений, которые «поддерживают» или определяют границу принятия решения. Чтобы обучить классификатор, мы находим оптимальную разделяющую <a href="__GHOST_URL__/gipierploskost/">Гиперплоскость (Hyperplane)</a>, которая "окружена" полями и позволяет классифицировать новые записи в дальнейшем.</p>		polie	2021-07-25		
124	Инсайт (Insight)		<p>Инсайт (озарение) – это ценность, полученная с помощью анализа, как в ходе <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">Разведочного анализа данных (EDA)</a>, так и в ходе обучения <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Такие открытия, полученные с помощью аналитики, могут быть невероятно мощными стимуляторами развития бизнеса, поскольку позволяют выявлять возможности.</p><p>К примеру, при <a href="__GHOST_URL__/modielirovaniie-ottoka/">Моделировании оттока (Churn Modeling)</a> можно узнать, какие факторы заставляют пользователей уходить с целевой продающей страницы сайта. Это позволяет людям, принимающим решения, вносить изменения в бизнес-процессы.</p><p>Один из лучших способов понять и передать значимую информацию из данных – использовать инструменты, которые помогают визуализировать результаты модели и предоставляют различные способы исследования и понимания ваших данных. Это дает реальную ценность для бизнеса в виде увеличения рентабельности инвестиций в рекламу, более точных прогнозов невозврата кредита и многого другого. Ясность на основе анализа данных позволяет пользователям принимать более обоснованные решения на основе повышенной интерпретируемости модели, позволяя аналитикам и другим пользователям объяснять результаты модели ключевым заинтересованным сторонам.</p><p>Инструменты визуализации данных помогают пользователям понять и объяснить выводы из результатов модели машинного обучения. Будь то простые графические представления, такие как облака слов, или более сложные и гибкие инструменты визуализации данных, такие как информационные панели <a href="__GHOST_URL__/tableau/">Tableau</a>, эти инструменты упрощают понимание и передачу ценности, раскрытой в модели, и способствуют более эффективному принятию бизнес-решений.</p><p>Данные важны, потому что инсайт позволяет пользователям с любым уровнем подготовки понять, что делает модель «за кулисами», что особенно важно, когда речь идет о формализованных отраслях, таких как банковское дело и здравоохранение. Если мы не понимаем, почему модель делает такие выводы, практическая полезность ее ограничена.</p><p>Пример. Банк собирает данные о своих клиентах с целью определить, кто готов приобрести кредитный продукт. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt</code></pre><p>Загрузим <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>, содержащий результаты анкетирования клиентов:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/62xm9ymoaunnfg6/bank-full.csv?dl=1', sep = ';')\ndf.head()</code></pre><p>Обширный <a href="__GHOST_URL__/datafrieim/">Датафрейм (Dataframe)</a> содержит, помимо прочих, данные о семейном статусе клиента, его кредитной истории и даже длительности телефонного разговора:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/08/eda-sample.png" class="kg-image" alt loading="lazy" width="2000" height="276" srcset="__GHOST_URL__/content/images/size/w600/2021/08/eda-sample.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/eda-sample.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/08/eda-sample.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/08/eda-sample.png 2400w"></figure><p>Для краткости продемонстрируем обнаружение инсайта на <a href="__GHOST_URL__/priznak/">Признаке (Feature)</a> "Длительность [холодного / теплого звонка]". Построим <a href="__GHOST_URL__/gistoghramma/">Гистограмму (Histogram)</a> распределения длительности разговора:</p><pre><code class="language-python">ax = df['Длительность'].plot.hist(bins = 12, xlim = (0, 2000), figsize = (15, 5))</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/08/insight-duration-hist.png" class="kg-image" alt loading="lazy" width="1002" height="387" srcset="__GHOST_URL__/content/images/size/w600/2021/08/insight-duration-hist.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/insight-duration-hist.png 1000w, __GHOST_URL__/content/images/2021/08/insight-duration-hist.png 1002w"></figure><p>А теперь выделим из общей массы записей те, что принесли компании продажу – приобретение кредитного продукта. Для таких записей используется столбец "Доходность": значение в ячейке равно "Присутствует". Методом <code>df.loс()</code> мы фильтруем набор и строим гистограмму для новой <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a>:</p><pre><code class="language-python">df_agreed = df.loc[df['Доходность'] == "Присутствует"]\nax = df_agreed['Длительность'].plot.hist(bins = 12, xlim = (0, 2000), figsize = (15, 5))</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/08/insight-filtered-duration-hist.jpg" class="kg-image" alt loading="lazy" width="989" height="387" srcset="__GHOST_URL__/content/images/size/w600/2021/08/insight-filtered-duration-hist.jpg 600w, __GHOST_URL__/content/images/2021/08/insight-filtered-duration-hist.jpg 989w"></figure><p>Нетрудно заметить, что максимальное значение координаты Y резко сократилось с ~33000 до ~830, однако инсайт кроется именно в оптимальной длительности разговора: <em>подавляющее большинство продаж выполняется во время звонков, чья длительность не превышает 575 секунд</em>. <em>Клиенты не любят длинные детальные разговоры</em>.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1ir5VXk9QuDHz30K_hNqJHg3tMwtnTUc-?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.datarobot.com/wiki/insights/">datarobot.com</a></p>		insait	2021-08-01		
125	Гистограмма (Histogram)		<p>Гистограмма – популярный график, используемый во время <a href="__GHOST_URL__/razvedochnyy-analiz-dannykh-chast-1/">Разведочного анализа данных (EDA)</a>, а также на других стадиях создания <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Подобный способ используется, чтобы визуализировать распределение вероятностей значений того или иного <a href="__GHOST_URL__/priznak/">Признака (Feature)</a>.</p><p>Пример. Банк собирает данные о своих клиентах с целью определить, кто готов приобрести кредитный продукт. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt</code></pre><p>Загрузим <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>, содержащий результаты анкетирования клиентов:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/62xm9ymoaunnfg6/bank-full.csv?dl=1', sep = ';')\ndf.head()</code></pre><p>Обширный <a href="__GHOST_URL__/datafrieim/">Датафрейм (Dataframe)</a> содержит, помимо прочих, данные о семейном статусе клиента, его кредитной истории и даже длительности телефонного разговора:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/08/eda-sample.png" class="kg-image" alt loading="lazy" width="2000" height="276" srcset="__GHOST_URL__/content/images/size/w600/2021/08/eda-sample.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/eda-sample.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/08/eda-sample.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/08/eda-sample.png 2400w"></figure><p>Построим гистограммы распределения признака "Возраст":</p><pre><code class="language-python">ax = df['Возраст'].plot.hist(bins = 12, xlim = (0, 100), figsize = (15, 5))</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/08/histogram.jpg" class="kg-image" alt loading="lazy" width="998" height="387" srcset="__GHOST_URL__/content/images/size/w600/2021/08/histogram.jpg 600w, __GHOST_URL__/content/images/2021/08/histogram.jpg 998w"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1x3QVTfZDBoQ_rHZoLuuCNPkIfWeaVH9T?usp=sharing">здесь</a>.</p>		gistoghramma	2021-08-01		
126	Сущность (Entity)		<p>Сущность (Entity) – это популярный компонент конструктора чат-бота, перечень разновидностей синтаксической функции подлежащего, сказуемого, определителя, дополнения, обстоятельства, служебных частей речи. Например, для чат-бота, автоматизирующего сбор заявок на курсы рисования, сущностью является 'enlist' (записать) – перечень синонимов слова и подходящих словоформ:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://avatars.mds.yandex.net/get-zen_doc/1718877/pub_610673f549aa541e4ac9393e_61067578906df03da94f6ed2/scale_1200" class="kg-image" alt loading="lazy"><figcaption>Интерфейс пополнения справочника сущности "записать" DialogFlow</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://avatars.mds.yandex.net/get-zen_doc/1863556/pub_610673f549aa541e4ac9393e_610676249bd3fc59f050903e/scale_1200" class="kg-image" alt loading="lazy"><figcaption>Интерфейс пополнения справочника сущности "курс" Aimylogic</figcaption></figure><p>Согласно структуре большинства языков, сущности можно автоматически сочетать между собой с помощью специального синтаксиса. Это означает, что вместо ручной декларации всех возможных вариантов фраз со значением "запиши меня на курсы" мы указываем лишь все возможные используемые сказуемые (запиши), предлоги (на) и дополнения (курсы):</p><figure class="kg-card kg-image-card"><img src="https://avatars.mds.yandex.net/get-zen_doc/2746730/pub_610673f549aa541e4ac9393e_6106770a906df03da9540701/scale_1200" class="kg-image" alt loading="lazy"></figure><p>Понравилась статья? Поддержите нас, поделившись статьей в социальных сетях и <a href="https://zen.yandex.ru/id/5fd12882382a85570c79c48c" rel="noopener noreferrer">подписавшись на канал</a>. И попробуйте курсы на <a href="https://bit.ly/hkudemy" rel="noopener noreferrer">Udemy</a>.<br></p>		sushchnost	2021-08-01		
127	Полнота (Recall)		<p>Полнота (Отзыв, чувствительность, истинная положительная скорость) – это мера того, насколько правильно наша <a href="__GHOST_URL__/modiel/">Модель (Model)</a> идентифицирует истинно положительные наблюдения. Термин тесно связан с <a href="__GHOST_URL__/matritsa-oshibok/">Матрицей ошибок (Confusion Matrix)</a>. Например, для всех пациентов, которые на самом деле имеют сердечные заболевания, полнота говорит нам, сколько из них мы <em>правильно</em> определили как имеющих сердечные заболевания. </p><h3 id="%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5">Введение</h3><p>Спросите любого специалиста по <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинному обучению (ML)</a> о самых запутанных концепциях в их учебном пути, и неизменно ответ склоняется в сторону темы "<a href="__GHOST_URL__/tochnost-izmierienii/">Доля правильных ответов (Accuracy)</a> и полнота". Разницу между ними на самом деле легко запомнить, но только после того, как мы действительно поймем, что означает каждый термин. Довольно часто эксперты склонны давать запутанные объяснения, которые еще больше запутывают новичков. Итак, давайте поставим точки над 'i'.</p><h3 id="%D0%B4%D0%BE%D0%BB%D1%8F-%D0%BF%D1%80%D0%B0%D0%B2%D0%B8%D0%BB%D1%8C%D0%BD%D1%8B%D1%85-%D0%BE%D1%82%D0%B2%D0%B5%D1%82%D0%BE%D0%B2-%D0%B8-%D0%BF%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0">Доля правильных ответов и полнота</h3><p>Для любой модели соответствие реальности чрезвычайно важно. Это включает в себя достижение баланса между Недообучением (Underfitting) и <a href="__GHOST_URL__/pierieobuchieniie/">Переобучением (Overfitting)</a>, или, другими словами, Дилемма смещения-дисперсии (Bias-Variance Trade-off).</p><p>Однако, когда дело доходит до классификации, существует еще один компромисс, который часто упускается из виду в пользу вышеупомянутой дилеммы. Это дилемма Accuracy и Recall. Несбалансированные классы часто встречаются в наборах данных, и когда речь заходит о конкретных случаях использования, мы действительно хотели бы придать большее значение метрикам доли правильных ответов и полноты, а также тому, как достичь баланса между ними.</p><p>Но как это сделать? Мы рассмотрим метрики оценки классификации, сосредоточившись на Accuracy и полноте, а также узнаем, как вычислить эти метрики, освоив набор данных и простой алгоритм классификации. Итак, давайте начнем!</p><h3 id="%D0%BF%D0%BE%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0-%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8">Постановка задачи</h3><p>Я твердо верю в обучение на практике. Поэтому на протяжении всей этой статьи мы будем говорить в практическом плане – с помощью набора данных.</p><p>Давайте рассмотрим популярный набор данных о сердечных заболеваниях, доступный в репозитории UCI. Здесь мы должны предсказать, страдает ли пациент от сердечного заболевания.</p><p>Поскольку эта статья посвящена исключительно метрикам оценки моделей, мы будем использовать для прогнозирования самый простой классификатор – <a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метод K-ближайших соседей (kNN)</a>.</p><p>Как всегда, мы начнем с импорта необходимых библиотек и пакетов:</p><pre><code class="language-python">import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n%matplotlib inline</code></pre><p>Затем давайте посмотрим на данные и целевые переменные, с которыми мы имеем дело:</p><pre><code class="language-python">data_file_path = 'https://www.dropbox.com/s/88u0lwlme4xz6iz/heart.csv?dl=1'\ndata_df = pd.read_csv(data_file_path)\n\n#To get information on the number of entries and the datatypes of the features\ndata_df.head()</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/08/recall-heart-dataset.png" class="kg-image" alt loading="lazy" width="1546" height="376" srcset="__GHOST_URL__/content/images/size/w600/2021/08/recall-heart-dataset.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/recall-heart-dataset.png 1000w, __GHOST_URL__/content/images/2021/08/recall-heart-dataset.png 1546w" sizes="(min-width: 1200px) 1200px"></figure><p>Давайте проверим, есть ли у нас пропущенные значения:</p><pre><code class="language-python">data_df.isnull().sum()</code></pre><p>Пропущенных значений нет:</p><pre><code class="language-python">age         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ntarget      0\ndtype: int64</code></pre><p>Теперь мы можем посмотреть, сколько пациентов на самом деле страдают от сердечных заболеваний (1), а сколько нет (0):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/recall-heart-disease-patients.jpg" class="kg-image" alt loading="lazy" width="476" height="365"></figure><p>Давайте продолжим, разделив наши обучающие и тестовые данные на <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a> и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a>. Поскольку мы используем kNN, необходимо также подвергнуть датасет <a href="__GHOST_URL__/standartizatsiia/">Стандартизации (Standartization)</a>:</p><pre><code class="language-python">y = data_df["target"].values\nx = data_df.drop(["target"], axis = 1)\n\n# Стандартизация обязательна для kNN\nss = StandardScaler()\nx = ss.fit_transform(x)\n\n# Разделение датасета на тренировочную и тестовую части\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3) # 70% training and 30% test\n</code></pre><p>Интуиция, стоящая за выбором наилучшего значения k, выходит за рамки этой статьи, но мы должны знать, что можем определить оптимальное значение k, когда получим самый высокий тестовый балл для этого значения. Для этого мы можем оценить результаты обучения и тестирования до 20 ближайших соседей:</p><pre><code class="language-python">train_score = []\ntest_score = []\nk_vals = []\n\nfor k in range(1, 21):\n    k_vals.append(k)\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    \n    tr_score = knn.score(X_train, y_train)\n    train_score.append(tr_score)\n    \n    te_score = knn.score(X_test, y_test)\n    test_score.append(te_score)</code></pre><p>Чтобы оценить максимальный тестовый балл и связанные с ним значения k, выполните следующую команду:</p><pre><code class="language-python">max_test_score = max(test_score)\ntest_scores_ind = [i for i, v in enumerate(test_score) if v == max_test_score]\nprint('Максимальный тестовый скор {} and k = {}'.format(max_test_score * 100, list(map(lambda x: x + 1, test_scores_ind))))</code></pre><p>Таким образом, мы получили оптимальное значение k, равное 10 с оценкой 85,7:</p><pre><code class="language-python">Максимальный тестовый скор: 85.71428571428571 and k = [10]</code></pre><p>Мы доработаем одно из этих значений и соответствующим образом подгоним модель:</p><pre><code class="language-python">knn = KNeighborsClassifier(10)\n\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)</code></pre><p>Мы получаем хороший результат, на первый взгляд:</p><pre><code class="language-python">0.8571428571428571</code></pre><p>Но как мы оцениваем, является ли эта модель "хорошей" или нет? Для этого мы используем матрицу ошибок:</p><pre><code class="language-python">y_pred = knn.predict(X_test)\nconfusion_matrix(y_test, y_pred)\npd.crosstab(y_test, y_pred, rownames = ['Фактические'], colnames =['Предсказанные'], margins = True)</code></pre><p>Матрица путаницы помогает нам понять, насколько верны были наши прогнозы и насколько они соответствуют реальным значениям:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/recall-confusion-matrix.png" class="kg-image" alt loading="lazy" width="421" height="310"></figure><p>Мы уже знаем, что наши тестовые данные состояли из 91 <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a>. Это значение 4-й строки и 4-го столбца в конце. Мы также замечаем, что существуют некоторые фактические и прогнозируемые значения. Фактические значения – это количество точек данных, которые первоначально были разделены на 0 или 1. Прогнозируемые значения – это количество точек данных, которые наша модель kNN предсказала как 0 или 1.</p><p>Фактические значения:</p><ul><li>Пациенты, у которых на самом деле нет сердечных заболеваний = 41</li><li>Пациенты, у которых на самом деле есть болезнь сердца = 50</li></ul><p>Прогнозируемые значения таковы:</p><ul><li>Число пациентов, которым было предсказано, что у них нет сердечных заболеваний = 40</li><li>Количество пациентов, у которых было предсказано наличие заболевания сердца = 51</li></ul><p>Все значения, которые мы получаем выше, имеют термины. Давайте пройдемся по ним один за другим:</p><ul><li>Случаи, когда у пациентов на самом деле не было болезни сердца и наша модель так и предсказывала, называются истинно отрицательными предсказаниями (True Negatives). Их 33.</li><li>Случаи, когда у пациентов действительно есть болезнь сердца, и наша модель также предсказывает, что она есть, называются Истинно позитивное (True Positives). В нашей матрице таких значений 43.</li><li>Однако есть некоторые случаи, когда у пациента на самом деле нет сердечных заболеваний, но наша модель предсказала, что они есть. Этот вид ошибки является ошибкой типа I, и мы называем значения Ложно позитивными (False Positives). Для нашей матрицы Ложных срабатываний 8.</li><li>Точно так же есть случаи, когда у пациента действительно есть болезнь сердца, но наша модель предсказала, что это не так. Этот вид ошибки является Ошибкой типа II, и мы называем значения Ложно отрицательными (False Negatives). Для нашей матрицы таких 7.</li></ul><h3 id="%D1%87%D1%82%D0%BE-%D1%82%D0%B0%D0%BA%D0%BE%D0%B5-%D0%B4%D0%BF%D0%BE">Что такое ДПО?</h3><p>Итак, теперь мы подошли к сути этой статьи. Что такое Доля правильных ответов (ДПР)? И какое отношение к этому имеет все вышеперечисленное обучение?</p><p>В простейших терминах, доля правильных ответов – это соотношение между Истинно позитивными и всеми позитивными предсказаниями. В нашем случае это было бы частным от числа пациентов, которых мы правильно идентифицируем, к общему числу пациентов, которые действительно больны. Рассчитывается с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$ДПО\\space{=}\\space{\\frac{TP}{TP + FP}},\\space{где}$$<br>\n$$TP\\space{–}\\space{истинно}\\space{позитивные}\\space{предсказания}$$<br>\n$$FP\\space{–}\\space{ложно}\\space{позитивные}\\space{предсказания}$$</p>\n<!--kg-card-end: markdown--><p>Какова Accuracy нашей модели? Она равна примерно 84%. Важно, чтобы мы не начинали лечить пациента, у которого на самом деле нет сердечного заболевания, из-за результатов модели.</p><h3 id="%D1%87%D1%82%D0%BE-%D1%82%D0%B0%D0%BA%D0%BE%D0%B5-%D0%BF%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0">Что такое полнота?</h3><p>Полнота – это мера того, насколько правильно наша модель идентифицирует Истинные Положительные моменты. Таким образом, для всех пациентов, которые на самом деле имеют сердечные заболевания, вспоминание говорит нам, сколько из них мы правильно определили как имеющие сердечные заболевания. Рассчитывается с помощью формулы:</p><!--kg-card-begin: markdown--><p>$$Полнота\\space{=}\\space{\\frac{TP}{TP + FN}},\\space{где}$$<br>\n$$TP\\space{–}\\space{истинно}\\space{позитивные}\\space{предсказания}$$<br>\n$$FN\\space{–}\\space{ложно}\\space{отрицательные}\\space{предсказания}$$</p>\n<!--kg-card-end: markdown--><p>Для нашей модели, полнота равна 86%. Он также определяет, насколько точно наша модель способна идентифицировать соответствующие данные. Мы называем это чувствительностью, или истинной положительной скоростью. Что делать, если у  есть болезнь сердца, но ему не дают никакого лечения, потому что наша модель предсказала это? Именно такой ситуации мы и хотели бы избежать!</p><h3 id="%D1%81%D0%B0%D0%BC%D0%B0%D1%8F-%D0%BF%D1%80%D0%BE%D1%81%D1%82%D0%B0%D1%8F-%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B0-%D0%B4%D0%BB%D1%8F-%D0%BF%D0%BE%D0%BD%D0%B8%D0%BC%D0%B0%D0%BD%D0%B8%D1%8F-%E2%80%93-%D0%B4%D0%BE%D0%BB%D1%8F-%D0%BF%D1%80%D0%B0%D0%B2%D0%B8%D0%BB%D1%8C%D0%BD%D1%8B%D1%85-%D0%BE%D1%82%D0%B2%D0%B5%D1%82%D0%BE%D0%B2">Самая простая метрика для понимания – доля правильных ответов</h3><p>Теперь мы подошли к одной из самых простых метрик – доля правильных ответов, отношения общего числа правильных предсказаний и общего числа предсказаний. Рассчитывается по формуле:</p><!--kg-card-begin: markdown--><p>$$ДПО\\space{=}\\space{\\frac{TP}{TP + FN}},\\space{где}$$<br>\n$$TP\\space{–}\\space{истинно}\\space{позитивные}\\space{предсказания}$$<br>\n$$FN\\space{–}\\space{ложно}\\space{отрицательные}\\space{предсказания}$$</p>\n<!--kg-card-end: markdown--><p>Для нашей модели ДПО равна ~83%. Использование accuracy в качестве определяющей метрики имеет смысл, но желательно использовать ДПО и полноту. Могут быть и другие ситуации, когда наша accuracy очень высока, но полнота низка. В идеале для нашей модели мы хотели бы полностью избежать любых ситуаций, когда у пациента есть болезнь сердца, но наша модель классифицирует его как не имеющего ее.</p><p>С другой стороны, в тех случаях, когда пациент не страдает сердечными заболеваниями и наша модель предсказывает обратное, мы также хотели бы избежать лечения здорового человека (особенно важно, когда входные параметры могут указывать на другое заболевание, но в конечном итоге мы лечим его / ее от сердечного заболевания).</p><p>Хотя мы стремимся к высокой accuracy и большому значению полноты, достичь того и другого одновременно невозможно. Например, если мы изменим модель на ту, которая дает нам высокую отзывчивость, мы можем обнаружить всех пациентов, у которых действительно есть сердечные заболевания, но в конечном итоге мы можем дать лечение многим пациентам, которые не страдают от них.</p><p>Точно так же, если мы стремимся к высокой accuracy, чтобы избежать любого неправильного и ненужного лечения, мы в конечном итоге получаем много пациентов, у которых на самом деле есть болезнь сердца, протекающая без какого-либо лечения.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1Bbnj3AZzYXtGSmUvZ5oZ_rMQwNEPSgD0?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/">Purva Huilgol</a></p>		otzyv	2021-08-04		
128	Tableau		<p>Tableau – это платформа визуальной аналитики, которая меняет способ использования данных для решения проблем, позволяя людям и организациям максимально использовать данные. Эта аналитическая платформа облегчает людям изучение данных и управление ими, а также ускоряет обнаружение и обмен идеями, которые могут изменить бизнес-процессы.</p><p>Миссия Tableau – помогать людям видеть и понимать данные, поэтому продукт предназначен для аналитиков, <a href="__GHOST_URL__/data-saiientist/">Дата-сайентистов (Data Scientist)</a>, студентов, преподавателей и руководителей. Это самая мощная, безопасная и гибкая платформа сквозной аналитики.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/08/--------------2021-08-07---22.20.14.png" class="kg-image" alt loading="lazy" width="2000" height="1099" srcset="__GHOST_URL__/content/images/size/w600/2021/08/--------------2021-08-07---22.20.14.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/--------------2021-08-07---22.20.14.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/08/--------------2021-08-07---22.20.14.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/08/--------------2021-08-07---22.20.14.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption>Консоль проекта о вовлеченности пользователей</figcaption></figure><p>Tableau была основана в 2003 году в результате проекта по информатике в Стэнфорде, целью которого было улучшить поток анализа и сделать данные более доступными для людей с помощью визуализации. Соучредители Крис Столте (Chris Stolte), Пэт Ханрахан (Pat Hanrahan) и Кристиан Шабо (Christian Chabot) разработали и запатентовали основную технологию VizQL, которая визуально выражает данные, переводя запросы данных в интуитивно понятный интерфейс.</p><p>Консоль использует <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a>, статистику, естественный язык и полуавтоматизированную подготовку данных для повышения творческих способностей человека в анализе. И Tableau предлагает не только полную интегрированную аналитическую платформу, но и проверенные вспомогательные ресурсы, чтобы помочь клиентам развернуть и масштабировать культуру, основанную на данных, которая обеспечивает устойчивость и ценность за счет значительных результатов.<br> Tableau был приобретен Salesforce в 2019 году.</p><p>Краткий обзор интерфейса:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/_R0KHeoKwCc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>Автор оригинальной статьи: <a href="https://www.tableau.com/why-tableau/what-is-tableau">tableau.com</a></p>		tableau	2021-08-07		
129	Альтернативная гипотеза (Alternative Hypothesis)		<p>Альтернативная гипотеза – предположение, которое предстоит тестировать на истинность в рамках статистического анализа. Для наглядности ее, как правило, иллюстрируют примером в сочетании с Нулевой гипотезой (Null Hypothesis).</p><p>Пример. Производитель мыла утверждает, что ее продукт убивает в среднем 99% микробов. Чтобы проверить заявление этой компании, мы сформулируем нулевую и альтернативную гипотезы.</p><p>Нулевая: Среднестатистическое количество уничтоженных мылом микробов равно 99%.</p><p>Альтернативная: Мыло в среднем уничтожает менее 99% процентов микробов.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/null-hypothesis_definitive_definitive.png" class="kg-image" alt loading="lazy" width="1000" height="588" srcset="__GHOST_URL__/content/images/size/w600/2021/03/null-hypothesis_definitive_definitive.png 600w, __GHOST_URL__/content/images/2021/03/null-hypothesis_definitive_definitive.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>Мы доверяем нулевой гипотезе до тех пор, пока в <a href="__GHOST_URL__/vyborka/">Выборке (Sample)</a> достаточно доказательств, подтверждающих ее истинность. В ином случае мы отвергаем нулевую гипотезу и поддерживаем альтернативную. Если выборка не может предоставить достаточных доказательств для того, чтобы отвергнуть нулевую гипотезу, нам придется изучить несколько других выборок или всю <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральную совокупность (Population)</a>.</p>		altiernativnaia-ghipotieza	2021-08-08		
130	Поэлементная кросс-валидация (LOOCV)		<p>Поэлементная кросс-валидация (Leave-One-Out Cross Validation) – разновидность проверки обобщающей способности <a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модели (Model)</a><a href="__GHOST_URL__/mashinnoie-obuchieniie/" rel="noopener noreferrer">Машинного обучения (ML)</a>, использующая множественные сплиты на <a href="__GHOST_URL__/trienirovochnyie-dannyie/" rel="noopener noreferrer">Тренировочные данные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/" rel="noopener noreferrer">Тестовые данные (Test Data)</a>.</p><p>Чтобы оценить производительность модели в наборе данных, нам нужно измерить, насколько хорошо прогнозы, сделанные ею, соответствуют наблюдаемым данным.</p><p>Самый распространенный способ измерить это – использовать <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратическую ошибку (MSE)</a>, которая рассчитывается следующим образом:</p><!--kg-card-begin: markdown--><p>$$MSE = \\frac{1}{n} × \\sum_{i=1}^n (y_i - \\widetilde{y}_i)^2$$<br>\n$$MSE\\space{}{–}\\space{Среднеквадратическая}\\space{ошибка,}$$<br>\n$$n\\space{}{–}\\space{количество}\\space{наблюдений,}$$<br>\n$$y_i\\space{}{–}\\space{фактическая}\\space{координата}\\space{наблюдения,}$$<br>\n$$\\widetilde{y}_i\\space{}{–}\\space{предсказанная}\\space{координата}\\space{наблюдения,}$$</p>\n<!--kg-card-end: markdown--><p>На практике мы используем следующую последовательность для расчета этой метрики:</p><ul><li>Разделим набор данных на тренировочные и тестовые данные:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/image-2.png" class="kg-image" alt loading="lazy" width="768" height="855" srcset="__GHOST_URL__/content/images/size/w600/2021/08/image-2.png 600w, __GHOST_URL__/content/images/2021/08/image-2.png 768w" sizes="(min-width: 720px) 720px"></figure><ul><li>Построим модель, используя только данные из обучающей выборки:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/image-3.png" class="kg-image" alt loading="lazy" width="768" height="585" srcset="__GHOST_URL__/content/images/size/w600/2021/08/image-3.png 600w, __GHOST_URL__/content/images/2021/08/image-3.png 768w" sizes="(min-width: 720px) 720px"></figure><ul><li>Используем модель, чтобы делать прогнозы на тестовой выборке и измерять среднеквадратическую ошибку.</li></ul><p>Тестовая MSE дает нам представление о том, насколько хорошо модель будет работать с данными, которые она ранее не видела, то есть данными, которые не использовались для обучения.</p><p>Однако недостатком использования только одного набора тестирования является то, что тестовая MSE может сильно различаться в зависимости от того, какие <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> использовались в наборах для обучения и тестирования.</p><p>Один из способов избежать этой проблемы – "подогнать" модель несколько раз, используя новые наборы для обучения и тестирования каждый раз, а затем вычислить среднее значение всех тестовых MSE.</p><p>Этот общий метод известен как <a href="__GHOST_URL__/kross-validatsiia/">Перекрестная проверка (Cross Validation)</a>, а одна его конкретная форма – как <em>Поэлементная кросс-валидация</em>. LOOCV использует следующий подход для оценки модели:</p><ul><li>Разделим набор данных на обучающий и тестовый наборы, используя все наблюдения, кроме одного, как часть обучающего набора:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/image-4.png" class="kg-image" alt loading="lazy" width="768" height="834" srcset="__GHOST_URL__/content/images/size/w600/2021/08/image-4.png 600w, __GHOST_URL__/content/images/2021/08/image-4.png 768w" sizes="(min-width: 720px) 720px"></figure><p>Обратите внимание: мы оставляем только одно наблюдение «за пределами» обучающей выборки. Именно потому метод получил такое название.</p><ul><li>Построим модель, используя только данные из обучающей выборки:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/image-5.png" class="kg-image" alt loading="lazy" width="768" height="583" srcset="__GHOST_URL__/content/images/size/w600/2021/08/image-5.png 600w, __GHOST_URL__/content/images/2021/08/image-5.png 768w" sizes="(min-width: 720px) 720px"></figure><ul><li>Используем модель, чтобы предсказать значение для того самого последнего тестового наблюдения, и вычислить MSE:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/image-6.png" class="kg-image" alt loading="lazy" width="768" height="579" srcset="__GHOST_URL__/content/images/size/w600/2021/08/image-6.png 600w, __GHOST_URL__/content/images/2021/08/image-6.png 768w" sizes="(min-width: 720px) 720px"></figure><ul><li>Наконец, мы повторяем этот процесс n раз (где n –- общее количество наблюдений в наборе), каждый раз исключая разные наблюдения.</li></ul><p>Напоследок, мы вычисляем среднее значение MSE.</p><h3 id="%D0%BF%D0%BB%D1%8E%D1%81%D1%8B-%D0%B8-%D0%BC%D0%B8%D0%BD%D1%83%D1%81%D1%8B-loocv">Плюсы и минусы LOOCV</h3><p>Поэлементная кросс-валидация обладает следующими преимуществами:</p><ul><li>Она обеспечивает гораздо менее предвзятую оценку тестовой MSE по сравнению с классической кросс-валидацией с единственным тестовым набором, поскольку мы неоднократно подгоняем модель.</li><li>Как правило, тестовая средняя MSE более показательна.</li></ul><p>Однако метод имеет следующие недостатки:</p><ul><li>Это может занять много времени при большом числе записей</li><li>Это также может занять много времени, если модель особенно сложна</li><li>Это может быть дорогостояще в вычислительном отношении.</li></ul><p>К счастью, современные вычисления стали настолько эффективными в большинстве областей, что LOOCV – гораздо более разумный метод для использования по сравнению с тем, что было много лет назад.</p><p>Обратите внимание, что LOOCV можно использовать как в <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a>, так и в <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>. Для задач регрессии он вычисляет тестовую MSE как среднеквадратичную разницу между предсказаниями и наблюдениями, тогда как в задачах классификации он вычисляет тестовую MSE как процент наблюдений, правильно классифицированных во время n повторных подгонок модели.</p><h3 id="loocv-scikit-learn">LOOCV: Scikit-learn</h3><p>Давайте посмотрим, как поэлементная кросс-валидация реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport sklearn\nfrom sklearn.model_selection import LeaveOneOut</code></pre><p>Сгенерируем небольшой Датасет (Dataset), инициализируем метод LeaveOneOut и создадим один сплит на обучающее и тестовое наблюдения:</p><pre><code class="language-python"># Предикторы\nX = np.array([[1, 2], [3, 4]])\n# Цедевая переменная\ny = np.array([1, 2])\n\nloo = LeaveOneOut()\nloo.get_n_splits(X)</code></pre><p>Метод <code>get_n_splits()</code> сообщает, ко всему прочему, число наблюдений:</p><pre><code class="language-python">2</code></pre><p>Теперь запустим классификацию для первого и второго сплитов, то есть определим дважды, к какому классу какой список внутри X относится: </p><pre><code class="language-python">for train_index, test_index in loo.split(X):\n    print("Тренировка:", train_index, "Тест:", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    print(X_train, X_test, y_train, y_test)</code></pre><p>Модель перекрестно прошлась по всем наблюдениям и безошибочно классифицировала пару <code>[3, 4]</code> как запись класса 2, а <code>[1, 2]</code> как запись класса 1:</p><pre><code class="language-python">Тренировка: [1] Тест: [0]\n[[3 4]] [[1 2]] [2] [1]\nТренировка: [0] Тест: [1]\n[[1 2]] [[3 4]] [1] [2]</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://drive.google.com/file/d/11aulHgs4ovjXx_eNGZdK908ai0LE9dtn/view?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.statology.org/leave-one-out-cross-validation/">Zach</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html">scikit-learn.org</a></p>		poeliemientnaia	2021-08-12		
131	Намерение (Intent)		<p>Намерение – одна из центральных концепций при разработке <a href="__GHOST_URL__/chat-bot/">Чат-ботов (Chatbot)</a>, совокупность алгоритмов, осуществляемых системой, распознавшей потребность пользователя.</p><h3 id="%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%82%D1%83%D1%80%D0%B0">Структура</h3><p>В зависимости от платформы-конструктора или API структура намерения разнится. Однако с практикой сформировались характерные компоненты. Рассмотрим их на примере DialogFlow – популярного продукта Google.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/--------------2021-08-12---22.58.15.png" class="kg-image" alt loading="lazy" width="1576" height="1544" srcset="__GHOST_URL__/content/images/size/w600/2021/08/--------------2021-08-12---22.58.15.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/--------------2021-08-12---22.58.15.png 1000w, __GHOST_URL__/content/images/2021/08/--------------2021-08-12---22.58.15.png 1576w" sizes="(min-width: 720px) 720px"></figure><ul><li>Контексты (Contexts): позволяют настроить сложную многоуровневую связь между намерениями. Например, создав контекстом awaitingPhone связку между намерением "Запрос имени" и "Запрос телефона" мы обяжем систему воспринимать числовой ввод как телефон. Вкупе с Последующими намерениями (Follow-up Intents), позволяет реализовывать сложную логику диалога с множеством похожих точек ветвления.</li><li>События (Events): создаваемое разработчиком на бэкенде событие <code>checkPhone</code> (к примеру, проверка телефона на наличие в базе компании), позволяет исполнить стороннюю логику с использованием номера телефона. Термин тесно связан с понятием <a href="__GHOST_URL__/viebkhuk/">Вебхук (Webhook)</a>.</li><li>Тренировочные фразы (Training Phrases): раздел, принимающий примеры пользовательского ввода и позволяющий непосредственно использовать <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a> для поиска закономерностей. Достойно работает на больших объемах обучающих данных.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/--------------2021-08-12---22.58.34.png" class="kg-image" alt loading="lazy" width="1576" height="1406" srcset="__GHOST_URL__/content/images/size/w600/2021/08/--------------2021-08-12---22.58.34.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/--------------2021-08-12---22.58.34.png 1000w, __GHOST_URL__/content/images/2021/08/--------------2021-08-12---22.58.34.png 1576w" sizes="(min-width: 720px) 720px"></figure><ul><li>Действия и параметры (Action and parameters): поле 'Enter action name' значение задается пользователем вручную или генерируется с помощью названий контекстов. Позволяет реализовывать кастомную логику с помощью встроенного редактора Inline Editor в разделе Fulfillment.</li><li>Ответы (Responses): ответы виртуального ассистента в случае успешного распознавания пользовательского ввода.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/--------------2021-08-12---23.02.15.png" class="kg-image" alt loading="lazy" width="1726" height="376" srcset="__GHOST_URL__/content/images/size/w600/2021/08/--------------2021-08-12---23.02.15.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/--------------2021-08-12---23.02.15.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/08/--------------2021-08-12---23.02.15.png 1600w, __GHOST_URL__/content/images/2021/08/--------------2021-08-12---23.02.15.png 1726w" sizes="(min-width: 720px) 720px"></figure><ul><li>Исполнение (Fulfillment): свитчер 'Enable webhook call for this intent' позволяет обратиться к кастомной логике разработчика и заполнить другой параметр определенным значением помимо телефона (к примеру, статус активности номера).</li></ul>		namierieniie	2021-08-14		
132	Тепловая карта (Heatmap)		<p>Тепловая карта – популярный способ визуализации закономерностей в <a href="__GHOST_URL__/dataset/">Датасете (Dataset)</a>, использующий сетку n на m, где в каждой ячейке – <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее арифметическое значение (Mean)</a> соответствующей группы <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>.</p><p><strong>Среднее геометрическое: Seaborn</strong></p><p>Тепловая карта по умолчанию – это соответсвующий класс библиотеки Seaborn. Посмотрим, как она выглядит в таком исполнении. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import seaborn as sns</code></pre><p>Используем встроенный датасет <code>flights</code> с графиками полетов по годам. Запросим методом <code>pivot()</code> такое представление карты, что на осях x и расположатся годы и месяцы соответственно, а основанием для расчета оттенка ячейки станет <a href="__GHOST_URL__/priznak/">Признак (Feature)</a> "число пассажиров" ("passengers"):</p><pre><code class="language-python">flights = sns.load_dataset("flights")\nflights = flights.pivot("month", "year", "passengers")\nax = sns.heatmap(flights)</code></pre><p>График демонстрирует в том числе и сниженную частоту полетов в июле и августе (это датасет родом из страны с комфортным летом: тогда жители предпочитают отдыхать дома). Более того, в период с 1949 по 1960 летающие предпочитали бронировать билеты на первую половину месяца:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/08/heatmap.jpg" class="kg-image" alt loading="lazy" width="463" height="364"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1BUjpyIxWnAmdbmRZjyoUCovFp4d1XwdE?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://seaborn.pydata.org/generated/seaborn.heatmap.html">seaborn.pydata.org</a></p>		tieplovaia-karta	2021-08-15		
133	Софтмакс (Softmax)		<p>Софтмакс – функция, превращающая логиты (наборы чисел) в вероятности, причем сумма последних равна единице. Функция выводит в качестве результата вектор, представляющий распределения вероятностей списка потенциальных результатов. Это также основной элемент, используемый в задачах <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a> <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокого обучения (Deep Learning)</a>. </p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/08/softmax.jpg" class="kg-image" alt loading="lazy" width="2000" height="800" srcset="__GHOST_URL__/content/images/size/w600/2021/08/softmax.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/08/softmax.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2021/08/softmax.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2021/08/softmax.jpg 2400w"><figcaption>Функция превращает логиты [2.0, 1.0, 0.1] в вероятности [0.7, 0.2, 0.1]</figcaption></figure><p>В глубоком обучении термин "логитовый слой" обычно используется для последнего слоя <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сети (Neural Network)</a> задач классификации, которая преобразует необработанные значения прогноза в виде действительных чисел в диапазоне от [-∞, +∞]. Логиты – это необработанные результаты, полученные на последнем уровне нейронной сети до того, как произойдет активация.<br><br>Softmax превращает логиты в вероятности, получая экспоненту e каждого значения, а затем подвергая <a href="__GHOST_URL__/normalizatsiia/">Нормализации (Normalization)</a> каждое e, то есть разделяя на их сумму, чтобы сумма всех экспонент равнялась единице. </p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/08/softmax-relu-fcnn-1.jpg" class="kg-image" alt loading="lazy" width="995" height="603" srcset="__GHOST_URL__/content/images/size/w600/2021/08/softmax-relu-fcnn-1.jpg 600w, __GHOST_URL__/content/images/2021/08/softmax-relu-fcnn-1.jpg 995w"></figure><p>Как показано выше, входные данные Softmax являются выходными для Полносвязного слоя (Fully Connected Layer), непосредственно предшествующего ему, и являются выходными данными всей нейронной сети. Эти выходные данные представляют собой распределение вероятностей всех кандидатов в классы меток.</p><p>Softmax – это не черный ящик. Он состоит из двух компонентов: специальное число e для некоторой степени, деленной на определенную сумму.<br>y_i относится к каждому элементу в векторе логитов y. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np</code></pre><p>Создадим игрушечный логитовый слой:</p><pre><code class="language-python">logits = [2.0, 1.0, 0.1]\nexps = [np.exp(i) for i in logits]</code></pre><p>Мы используем <code>np.exp(power)</code>, чтобы возвести число <code>e</code> (~2,718) к любой желаемой степени. Мы перебираем каждую степень i и вычисляем экспоненту. Результат сохраняется в списке под названием exps (экспоненты).</p><p>Почему бы просто не разделить каждый логит на их сумму? Зачем нужны экспоненты? Логиты – это логарифм шансов, он лежит в пределах от минус- до плюс-бесконечности. Когда логиты отрицательны или обладают разными знаками, правильной нормализации выполнить не удастся. Возведение в степень решает эту проблему.</p><pre><code class="language-python">print(np.exp(100))\nprint(np.exp(-100))\nprint(3.720076e-44 &gt; 0)</code></pre><p>Кстати, число Эйлера <code>e</code> также упрощает дальнейшие вычисления. Логарифм произведений можно легко превратить в суммы для удобного вычисления производной: <code>log(a * b) = log(a) + log(b)</code>.</p><p>Замена <code>i</code> на <code>logit</code> – еще один подробный способ: <code>exps = [np.exp (logit) for logit in logits</code>. Обратите внимание на использование существительных во множественном и единственном числе. Это сделано намеренно.</p><p>Мы только что вычислили делимое функции Softmax. Для каждого логита мы взяли экспоненциальную степень e. Каждый преобразованный логит j необходимо нормализовать, чтобы сумма всех конечных результатов, которые являются вероятностями, была равна единице. </p><p>Мы вычисляем сумму всех преобразованных логитов и сохраняем эту сумму в одной переменной  <code>sum_of_exps</code>, которую мы будем использовать для нормализации.</p><pre><code class="language-python">sum_of_exps = sum(exps)</code></pre><p>Теперь мы готовы написать последнюю часть нашей функции Softmax: каждый преобразованный логит должен быть нормализован с помощью <code>sum_of_exps</code>:</p><pre><code class="language-python">softmax = [j/sum_of_exps for j in exps]</code></pre><p>Мы захватываем каждый преобразованный логит, используя <code>[j for j in exps]</code>, делим каждый <code>j</code> на <code>sum_of_exps</code>. Это нам список:</p><pre><code class="language-python">print(softmax)\nprint(sum(softmax))</code></pre><p>Сумма элементов равна единице:</p><pre><code class="language-python">[0.6590011388859679, 0.2424329707047139, 0.09856589040931818]\n1.0</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1tNGU2PcmVhN-RgVWmzGnYpJ7FOkFj7vr?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d">Uniqtech</a></p>		softmaks	2021-08-20		
134	Пропуск (Omission)		<p>Пропуск – пример принципа Неопределенности (Uncertainty) в Машинном обучении (ML), отсутствующее значение ячейки:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/08/omission.png" class="kg-image" alt loading="lazy" width="2000" height="1000" srcset="__GHOST_URL__/content/images/size/w600/2021/08/omission.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/omission.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/08/omission.png 1600w, __GHOST_URL__/content/images/2021/08/omission.png 2324w" sizes="(min-width: 1200px) 1200px"><figcaption>Пропуски в ячейках B7, D4 и E6</figcaption></figure><h3 id="%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0-%D0%BF%D1%80%D0%BE%D0%BF%D1%83%D1%81%D0%BA%D0%BE%D0%B2">Обработка пропусков</h3><p>Поскольку <a href="__GHOST_URL__/modiel/">Модели (Model)</a> выдвигают требование (необходим полный датасет), выработалось несколько способов обработать – заполнить ячейки с отсутствующими значениями:</p><ul><li>Заполнение <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Средним арифметическим (Mean)</a> для числовых <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, например, для столбца "Длительность звонка" на иллюстрации выше</li><li>Заполнение <a href="__GHOST_URL__/moda/">Модой (Mode)</a> для <a href="__GHOST_URL__/katieghorialnaia-pieriemiennaia/">Категориальных переменных (Categorical Variable)</a> для столбца "Образование"</li><li>Удаление неполных записей (избавиться от строк 4, 6 и 7)</li><li>Заполнение предыдущим / последующим значением (метод Pandas <code>fillna()</code>) и прочие.</li></ul>		propusk	2021-08-21		
135	Kaggle		<p>Kaggle (kaggle.com) – популярная веб-среда для разработки, специализирующаяся на <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a>. К основным достоинствам функционала относят:</p><ul><li>Бесплатная исполняемая среда для разработки и отладки кода в формате <a href="__GHOST_URL__/noutbuk/">Ноутбука (Notebook)</a></li><li>Успешная система стимулирования пользователей создавать контент и поддерживать дискуссии в комментариях (см. <a href="Kaggle Progression System">Система прогрессии Kaggle</a>)</li><li>Открытая база датасетов</li><li>Площадка для соревнований в области Мащинного обучения, <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a> и прочих связанных областей</li><li>Курсы</li></ul><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/08/kaggle-rachael-tatman-profile.jpg" class="kg-image" alt loading="lazy" width="1052" height="751" srcset="__GHOST_URL__/content/images/size/w600/2021/08/kaggle-rachael-tatman-profile.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/08/kaggle-rachael-tatman-profile.jpg 1000w, __GHOST_URL__/content/images/2021/08/kaggle-rachael-tatman-profile.jpg 1052w"><figcaption>Регалии сотрудницы Kaggle Рэйчел Тэтман</figcaption></figure><p>Чтобы "общаться с продуктом на ты", рекомендую <a href="https://www.youtube.com/playlist?list=PLqFaTIg4myu8gbDh6oBl7XRYNBlthpDEW">вводный плейлист Тэтман</a> в соавторстве с другими сотрудниками компании.  </p>		kaggle	2021-08-22		
136	Классификация (Classification)		<p>Классификация – это крупнейшая задача <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, которая ставит своей целью назначить метку класса <a href="__GHOST_URL__/nabliudieniie/">Наблюдениям (Observation)</a> из предметной области, например, сортировка электронных писем на "спам" и "не спам".</p><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%BE%D0%B5-%D0%BF%D1%80%D0%BE%D0%B3%D0%BD%D0%BE%D0%B7%D0%BD%D0%BE%D0%B5-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5">Классификационное прогнозное моделирование</h3><p>В машинном обучении классификация относится к задаче прогнозного моделирования, когда метка класса прогнозируется для данного примера входных данных.</p><p>Примеры проблем классификации:</p><ul><li>Классифицировать рукописный символ как букву или цифру (Handwriting Recognition)</li><li>Учитывая недавнее поведение пользователя, предсказать, откажется он от сервиса сайта или нет – <a href="__GHOST_URL__/modielirovaniie-ottoka/">Моделирование оттока (Churn Modeling)</a></li></ul><p>Для классификации требуется обучающий набор с множеством записей, из которых можно учиться.</p><p><a href="__GHOST_URL__/modiel/">Модель (Model)</a> будет использовать набор обучающих данных и вычислит, как лучше всего сопоставить примеры входных данных с конкретными метками классов. Таким образом, <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные данные (Train Data)</a> должны быть достаточно репрезентативным для проблемы и иметь достаточно примеров каждой метки класса.</p><p>Метки классов часто являются строковыми значениями, например «Спам», «не спам» и будут "конвертированы" в числовые значения, прежде чем будут переданы в алгоритм моделирования. Это называют Кодированием (Encoding), когда каждой метке класса присваивается уникальное целое число, например «Спам» = 0, «не спам» = 1.</p><p>Алгоритмы классификационного прогнозного моделирования оцениваются на основе их результатов. <a href="__GHOST_URL__/tochnost-izmierienii/">Точность измерений (Accuracy)</a> – это популярный показатель, используемый для оценки производительности модели на основе предсказанных меток классов. Точность классификации не идеальна, но это хорошая отправная точка для многих задач классификации.</p><p>Вместо меток классов для некоторых задач может потребоваться прогнозирование вероятности "членства" в классе для того или иного наблюдения. Это обеспечивает дополнительную неопределенность в прогнозе, которую затем может использовать программное обеспечение. Популярной диагностикой для оценки предсказанных вероятностей является ROC-кривая (ROC AUC).</p><p>Вы можете встретиться с четырьмя основными типами классификации:</p><ul><li>Бинарная классификация (Binary Classification)</li><li>Мультиклассовая классификация (Multi-Class Classification)</li><li>Классификация по нескольким меткам (Multi-Label Classification)</li><li>Несбалансированная классификация (Imbalanced Classification)</li></ul><h3 id="%D0%B1%D0%B8%D0%BD%D0%B0%D1%80%D0%BD%D0%B0%D1%8F-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">Бинарная классификация</h3><p>Двоичная классификация предполагает два возможных класса меток. Примеры:</p><ul><li>Обнаружение спама в электронной почте (спам или нет)</li><li>Прогнозирование оттока (отток или нет)</li><li>Прогноз конверсии (купит или нет)</li></ul><p>Обычно такие задачи включают один класс, который является нормальным состоянием, и другой, который является ненормальным.</p><p>Например, «не спам» – это нормальное состояние, а «спам» – ненормальное состояние. Другой пример: «рак не обнаружен» – это нормальное состояние задачи медицинской диагностики, а «рак обнаружен» – это ненормальное состояние. Классу для нормального состояния присваивается метка 0, а классу с ненормальным состоянием – 1.</p><p>Модель в этом случае предсказывает распределение вероятностей Бернулли для каждого примера. К примеру, в случае с диагностикой пациентов <a href="__GHOST_URL__/alghoritm/">Алгоритм (Algorithm)</a> сообщит, что пациент № 3049 с вероятностью 85% болен раком.</p><p>Популярные алгоритмы, которые можно использовать для двоичной классификации:</p><ul><li><a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/">Логистическая регрессия (Logistic Regression)</a></li><li><a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метод K-ближайших соседей (k-Nearest Neighbours)</a></li><li><a href="__GHOST_URL__/dierievo-rieshienii/">Дерево решений (Decision Tree)</a></li><li><a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a></li><li>Наивный байесовский классификатор (Naive Bayes)</li></ul><p>Некоторые алгоритмы специально разработаны для двоичной классификации и изначально не поддерживают более двух классов (это логистическая регрессия и метод опорных векторов).</p><p>Давайте проанализируем набор данных, чтобы развить интуицию в решении задач двоичной классификации. Мы можем использовать функцию <code>make_blobs()</code> для синтеза игрушечного <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>.</p><p>В приведенном ниже примере создается набор данных из 1000 примеров, которые принадлежат одному из двух классов, каждый с двумя входными функциями. Для начала импортируем необходимые для всех типов классификации библиотеки:</p><pre><code class="language-python">import numpy\nfrom numpy import where\n\nimport collections\nfrom collections import Counter\n\nimport sklearn\nfrom sklearn.datasets import make_blobs, make_classification, make_multilabel_classification, make_blobs\n\nimport matplotlib\nfrom matplotlib import pyplot</code></pre><p>Сначала создаем набор данных, состоящий из 1000 примеров, разделенных на <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a> X и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a> y.</p><pre><code class="language-python"># Создадим игрушечный датасет\nX, y = make_blobs(n_samples = 1000, centers = 2, random_state = 1)\n\n# Отобразим размер датасета\nprint(X.shape, y.shape)\n\n# Отобразим общее число представителей того или иного класса \ncounter = Counter(y)\nprint(counter)\n\n# Отобразим метки первых 10 наблюдений:\nfor i in range(10):\n\tprint(X[i], y[i])\n\n# Построим график, причем "представители" классов окрашены разным цветами\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label = str(label))\npyplot.legend()\npyplot.show()</code></pre><p>Наконец создается <a href="__GHOST_URL__/tochechnaya-diagramma/">Точечная диаграмма (Scatterplot)</a>, а точки окрашиваются в соответствии со значением их класса. Мы видим два различных кластера, которые легко различить:</p><pre><code class="language-python">(1000, 2) (1000,)\nCounter({0: 500, 1: 500})\n[-3.05837272  4.48825769] 0\n[-8.60973869 -3.72714879] 1\n[1.37129721 5.23107449] 0\n[-9.33917563 -2.9544469 ] 1\n[-11.57178593  -3.85275513] 1\n[-11.42257341  -4.85679127] 1\n[-10.44518578  -3.76476563] 1\n[-10.44603561  -3.26065964] 1\n[-0.61947075  3.48804983] 0\n[-10.91115591  -4.5772537 ] 1</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/classification-binary.jpg" class="kg-image" alt loading="lazy" width="457" height="334"></figure><h3 id="%D0%BC%D1%83%D0%BB%D1%8C%D1%82%D0%B8%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%BE%D0%B2%D0%B0%D1%8F-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">Мультиклассовая классификация</h3><p>Мультиклассовая классификация предполагает, что классов более двух. Примеры включают:</p><ul><li>Классификация лиц</li><li>Классификация видов растений</li><li>Оптическое распознавание символов</li></ul><p>В отличие от бинарной классификации, мультиклассовая классификация не имеет понятия нормальных и аномальных исходов. Вместо этого примеры классифицируются как принадлежащие к одному из ряда известных классов.</p><p>Для некоторых задач количество меток классов может быть очень большим. Например, модель может предсказать фотографию как принадлежащую одному из тысяч или десятков тысяч лиц в системе распознавания лиц.</p><p>Обычно такую задачу отрабатывают с помощью модели, которая прогнозирует Распределение вероятностей Мультинулли (Multinoulli Probability Distribution) для каждого примера.</p><p>Распределение Мультинулли – это дискретное распределение вероятностей, которое охватывает случай, когда событие будет иметь категориальный исход, например K в {1, 2, 3,…, K}. Для классификации это означает, что модель предсказывает вероятность принадлежности примера к той или иной метке класса.</p><p>Многие алгоритмы, используемые для двоичной классификации, могут использоваться и для мультиклассовой:</p><ul><li>Метод k-ближайших соседей</li><li>Дерево решени</li><li>Наивный байесовский классификатор</li><li><a href="__GHOST_URL__/sluchainyi-lies/">Случайный лес (Random Forest)</a></li><li>Градиентный бустинг (Gradient Boosting)</li></ul><p>Такая классификация использует бинарную для каждого класса по сравнению со всеми другими (one-vs-rest) или одного для каждой пары классов (one-vs-one):</p><ul><li>"Один против остальных" (one-vs-rest): создаем одну модель бинарной классификации для каждого класса по сравнению со всеми другими классами</li><li>"Один против одного" (one-vs-one): создаем одну модель бинарной классификации для каждой пары классов</li></ul><p>Алгоритмы мультиклассовой классификации:</p><ul><li>Логистическая регрессия</li><li>Машина опорных векторов</li></ul><p>Давайте подробнее рассмотрим набор данных, чтобы развить интуицию для решения подобных задач. Мы использовать функцию <code>make_blobs()</code> для синтеза набора:</p><pre><code class="language-python"># Создадим игрушечный датасет\nX, y = make_blobs(n_samples = 1000, centers = 3, random_state = 1)\n\n# Отобразим размер датасета\nprint(X.shape, y.shape)\n\n# Отобразим общее число представителей того или иного класса \ncounter = Counter(y)\nprint(counter)\n\n# Отобразим метки первых 10 наблюдений:\nfor i in range(10):\n\tprint(X[i], y[i])\n \n# Построим график, причем "представители" классов окрашены разным цветами\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label = str(label))\npyplot.legend()\npyplot.show()</code></pre><p>В приведенном примере создается набор из 1000 примеров, которые принадлежат одному из трех классов. Наконец, создается диаграмма, а точки окрашиваются в соответствии со значением их класса:</p><pre><code class="language-python">(1000, 2) (1000,)\nCounter({0: 334, 1: 333, 2: 333})\n[-3.05837272  4.48825769] 0\n[-8.60973869 -3.72714879] 1\n[1.37129721 5.23107449] 0\n[-9.33917563 -2.9544469 ] 1\n[-8.63895561 -8.05263469] 2\n[-8.48974309 -9.05667083] 2\n[-7.51235546 -7.96464519] 2\n[-7.51320529 -7.46053919] 2\n[-0.61947075  3.48804983] 0\n[-10.91115591  -4.5772537 ] 1</code></pre><p>Мы можем видеть три различных кластера, которые легко различить.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/classification-multiclass.png" class="kg-image" alt loading="lazy" width="473" height="334"></figure><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE-%D0%BD%D0%B5%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%B8%D0%BC-%D0%BC%D0%B5%D1%82%D0%BA%D0%B0%D0%BC">Классификация по нескольким меткам</h3><p>Такая классификация предполагает, что имеется две или более метки классов.</p><p>Рассмотрим пример классификации объектов на фотографии, причем изображение может иметь несколько объектов, таких как «велосипед», «яблоко», «человек» и т.д. Это не похоже на двоичную и мультиклассовую классификации, где для каждого примера прогнозируется одна метка класса.</p><p>По сути, это модель, которая делает несколько прогнозов двоичной классификации для каждого примера.</p><p>Алгоритмы классификации, используемые для двоичной или мультиклассовой классификации, не могут использоваться напрямую для классификации по нескольким меткам. Могут использоваться специализированные версии стандартных алгоритмов:</p><ul><li>Деревья решений с несколькими ярлыками</li><li>Случайные леса с несколькими ярлыками</li><li>Градиентный бустинг с несколькими ярлыками</li></ul><p>Затем давайте более подробно рассмотрим набор данных, чтобы развить интуицию для такого типа задач.</p><p>Мы используем функцию <code>make_multilabel_classification()</code> для синтеза набора данных с несколькими метками:</p><pre><code class="language-python"># Создадим игрушечный датасет\nX, y = make_multilabel_classification(n_samples = 1000, n_features = 2, n_classes = 3, n_labels = 2, random_state = 1)\n\n# Отобразим размер датасета\nprint(X.shape, y.shape)\n\n# Отобразим метки первых 10 наблюдений:\nfor i in range(10):\n\tprint(X[i], y[i])</code></pre><p>В приведенном ниже примере создается набор данных из 1000 примеров, каждый с двумя входными функциями. Есть три класса, каждый из которых может иметь одну из двух меток (0 или 1).</p><pre><code class="language-python">(1000, 2) (1000, 3)\n[18. 35.] [1 1 1]\n[22. 33.] [1 1 1]\n[26. 36.] [1 1 1]\n[24. 28.] [1 1 0]\n[23. 27.] [1 1 0]\n[15. 31.] [0 1 0]\n[20. 37.] [0 1 0]\n[18. 31.] [1 1 1]\n[29. 27.] [1 0 0]\n[29. 28.] [1 1 0]</code></pre><h3 id="%D0%BD%D0%B5%D1%81%D0%B1%D0%B0%D0%BB%D0%B0%D0%BD%D1%81%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%B0%D1%8F-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">Несбалансированная классификация</h3><p>Несбалансированная классификация относится к задачам классификации, в которых количество примеров в классах распределяется неравномерно.</p><p>Как правило, такие задачи представляют собой задачи двоичной классификации, где большинство примеров в наборе обучающих данных относятся к нормальному классу, а меньшая часть примеров относится к ненормальному классу.</p><p>Примеры:</p><ul><li><a href="__GHOST_URL__/obnaruzhieniie-moshiennichieskikh-opieratsii/">Обнаружение мошеннических операций (Fraud Detection)</a></li><li>Обнаружение выбросов</li><li>Медицинские диагностические тесты</li></ul><p>Эти проблемы моделируются как задачи бинарной классификации, хотя могут потребоваться специальные методы.</p><p>Примеры алгоритмов:</p><ul><li>Случайное недосэмплирование (Random Undersampling)</li><li>Алгоритм SMOTE (SMOTE Oversampling)</li></ul><p>Могут использоваться специализированные алгоритмы моделирования, которые уделяют больше внимания классу меньшинства при подгонке модели к набору обучающих данных, например, Чувствительные к стоимости алгоритмы (Cost-Sensitive Algorithm).</p><p>Примеры:</p><ul><li>Логистическая регрессия с учетом затрат</li><li>Деревья принятия решений с учетом затрат</li><li>Чувствительные к стоимости машины опорных векторов</li></ul><p>Наконец, могут потребоваться альтернативные показатели производительности, поскольку точности классификации может вводить в заблуждение.</p><p>Примеры включают:</p><ul><li>Точность (Precision)</li><li>Отзыв (Recall)</li><li>Критерий F1 (F1 Score)</li></ul><p>Давайте подробно рассмотрим набор данных, чтобы развить интуицию в отношении несбалансированных проблем классификации.</p><p>Мы используем функцию <code>make_classification()</code> для синтеза набора данных:</p><pre><code class="language-python"># Создадим игрушечный датасет\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_classes=2, n_clusters_per_class=1, weights=[0.99,0.01], random_state=1)\n\n# Отобразим размер датасета\nprint(X.shape, y.shape)\n\n# Отобразим общее число представителей того или иного класса \ncounter = Counter(y)\nprint(counter)\n\n# Отобразим метки первых 10 наблюдений:\nfor i in range(10):\n\tprint(X[i], y[i])\n\n# Построим график, причем "представители" классов окрашены разным цветами\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()</code></pre><p>В приведенном ниже примере создается набор данных из 1000 примеров, которые принадлежат одному из двух классов, каждый с двумя предикторами:</p><pre><code class="language-python">(1000, 2) (1000,)\nCounter({0: 983, 1: 17})\n[0.86924745 1.18613612] 0\n[1.55110839 1.81032905] 0\n[1.29361936 1.01094607] 0\n[1.11988947 1.63251786] 0\n[1.04235568 1.12152929] 0\n[1.18114858 0.92397607] 0\n[1.1365562  1.17652556] 0\n[0.46291729 0.72924998] 0\n[0.18315826 1.07141766] 0\n[0.32411648 0.53515376] 0</code></pre><p>Для набора создается диаграмма рассеяния, а точки окрашиваются в соответствии со значением их класса:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/08/classification-multilabel.jpg" class="kg-image" alt loading="lazy" width="457" height="334"></figure><p>Мы можем видеть один основной кластер для примеров, принадлежащих классу 0, и несколько разрозненных примеров, принадлежащих классу 1. Интуиция подсказывает, что моделировать наборы данных с этим свойством несбалансированных меток классов сложнее.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1JMdx-xiovsCCSSCDOZM6iY0F2fpz5FaE?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/types-of-classification-in-machine-learning/">Jason Brownlee</a></p>		klassifikatsiia	2021-08-25		
137	Кликстрим (Clickstream)		<p>Кликстрим (поток кликов) – это запись пользовательской активности на сайте или в приложении: переходы по страницам / разделам, клики по изображениям, выставление оценок и т.д. Используется для решения задачи <a href="__GHOST_URL__/modielirovaniie-ottoka/">Моделирование оттока (Churn Modeling)</a>.</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/08/--------------2021-08-29---21.32.25.png" class="kg-image" alt loading="lazy" width="2000" height="399" srcset="__GHOST_URL__/content/images/size/w600/2021/08/--------------2021-08-29---21.32.25.png 600w, __GHOST_URL__/content/images/size/w1000/2021/08/--------------2021-08-29---21.32.25.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/08/--------------2021-08-29---21.32.25.png 1600w, __GHOST_URL__/content/images/2021/08/--------------2021-08-29---21.32.25.png 2338w"><figcaption>Запись активности пользователей: дата, заказ, страна, ID сессии, переходы по целевым страницам 1 и 2, цвета и места</figcaption></figure><p><a href="https://www.kaggle.com/tunguz/clickstream-data-for-online-shopping">Датасет Kaggle о пользователях онлайн-магазина</a></p>		klikstrim	2021-08-29		
138	Метод локтя (Elbow Rule)		<p>Метод локтя – один из самых известных методов, с помощью которого вы можете выбрать правильное значение k и повысить производительность <a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модели (Model)</a>. Этот эмпирический метод вычисляет сумму квадратов расстояний между точками и вычисляет <a href="__GHOST_URL__/sriednieie-znachieniie/#-" rel="noopener noreferrer">Среднее значение (Mean)</a>.</p><p>Пример. Предположим, мы пошли в магазин за овощами и увидели, что они будут расположены на полках по типу. Вся морковь хранится в одном месте, картошка – в другом. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/k-means-borders-1.png" class="kg-image" alt loading="lazy" width="490" height="375"></figure><p>До применения кластеризации (появления окрашенных зон и обозначения записей разными иконками) перепутать категорию довольно легко. Неопытные мерчендайзеры до сих пор кладут арбузы в отдел ягод, хоть и правы с научной точки зрения. </p><p>Метод k-средних пытается сгруппировать похожие элементы в три этапа:</p><ol><li>Выберем значение k</li><li>Инициализируем центроиды (разделительные линии)</li><li>Выберем группу и найдем среднее значение расстояния между точками.</li></ol><p>Давайте разберемся в вышеуказанных шагах с помощью иллюстраций. Допустим, мы на глаз кластеризовали наблюдения, причислив половину к белой категории, оставшуюся часть – к розовой. </p><p>Шаг 1. Мы случайным образом выбираем значение K, равное 2:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stagesselect-2-points.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stagesselect-2-points.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stagesselect-2-points.png 1000w"></figure><p>Существуют различные методы, с помощью которых мы можем выбрать правильные значения параметра k. Об этом позже.</p><p>Шаг 2. Соединим две выбранные максимально удаленные точки, обозначенные белой полупрозрачной обводкой. Теперь, чтобы определить центроид, мы построим перпендикуляр к этой линии:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stagescentroid.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stagescentroid.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stagescentroid.png 1000w"></figure><p>Если вы заметили, одна белая точка попала в группу розовых, и теперь относится к другой группе, чем предположено изначально.</p><p>Шаг 3. Мы соединим две другие удаленные точки, проведем к ним перпендикулярную линию и найдем центроид. Теперь некоторые белые точки преобразуются в розовые:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stages-centroid-2.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stages-centroid-2.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stages-centroid-2.png 1000w"></figure><p>Этот процесс будет продолжаться до тех пор, пока мы не переберем все возможные сочетания пар дистанцированных точек и не уточним границы кластеров. Стабильность центроидов определяется путем сравнения абсолютного значения изменения среднего Евклидова расстояния (Euclidian Distance) между наблюдениями и их соответствующими центроидами с пороговым значением.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/05/k-means-stages-result.png" class="kg-image" alt loading="lazy" width="1000" height="400" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-stages-result.png 600w, __GHOST_URL__/content/images/2021/05/k-means-stages-result.png 1000w"></figure><h3 id="%D0%BA%D0%B0%D0%BA-%D0%B2%D1%8B%D0%B1%D1%80%D0%B0%D1%82%D1%8C-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-k">Как выбрать значение k?</h3><p>Одна из самых сложных задач в этом алгоритме кластеризации – выбрать правильные значения k. Существует два метода –  <a href="__GHOST_URL__/silhouette-method/">Метод силуэта (Silhouette Method)</a> и метод локтя.</p><p>Рассмотрим "локтевой" способ. Когда значение k равно 1, сумма квадрата внутри кластера будет большой. По мере увеличения значения k сумма квадратов расстояний внутри кластера будет уменьшаться.</p><p>Наконец, мы построим график между значениями k и суммой квадрата внутри кластера, чтобы получить значение k. Мы внимательно рассмотрим график. В какой-то момент значение по оси x резко уменьшится. Эта точка будет считаться оптимальным значением k:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/05/k-means-elbow.png" class="kg-image" alt loading="lazy" width="1001" height="408" srcset="__GHOST_URL__/content/images/size/w600/2021/05/k-means-elbow.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/k-means-elbow.png 1000w, __GHOST_URL__/content/images/2021/05/k-means-elbow.png 1001w"><figcaption>Ось x – количество кластеров k, y – сумма квадрат расстояний между точками</figcaption></figure>		mietod-loktia	2021-08-29		
139	Гиперпараметр\t(Hyperparameter)		<p>Гиперпараметр – это конфигурация <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, оптимальные настройки, которые невозможно вычислить с помощью <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> и предстоит определить в ходе итеративного обучения:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/colab.research.google.com_github_jakevdp_PythonDataScienceHandbook_blob_master_notebooks_05.03-Hyperparameters-and-Model-Validation.ipynb.png" class="kg-image" alt loading="lazy" width="1848" height="216" srcset="__GHOST_URL__/content/images/size/w600/2021/09/colab.research.google.com_github_jakevdp_PythonDataScienceHandbook_blob_master_notebooks_05.03-Hyperparameters-and-Model-Validation.ipynb.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/colab.research.google.com_github_jakevdp_PythonDataScienceHandbook_blob_master_notebooks_05.03-Hyperparameters-and-Model-Validation.ipynb.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/colab.research.google.com_github_jakevdp_PythonDataScienceHandbook_blob_master_notebooks_05.03-Hyperparameters-and-Model-Validation.ipynb.png 1600w, __GHOST_URL__/content/images/2021/09/colab.research.google.com_github_jakevdp_PythonDataScienceHandbook_blob_master_notebooks_05.03-Hyperparameters-and-Model-Validation.ipynb.png 1848w" sizes="(min-width: 720px) 720px"><figcaption>Автоматически подобранные гиперпараметры полиномиальной регрессии</figcaption></figure><p>Пример. Запустите 8 ячеек ноутбука <a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb#scrollTo=6GBb0a3XhEWQ">"Validation in Practice: Grid Search"</a> из документации <a href="__GHOST_URL__/google-colab/">Colab</a>: здесь утилита <a href="https://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html">GridSearchCV</a> автоматически подбирает гиперпараметры Полиномиальной регрессии (Polynomial Regression):</p><ul><li>linearregression__fit_intercept: пересечение (intercept) – это место, где прямая <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regression)</a> пересекает ось y. Если значение гиперпараметра здесь равно False, то линейная регрессия неприменима к данной модели (GridSearchCV перебирает несколько из них).</li><li>linearregression__normalize: вероятно, означает, что при попытке определить уравнение линейной регрессии к исходным данным применялась <a href="__GHOST_URL__/normalizatsiia/">Нормализация (Normalization)</a>.</li><li>polynomialfeatures__degree (степень многочленов): представьте, что <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a> y можно описать уравнением <code>6x<sup>5</sup> + 5x<sup>4</sup>− 3x<sup>2</sup>+ x</code>. Пятая степень (<code>6x<sup>5</sup></code>) – это и есть [наивысшая] степень многочленов.</li></ul><p>Существует важное отличие между параметрами модели и ее гиперпараметрами: если первые зачастую подбираются автоматически, то вторые определяются в ходе обучения <a href="__GHOST_URL__/alghoritm/">Алгоритма (Algorithm)</a> <a href="__GHOST_URL__/data-saiientist/">Дата-сайентистом (Data Scientist)</a> и помогают определить наилучшее уравнение, описывающее зависимость <a href="__GHOST_URL__/priediktor/">Предикторов (Predictor Variable)</a> и целевой переменной.</p><p>Примеры параметров:</p><ul><li>Веса (Weights) в искусственной <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сети (Neural Network)</a></li><li>Опорные векторы в <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Методе опорных векторов (SVM)</a></li></ul><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/">Jason Brownlee</a></p>		gipierparamietr	2021-09-05		
140	Гиперплоскость (Hyperplane)		<p>Гиперплоскость – это граница решений, которая помогают отнести к той или иной группе точки данных в соответствующих <a href="__GHOST_URL__/alghoritm/">Алгоритмах (Algorithm)</a> <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, например, в <a href="__GHOST_URL__/mietod-opornykh-viektorov/" rel="noopener noreferrer">Методе опорных векторов (SVM)</a>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image.png" class="kg-image" alt loading="lazy" width="1400" height="592" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image.png 1000w, __GHOST_URL__/content/images/2021/09/image.png 1400w" sizes="(min-width: 720px) 720px"><figcaption>Гиперплоскости в двумерном (слева) и трехмерном пространствах признаков</figcaption></figure><p>Точки данных, находящиеся по обе стороны от гиперплоскости, можно отнести к разным классам. Кроме того, размер гиперплоскости зависит от количества <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>. Если их количество равно двум, тогда гиперплоскость представляет собой линию. Если количество входных объектов равно 3, то гиперплоскость становится двумерной. Когда количество функций превышает три, визуализировать ее возможно только с введением условностей.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">Rohith Gandhi</a></p>		gipierploskost	2021-09-05		
141	Граф (Graph)		<p>Графы –- это сети из Узлов (Node) и Граней (Edges); структуры данных,  используемые для различных задач <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> – <a href="__GHOST_URL__/klassifikatsiia/">Классификация (Classification)</a>, Кластеризация (Clustering) и <a href="__GHOST_URL__/rieghriessiia/">Регрессия (Regression)</a>.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/09/graph.png" class="kg-image" alt loading="lazy" width="1799" height="552" srcset="__GHOST_URL__/content/images/size/w600/2021/09/graph.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/graph.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/graph.png 1600w, __GHOST_URL__/content/images/2021/09/graph.png 1799w" sizes="(min-width: 1200px) 1200px"></figure><p>С помощью изображения выше мы можем ознакомиться с двумя разными взглядами на взаимодействия персонажей романа «Отверженные», где два ´<br>узла-окружности соединяются, если взаимодействуют соответствующие персонажи. </p><p>Цвета графа слева подчеркивает различия сообществ, описываемых в романе: узлы имеют один и тот же цвет, если они принадлежат к одному сообществу.</p><p>Напротив, цвет на правом рисунке обозначает местоположение персонажей; «соединительные узлы» окрашены в синий цвет. </p><p> Цветовая разметка обоих графок была сгенерирована с использованием <a href="https://snap.stanford.edu/node2vec/">node2vec</a>.</p><p>Автор оригинальной статьи: <a href="https://www-cs.stanford.edu/people/jure/pubs/graphrepresentation-ieee17.pdf">William L. Hamilton, Rex Ying, Jure Leskovec</a></p>		graf	2021-09-05		
142	Остаток (Residual)		<p>Остатки – это разница между фактическим целевым и спрогнозированным значением, ключевое понятие в задачах Регрессии (Regression). Это элемент формул ее метрик: <a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Cреднеквадратической ошибки (MSE)</a>, <a href="__GHOST_URL__/sriedniaia-absoliutnaia-oshibka/">Cредней абсолютной ошибки (MAE)</a>, средней абсолютной процентная ошибка (MAPE).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-1.png" class="kg-image" alt loading="lazy" width="1043" height="553" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-1.png 1000w, __GHOST_URL__/content/images/2021/09/image-1.png 1043w" sizes="(min-width: 720px) 720px"><figcaption>Чем сильнее точки "не укладываются" в прямую, тем несостоятельнее линейная регрессия</figcaption></figure><p>График остатков, соответственно, – это тип графика, который отображает сопоставленные значения с остаточными в регрессионной <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. Такой способ визуалиации часто используется для оценки способности модели линейной регрессии описать зависимости в <a href="__GHOST_URL__/dataset/">Датасете (Dataset)</a> и для проверки Гетероскедастичности (Heteroscedasticity) – изменчивости распределения точек данных относительно прямо.</p><h3 id="%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BE%D0%BA-statsmodels">Остаток: statsmodels</h3><p>Посмотрим, какими бывают графики остатков. Для этого импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols</code></pre><p>Создадим небольшой Датафрейм (DataFrame), содержащий данные о результативности баскетболистов и протянем модель линейной регрессии, чтобы подтвердить или опровергнуть линейную зависимость между рейтингом игрока и количеством принесенных очков:</p><pre><code class="language-python">df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],\n                   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],\n                   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],\n                   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})\n\n# Применим к данным модель линейной регрессии\nmodel = ols('rating ~ points', data = df).fit()</code></pre><p>Теперь настало время визуализировать отношения между этими метриками. Функция <code>sm.graphics.plot_regress_exog()</code> предлагает четыре вида остаточных графиков, и в некоторые из них стоит вглядеться, прежде чем наступит понимание:</p><pre><code class="language-python"># Зададим размер графика\nfig = plt.figure(figsize = (12, 8))\n\n# Создадим графики\nfig = sm.graphics.plot_regress_exog(model, 'points', fig = fig)</code></pre><p>На субграфике слева сверху мы видим восемь "поплавков", причем красным обозначается фактический рейтинг игрока, а синим – предполагаемый моделью. Таких поплавков на два меньше общего числа записей, поскольку число очков двух пар игроков совпадают:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/residual.png" class="kg-image" alt loading="lazy" width="944" height="655" srcset="__GHOST_URL__/content/images/size/w600/2021/09/residual.png 600w, __GHOST_URL__/content/images/2021/09/residual.png 944w"><figcaption>Рейтинг и число очков у баскетболиста не столь взаимосвязаны, как кажется неопытному наблюдателю</figcaption></figure><p>График частичной регрессии (Partial Regression Plot) снизу слева пытается показать эффект добавления другой переменной в модель, которая уже имеет одну или несколько независимых переменных.</p><p>Его сосед справа сверху дает возможность акцентировать внимание на <a href="__GHOST_URL__/dispiersiia/">Дисперсии  (Variance)</a> – отклонения точек относительно прямой. </p><p>Последний субграфик "компоненты и компоненты плюс остатки" (Component and Component-Plus-Residual – CCPR) справа снизу позволяет судить о влиянии одного предиктора на целевую переменную, принимая во внимание влияние других независимых переменных. Два оставшихся <a href="__GHOST_URL__/priznak/">Признака (Feature)</a> также в немалой степени влияют на рейтинг:</p><ul><li>Ассисты<em><strong> (</strong></em>assists) – передачи, после которых мяч удачно заброшен в корзину</li><li>Подборы (rebounds) – спортивная «кража» мяча&amp;</li></ul><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1KOHBvrR72z7_AsMJwj1WGvialQcccBkH?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/the-unreasonable-richness-of-residual-plot-17f9104a687a">Pararawendy Indarjo</a>, <a href="https://www.statology.org/residual-plot-python/">Zach</a></p>		ostatok	2021-09-12		
143	Метод силуэта (Silhouette Method)		<p>Метод силуэтов – способ изучения разделительного расстояния между результирующими кластерами <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>, часто используемый вместе с <a href="__GHOST_URL__/mietod-k-sriednikh/">Методом K-средних (K-Means)</a>. График силуэта отображает меру того, насколько близко каждая точка в одном кластере находится к точкам в соседних кластерах, и, таким образом, обеспечивает способ визуальной оценки количества кластеров. Эта мера имеет диапазон [-1, 1]:</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-2.png" class="kg-image" alt loading="lazy" width="1800" height="700" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/image-2.png 1600w, __GHOST_URL__/content/images/2021/09/image-2.png 1800w"><figcaption>Силуэт – одна из цветных фигур на субграфике справа</figcaption></figure><p>Коэффициенты силуэта (так называются эти значения) около +1 указывают на то, что образец находится далеко от соседних кластеров. Значение, близкое к нулю указывает, что выборка находится на границе принятия решения между двумя соседними кластерами или очень близко к ней, а отрицательные значения указывают на то, что эти выборки могли быть назначены неправильному кластеру.</p><p>В этом примере анализ силуэта используется для выбора оптимального значения для числа кластеров (n_clusters). Графики ниже показывают, что значения n_clusters 3, 5 и 6 – плохой выбор для данных данных из-за наличия кластеров с оценками силуэта ниже среднего, а также из-за значительных колебаний в размере участков силуэта. Анализ силуэта более неоднозначен при выборе между 2 и 4:</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-3.png" class="kg-image" alt loading="lazy" width="1800" height="700" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-3.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-3.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/image-3.png 1600w, __GHOST_URL__/content/images/2021/09/image-3.png 1800w"><figcaption>3 кластера</figcaption></figure><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-4.png" class="kg-image" alt loading="lazy" width="1800" height="700" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-4.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/image-4.png 1600w, __GHOST_URL__/content/images/2021/09/image-4.png 1800w"><figcaption>4 кластера</figcaption></figure><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-5.png" class="kg-image" alt loading="lazy" width="1800" height="700" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-5.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/image-5.png 1600w, __GHOST_URL__/content/images/2021/09/image-5.png 1800w"><figcaption>5 кластеров</figcaption></figure><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-6.png" class="kg-image" alt loading="lazy" width="1800" height="700" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-6.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-6.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/image-6.png 1600w, __GHOST_URL__/content/images/2021/09/image-6.png 1800w"><figcaption>6 кластеров</figcaption></figure><p>Также по толщине силуэта можно визуализировать размер кластера. График силуэта для кластера 0, когда n_clusters равно 2, больше по размеру из-за группирования 3 субкластеров в один большой кластер. Однако, когда n_clusters равно 4, все графики имеют более или менее одинаковую толщину и, следовательно, имеют аналогичные размеры, что также можно проверить на помеченном графике разброса справа.</p><p>Автор оригинальной статьи: <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html">scikit-learn.org</a></p>		silhouette-method	2021-09-12		
144	Стационарность (Stationarity)		<p>Стационарность – допущение, предполагающее одинаковую <a href="__GHOST_URL__/kovariatsiia/">Ковариацию (Covariance)</a> <a href="__GHOST_URL__/vyborka/">Выборок (Sample)</a> одного размера. Как правило, применяется относительно <a href="__GHOST_URL__/vriemiennoi-riad/">Временных рядов (Time Series)</a>:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/stationarity.png" class="kg-image" alt loading="lazy" width="705" height="698" srcset="__GHOST_URL__/content/images/size/w600/2021/09/stationarity.png 600w, __GHOST_URL__/content/images/2021/09/stationarity.png 705w"><figcaption>Стационарный временной ряд (сверху) и нестационарный</figcaption></figure><p>Чтобы некоторый временной ряд был классифицирован как стационарный, он должен удовлетворять трем условиям:</p><ul><li>Постоянное <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее арифметическое (Mean)</a> выборок</li><li>Постоянная <a href="__GHOST_URL__/dispiersiia/">Дисперсия (Variance)</a></li><li>Постоянная ковариация между периодами одинакового расстояния. То есть мера линейной зависимости между периодами времени одинаковой длины (скажем, 10 дней / часов / минут) должна быть идентична ковариации некоторого другого периода такой же длины.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-8.png" class="kg-image" alt loading="lazy" width="1290" height="408" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-8.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-8.png 1000w, __GHOST_URL__/content/images/2021/09/image-8.png 1290w" sizes="(min-width: 720px) 720px"><figcaption>Только крайний левый ряд обладает одинаковыми средним и дисперсией</figcaption></figure><p>Зачем нам стационарность? Самые важные причины:</p><ul><li>Стационарные процессы легче анализировать</li><li>Стационарность предполагается большинством <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.</li></ul><p>Для проверки стационарности используют <a href="__GHOST_URL__/tiest-diki-fulliera/">Тест Дики-Фуллера (Dickey-Fuller Test)</a>.</p>		statsionarnost	2021-09-12		
145	Слабый искусственный интеллект (Weak AI)		<p>Слабый (узкий) искусственный интеллект – это тип <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a>, который ограничен определенной или узкой областью. Слабый ИИ имитирует человеческое познание. Он может принести пользу обществу, автоматизируя трудоемкие задачи и анализируя данные способами, которые людям порой не под силу или чрезвычайно трудозатратны. Слабый ИИ можно противопоставить сильному, теоретической форме машинного интеллекта, который равен человеческому.</p><p>Слабому ИИ не хватает человеческого сознания, хотя временами он может это имитировать. Классической иллюстрацией слабого ИИ является мысленный эксперимент Джона Сирла «Китайская комната». </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://images.unsplash.com/photo-1512972972907-6d71529c5e92?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fGNoaW5lc2UlMjBpbnRlcmlvcnxlbnwwfHx8fDE2MzIwNDQ3MTA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" class="kg-image" alt loading="lazy" width="5438" height="3625" srcset="https://images.unsplash.com/photo-1512972972907-6d71529c5e92?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fGNoaW5lc2UlMjBpbnRlcmlvcnxlbnwwfHx8fDE2MzIwNDQ3MTA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=600 600w, https://images.unsplash.com/photo-1512972972907-6d71529c5e92?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fGNoaW5lc2UlMjBpbnRlcmlvcnxlbnwwfHx8fDE2MzIwNDQ3MTA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1000 1000w, https://images.unsplash.com/photo-1512972972907-6d71529c5e92?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fGNoaW5lc2UlMjBpbnRlcmlvcnxlbnwwfHx8fDE2MzIwNDQ3MTA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1600 1600w, https://images.unsplash.com/photo-1512972972907-6d71529c5e92?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fGNoaW5lc2UlMjBpbnRlcmlvcnxlbnwwfHx8fDE2MzIwNDQ3MTA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2400 2400w" sizes="(min-width: 720px) 720px"><figcaption>Фото: <a href="https://unsplash.com/@dchestudio?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Daniel Chen</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption></figure><p>В этом эксперименте говорится, что человек за пределами комнаты может разговаривать на китайском языке с человеком внутри комнаты, которому дают инструкции о том, как отвечать на вопросы на китайском языке. В этом эксперименте кажется, что человек в комнате говорит по-китайски. На самом деле, он не мог бы сказать или понять ни слова по-китайски без инструкций, которые ему давали. Человек хорошо выполняет инструкции, а не говорит по-китайски. </p><p>Узкие системы ИИ не обладают общим интеллектом; у них есть свой, "особый". Искусственный интеллект, который знает, как проехать из точки А в точку Б, обычно не способен вызвать Вас на игру в шахматы. Точно так же ИИ, который может притвориться, что говорит с вами по-китайски, вероятно, не сможет подмести ваши полы.</p><p>Слабый ИИ помогает превратить <a href="__GHOST_URL__/bolshiie-dannyie/">Большие данные (Big Data)</a> в полезную информацию, обнаруживая закономерности и делая прогнозы. Примеры слабого ИИ включают новостную ленту Facebook, предлагаемые покупки Amazon и Siri – технологию Apple, которая отвечает на устные вопросы пользователей. Спам-фильтры электронной почты – еще один пример слабого ИИ: компьютер использует алгоритм, чтобы узнать, какие сообщения могут быть рекламой, а затем перенаправляет их в соответствующую папку.</p><h3 id="%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%B8%D1%8F">Ограничения</h3><p>Помимо ограниченных возможностей, некоторые из проблем со слабым ИИ включают возможность причинения вреда в случае сбоя системы. Например, беспилотный автомобиль, который неправильно рассчитывает местоположение встречного транспортного средства и вызывает смертельное столкновение. У системы также есть возможность причинить вред, если она используется кем-то, кто желает нанести урон, – скажем, террористом, который использует беспилотный автомобиль для установки взрывчатых веществ в людном месте.</p><p>Еще одна проблема, связанная со слабым ИИ, – это потеря рабочих мест из-за автоматизации растущего числа задач. Взлетит ли уровень безработицы, или общество найдет новые способы, позволяющие людям быть экономически продуктивными? Хотя перспектива того, что большой процент рабочих потеряет работу, пугает, сторонники теории утверждают, что также разумно ожидать, что если это произойдет, появятся новые рабочие места, которые мы пока не можем предсказать, поскольку использование ИИ становится все более всеобъемлющим.</p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/w/weak-ai.asp">Jake Frankenfield</a></p>		slabyi-iskusstviennyi-intielliekt	2021-09-19		
146	Сильный искусственный интеллект (Strong AI)		<p>Сильный (общий) искусственный интеллект (AGI) – теоретическая форма <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a>, используемая для описания определенного образа мышления: если исследователи смогут разработать машину с интеллектом, равным человеческому, у него будет самосознание, способное решать проблемы, учиться и планировать будущее.</p><p>Сильный ИИ нацелен на создание интеллектуальных машин, неотличимых от человеческого разума. Но, как и ребенок, ИИ-машине придется учиться через вводимые данные и опыт, постоянно развивая свои способности с течением времени.</p><p>Хотя исследователи искусственного интеллекта как в академических кругах, так и в частном секторе инвестируют в создание общего искусственного интеллекта, сегодня он существует только как теоретическая концепция, а не материальная реальность. Одни склоняются к оптимистичному прогнозу: мы можем достичь невероятных высот в области искусственного интеллекта; другие скажут, что сильные системы искусственного интеллекта даже не могут быть разработаны. Пока критерии успеха, такие как интеллект и понимание, не будут четко определены, последние верны в своем убеждении. На данный момент многие используют тест Тьюринга для оценки интеллекта системы ИИ.</p><h3 id="%D1%82%D0%B5%D1%81%D1%82-%D1%82%D1%8C%D1%8E%D1%80%D0%B8%D0%BD%D0%B3%D0%B0">Тест Тьюринга</h3><p>Алан Тьюринг разработал тест Тьюринга в 1950 году и обсуждал его в своей статье «Вычислительные машины и интеллект». Первоначально известный как "игра в имитацию", тест оценивает, можно ли отличить поведение машины от поведения человека. В этом тесте участвует человек, известный как «дознаватель», который пытается определить разницу между результатами, созданными компьютером, и результатами, созданными человеком, с помощью серии вопросов. Если дознаватель не может надежно отличить машины от людей, машина проходит испытание. Однако, если оценщик может правильно идентифицировать человеческие реакции, это не позволит причислить машину к категории "интеллектуальная".</p><p>Хотя нет установленных руководящих принципов оценки для теста Тьюринга, ученый указал, что человек-оценщик будет иметь только шанс в 70% правильно определить человеческий или компьютерный разговор через 5 минут. Тест Тьюринга привел к всеобщему признанию идеи машинного интеллекта.</p><p>Однако исходный тест проверяет только два набора навыков – чат или шахматы. Сильный ИИ должен одинаково хорошо выполнять множество задач, что привело к разработке расширенного теста Тьюринга. Этот тест оценивает текстовые, визуальные и слуховые характеристики ИИ и сравнивает их с результатами, созданными человеком. Эта версия используется в знаменитом конкурсе на приз Лебнера, где судья-человек угадывает, был ли результат создан человеком или компьютером.</p><h3 id="%D1%82%D0%B5%D0%BD%D0%B4%D0%B5%D0%BD%D1%86%D0%B8%D0%B8">Тенденции</h3><p>Хотя явных примеров сильного искусственного интеллекта нет, область быстро обновляется. Возникла другая теория искусственного интеллекта, известная как Искусственный суперинтеллект (ASI) или суперинтеллект. Этот тип превосходит сильный ИИ по способностям. Тем не менее, Super AI по-прежнему остается чисто умозрительной концепцией, поскольку нам еще предстоит создать примеры Strong AI.</p><p>Есть области, в которых ИИ играет более важную роль, например:</p><ul><li><strong>Кибербезопасность</strong>: искусственный интеллект возьмет на себя больше функций в мерах кибербезопасности организаций, включая обнаружение правонарушений, мониторинг и анализ угроз, реагирование на инциденты и анализ рисков.</li><li><strong>Развлечения и создание контента</strong>: программы по информатике уже становятся все лучше и лучше в создании контента, будь то копирайтинг, поэзия, видеоигры или даже создание фильмов. Приложение ИИ для генерации текста GBT-3 OpenAI уже создает контент, который практически невозможно отличить от копии, написанной людьми.</li><li><strong>Распознавание и прогнозирование поведения</strong>: алгоритмы прогнозирования сделают ИИ сильнее, начиная от приложений для прогнозирования погоды и фондового рынка и заканчивая, что еще более интересно, прогнозированием поведения человека. Это также вызывает вопросы относительно скрытых предубеждений и этического ИИ. Некоторые исследователи ИИ в сообществе ИИ настаивают на наборе антидискриминационных правил, которые часто ассоциируются с понятием <a href="__GHOST_URL__/spraviedlivost/">Справедливости (Fairness)</a>.</li></ul><p>Автор оригинальной статьи: <a href="https://www.ibm.com/cloud/learn/strong-ai">IBM Cloud Education</a></p>		silnyi-iskusstviennyi-intielliekt	2021-09-19		
147	Обнаружение мошеннических операций (Fraud Detection)	Обнаружение мошеннических операций – одна из популярнейших задач Машинного обучения (ML), нацеленная на выделение правонарушений из общего потока событий.	<p>Обнаружение мошеннических операций &ndash; одна из популярнейших задач <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, нацеленная на выделение правонарушений из общего потока событий. Рассмотрим в качестве примера распознавание воровства средств с банковских карт.</p>\r\n<p>Для начала импортируем необходимые библиотеки:</p>\r\n<pre><code class="language-python">import pandas as pd\r\nimport numpy as np\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\nfrom sklearn.manifold import TSNE\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom xgboost import XGBClassifier\r\nfrom xgboost import plot_importance\r\nimport xgboost as xgb\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.model_selection import KFold\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.model_selection import cross_val_predict\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn import svm\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.neural_network import MLPClassifier\r\n\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn import metrics\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom sklearn.metrics import average_precision_score\r\nfrom sklearn.metrics import roc_curve, auc</code></pre>\r\n<p>Импортируем хронологию операций:</p>\r\n<pre><code class="language-python">data = pd.read_csv('../input/creditcardfraud/creditcard.csv')</code></pre>\r\n<p>Посмотрим, из чего состоит <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>:</p>\r\n<pre><code class="language-python">data.info()</code></pre>\r\n<pre><code class="language-python">&lt;class 'pandas.core.frame.DataFrame'&gt;\r\nRangeIndex: 284807 entries, 0 to 284806\r\nData columns (total 31 columns):\r\n #   Column  Non-Null Count   Dtype  \r\n---  ------  --------------   -----  \r\n 0   Time    284807 non-null  float64\r\n 1   V1      284807 non-null  float64\r\n 2   V2      284807 non-null  float64\r\n 3   V3      284807 non-null  float64\r\n 4   V4      284807 non-null  float64\r\n 5   V5      284807 non-null  float64\r\n 6   V6      284807 non-null  float64\r\n 7   V7      284807 non-null  float64\r\n 8   V8      284807 non-null  float64\r\n 9   V9      284807 non-null  float64\r\n 10  V10     284807 non-null  float64\r\n 11  V11     284807 non-null  float64\r\n 12  V12     284807 non-null  float64\r\n 13  V13     284807 non-null  float64\r\n 14  V14     284807 non-null  float64\r\n 15  V15     284807 non-null  float64\r\n 16  V16     284807 non-null  float64\r\n 17  V17     284807 non-null  float64\r\n 18  V18     284807 non-null  float64\r\n 19  V19     284807 non-null  float64\r\n 20  V20     284807 non-null  float64\r\n 21  V21     284807 non-null  float64\r\n 22  V22     284807 non-null  float64\r\n 23  V23     284807 non-null  float64\r\n 24  V24     284807 non-null  float64\r\n 25  V25     284807 non-null  float64\r\n 26  V26     284807 non-null  float64\r\n 27  V27     284807 non-null  float64\r\n 28  V28     284807 non-null  float64\r\n 29  Amount  284807 non-null  float64\r\n 30  Class   284807 non-null  int64  \r\ndtypes: float64(30), int64(1)\r\nmemory usage: 67.4 MB</code></pre>\r\n<p>Кроме <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a> &laquo;Время&raquo; (Time), &laquo;Количество&raquo; (Amount) и &laquo;Класс&raquo; (Class) другие не стоит интерпретировать в одиночку. Но все мы знаем, что значения столбцов V1 - V28 были преобразованы с помощью <a href="__GHOST_URL__/mietod-ghlavnykh-komponient/">Анализа главных компонент (PCA)</a>. Эти загадочные колонки &ndash; результат защиты конфиденциальных данных пользователей.</p>\r\n<pre><code class="language-python">&lt;class 'pandas.core.frame.DataFrame'&gt;\r\nRangeIndex: 284807 entries, 0 to 284806\r\nData columns (total 31 columns):\r\n #   Column  Non-Null Count   Dtype  \r\n---  ------  --------------   -----  \r\n 0   Time    284807 non-null  float64\r\n 1   V1      284807 non-null  float64\r\n 2   V2      284807 non-null  float64\r\n 3   V3      284807 non-null  float64\r\n 4   V4      284807 non-null  float64\r\n 5   V5      284807 non-null  float64\r\n 6   V6      284807 non-null  float64\r\n 7   V7      284807 non-null  float64\r\n 8   V8      284807 non-null  float64\r\n 9   V9      284807 non-null  float64\r\n 10  V10     284807 non-null  float64\r\n 11  V11     284807 non-null  float64\r\n 12  V12     284807 non-null  float64\r\n 13  V13     284807 non-null  float64\r\n 14  V14     284807 non-null  float64\r\n 15  V15     284807 non-null  float64\r\n 16  V16     284807 non-null  float64\r\n 17  V17     284807 non-null  float64\r\n 18  V18     284807 non-null  float64\r\n 19  V19     284807 non-null  float64\r\n 20  V20     284807 non-null  float64\r\n 21  V21     284807 non-null  float64\r\n 22  V22     284807 non-null  float64\r\n 23  V23     284807 non-null  float64\r\n 24  V24     284807 non-null  float64\r\n 25  V25     284807 non-null  float64\r\n 26  V26     284807 non-null  float64\r\n 27  V27     284807 non-null  float64\r\n 28  V28     284807 non-null  float64\r\n 29  Amount  284807 non-null  float64\r\n 30  Class   284807 non-null  int64  \r\ndtypes: float64(30), int64(1)\r\nmemory usage: 67.4 MB\r\n</code></pre>\r\n<p>Посмотрим, насколько наши данные сбалансированы:</p>\r\n<pre><code class="language-python">plt.figure(figsize=(10,10))\r\nsns.countplot(\r\n    y="Class", \r\n    data=data,\r\n    facecolor=(0, 0, 0, 0),\r\n    linewidth=5, \r\n    edgecolor=sns.color_palette("dark", 2))\r\n\r\nplt.title('Fraudulent Transaction Summary')\r\nplt.xlabel('Count')\r\nplt.ylabel('Fraudulent Transaction   Non-Fraudulent Transaction', fontsize=12)</code></pre>\r\n<p>Мы имеем дело с Несбалансированным датасетом (Imbalanced Dataset), то есть соотношение представителей класса неравное.</p>\r\n<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/image-9.png" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-9.png 600w, __GHOST_URL__/content/images/2021/09/image-9.png 602w" alt="" width="602" height="604" loading="lazy" />\r\n<figcaption>На столбчатой диаграмме почти не видно красный столбец с транзакциями мошенников&nbsp;</figcaption>\r\n</figure>\r\n<p>График показывает, что существует огромная разница между классами операций. Несбалансированные данные могут вызвать проблемы <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, такие как неправильная <a href="__GHOST_URL__/tochnost-izmierienii/">Точность (Accuracy)</a>. В этом проекте мы будем использовать Метод удаления примеров мажоритарного класса (Undersampling Method).</p>\r\n<p>Преобразуем признак "Класс" в категориальный:</p>\r\n<pre><code class="language-python">data['Class']= data['Class'].astype('category')</code></pre>\r\n<p>Посмотрим, как транзакции распределены по времени. Time &ndash; &nbsp;это количество секунд, прошедших между рассматриваемой и первой транзакцией в наборе данных:</p>\r\n<pre><code class="language-python">plt.figure(figsize=(15,10))\r\nsns.distplot(data['Time'])</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---13.52.17.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---13.52.17.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---13.52.17.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---13.52.17.png 1404w" alt="" width="1404" height="944" loading="lazy" /></figure>\r\n<p>Следующим делом посмотрим на распределение признака "Количество":</p>\r\n<pre><code class="language-python">plt.figure(figsize=(10,10))\r\nsns.distplot(data['Amount'])</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---13.53.45.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---13.53.45.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---13.53.45.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---13.53.45.png 1276w" alt="" width="1276" height="1178" loading="lazy" /></figure>\r\n<p>Приведенные выше графики показывают, что столбцы "Время" и "Количество" необходимо подвергнуть <a href="__GHOST_URL__/standartizatsiia/">Стандартизации (Standartization)</a>. Этот метод позволит создавать признаки, которые имеют схожие диапазоны значений.</p>\r\n<p>Перед стандартизацией я хочу создать функцию &laquo;Час&raquo;, которая поможет лучше использовать &laquo;Время&raquo; и его связь с остальными столбцами.</p>\r\n<pre><code class="language-python">data['Hour'] = data['Time'].apply(lambda x: np.ceil(float(x)/3600) % 24)\r\n\r\npd.pivot_table(\r\n    columns="Class", \r\n    index="Hour", \r\n    values= 'Amount', \r\n    aggfunc='count', \r\n    data=data)</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---21.55.46.png" alt="" width="302" height="404" loading="lazy" /></figure>\r\n<p>Посмотрим, в какое время дня мошенники наиболее активны и сравним с активностью нормальных операций:</p>\r\n<pre><code class="language-python">#Hour vs Class\r\nfig, axes = plt.subplots(2, 1, figsize=(15, 10))\r\n\r\nsns.countplot(\r\n    x="Hour",\r\n    data=data[data['Class'] == 0], \r\n    color="#98D8D8",  \r\n    ax=axes[0])\r\naxes[0].set_title("Non-Fraudulent Transaction")\r\n\r\n\r\nsns.countplot(\r\n    x="Hour",\r\n    data=data[data['Class'] == 1],\r\n    color="#F08030", \r\n    ax=axes[1])\r\naxes[1].set_title("Fraudulent Transaction")</code></pre>\r\n<figure class="kg-card kg-image-card kg-width-wide"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---21.57.05.png" sizes="(min-width: 1200px) 1200px" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---21.57.05.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---21.57.05.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---21.57.05.png 1406w" alt="" width="1406" height="944" loading="lazy" /></figure>\r\n<p>Приведенные выше графики показывают, что обычные и мошеннические транзакции совершались каждый час. Для мошеннических транзакций третий и двенадцатый часы &ndash; самые "горячие".</p>\r\n<h3 id="%D0%BF%D0%BE%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5-%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8">Понижение размерности</h3>\r\n<p>Результаты исследования данных показывают, что набор данных большой, а размеры классов несбалансированы, поэтому уменьшение размерности поможет интерпретировать результаты. Для этого будет использоваться Стохастическое вложение соседей с t-распределением (t-SNE). Этот метод хорошо работает с данными большого размера и "проецирует" их в двух- или трехмерном пространстве.</p>\r\n<pre><code class="language-python">data_nonfraud = data[data['Class'] == 0].sample(2000)\r\ndata_fraud  = data[data['Class'] == 1]\r\n\r\ndata_new = data_nonfraud.append(data_fraud).sample(frac=1)\r\nX = data_new.drop(['Class'], axis = 1).values\r\ny = data_new['Class'].values\r\n\r\ntsne = TSNE(n_components=2, random_state=42)\r\nX_transformation = tsne.fit_transform(X)\r\n\r\nplt.figure(figsize=(10, 10))\r\nplt.title("t-SNE Dimensionality Reduction")\r\n\r\ndef plot_data(X, y):\r\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], label="Non_Fraudulent", alpha=0.5, linewidth=0.15, c='#17becf')\r\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], label="Fraudulent", alpha=0.5, linewidth=0.15, c='#d62728')\r\n    plt.legend()\r\n    return plt.show()\r\n\r\nplot_data(X_transformation, y)</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.02.08.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---22.02.08.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---22.02.08.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.02.08.png 1212w" alt="" width="1212" height="1188" loading="lazy" /></figure>\r\n<p>На приведенном выше графике показано, что мошеннические и нормальные транзакции плохо разделены на два разных кластера в двухмерном пространстве. Это означает, что два типа операций сильно похожи. Также этот график демонстрирует, что показаний точности недостаточно для выбора лучшего алгоритма.</p>\r\n<h3 id="%D1%81%D1%82%D0%B0%D0%BD%D0%B4%D0%B0%D1%80%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F">Стандартизация</h3>\r\n<pre><code class="language-python">data[['Time', 'Amount']] = StandardScaler().fit_transform(data[['Time', 'Amount']])</code></pre>\r\n<h3 id="%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%B3%D0%B8%D0%BF%D0%B5%D1%80%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2">Оптимизация гиперпараметров</h3>\r\n<p>Этот метод помогает найти оптимальные параметры для алгоритмов машинного обучения. <a href="__GHOST_URL__/alghoritm-poiska-po-sietkie/">Алгоритм поиска по сетке (Grid Search)</a> будет использоваться для настройки <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a>. Затем будет выполнен <a href="__GHOST_URL__/ekstremalnyy-gradiyentnyy-busting/">Экстремальный градиентный бустинг (XGBoost)</a> для построения графика <a href="__GHOST_URL__/vazhnost-priznaka/">Важности признаков (Feature Importance)</a>. Этот график помогает выбрать параметры, которые будут использоваться в <a href="__GHOST_URL__/modiel/">Модели (Model)</a>.</p>\r\n<pre><code class="language-python">train_data, label_data = data.iloc[:,:-1],data.iloc[:,-1]\r\n\r\ndata_dmatrix = xgb.DMatrix(data=train_data, label= label_data)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(\r\n                                    train_data, label_data, test_size=0.3,random_state=42)\r\n                                    \r\nparams = {\r\n    'objective':'reg:logistic',\r\n    'colsample_bytree': 0.3,\r\n    'learning_rate': 0.1,\r\n    'bootstrap': True, \r\n    'criterion': 'gini', \r\n    'max_depth': 4, \r\n    'max_features': 'auto', \r\n    'n_estimators': 50\r\n}\r\nxg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\r\n\r\n#Feature importance graph\r\nplt.rcParams['figure.figsize'] = [20, 10]\r\nxgb.plot_importance(xg_reg)</code></pre>\r\n<figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.10.21.png" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---22.10.21.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---22.10.21.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.10.21.png 1414w" alt="" width="1414" height="732" loading="lazy" />\r\n<figcaption>График важности признаков</figcaption>\r\n</figure>\r\n<p>На приведенном выше графике показано, что самый важный столбец &ndash; это V16. Параметры с наименьшей важностью - V13, V25, Time, V20, V22, V8, V15, V19 и V2 будут удалены из данных перед построением модели.</p>\r\n<pre><code class="language-python">data_model = data.drop(['V13', 'V25', 'Time', 'V20', 'V22', 'V8', 'V15', 'V19', 'V2'], axis=1)</code></pre>\r\n<h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D1%83%D0%B4%D0%B0%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D0%BE%D0%B2-%D0%BC%D0%B0%D0%B6%D0%BE%D1%80%D0%B8%D1%82%D0%B0%D1%80%D0%BD%D0%BE%D0%B3%D0%BE-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B0">Метод удаления примеров мажоритарного класса</h3>\r\n<p>Перед построением модели будет применен метод случайного недосэмплирования. В этом проекте было выбрано 5% нормальных транзакций.</p>\r\n<pre><code class="language-python">data_under_nonfraud = data_model[data_model['Class'] == 0].sample(15000)\r\ndata_under_fraud  = data_model[data_model['Class'] == 1]\r\n\r\ndata_undersampling = data_under_nonfraud.append(data_under_fraud, \r\n                                                ignore_index=True, sort=False)\r\n                                                \r\nplt.figure(figsize=(10,10))\r\nsns.countplot(y="Class", data=data_undersampling,palette='Dark2')\r\nplt.title('Fraudulent Transaction Summary')\r\nplt.xlabel('Count')\r\nplt.ylabel('Fraudulent Transaction,        Non-Fraudulent Transaction')</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.14.09.png" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---22.14.09.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---22.14.09.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.14.09.png 1216w" alt="" width="1216" height="1220" loading="lazy" /></figure>\r\n<p>Новые данные будут случайным образом разделены на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные данные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>. Доля первых составляет 70%, вторых &ndash; 30%.</p>\r\n<h3 id="k-%D0%B1%D0%BB%D0%BE%D1%87%D0%BD%D0%B0%D1%8F-%D0%BA%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F">k-блочная кросс-валидация</h3>\r\n<pre><code class="language-python">kfold_cv=KFold(n_splits=5, random_state=42, shuffle=True)\r\n\r\nfor train_index, test_index in kfold_cv.split(X,y):\r\n    X_train, X_test = X[train_index], X[test_index]\r\n    y_train, y_test = y[train_index], y[test_index]</code></pre>\r\n<h3 id="%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9-%D0%BB%D0%B5%D1%81">Случайный лес</h3>\r\n<pre><code class="language-python">modelRF = RandomForestClassifier(\r\n    n_estimators=500, \r\n    criterion = 'gini', \r\n    max_depth = 4, \r\n    class_weight='balanced', \r\n    random_state=42\r\n).fit(X_train, y_train)\r\n\r\n# Obtain predictions from the test data \r\npredict_RF = modelRF.predict(X_test)</code></pre>\r\n<h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2">Метод опорных векторов</h3>\r\n<pre><code class="language-python">modelSVM = svm.SVC(\r\n    kernel='rbf', \r\n    class_weight='balanced', \r\n    gamma='scale', \r\n    probability=True, \r\n    random_state=42\r\n).fit(X_train, y_train)\r\n\r\n# Obtain predictions from the test data \r\npredict_SVM = modelSVM.predict(X_test)</code></pre>\r\n<h3 id="%D0%BB%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F">Логистическая регрессия</h3>\r\n<pre><code class="language-python">modelLR = LogisticRegression(\r\n    solver='lbfgs', \r\n    multi_class='multinomial',\r\n    class_weight='balanced', \r\n    max_iter=500, \r\n    random_state=42\r\n).fit(X_train, y_train)\r\n\r\n# Obtain predictions from the test data \r\npredict_LR = modelLR.predict(X_test)</code></pre>\r\n<h3 id="%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D1%8B%D0%B9-%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD">Многослойный перцептрон</h3>\r\n<pre><code class="language-python">modelMLP = MLPClassifier(\r\n    solver='lbfgs', \r\n    activation='logistic', \r\n    hidden_layer_sizes=(100,),\r\n    learning_rate='constant', \r\n    max_iter=1500, \r\n    random_state=42\r\n).fit(X_train, y_train)\r\n\r\n# Obtain predictions from the test data \r\npredict_MLP = modelMLP.predict(X_test)</code></pre>\r\n<h3 id="%D1%81%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%B2">Сравнение методов</h3>\r\n<pre><code class="language-python">RF_matrix = confusion_matrix(y_test, predict_RF)\r\nSVM_matrix = confusion_matrix(y_test, predict_SVM)\r\nLR_matrix = confusion_matrix(y_test, predict_LR)\r\nMLP_matrix = confusion_matrix(y_test, predict_MLP) \r\n\r\nfig, ax = plt.subplots(1, 2, figsize=(15, 8))\r\n\r\nsns.heatmap(RF_matrix, annot=True, fmt="d",cbar=False, cmap="Paired", ax = ax[0])\r\nax[0].set_title("Random Forest", weight='bold')\r\nax[0].set_xlabel('Predicted Labels')\r\nax[0].set_ylabel('Actual Labels')\r\nax[0].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\r\nax[0].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\r\n\r\nsns.heatmap(SVM_matrix, annot=True, fmt="d",cbar=False, cmap="Dark2", ax = ax[1])\r\nax[1].set_title("Support Vector Machine", weight='bold')\r\nax[1].set_xlabel('Predicted Labels')\r\nax[1].set_ylabel('Actual Labels')\r\nax[1].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\r\nax[1].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\r\n\r\nfig, axe = plt.subplots(1, 2, figsize=(15, 8))\r\n\r\nsns.heatmap(LR_matrix, annot=True, fmt="d",cbar=False, cmap="Pastel1", ax = axe[0])\r\naxe[0].set_title("Logistic Regression", weight='bold')\r\naxe[0].set_xlabel('Predicted Labels')\r\naxe[0].set_ylabel('Actual Labels')\r\naxe[0].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\r\naxe[0].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\r\n\r\nsns.heatmap(MLP_matrix, annot=True, fmt="d",cbar=False, cmap="Pastel1", ax = axe[1])\r\naxe[1].set_title("Multilayer Perceptron", weight='bold')\r\naxe[1].set_xlabel('Predicted Labels')\r\naxe[1].set_ylabel('Actual Labels')\r\naxe[1].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\r\naxe[1].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])</code></pre>\r\n<p>Для несбалансированных данных результаты матрицы путаницы могут быть неверными. Однако полезно сказать, сколько мошеннических транзакций предсказано верно. На основе графиков <a href="__GHOST_URL__/mnoghosloinyi-piertsieptron/">Многослойного персептрона (MLP)</a>, <a href="__GHOST_URL__/sluchainyi-lies/">Случайного леса (Random Forest)</a> и <a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/">Логистической регрессии (Logistic Regression)</a> предсказывают одну и ту же долю мошеннических транзакций (сумма нижних двух ячеек каждой из матриц равна 109).</p>\r\n<figure class="kg-card kg-image-card kg-width-wide"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.20.08.png" sizes="(min-width: 1200px) 1200px" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---22.20.08.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---22.20.08.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.20.08.png 1228w" alt="" width="1228" height="1422" loading="lazy" /></figure>\r\n<pre><code class="language-python">print("Classification_RF:")\r\nprint(classification_report(y_test, predict_RF))\r\nprint("Classification_SVM:")\r\nprint(classification_report(y_test, predict_SVM))\r\nprint("Classification_LR:")\r\nprint(classification_report(y_test, predict_LR))\r\nprint("Classification_MLP:")\r\nprint(classification_report(y_test, predict_MLP))</code></pre>\r\n<p>В приведенной ниже таблице показаны результаты по точности, <a href="__GHOST_URL__/otzyv/">Отзыву (Recall)</a> и Критерий F1 (F1 Score).</p>\r\n<ul>\r\n<li>Модель логистической регрессии &nbsp;имеет самый высокий уровень отзыва. Это означает, что она лучше "разыскивает" фактическую мошенническую транзакцию. Однако, когда мы смотрим на показатель точности, логистическая регрессия показывает один из самых худших результатов.</li>\r\n<li>Наивысший удалось достигнуть случайному лесу. Высокая точность связана с низким уровнем ложных срабатываний, поэтому можно сказать, что модель случайного леса предсказывает наименьшее количество ложных мошеннических транзакций.</li>\r\n<li>Критерий F1 дает лучшее объяснение на том основании, что он рассчитывается из <a href="__GHOST_URL__/sriednieie-znachieniie/#--2">Гармонических средних значений (Harmonic Mean)</a> точности и отзыва. F1 &ndash; это лучшая метрика для выбора наиболее предсказуемой модели. В свете этой информации мы можем сказать, что алгоритм Случайного леса является наилучшим.</li>\r\n</ul>\r\n<pre><code class="language-python">Classification_RF:\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.98      0.99      0.99       389\r\n           1       0.98      0.92      0.95       109\r\n\r\n    accuracy                           0.98       498\r\n   macro avg       0.98      0.96      0.97       498\r\nweighted avg       0.98      0.98      0.98       498\r\n\r\nClassification_SVM:\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.83      0.50      0.62       389\r\n           1       0.26      0.64      0.37       109\r\n\r\n    accuracy                           0.53       498\r\n   macro avg       0.55      0.57      0.50       498\r\nweighted avg       0.71      0.53      0.57       498\r\n\r\nClassification_LR:\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.98      0.96      0.97       389\r\n           1       0.86      0.94      0.89       109\r\n\r\n    accuracy                           0.95       498\r\n   macro avg       0.92      0.95      0.93       498\r\nweighted avg       0.95      0.95      0.95       498\r\n\r\nClassification_MLP:\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.86      1.00      0.92       389\r\n           1       0.98      0.41      0.58       109\r\n\r\n    accuracy                           0.87       498\r\n   macro avg       0.92      0.71      0.75       498\r\nweighted avg       0.88      0.87      0.85       498\r\n</code></pre>\r\n<p>Окончательное сравнение будет выполнено с <a href="__GHOST_URL__/roc-krivaia/">ROC-кривая (AUC ROC)</a>:</p>\r\n<pre><code class="language-python">#RF AUC\r\nrf_predict_probabilities = modelRF.predict_proba(X_test)[:,1]\r\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_predict_probabilities)\r\nrf_roc_auc = auc(rf_fpr, rf_tpr)\r\n\r\n#SVM AUC\r\nsvm_predict_probabilities = modelSVM.predict_proba(X_test)[:,1]\r\nsvm_fpr, svm_tpr, _ = roc_curve(y_test, svm_predict_probabilities)\r\nsvm_roc_auc = auc(svm_fpr, svm_tpr)\r\n\r\n#LR AUC\r\nlr_predict_probabilities = modelLR.predict_proba(X_test)[:,1]\r\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_predict_probabilities)\r\nlr_roc_auc = auc(lr_fpr, lr_tpr)\r\n\r\n#MLP AUC\r\nmlp_predict_probabilities = modelMLP.predict_proba(X_test)[:,1]\r\nmlp_fpr, mlp_tpr, _ = roc_curve(y_test, mlp_predict_probabilities)\r\nmlp_roc_auc = auc(mlp_fpr, mlp_tpr)\r\nplt.figure()\r\nplt.plot(rf_fpr, rf_tpr, color='red',lw=2,\r\n         label='Random Forest (area = %0.2f)' % rf_roc_auc)\r\n\r\nplt.plot(svm_fpr, svm_tpr, color='blue',lw=2, \r\n         label='Support Vector Machine (area = %0.2f)' % svm_roc_auc)\r\n\r\nplt.plot(lr_fpr, lr_tpr, color='green',lw=2, \r\n         label='Logistic Regression (area = %0.2f)' % lr_roc_auc)\r\n\r\nplt.plot(mlp_fpr, mlp_tpr, color='orange',lw=2, \r\n         label='Multilayer Perceptron (area = %0.2f)' % mlp_roc_auc)\r\n\r\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\r\nplt.xlim([0.0, 1.0])\r\nplt.ylim([0.0, 1.05])\r\nplt.xlabel('False Positive Rate')\r\nplt.ylabel('True Positive Rate')\r\nplt.title('ROC Curve')\r\nplt.legend(loc="lower right")\r\nplt.show()</code></pre>\r\n<p>Основываясь на кривой, мы можем сказать, что алгоритмы логистической регрессии, случайного леса и нейронной сети &ndash; многослойного персептрона имеют почти одинаковые результаты AUC. У отличной модели AUC близка к 1, что означает, что у нее хороший показатель отделимости.</p>\r\n<p>Этот вывод можно продемонстрировать и по результатам кривой ROC. Эти алгоритмы склоняются к истинно положительной скорости, а не к ложноположительной. В результате можно сказать, что эти алгоритмы имеют хорошую производительность классификации.</p>\r\n<figure class="kg-card kg-image-card kg-width-full"><img class="kg-image" src="__GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.34.05.png" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-19---22.34.05.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-19---22.34.05.png 1000w, __GHOST_URL__/content/images/2021/09/--------------2021-09-19---22.34.05.png 1410w" alt="" width="1410" height="736" loading="lazy" /></figure>\r\n<p>Наконец, мы можем вычислить средний балл точности для этих трех моделей. Результаты показывают, что все модели имеют почти одинаковый балл.</p>\r\n<pre><code class="language-python">print("Average precision score of Logistic Regression", average_precision_score(y_test, modelLR.predict_proba(X_test)[:,1]))\r\nprint("Average precision score of Random Forest", average_precision_score(y_test, modelRF.predict_proba(X_test)[:,1]))\r\nprint("Average precision score of Multilayer Perceptron", average_precision_score(y_test, modelMLP.predict_proba(X_test)[:,1]))</code></pre>\r\n<pre><code class="language-python">Average precision score of Logistic Regression 0.9651191598439374\r\nAverage precision score of Random Forest 0.9728045908653973\r\nAverage precision score of Multilayer Perceptron 0.8624254915524178</code></pre>\r\n<p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://www.kaggle.com/akashdeepkuila/credit-card-fraud-detection">здесь</a>.</p>\r\n<p>Автор: <a href="https://www.kaggle.com/akashdeepkuila/credit-card-fraud-detection/data">Akashdeep Kuila</a></p>		obnaruzhieniie-moshiennichieskikh-opieratsii	2021-09-19	Основы	https://images.unsplash.com/photo-1554672723-b208dc85134f?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3570&q=80
148	Точка пересечения (Intercept)		<p>Точка пересечения (B<sub>0</sub>) – константа уравнения <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regression)</a>, характеризующая сдвиг прямой относительно точки начала координат:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-15.png" class="kg-image" alt loading="lazy" width="720" height="480" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-15.png 600w, __GHOST_URL__/content/images/2021/09/image-15.png 720w" sizes="(min-width: 720px) 720px"><figcaption>Intercept в данном случае равен 34,3</figcaption></figure><p>В линейной регрессии мы хотим спрогнозировать <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a>  y с помощью <a href="__GHOST_URL__/priediktor/">Предиктора (Predictor Variable)</a> x и предполагаем, что между этими двумя переменными существует линейная связь. И обычно, помимо этой связи, мы предполагаем, что есть коэффициент наклона B<sub>1</sub> и дополнительный сдвиг B<sub>0</sub>:</p><!--kg-card-begin: markdown--><p>$$y = B_1 * x + B_0\\space{где}$$<br>\n$$y\\space{-}\\space{целевая}\\space{переменная,}$$<br>\n$$B_1\\space{-}\\space{коэффициент}\\space{наклона}\\space{прямой,}$$<br>\n$$B_0\\space{-}\\space{точка}\\space{пересечения}$$</p>\n<!--kg-card-end: markdown--><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/should-i-inlcude-the-intercept-in-my-machine-learning-model-2efa2a3eb58b">Gonzalo Ferreiro Volpi</a><br></p>		tochka-pieriesiechieniia	2021-09-26		
149	Мультиколлинеарность (Multicollinearity)		<p>Мультиколлинеарность – это наличие высокой взаимной корреляции между двумя или более независимыми переменными в модели <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a>. Мультиколлинеарность может привести к искаженным или вводящим в заблуждение результатам, когда исследователь или аналитик пытается определить, насколько эффективно каждая независимая переменная может быть использована наиболее для прогнозирования значения <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> в статистической модели.</p><p>В общем, мультиколлинеарность может привести к более широким Доверительным интервалам (Confidence Interval), которые дают менее надежные вероятности с точки зрения влияния независимых переменных в <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. То есть статистические выводы из нее могут быть ненадежными.</p><p>Примером может служить многомерная регрессионная модель, которая пытается предвидеть доходность акций на основе таких элементов, как отношение цены к прибыли (отношения P / E), рыночная капитализация или другие данные. Доходность акций является целевой переменной, а различные части финансовых данных – <a href="__GHOST_URL__/priediktor/">переменными-предикторами (Predictor Variable)</a>.</p><p>Мультиколлинеарность в модели множественной регрессии указывает на то, что коллинеарные независимые переменные каким-то образом связаны, хотя взаимосвязь может быть, а может и не быть случайной. Например, прошлые результаты могут быть связаны с рыночной капитализацией, поскольку акции, показавшие хорошие результаты в прошлом, будут иметь растущую рыночную стоимость.</p><p>Другими словами, мультиколлинеарность может существовать, когда две независимые переменные сильно коррелированы. Это также может произойти, если независимая переменная вычисляется из других переменных в наборе данных или если две независимые переменные дают аналогичные и повторяющиеся результаты.</p><p>Один из наиболее распространенных способов устранения проблемы мультиколлинеарности - удаление всех таких переменных, кроме одной. Также можно устранить мультиколлинеарность, объединив две или более коллинеарных переменных в одну искусственную переменную. Затем можно провести статистический анализ для изучения взаимосвязи между указанной зависимой переменной и только одной независимой переменной.</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/--------------2021-09-26---14.16.02-1.png" class="kg-image" alt loading="lazy" width="2000" height="439" srcset="__GHOST_URL__/content/images/size/w600/2021/09/--------------2021-09-26---14.16.02-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/--------------2021-09-26---14.16.02-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/09/--------------2021-09-26---14.16.02-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/09/--------------2021-09-26---14.16.02-1.png 2400w"><figcaption>Пример мультиколлинеарного датасета</figcaption></figure><p>На изображении выше два предиктора – площадь поверхности тела (Body surface area) и вес (Weight) сильно коррелированы, что подразумевает наличие мультиколлинеарности. Чтобы правильно предсказать кровяное давление (blood pressure), эти признаки стоит, к примеру, объединить.</p><p>В сфере инвестирования аналитики рынка хотят избегать использования технических индикаторов, которые коллинеарны из следующих соображений: они основаны на очень похожих или связанных исходных данных, склонны выявлять аналогичные прогнозы относительно зависимой переменной движения цены. Вместо этого анализ рынка должен основываться на разных независимых переменных, чтобы гарантировать всесторонний анализ.</p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/m/multicollinearity.asp">Gordon Scott</a></p>		multikolliniearnost	2021-09-26		
150	Круговая диаграмма (Piechart)		<p>Круговая диаграмма – это тип визуализации данных, который отображает данные в виде круговой диаграммы. Весь «пирог» представляет собой 100 процентов целого, в то время как «кусочки» пирога представляют собой его части.</p><h3 id="%D0%BA%D1%80%D1%83%D0%B3%D0%BE%D0%B2%D0%B0%D1%8F-%D0%B4%D0%B8%D0%B0%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0-seaborn">Круговая диаграмма: Seaborn</h3><p>Посмотрим, как визуализировать данные подобным образом с помощью библиотеки Seaborn:</p><pre><code class="language-python">import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Инициализация данных\ndata = [15, 25, 25, 30, 5]\nlabels = ['Group 1', 'Group 2', 'Group 3', 'Group 4', 'Group 5']\n\n# Настройка цветовой гаммы\ncolors = sns.color_palette('pastel')[0:5]\n\n# Рендеринг круговой диаграммы\nplt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\nplt.show()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/09/image-16.png" class="kg-image" alt loading="lazy" width="1024" height="872" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-16.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-16.png 1000w, __GHOST_URL__/content/images/2021/09/image-16.png 1024w" sizes="(min-width: 720px) 720px"></figure><p>Автор оригинальной статьи: <a href="https://www.statology.org/seaborn-pie-chart/">Zach</a></p>		krughovaia-diaghramma	2021-09-26		
151	Мешок слов (Bag of Words)		<p>Мешок слов (BoW) – это способ представления текстовых данных при моделировании в <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a>.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/10/image-1.png" class="kg-image" alt loading="lazy" width="602" height="452" srcset="__GHOST_URL__/content/images/size/w600/2021/10/image-1.png 600w, __GHOST_URL__/content/images/2021/10/image-1.png 602w"></figure><p>Модель набора слов проста для понимания и реализации и зарекомендовала себя с большим успехом в таких задачах, как Языковое моделирование (Language Modeling) и Классификация документов (Document Classification).</p><h3 id="%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0-%D1%81-%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%BC">Проблема с текстом</h3><p>Проблема с моделированием текста заключается в том, что он беспорядочный, а большинство <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a> машинного обучения предпочитают входные и выходные данные фиксированной длины.</p><p>Алгоритмы машинного обучения не могут работать напрямую с необработанным текстом: его необходимо преобразовать в числа а точнее, в векторы чисел. При языковой обработке векторы выводятся из текстовых данных, чтобы отразить различные лингвистические свойства текста. Это называется извлечением или Кодированием (Encoding) признаков. BoW – как раз и есть один из таких методов.</p><p>Подход очень простой и гибкий, и его можно использовать множеством способов для извлечения функций из документов.</p><p>Пакет слов – это представление текста, которое описывает "характер" присутствия слов в документе. Это подразумевает две вещи:</p><ul><li>Словарь – список уникальных присутствующих в тексте слов</li><li>Мера присутствия таких слов в тексте</li></ul><p>Это называется «мешком» слов, потому что всякая информация о порядке или структуре слов в документе отбрасывается. Модель заботится только о том, встречаются ли известные слова в документе, а не об их положении. В этом подходе мы изучаем на гистограмму частоты употребления слов в тексте, то есть рассматриваем ее как признак-столбец.</p><p>Интуиция подсказывает человеку, что тексты похожи, если имеют похожее содержание. Кроме того, только по содержанию мы можем кое-что узнать о значении документа.</p><p>Пакет слов может быть настолько простым или сложным, насколько нам хочется. Сложность возникает как при принятии решения о том, как составить словарь известных слов – <a href="www.helenkapatsa.ru/token/">Токенов (Token)</a>, так и при оценке меры их "присутствия". Мы рассмотрим обе эти проблемы более подробно.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-%D0%BC%D0%B5%D1%88%D0%BA%D0%B0-%D1%81%D0%BB%D0%BE%D0%B2">Пример модели мешка слов</h3><p><strong>Шаг 1. Соберите данные</strong><br>Ниже приведен фрагмент книги Чарльза Диккенса «Повесть о двух городах»:</p><p><em>It was the best of times,</em><br><em>It was the worst of times,</em><br><em>It was the age of wisdom,</em><br><em>It was the age of foolishness,</em></p><p>В этом небольшом примере давайте рассматривать каждую строку как отдельный «документ», а все четверостишие – как Корпус (Corpus) документов.</p><p><strong>Шаг 2: Составьте словарь</strong><br>Теперь мы можем составить список всех слов:</p><ul><li>“it”</li><li>“was”</li><li>“the”</li><li>“best”</li><li>“of”</li><li>“times”</li><li>“worst”</li><li>“age”</li><li>“wisdom”</li><li>“foolishness”</li></ul><p>Это словарь из 10 лексем корпуса, содержащего 24 слова.</p><p><strong>Шаг 3. Создайте векторы документа</strong></p><p>Следующим шагом будет оценка слов в каждом документе. Цель состоит в том, чтобы превратить каждый документ с произвольным текстом в вектор, который мы можем использовать в качестве Входных данных (Input Data) для <a href="__GHOST_URL__/modiel/">Модели (Model)</a> машинного обучения.</p><p>Самый простой метод оценки – отметить наличие слов как логическое значение, 0 – отсутствие, 1 – присутствие. Поскольку в словаре есть 10 слов, мы создадим таблицу, описывающую присутствие того или иного слова в документе № 1, то есть первой строке:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/10/bag-of-words.png" class="kg-image" alt loading="lazy" width="218" height="456"></figure><p>Двоичный вектор документа будет выглядеть следующим образом:</p><pre><code class="language-python">[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</code></pre><p>Остальные три документа выглядели бы следующим образом:</p><pre><code class="language-python">"it was the worst of times" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n"it was the age of wisdom" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n"it was the age of foolishness" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]</code></pre><p>Теперь у нас есть последовательный способ извлечения функций из любого документа в нашем корпусе, и эти вектора подходят для моделирования.</p><p>Новые документы, слова из которых лишь отчасти "покрыты" ранее составленным словарем, по-прежнему могут кодироваться, при этом оцениваются только вхождение известных слов, а неизвестные игнорируются.</p><h3 id="%D1%83%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D0%BB%D0%BE%D0%B2%D0%B0%D1%80%D0%BD%D1%8B%D0%BC-%D0%B7%D0%B0%D0%BF%D0%B0%D1%81%D0%BE%D0%BC">Управление словарным запасом</h3><p>По мере увеличения размера словаря увеличивается и векторное представление документов.</p><p>В предыдущем примере длина вектора документа равна количеству известных слов.</p><p>Для очень большого корпуса, например, тысячи книг, длина вектора – тысячи или миллионы позиций. Кроме того, каждый документ может содержать очень мало известных слов. В результате получается вектор с множеством нулевых оценок, называемый Разреженным вектором (Sparse Vector).</p><p>Для разреженных векторов требуется больше памяти и вычислительных ресурсов при моделировании, а огромное количество позиций или измерений может сделать процесс моделирования очень сложным для традиционных алгоритмов. Таким образом, при использовании мешка слов возникает необходимость уменьшить размер словарного запаса.</p><p>Существуют простые методы очистки текста, которые можно использовать в качестве первого шага, например:</p><ul><li>Игнорирование регистра</li><li>Игнорирование знаков препинания</li><li>Игнорирование часто используемых неинформативных так называемых стоп-слов, например «а», «из» и т.д.</li><li>Исправление слов с ошибками</li><li>Сокращение слов до их граммтической основы – Cnемминг (Stemming)</li></ul><p>Более сложный подход – создать словарь сгруппированных слов. Это одновременно изменяет объем словарного запаса и позволяет мешку слов выделить больше смысла из документа.</p><p>При таком подходе каждое слово или лексема называются «грамм». Создание словаря пар из двух слов, в свою очередь, называется моделью Биграмм (Bigram). Опять же, моделируются только биграммы, которые появляются в корпусе, а не все возможные биграммы.</p><p>N-грамм – это последовательность слов из N-знаков: биграмма – это последовательность из двух слов, таких как «пожалуйста, переверни», «переверни это» или «домашнее задание»; и триграмма – это последовательность из трех слов, например «пожалуйста, переверни свою» или «своя домашняя работа».</p><p>Например, биграммы в первой строке текста в предыдущем разделе: «Это были лучшие времена» выглядят следующим образом:</p><ul><li>“it was”</li><li>“was the”</li><li>“the best”</li><li>“best of”</li><li>“of times”</li></ul><p>Словарь, который затем отслеживает тройки слов, называется моделью триграммы, а общий подход называется моделью <a href="__GHOST_URL__/n-ghramm/">N-граммы (N-gram)</a>, где N – количество сгруппированных слов.</p><p>Часто биграммы показывают лучшие результаты, чем модели Ngram, где N равен 1. </p><h3 id="%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B0-%D1%81%D0%BB%D0%BE%D0%B2">Оценка слов</h3><p>После того, как словарный запас выбран, необходимо подсчитать наличие слов в примерах документов. В проработанном выше примере мы уже видели один очень простой подход к оценке: бинарная оценка наличия или отсутствия слов.</p><p>Некоторые дополнительные простые методы оценки включают в себя:</p><ul><li><strong>Подсчет</strong>: сколько раз каждое слово встречается в документе.</li><li><strong>Частота</strong> появления каждого слова в документе</li></ul><h3 id="%D1%85%D1%8D%D1%88%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D1%81%D0%BB%D0%BE%D0%B2">Хэширование слов</h3><p>Хэш-функция сопоставляет данные с набором чисел фиксированного размера. Например, мы используем их , преобразуя имена в числа для скорейшего поиска.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/10/image-2.png" class="kg-image" alt loading="lazy" width="1050" height="700" srcset="__GHOST_URL__/content/images/size/w600/2021/10/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2021/10/image-2.png 1000w, __GHOST_URL__/content/images/2021/10/image-2.png 1050w" sizes="(min-width: 720px) 720px"></figure><p>Мы можем использовать Хеширование слов (Word Hashing) в нашем словаре. Это решает проблему наличия очень большого словарного запаса для большого текстового корпуса, потому что мы можем выбрать размер хэш-пространства, который, в свою очередь, равен размеру векторного представления документа.</p><h3 id="tf-idf">TF-IDF</h3><p>Проблема с оценкой частоты слов заключается в том, что в документе преобладают очень часто встречающиеся слова, но они могут не содержать столько информации для модели, сколько более редкие, специфические для предметной области слова.</p><p>Один из подходов состоит в том, чтобы изменить частоту слов в зависимости от того, как часто они появляются во всех документах, тем самым "штрафуя" часто встречающиеся предлоги ("at"), артикли ("the") и т.д. Такой подход к оценке называется Мера оценки важности слова в контексте документа (TF-IDF), где:</p><ul><li>Term Frequency (TF) –оценка частоты встречаемости слова в текущем документе.<br>Inverse Document Frequency (IDF) – оценка того, насколько редко слово встречается в документах.</li></ul><h3 id="%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%BC%D0%B5%D1%88%D0%BA%D0%B0-%D1%81%D0%BB%D0%BE%D0%B2">Ограничения мешка слов</h3><p>Модель набора слов очень проста для понимания и реализации и предлагает большую гибкость для настройки ваших конкретных текстовых данных. Тем не менее, она страдает некоторыми недостатками, такими как:</p><ul><li><strong>Словарь</strong> требует тщательного проектирования, особенно управления размером, что влияет на разреженность представлений документа.</li><li><strong>Редкость</strong>: разреженные представления труднее моделировать как по вычислительным (пространственная и временная сложность), так и по информационным причинам. Модели должны использовать так мало информации в таком большом пространстве представлений.</li><li><strong>Значение</strong>: при отказе от порядка слов игнорируется контекст и, в свою очередь, значение слов в документе. Контекст и значение могут многое дать нашей модели. Человеку очевидна разница между фразами "this is interesting" и "is this interesting".</li></ul><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/what-is-deep-learning/">Jason Brownlee</a></p>		mieshok-slov	2021-10-03		
152	PyTorch		<p>PyTorch – это библиотека <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> с открытым исходным кодом, позволяющая решать огромное количество задач, в числе которых:</p><ul><li>Распознавание изображений (Image Recognition)</li><li>Обнаружение мошеннических операций (Fraud Detection)</li><li>Распознавание рукописного текста (Hadrwriting Recognition) и проч.</li></ul><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/10/pytorch.org_.png" class="kg-image" alt loading="lazy" width="2000" height="800" srcset="__GHOST_URL__/content/images/size/w600/2021/10/pytorch.org_.png 600w, __GHOST_URL__/content/images/size/w1000/2021/10/pytorch.org_.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/10/pytorch.org_.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/10/pytorch.org_.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption>Шапка официального сайта pytorch.org</figcaption></figure><p>Основные преимущества:</p><ul><li>Готовность к развертыванию: сопутствующие сервисы TorchScript – программа для создания и оптимизации Модели (Model) и TorchServe – фреймворк для развертывания обученной модели</li><li>Оптимизация производительности</li><li>Надежная экосистема расширяет PyTorch и поддерживает разработку в области <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерного зрения (CV)</a>, <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a> и других областях.</li><li>Облачная поддержка: PyTorch хорошо поддерживается на основных облачных платформах, обеспечивая беспроблемную разработку и легкое масштабирование.</li></ul><h3 id="%D0%BF%D1%80%D0%BE%D1%81%D1%82%D0%B5%D0%B9%D1%88%D0%B0%D1%8F-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D0%BD%D0%B0-pytorch">Простейшая модель на PyTorch</h3><p>Посмотрим, как работает фреймворк и насколько кратким может быть код. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np </code></pre><p>Мы инициируем линейную регрессию. Мы используем функцию, которая комбинирует входные значения x и веса w линейно. Создадим обучающие выборки x и y: тип данных – ‘numpy.float32’. Y – это просто удвоенные значения x. Инициализируем наши веса, равные нулю для начала:</p><pre><code class="language-python">X = np.array([1, 2, 3, 4], dtype=np.float32)\nY = np.array([2, 4, 6, 8], dtype=np.float32)\n\nw = 0.0</code></pre><p>А теперь нам нужно создать прогноз, рассчитать потери и градиент. Каждый из этих шагов мы выполним вручную. Инициализируем функцию "Прямой проход" (Forward Pass), и она получит x в качестве аргумента. "На выходе" у нашей функции W, умноженный на X</p><pre><code class="language-python">def forward(x):\n    return w * x</code></pre><p>Здесь мы определяем потерю функции, которая зависит от прогнозируемых y. В случае линейной регрессии, это среднеквадратическая ошибка. Мы можем рассчитать это, возведя разницы между предсказанными и реальными значениями y в квадрат и взяв среднее от полученного:<br>Нам предстоит теперь вручную рассчитать градиент потерь согласно нашим параметрам.</p><pre><code class="language-python">def loss(y, y_pred):\n    return ((y_pred - y)**2).mean()</code></pre><p>Давайте посмотрим на среднеквадратичную ошибку. Формула выглядит так:<br>У нас есть производная. Частное производных J и весов равно отношению единицы к N, умноженное на 2x и (w * x - y):<br>Мы реализуем это с помощью функции gradient следующим образом. Мы можем сделать это в одной строке, поэтому возвращаем numpy., умноженное на два X, разница между предсказанным и реальным y. И, конечно, нам также понадобится среднее:</p><pre><code class="language-python"># # MSE = 1 / N * (w* x - y) ** 2\n# dJ / dw = 1 / N * 2x * (w * x - y)\ndef gradient(x, y, y_pred):\n    return np.dot(2*x, y_pred - y).mean()</code></pre><p>Выведем наш прогноз перед обучением:</p><pre><code class="language-python">print(f'Предсказание перед обучением: f(5) = {forward(5):.3f}')</code></pre><p>До наших тренировок прогноз равен нулю.</p><pre><code class="language-python">Предсказание перед обучением: f(5) = 0.000</code></pre><p>А теперь приступим к обучению и определим некоторые параметры. Нам нужна скорость обучения, которая равна нулю. Определим число итераций, равное 20:</p><pre><code class="language-python">learning_rate = 0.01\nn_iters = 20</code></pre><p>Теперь давайте выполним наш цикл обучения. Выполним прямой проход, функцией forward(). Затем рассчитаем потери, потому Y теперь нам нужен, чтобы получить градиенты. И теперь мы должны обновить наши веса. Формула обновления в алгоритме градиентного спуска заключается в том, что мы просто меняем на противоположное скорость обучения градиента, а затем умножаем ее на наш градиент. </p><p>Итак, это формула обновления. Допустим, мы также хотим вывести здесь некоторую информацию, поэтому мы выводим на экран каждый шаг, если он не равен 0. Мы хотим отобразить данные о каждой эпохе. Мы инкриминируем число эпох на один, потому что нумерация смещена, а затем отображаем веса с тремя знаками после запятой. Потери отобразим по восьмой знак после запятой:</p><pre><code class="language-python">for epoch in range(n_iters):\n    y_pred = forward(X)\n    l = loss(Y, y_pred)\n    dw = gradient(X, Y, y_pred)\n    w -= learning_rate * dw\n    if epoch % 2 == 0:\n        print(f'Эпоха {epoch+1}: w = {w:.3f}, потери = {l:.8f}')</code></pre><p>Смотрите, как снижаются потери с каждой эпохой:</p><pre><code class="language-python">Эпоха 1: w = 1.200, потери = 30.00000000\nЭпоха 3: w = 1.872, потери = 0.76800019\nЭпоха 5: w = 1.980, потери = 0.01966083\nЭпоха 7: w = 1.997, потери = 0.00050331\nЭпоха 9: w = 1.999, потери = 0.00001288\nЭпоха 11: w = 2.000, потери = 0.00000033\nЭпоха 13: w = 2.000, потери = 0.00000001\nЭпоха 15: w = 2.000, потери = 0.00000000\nЭпоха 17: w = 2.000, потери = 0.00000000\nЭпоха 19: w = 2.000, потери = 0.00000000</code></pre><p>Напомню, наша формула y равен 2 на X, поэтому наша W равна двум в начале. Мы видим, что с каждым шагом тренировки увеличиваются веса и уменьшаются потери. </p><p>Давайте используем интерполяцию – смешение текстовых данных и вычисляемого значения. Предположим, что мы хотим предсказать значение y при х, равное 5; это 10. Отобразим результат как число с трем знаками после запятой:</p><pre><code class="language-python">print(f'Предсказание после обучением: f(5) = {forward(5):.3f}')</code></pre><p>После обучения наша модель предсказывает с точностью 1,9999:</p><pre><code class="language-python">Предсказание после обучением: f(5) = 10.000</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://www.kaggle.com/helenkapatsa/gradient-descent-manual">здесь</a>.</p>		pytorch	2021-10-03		
153	Ноутбук (Notebook)		<p>Ноутбук – коллекция ячеек с исполняемым кодом и результатом его работы:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/10/--------------2021-10-03---15.41.56.png" class="kg-image" alt loading="lazy" width="2000" height="1135" srcset="__GHOST_URL__/content/images/size/w600/2021/10/--------------2021-10-03---15.41.56.png 600w, __GHOST_URL__/content/images/size/w1000/2021/10/--------------2021-10-03---15.41.56.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/10/--------------2021-10-03---15.41.56.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/10/--------------2021-10-03---15.41.56.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption>Ноутбук kaggle.com</figcaption></figure><p>Самыми популярными программами и порталами, позволяющими работать с ноутбуками, являются:</p><ul><li><a href="__GHOST_URL__/google-colab/">Google Colab</a> (использует облачные мощности)</li><li>Jupyter Notebook (использует мощность Вашего ПК)</li><li>Kaggle.com (использует облачные мощности)</li></ul><p>Каждый из облачных сервисов по состоянию на 2021 г., выделяет порядка 16 Гб оперативной памяти, а также выполняет предварительную настройку среды разработки, включающую установку популярных библиотек. Это делает браузерные сервисы невероятно удобным инструментом работы.</p>		noutbuk	2021-10-03		
154	Импульс (Momentum)		<p>Импульс – это расширение алгоритма <a href="__GHOST_URL__/optimizatsiia/">Оптимизации (Optimization)</a> Градиентного спуска (Gradient Descent), которое позволяет функции поиска создавать инерцию в пространстве, преодолевать колебания шумных градиентов и двигаться по ровным участкам пространства с целью достигнуть точки Локального минимума (Local Minima).</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/10/momentum.gif" class="kg-image" alt loading="lazy" width="620" height="480" srcset="__GHOST_URL__/content/images/size/w600/2021/10/momentum.gif 600w, __GHOST_URL__/content/images/2021/10/momentum.gif 620w"><figcaption>Виды алгоритмов оптимизации</figcaption></figure><p>Градиентный спуск – это алгоритм оптимизации, который следует отрицательному градиенту целевой функции, чтобы найти ее минимум.</p><p>Проблема с градиентным спуском заключается в том, что он может перемещаться по пространству поиска в задачах оптимизации, которые имеют большое количество кривизны или <a href="__GHOST_URL__/shum/">Шума (Noise)</a>, и застревает в плоских участках, где нет градиента.</p><p>Основная идея импульса в <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a> – увеличить скорость обучения. Эта концепция – один из тех маленьких наворотов, которые, может, не так важны, но в действительности позволяют сэкономить время и значительно упрощают процесс.</p><p>Он в основном используется в <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетях (Neural Network)</a>, учитывая, что размер данных здесь провоцирует значительные временные затраты. Порой градиентный спуск может занять много времени, когда <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> достаточно велик.</p><p>Плюсы:</p><ul><li>Может использоваться для обработки шумных градиентов</li><li>Может работать с очень маленькими градиентами</li></ul><p>Минусы:</p><ul><li>Вносит дополнительную сложность в модель</li></ul><p>Прежде чем мы начнем, вот небольшой пересмотр основ градиентного спуска. Мы будем обсуждать 2 типа методов обновления – Простое обновление импульса, Обновление импульса Нестерова.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/10/image-6.png" class="kg-image" alt loading="lazy" width="275" height="183"></figure><p>Идея импульса заключается в том, чтобы подобрать именно такую величину шага (скорость обучения), чтобы целевая функция попала в точку локального минимума, а не перескочила ее.</p><h3 id="%D0%BF%D1%80%D0%BE%D1%81%D1%82%D0%BE%D0%B5-%D0%BE%D0%B1%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B8%D0%BC%D0%BF%D1%83%D0%BB%D1%8C%D1%81%D0%B0">Простое обновление импульса</h3><p>Урок физики начался. Ну вот как это происходит.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/10/image-7.png" class="kg-image" alt loading="lazy" width="750" height="391" srcset="__GHOST_URL__/content/images/size/w600/2021/10/image-7.png 600w, __GHOST_URL__/content/images/2021/10/image-7.png 750w" sizes="(min-width: 720px) 720px"></figure><p>Подумайте о потере как о американских горках, представьте, что она имеет потенциальную энергию PE, равную произведению массы, силы тяжести и высоты.</p><p>Когда градиент находится наверху, мы хотим, чтобы он шел вниз быстрее, а когда внизу, – чтобы он замедлялся и "поймал" точку минимума.</p><!--kg-card-begin: markdown--><p>$$v_2 = mu * v_1 - LR * dw $$<br>\n$$v_2\\space{}{–}\\space{скорость}\\space{новой}\\space{эпохи,}$$<br>\n$$mu\\space{}{–}\\space{ммпульс,}$$<br>\n$$v_1\\space{}{–}\\space{скорость}\\space{предыдущей}\\space{эпохи,}$$<br>\n$$LR\\space{}{–}\\space{скорость}\\space{обучения,}$$<br>\n$$dw\\space{}{–}\\space{изменение}\\space{весов}$$</p>\n<!--kg-card-end: markdown--><p>v – как бы обозначает скорость, которая инициализируется нулем (с вершины холма), mu – импульс. Думайте об этом как о коэффициенте трения, который будет противодействовать v, когда он "идет с горки". Обычно значение находится в диапазоне (0,1–0,9) и изначально принимается равным 0,9.</p><p>Эта переменная гасит энергию системы, позволяя v снова в нужный момент принять нулевое значение в точке минимума. Иногда мы меняем значение mu с 0,5 до 0,9 в течение нескольких эпох для дальнейшей оптимизации, что обеспечивает относительно небольшой прирост скорости системы.</p><h3 id="%D0%B8%D0%BC%D0%BF%D1%83%D0%BB%D1%8C%D1%81-%D0%BD%D0%B5%D1%81%D1%82%D0%B5%D1%80%D0%BE%D0%B2%D0%B0">Импульс Нестерова</h3><p>Импульс Нестерова (Nesterov Momentum)<strong> – э</strong>то дальний родственник обычного обновления импульса, но он довольно популярен из-за последовательности в получении минимумов и скорости, с которой это происходит.</p><p>Итак, основная концепция импульса Нестерова заключается в том, что, если вы знаете скорость и направление объекта, вы можете предсказать его местоположение во времени T.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/10/image-8.png" class="kg-image" alt loading="lazy" width="800" height="254" srcset="__GHOST_URL__/content/images/size/w600/2021/10/image-8.png 600w, __GHOST_URL__/content/images/2021/10/image-8.png 800w" sizes="(min-width: 720px) 720px"></figure><p>(Слева) вместо того, чтобы идти в сторону градиентного шага, иногда движение идет в другом направлении, теряя время. (Справа) импульс Нестерова вычисляет шаг, который нужно сделать в будущем, и предпринимает корректирующие действия.</p><p>Скажем, текущий вектор в позиции <code>x</code>, скорость <code>mu * v</code>. Чтобы предсказать, где эта точка окажется во времени t (следующий шаг), используем уравнение <code>x + mu * v</code>. Мы можем использовать это как прогноз для функции и отрегулировать движение градиента, чтобы занять правильное положение.</p><p>Автор оригинальной статьи: <a href="https://medium.com/@abhinav.mahapatra10/ml-advanced-momentum-in-machine-learning-what-is-nesterov-momentum-ad37ce1935fc">Abhinav Mahapatra</a></p>		impuls	2021-10-17		
155	Автокорреляция (Autocorrelation)		<p>Автокорреляция (последовательная корреляция) – сила взаимосвязи <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> во <a href="__GHOST_URL__/vriemiennoi-riad/">Временном ряду (Time Series)</a>. Коррелограммы – графики автокорреляции и частичной автокорреляции, широко используются при анализе и прогнозировании временных рядов.</p><p>Пример. Используем Датасет (Dataset) минимальных суточных температур за 10 лет (1981–1990) в г. Мельбурн, Австралия. Единицы измерения – градусы Цельсия, всего 3650 наблюдений. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">from google.colab import auth, files\nimport gspread\nfrom oauth2client.client import GoogleCredentials\n\nimport pandas as pd\nfrom pandas import read_csv\n\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf</code></pre><p>Получим токен Google Cloud SDK, чтобы использовать Google-таблицу:</p><pre><code class="language-python">auth.authenticate_user()\ngc = gspread.authorize(GoogleCredentials.get_application_default())</code></pre><p>Загрузим набор данных:</p><pre><code class="language-python">weather = gc.open_by_url('https://docs.google.com/spreadsheets/d/1f4_ZvnFOhDnAS8gipVfZ0IPFfLMQwf_pDUPSYh-54MI/edit#gid=0')\nsheet = weather.sheet1\ndf = pd.DataFrame(sheet.get_all_records())\ndf.head()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/10/--------------2021-10-24---19.48.16.png" class="kg-image" alt loading="lazy" width="408" height="450"></figure><p>Визуализируем данные, чтобы увидеть сезонные колебания:</p><pre><code class="language-python">df.plot()\npyplot.show()</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/10/autocorrelation-initial.jpg" class="kg-image" alt loading="lazy" width="468" height="348"><figcaption>График набора данных минимальных суточных температур</figcaption></figure><p>Температуры в данном случае – серия Pandas, и создается линейный график временного ряда.</p><h3 id="%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F-%D0%B8-%D0%B0%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F">Корреляция и автокорреляция</h3><p>Статистическая корреляция отображает силу взаимосвязи между двумя переменными.</p><p>Мы можем предположить, что распределение каждой переменной соответствует распределению Гаусса (кривая колокола). В этом случае мы можем использовать Коэффициент корреляции Пирсона (Pearson Correlation Coefficient), чтобы суммировать корреляцию между переменными.</p><p>Коэффициент корреляции Пирсона – это число от -1 до 1, которое описывает отрицательную или положительную корреляцию соответственно. Нулевое значение указывает на отсутствие корреляции.</p><p>Мы можем вычислить корреляцию для наблюдений временного ряда, используя n-ное и (n-1)-е наблюдения. Последние еще называют <em>лагами. </em>Поскольку корреляция наблюдений временного ряда вычисляется со значениями того же ряда в предыдущие моменты времени, это еще называется <em>последовательной корреляцией</em>.</p><p>График автокорреляции временного ряда (AutoCorrelation Function – ACF) иногда называют коррелограммой. Давайте построим такой с помощью функции <code>plot_acf()</code> библиотеки statsmodels:</p><pre><code class="language-python">plot_acf(df['Temp'])\npyplot.show()</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/10/autocorrelation.jpg" class="kg-image" alt loading="lazy" width="480" height="364"><figcaption>Коррелограмма датасета минимальных суточных температур</figcaption></figure><p>При исполнении кода ячейки создается двухмерный график, демонстрирующий корреляцию между двумя временными рядами – исходным и сдвинутым на один день назад. Мы ограничили количество значений по оси x до 35, чтобы улучшить читаемость графика. </p><p>Поскольку температурная разница между 1 и 2-м июля, очевидно, небольшая, то и корреляция будет стремиться к единице (второй столбец графика слева). Но когда мы исследуем разницу температур между 1 июля и 4 августа (35 дней разницы – крайний столбец слева), коэффициент корреляции очевидно будет меньше.</p><p>Если вы хотите ознакомиться с пошаговой последовательностью построения графика автокорреляции, посмотрите обучающее <a href="https://youtu.be/ZjaBn93YPWo">видео</a> от Brandon Rohrer.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/10/--------------2021-10-24---20.12.36.png" class="kg-image" alt loading="lazy" width="2000" height="1005" srcset="__GHOST_URL__/content/images/size/w600/2021/10/--------------2021-10-24---20.12.36.png 600w, __GHOST_URL__/content/images/size/w1000/2021/10/--------------2021-10-24---20.12.36.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/10/--------------2021-10-24---20.12.36.png 1600w, __GHOST_URL__/content/images/2021/10/--------------2021-10-24---20.12.36.png 2332w" sizes="(min-width: 720px) 720px"><figcaption>Пошаговая процедура построения коррелограммы</figcaption></figure><p>Доверительные интервалы изображены в виде полупрозрачного голубого конуса. По умолчанию установлен доверительный интервал 95%. Если голубая точка каждого из 35 коэффициентов лежит за пределами этой фигуры, то является статистически значимой единицей.</p><h3 id="%D1%87%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%B0%D1%8F-%D0%B0%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F">Частичная автокорреляция</h3><p>Частичная автокорреляция (Partial Autocorrelation – PACF) – это сводка отношений между наблюдением во временном ряду с наблюдениями на предыдущих временных шагах с удаленными взаимосвязями промежуточных наблюдений. Частичная автокорреляция при лаге k возникает после устранения влияния любых корреляций с более короткими лагами.</p><p>Автокорреляция для наблюдения и наблюдения на предыдущем временном шаге состоит как из прямой корреляции, так и из косвенной корреляции. Эти косвенные корреляции являются линейной функцией корреляция наблюдения с наблюдениями на промежуточных временных шагах.</p><p>Именно эти косвенные корреляции пытается устранить функция частичной автокорреляции. Построим такой график для нашего температурного датасета:</p><pre><code class="language-python">plot_pacf(df['Temp'], lags=50)\npyplot.show()</code></pre><p>При выполнении примера создается двухмерный график частичной автокорреляции для первых 50 лагов:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/10/autocorrelation-partial-1.jpg" class="kg-image" alt loading="lazy" width="472" height="364"><figcaption>График частичной автокорреляции датасета минимальных суточных температур</figcaption></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1lf8BKrQ4FEVS-Qusx0jggESPF3KY39b9?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/">Jason Brownlee</a></p>		avtokorrieliatsiia	2021-10-24		
156	Техника переcэмплирования синтетического меньшинства (SMOTE)	Техника переcэмплирования синтетического меньшинства (Synthetic Minority Oversampling Technique – SMOTE) – метод подготовки Несбалансированного датасета (Imbalanced Dataset) к загрузке в Модель (Model) Машинного обучения (ML), предполагающий дублирование Наблюдений (Observation) класса, представител	<p>Техника переcэмплирования синтетического меньшинства (Synthetic Minority Oversampling Technique<strong> &ndash; </strong>SMOTE) &ndash; метод подготовки Несбалансированного датасета (Imbalanced Dataset) к загрузке в <a href="__GHOST_URL__/modiel/">Модель (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, предполагающий дублирование <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> класса, представителей которого в наборе меньше остальных.</p>\r\n<p>Зачастую наборы данных являются несбалансированными: например, при исследовании раковых заболеваний подавляющее большинство пациентов здоровы. При <a href="__GHOST_URL__/obnaruzhieniie-moshiennichieskikh-opieratsii/">Обнаружении мошеннических операций (Fraud Detection)</a> большая часть финансовых транзакций все же является законными. И это существенно влияет на эффективность модели.</p>\r\n<p>Проблема работы с несбалансированными наборами данных заключается в том, что большинство методов машинного обучения будут "игнорировать" класс меньшинства и, как следствие, будут иметь низкую производительность, хотя именно эти данные наиболее важны.</p>\r\n<p>Один из подходов к устранению несбалансированности &ndash; это дублирующая выборка класса меньшинства, хоть эти примеры и не добавляют в модель никакой новой информации и синтезированы из существующих записей. Этот тип перебалансировки и называется Техникой переcэмплирования синтетического меньшинства, или сокращенно SMOTE. Этот метод был описан Нитешем Чавла (Nitesh Chawla) в своей статье 2002 года.</p>\r\n<p>SMOTE работает, выбирая примеры, которые расположены близко в пространстве признаков. Сначала выбирается случайный пример из класса меньшинства. Затем для этого примера находятся k ближайших соседей (обычно k = 5). Выбирается случайно выбранный сосед и создается синтетический пример в случайно выбранной точке между двумя примерами в пространстве признаков:</p>\r\n<figure class="kg-card kg-image-card kg-card-hascaption"><img class="kg-image" src="__GHOST_URL__/content/images/2021/10/smote.jpg" sizes="(min-width: 720px) 720px" srcset="__GHOST_URL__/content/images/size/w600/2021/10/smote.jpg 600w, __GHOST_URL__/content/images/2021/10/smote.jpg 834w" alt="" width="834" height="325" loading="lazy" />\r\n<figcaption>Синтезирование примеров "поблизости" (по центру)</figcaption>\r\n</figure>\r\n<p>Общим недостатком этого подхода является то, что синтетические примеры создаются без учета класса большинства, что может привести к неоднозначным результатам, если существует сильное перекрытие классов.</p>\r\n<p>Давайте посмотрим на рабочий пример проблемы несбалансированной классификации.</p>\r\n<h3 id="smote-imbalanced-learn">SMOTE: imbalanced-learn</h3>\r\n<p>Посмотрим, как работает SMOTE и для этого установим библиотеку imbalanced-learn и другие необходимые компоненты:</p>\r\n<pre><code class="language-python">!pip install imbalanced-learn</code></pre>\r\n<pre><code class="language-python">import collections\r\nfrom collections import Counter\r\n\r\nimport sklearn\r\nfrom sklearn.datasets import make_classification\r\n\r\nimport matplotlib\r\nfrom matplotlib import pyplot\r\n\r\nimport numpy\r\nfrom numpy import where\r\n\r\nimport imblearn\r\nfrom imblearn.over_sampling import SMOTE</code></pre>\r\n<p>В этом разделе мы изучим SMOTE, применив его к проблеме несбалансированной Бинарной классификации (Binary Classification).</p>\r\n<p>Используем функцию scikit-learn <code>make_classification()</code> для создания набора данных синтетической двоичной классификации с 10 000 примерами и распределением классов 1 к 100:</p>\r\n<pre><code class="language-python">X, y = make_classification(n_samples = 10000, n_features = 2, \r\nn_redundant = 0, n_clusters_per_class = 1, weights = [0.99], flip_y = 0, random_state = 1)</code></pre>\r\n<p>Мы можем использовать объект Counter, чтобы подсчитать представителей каждого класса и удостовериться, что набор данных был создан правильно:</p>\r\n<pre><code class="language-python">counter = Counter(y)\r\nprint(counter)</code></pre>\r\n<pre><code class="language-python">Counter({0: 9900, 1: 100})</code></pre>\r\n<p>Cоздадим <a href="__GHOST_URL__/tochechnaya-diagramma/">Точечную диаграмму (Scatterplot)</a> и раскрасим примеры для каждого класса разным цветом, чтобы четко увидеть пространственную природу дисбаланса классов:</p>\r\n<pre><code class="language-python">for label, _ in counter.items():\r\n\trow_ix = where(y == label)[0]\r\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\r\npyplot.legend()\r\npyplot.show()</code></pre>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/10/smote-initial.jpg" alt="" width="480" height="348" loading="lazy" /></figure>\r\n<p>Создается график разброса, показывающий большую массу точек, принадлежащих классу большинства (синий), и небольшое количество точек класса меньшинства (оранжевый). Обратим внимание на небольшое перекрытие.</p>\r\n<p><em>Передискретизируем</em> класс меньшинства с помощью SMOTE и построим преобразованный набор данных. Используем встроенную в Python реализацию SMOTE:</p>\r\n<pre><code class="language-python">oversample = SMOTE()\r\nX, y = oversample.fit_resample(X, y)\r\n\r\ncounter = Counter(y)\r\nprint(counter)</code></pre>\r\n<p>Мы инициировали экземпляр SMOTE с параметрами по умолчанию, которые будут уравновешивать класс меньшинства:</p>\r\n<pre><code class="language-python">Counter({0: 9900, 1: 9900})</code></pre>\r\n<p>Посмотрим, как теперь распределены наши данные в двумерном пространстве:</p>\r\n<pre><code class="language-python">for label, _ in counter.items():\r\n\trow_ix = where(y == label)[0]\r\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\r\npyplot.legend()\r\npyplot.show()</code></pre>\r\n<p>Весьма причудливо выглядят синтезированные наблюдения на точечной диаграмме: они нарушают естественное представление:</p>\r\n<figure class="kg-card kg-image-card"><img class="kg-image" src="__GHOST_URL__/content/images/2021/10/smote-after.jpg" alt="" width="480" height="348" loading="lazy" /></figure>\r\n<p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1rDyX4vpswq2RLtgXf5AJh-uBu4xtQ9n0?usp=sharing">здесь</a>.</p>\r\n<p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/">Jason Brownlee</a></p>		smote-pieriesemplirovaniie	2021-10-31	Основы	https://images.unsplash.com/photo-1578496479273-878c39a9d5e0?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3570&q=80
157	Основанная на плотности пространственная кластеризация для приложений с шумами (DBSCAN)		<p>Основанная на плотности пространственная кластеризация для приложений с шумами (DBSCAN) – это метод <a href="__GHOST_URL__/niekontroliruiemoie-obuchieniie/">Обучения без учителя (Unsupervised Learning)</a>, при котором группируем точки <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> на основе определенных характеристик в <a href="Semi-Supervised Learning, ">Кластеры (Cluster)</a> <em>произвольной формы</em>:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/11/dbscan.jpg" class="kg-image" alt loading="lazy" width="1546" height="765" srcset="__GHOST_URL__/content/images/size/w600/2021/11/dbscan.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/11/dbscan.jpg 1000w, __GHOST_URL__/content/images/2021/11/dbscan.jpg 1546w" sizes="(min-width: 1200px) 1200px"><figcaption>Очевидное преимущество DBSCAN (справа) перед K-Means в случае необычной формы кластера</figcaption></figure><p>На изображении выше есть точки данных, расположенные в виде концентрических кругов. Мы видим три различных плотных кластера в виде концентрических кругов с некоторым <a href="__GHOST_URL__/shum/">Шумом (Noise)</a>. Мы запустили <a href="__GHOST_URL__/mietod-k-sriednikh/">Метод K-средних (K-Means)</a> и получили <em>четыре</em> кластера. Данные содержат шум, поэтому и выделили четвертую группу наблюдений, обозначенную фиолетовым цветом. К сожалению, методу не удалось корректно сгруппировать точки. Кроме того, он не смог должным образом обнаружить шум. Теперь давайте посмотрим на результаты кластеризации DBSCAN.</p><p>DBSCAN не только может правильно выделить кластеры сложной формы, но также отлично обнаруживает шум. Метод создан Мартином Эстером и коллегами в 1996 году. Это алгоритм, основанный на плотности, который работает с предположением, что кластеры представляют собой плотные области в пространстве, разделенные "пустующими" областями.</p><p>Он собирает "плотно сгруппированные» точки данных в один кластер. Самая интересная особенность кластеризации DBSCAN заключается в том, что она устойчива к <a href="__GHOST_URL__/vybros/">Выбросам (Outlier)</a>. Также не требуется указывать количество кластеров заранее.</p><p>DBSCAN требует только два параметра: epsilon (радиус круга, который должен быть создан вокруг каждой точки данных для проверки плотности) и minPoints (минимальное количество точек данных, необходимых внутри этого круга для того, чтобы эта точка данных была классифицирована как базовая).</p><p>Здесь у нас есть некоторые точки данных, представленные серым цветом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/image-2.png" class="kg-image" alt loading="lazy" width="257" height="197"></figure><p> Давайте посмотрим, как DBSCAN кластеризует эти точки данных.</p><p>DBSCAN создает окружность эпсилон-радиуса вокруг каждого точки данных и классифицирует их на базовую точку, граничную точку и шум:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/11/image-3.png" class="kg-image" alt loading="lazy" width="375" height="308"><figcaption>DBCAN с minPoints = 3</figcaption></figure><p>Точка данных является центральной, если круг вокруг нее не менее minPoints точек. Если количество точек меньше minPoints, то оно классифицируется как граничная точка, а если нет других точек в пределах эпсилон-радиуса, то точка рассматривается как шум.</p><p>Все точки данных с как минимум 3 точками в круге считаются основными точками и обозначены красным цветом. Все точки данных с менее чем 3, но более чем 1 точкой в ​​круге, включая ее саму, считаются граничными точками. Они представлены желтым цветом. Наконец, точки данных, внутри круга которых нет другой точки, считаются шумом (фиолетовый цвет).</p><p>Для определения местоположения точек данных в пространстве DBSCAN использует <a href="__GHOST_URL__/ievklidovo-rasstoianiie/">Евклидово расстояние (Euclidean Distance)</a>, хотя можно использовать и другие методы. Ему необходимо просканировать весь набор данных один раз, тогда как в других алгоритмах нам приходится делать это несколько раз.</p><h3 id="%D0%B4%D0%BE%D1%81%D1%82%D1%83%D0%BF%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%B8-c%D0%B2%D1%8F%D0%B7%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Доступность и cвязность</h3><p>Это две концепции, которые вам необходимо понять, прежде чем двигаться дальше. Доступность указывает, можно ли получить доступ к точке данных из другой точки данных прямо или косвенно, тогда как Связность указывает, принадлежат ли две точки данных к одному кластеру или нет. С точки зрения этих концепций, две точки в DBSCAN можно обозначить как:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/image-5.png" class="kg-image" alt loading="lazy" width="321" height="243"></figure><ul><li>Точки X и Y плотностно достижимые (Density Reachable) для точки O</li><li>Точки X и Y плотностно связаны (Density Connected)<strong> </strong>друг с другом</li><li>Точки O и A напрямую плотностно достижимы (Directly Density Reachable) друг другу</li></ul><h3 id="%D0%B2%D1%8B%D0%B1%D0%BE%D1%80-%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2-%D0%B2-%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8-dbscan">Выбор параметров в кластеризации DBSCAN</h3><p>DBSCAN очень чувствителен к значениям epsilon и minPoints. Поэтому очень важно понимать, как выбирать эти значения, ведь небольшое изменение этих значений может значительно изменить результаты.</p><p>Значение minPoints должно быть как минимум на единицу больше, чем количество <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a> в наборе данных. Нет смысла принимать minPoints за 1, потому что это приведет к тому, что каждая точка будет отдельным кластером. Следовательно, его значение должно быть не менее 3. </p><h3 id="dbscan-scikit-learn">DBSCAN: Scikit-learn</h3><p>Давайте посмотрим, как метод реализован в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport pandas as pd\nimport math\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors</code></pre><p>Здесь я создаю набор данных только с двумя признаками, чтобы мы могли легко его визуализировать. Для создания <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> я создал функцию PointsInCircum, которая принимает радиус и количество точек данных в качестве аргументов и возвращает массив точек данных, который при нанесении на график образует круг:</p><pre><code class="language-python">np.random.seed(42)\n\n# Функция, позволяющая создать кластеры в форме окружности \ndef PointsInCircum(r, n = 100): return [(math.cos(2 * math.pi / n * x) * r + np.random.normal(-30, 30), math.sin(2 * math.pi/ n * x) * r + np.random.normal(-30, 30)) for x in range(1, n + 1)]</code></pre><p>Одного круга недостаточно, чтобы увидеть способность DBSCAN к кластеризации. Поэтому создадим три концентрических круга разного радиуса. Кроме того, добавим шума, чтобы видеть, как метод с ним справляется:</p><pre><code class="language-python"># Создадим датафрейм в форме кругов\ndf = pd.DataFrame(PointsInCircum(500, 1000))\ndf = df.append(PointsInCircum(300, 700))\ndf = df.append(PointsInCircum(100, 300))\n\n# Добавим шум\ndf = df.append([(np.random.randint(-600, 600), np.random.randint(-600, 600)) for i in range(300)])</code></pre><p>Давайте построим эти точки данных и посмотрим, как они выглядят в пространстве функций. Здесь я использую диаграмму рассеяния для построения этих точек данных. Используйте следующий синтаксис:</p><pre><code class="language-python">plt.figure(figsize = (10, 10))\nplt.scatter(df[0], df[1], s = 15, color = 'grey')\nplt.title('Датасет', fontsize = 20)\nplt.xlabel('Признак 1', fontsize = 14)\nplt.ylabel('Признак 2', fontsize = 14)\nplt.show()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/dbscan-initial.jpg" class="kg-image" alt loading="lazy" width="711" height="698" srcset="__GHOST_URL__/content/images/size/w600/2021/11/dbscan-initial.jpg 600w, __GHOST_URL__/content/images/2021/11/dbscan-initial.jpg 711w"></figure><p>Пришло время внедрить DBSCAN и убедиться в его мощи. Давайте сначала запустим его без какой-либо оптимизации параметров и посмотрим на результаты.</p><pre><code class="language-python"># Инициализируем метод\ndbscan = DBSCAN()\ndbscan.fit(df[[0, 1]])</code></pre><pre><code class="language-python">DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n       metric_params=None, min_samples=5, n_jobs=None, p=None)\n</code></pre><p>Epsilon равен 0,5, а min_samples (minPoints) – 5. Давайте визуализируем результаты этой модели:</p><pre><code class="language-python">df['DBSCAN_labels'] = dbscan.labels_ \n\n# Отобразим точки на графике\nplt.figure(figsize = (10, 10))\nplt.scatter(df[0], df[1], c = df['DBSCAN_labels'], cmap = matplotlib.colors.ListedColormap(colors), s = 15)\nplt.title('DBSCAN', fontsize = 20)\nplt.xlabel('Признак 1', fontsize = 14)\nplt.ylabel('Признак 2', fontsize = 14)\nplt.show()</code></pre><p>Все точки данных теперь фиолетового цвета, что означает, что они рассматриваются как шум:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/dbscan-one-color.png" class="kg-image" alt loading="lazy" width="711" height="699" srcset="__GHOST_URL__/content/images/size/w600/2021/11/dbscan-one-color.png 600w, __GHOST_URL__/content/images/2021/11/dbscan-one-color.png 711w"></figure><p>Это потому, что значение epsilon очень мало, и мы не оптимизировали параметры. Следовательно, нам нужно найти значение epsilon и minPoints, а затем снова обучить нашу модель. </p><p>Для эпсилона я использую график K-расстояний – дистанций между точкой и ближайшей к ней точкой данных для всех точек в наборе:</p><pre><code class="language-python">neigh = NearestNeighbors(n_neighbors = 2)\nnbrs = neigh.fit(df[[0, 1]])\ndistances, indices = nbrs.kneighbors(df[[0, 1]])\n\n# Отобразим график расстояний между точками\ndistances = np.sort(distances, axis = 0)\ndistances = distances[:, 1]\nplt.figure(figsize = (20, 10))\nplt.plot(distances)\nplt.title('K-distance Graph', fontsize = 20)\nplt.xlabel('Data Points sorted by distance', fontsize = 14)\nplt.ylabel('Epsilon', fontsize = 14)\nplt.show()</code></pre><p>Построим наш график K-расстояний и найдем значение эпсилон:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/dbscan-k-distance.png" class="kg-image" alt loading="lazy" width="1260" height="700" srcset="__GHOST_URL__/content/images/size/w600/2021/11/dbscan-k-distance.png 600w, __GHOST_URL__/content/images/size/w1000/2021/11/dbscan-k-distance.png 1000w, __GHOST_URL__/content/images/2021/11/dbscan-k-distance.png 1260w" sizes="(min-width: 720px) 720px"></figure><p>Оптимальное значение эпсилон находится в точке максимальной кривизны на графике K-расстояний, которая в данном случае равна 30. Узнаем значение minPoints. На этот раз попробуем значение 6:</p><pre><code class="language-python">dbscan_opt = DBSCAN(eps = 30, min_samples = 6)\ndbscan_opt.fit(df[[0, 1]])</code></pre><pre><code class="language-python">DBSCAN(algorithm='auto', eps=30, leaf_size=30, metric='euclidean',\n       metric_params=None, min_samples=6, n_jobs=None, p=None)</code></pre><p>Самое удивительное в DBSCAN – это его способность отделять шум. </p><pre><code class="language-python">df['DBSCAN_opt_labels'] = dbscan_opt.labels_\ndf['DBSCAN_opt_labels'].value_counts()</code></pre><p>DBSCAN сгруппировал точки данных в три кластера, а также обнаружил шум в наборе данных, представленном фиолетовым цветом. Здесь 0, 1 и 2 – три разных кластера, а -1 – это шум. </p><pre><code class="language-python">0    1030\n 1     730\n 2     318\n-1     222\nName: DBSCAN_opt_labels, dtype: int64</code></pre><p>Давайте построим график результатов и посмотрим, что у нас получится:</p><pre><code class="language-python"># Обозначим кластеры разными цветами\ncolors = ['purple', 'red', 'blue', 'green']\nplt.figure(figsize = (10, 10))\nplt.scatter(df[0], df[1], c = df['DBSCAN_opt_labels'], cmap = matplotlib.colors.ListedColormap(colors), s = 15)\nplt.title('DBSCAN Clustering', fontsize = 20)\nplt.xlabel('Feature 1',fontsize=14)\nplt.ylabel('Feature 2',fontsize=14)\nplt.show()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/dbscan-final.jpg" class="kg-image" alt loading="lazy" width="711" height="697" srcset="__GHOST_URL__/content/images/size/w600/2021/11/dbscan-final.jpg 600w, __GHOST_URL__/content/images/2021/11/dbscan-final.jpg 711w"></figure><p>Здесь важно отметить, что, хотя DBSCAN создает кластеры на основе различной плотности, он испытывает трудности, если встретит группы наблюдений схожей плотности. </p><p>Кроме того, по мере увеличения размера данных этому <a href="__GHOST_URL__/alghoritm/">Алгоритму (Algorithm)</a> становится трудно создавать кластеры, и он становится жертвой <a href="__GHOST_URL__/prokliatiie-razmiernostiei/">Проклятия размерности (Curse of Dimensionality)</a>.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1cLTdCx5d_abn5fMXMZeIG4oSEy3lfBh4?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/">Abhishek Sharma</a></p>		osnovannaia-na-plotnosti-prostranstviennaia-klastierizatsiia-dlia-prilozhienii-s-shumami	2021-11-07		
158	Модель Бокса — Дженкинса (ARIMA)		<p>Модель Бокса — Дженкинса (Авторегрессионная интегрированная скользящая средняя, англ. Autoregressive Integrated Moving Average, ARIMA) – <a href="__GHOST_URL__/alghoritm/">Алгоритм (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, позволяющий делать прогнозы на основе <a href="__GHOST_URL__/vriemiennoi-riad/">Временных рядов (Time Series)</a>, т.е. исторических наблюдений.</p><p>Если у организации есть возможность лучше прогнозировать объемы продаж продукта, она будет в более выгодном положении для оптимизации уровней запасов. Это может привести к увеличению ликвидности денежных резервов организаций, уменьшению оборотных средств и повышению удовлетворенности клиентов за счет уменьшения количества невыполненных заказов.</p><p>В области машинного обучения существует определенный набор методов и приемов, которые особенно хорошо подходят для прогнозирования значения зависимой переменной в зависимости от времени. В этой статье мы рассмотрим Авторегрессионная интегрированная скользящая средняя (ARIMA).</p><p>Мы называем ряд точек данных, проиндексированных (т.е. нанесенных на график) в хронологическом порядке временными рядами. Временной ряд можно разбить на 3 компонента.</p><ul><li><strong>Тенденция (<strong>Trend</strong>)</strong>: движение данных вверх и вниз в течение длительного периода времени (например, повышение стоимости дома).</li><li><strong>Сезонность (<strong>Seasonality</strong>)</strong>: сезонные скачки (например, рост спроса на мороженое летом).</li><li><strong><a href="__GHOST_URL__/shum/">Шум (Noise)</a></strong>: всплески и спады через случайные промежутки времени</li></ul><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/11/image-6.png" class="kg-image" alt loading="lazy" width="680" height="412" srcset="__GHOST_URL__/content/images/size/w600/2021/11/image-6.png 600w, __GHOST_URL__/content/images/2021/11/image-6.png 680w"><figcaption>Сверху вниз: исходник, тенденция, сезонность и шум</figcaption></figure><p>Прежде чем применять какую-либо статистическую модель к временному ряду, мы хотим убедиться, что в нем есть <a href="__GHOST_URL__/statsionarnost/">Стацинарность (Stationarity)</a>:</p><p>Чтобы некоторый временной ряд был классифицирован как стационарный, он должен удовлетворять трем условиям:</p><ul><li>Постоянное <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее арифметическое (Mean)</a> выборок</li><li>Постоянная <a href="__GHOST_URL__/dispiersiia/">Дисперсия (Variance)</a></li><li>Постоянная ковариация между периодами одинакового расстояния. То есть мера линейной зависимости между периодами времени одинаковой длины (скажем, 10 дней / часов / минут) должна быть идентична ковариации некоторого другого периода такой же длины.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/image-8.png" class="kg-image" alt loading="lazy" width="1290" height="408" srcset="__GHOST_URL__/content/images/size/w600/2021/09/image-8.png 600w, __GHOST_URL__/content/images/size/w1000/2021/09/image-8.png 1000w, __GHOST_URL__/content/images/2021/09/image-8.png 1290w" sizes="(min-width: 720px) 720px"><figcaption>Только крайний левый ряд обладает одинаковыми средним и дисперсией</figcaption></figure><p>Если временной ряд является стационарным и имеет определенное поведение в течение заданного временного интервала, то можно с уверенностью предположить, что он будет иметь такое же поведение в какой-то более поздний момент времени. Большинство методов статистического моделирования предполагают или требуют, чтобы временные ряды были стационарными.</p><h3 id="%D1%81%D1%82%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Стационарность</h3><p>Библиотека statsmodels предоставляет набор функций для работы с временными рядами. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()</code></pre><p>Мы будем работать с набором данных, который содержит количество пассажиров самолета в определенный день:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/iw6ucqq3rhifb7r/AirPassengers.csv?dl=1', parse_dates = ['Month'], index_col = ['Month'])\ndf.head()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/--------------2021-11-14---18.08.15.png" class="kg-image" alt loading="lazy" width="243" height="271"></figure><p>Отобразим динамику на графике:</p><pre><code class="language-python">plt.xlabel('Date')\nplt.ylabel('Number of air passengers')\nplt.plot(df)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/ARIMA-initial.jpg" class="kg-image" alt loading="lazy" width="477" height="348"></figure><p>Убедимся, что временной ряд стационарен. Есть два основных способа определить, является ли данный временной ряд таковым:</p><ul><li>Скользящие среднее и <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Stabdard Deviation)</a>: временные ряды являются стационарными, если эти метрики остаются постоянными во времени (невооруженным глазом видно, являются ли линии прямыми и параллельными оси x).</li><li>Расширенный <a href="__GHOST_URL__/tiest-diki-fulliera/">Тест Дики-Фуллера (ADF)</a>: временной ряд считается стационарным, если <a href="__GHOST_URL__/p-znachieniie/">P-значение (P-Value)</a> низкое в соответствии с <a href="__GHOST_URL__/nulievaia-ghipotieza/">Нулевой гипотезой (Null Hypothesis)</a>, а критические значения Доверительными интервалами (Confidence Intervals) 1%, 5%, 10% максимально близки к статистике ADF.</li></ul><p>Для тех, кто не понимает разницу между средним и скользящим средним: 10-дневное скользящее среднее будет усреднять цены закрытия за первые 10 дней в качестве первой точки данных. Следующая запись исключит цену первого дня, но добавит цену 11-го день и возьмет среднее значение, и так далее, как показано ниже:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/image-7.png" class="kg-image" alt loading="lazy" width="261" height="464"></figure><pre><code class="language-python">rolling_mean = df.rolling(window = 12).mean()\nrolling_std = df.rolling(window = 12).std()\nplt.plot(df, color = 'blue', label = 'Original')\nplt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\nplt.plot(rolling_std, color = 'black', label = 'Rolling Std')\nplt.legend(loc = 'best')\nplt.title('Rolling Mean &amp; Rolling Standard Deviation')\nplt.show()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/ARIMA----------------------.jpg" class="kg-image" alt loading="lazy" width="461" height="349"></figure><p>Как видите, скользящие среднее и стандартное отклонение со временем растут. Таким образом, можно сделать вывод, что временной ряд не является стационарным.</p><pre><code class="language-python">result = adfuller(df['#Passengers'])\nprint('ADF Statistic: {}'.format(result[0]))\nprint('p-value: {}'.format(result[1]))\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t{}: {}'.format(key, value))</code></pre><pre><code class="language-python">ADF Statistic: 0.8153688792060418\np-value: 0.9918802434376409\nCritical Values:\n\t1%: -3.4816817173418295\n\t5%: -2.8840418343195267\n\t10%: -2.578770059171598</code></pre><p>Статистика ADF далека от критических значений, а P-значение превышает пороговое значение (0,05). Таким образом, можно сделать вывод, что временной ряд не является стационарным.</p><p>Взятие логарифма – это простой способ снизить скорость увеличения скользящего среднего.</p><pre><code class="language-python">df_log = np.log(df)\nplt.plot(df_log)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/ARIMA-logarithm.jpg" class="kg-image" alt loading="lazy" width="464" height="334"></figure><p>Давайте создадим функцию для запуска двух тестов, которые определяют, является ли данный временной ряд стационарным.</p><pre><code class="language-python">def get_stationarity(timeseries):\n    \n    # rolling statistics\n    rolling_mean = timeseries.rolling(window = 12).mean()\n    rolling_std = timeseries.rolling(window = 12).std()\n    \n    # rolling statistics plot\n    original = plt.plot(timeseries, color = 'blue', label = 'Original')\n    mean = plt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\n    std = plt.plot(rolling_std, color = 'black', label = 'Rolling Std')\n    plt.legend(loc = 'best')\n    plt.title('Rolling Mean &amp; Standard Deviation')\n    plt.show(block = False)\n    \n    # Dickey–Fuller test:\n    result = adfuller(timeseries['#Passengers'])\n    print('ADF Statistic: {}'.format(result[0]))\n    print('p-value: {}'.format(result[1]))\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t{}: {}'.format(key, value))</code></pre><p>Есть несколько преобразований, которые мы можем применить к временному ряду, чтобы сделать его стационарным. Например, вычесть скользящее среднее:</p><pre><code class="language-python">rolling_mean = df_log.rolling(window = 12).mean()\ndf_log_minus_mean = df_log - rolling_mean\ndf_log_minus_mean.dropna(inplace = True)\nget_stationarity(df_log_minus_mean)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/ARIMA------------------------------------------.jpg" class="kg-image" alt loading="lazy" width="466" height="349"></figure><pre><code class="language-python">ADF Statistic: -3.1629079913008504\np-value: 0.022234630001244333\nCritical Values:\n\t1%: -3.4865346059036564\n\t5%: -2.8861509858476264\n\t10%: -2.579896092790057</code></pre><p>Как мы видим, после вычитания среднего скользящее среднее и стандартное отклонение стали приблизительно горизонтальны. P-значение ниже порога 0,05, а статистика ADF близка к критическим значениям. Следовательно, временной ряд стал стационарным.</p><h3 id="%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D0%B0%D0%B2%D1%82%D0%BE%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%B8-ar">Модель авторегрессии (AR)</h3><p>Модели авторегрессии основываются на предположении, что прошлые значения влияют на текущие. Такие методы обычно используются при анализе природы, экономики и других изменяющихся во времени процессов. Пока справедливо предположение, мы можем построить модель <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regression)</a>, которая пытается предсказать значение зависимой переменной сегодня, учитывая значения, которые она "узнала" о предыдущих днях.</p><h3 id="%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%B7%D1%8F%D1%89%D0%B8%D1%85-%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%85-ma">Модель скользящих средних (MA)</h3><p>Предполагается, что значение зависимой переменной в текущий день зависит от <a href="__GHOST_URL__/oshibka/">Ошибки (Error)</a> предыдущих дней. </p><p>Модель ARIMA, как Вы уже догададись, – это комбинация AR и MA.</p><h3 id="arima">ARIMA: </h3><p>Модель ARIMA использует дифференцирование к модели ARMA. При дифференцировании текущее значение вычитается из предыдущего, и полученная разность используется для преобразования временного ряда в стационарный. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/11/image-8.png" class="kg-image" alt loading="lazy" width="700" height="182" srcset="__GHOST_URL__/content/images/size/w600/2021/11/image-8.png 600w, __GHOST_URL__/content/images/2021/11/image-8.png 700w"><figcaption>Данные до (справа) и после дифференцирования со смещением на единицу</figcaption></figure><p>Три целых числа (p, d, q) обычно используются для параметризации ARIMA:</p><ul><li>p: количество членов авторегрессии</li><li>d: количество несезонных различий</li><li>q: количество условий скользящей средней</li></ul><p>Стоит также познакомиться с двумя терминами, прежде чем приступить к моделированию будущего числа пассажиров.</p><p><strong>Функция автокорреляции (ACF) – </strong>корреляция между наблюдениями в текущий момент времени и наблюдениями во все предыдущие моменты времени. Мы можем использовать ACF для определения оптимального количества условий скользящей средней. Количество элементов определяет порядок модели.</p><p><strong>Функция частичной автокорреляции (PACF): к</strong>ак следует из названия, PACF является подмножеством ACF. PACF выражает корреляцию между наблюдениями, сделанными в два момента времени, с учетом любого влияния со стороны других точек данных. Мы можем использовать PACF, чтобы определить оптимальное количество элементов для использования в модели AR. Количество элементов определяет порядок модели.</p><p>Давайте посмотрим на пример. Горизонтальные фиолетовые пунктирные линии представляют уровни значимости. Вертикальные линии представляют значения ACF и PACF в определенный момент времени. Только вертикальные линии, которые превышают горизонтальные, считаются значимыми.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/image-9.png" class="kg-image" alt loading="lazy" width="539" height="248"></figure><p>Таким образом, мы использовали предыдущие два дня в уравнении авторегрессии.</p><p>ACF можно использовать для определения наилучших параметров модели MA.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/image-10.png" class="kg-image" alt loading="lazy" width="528" height="245"></figure><p>Таким образом, в уравнении скользящего среднего мы будем использовать только предыдущий день.</p><p>Возвращаясь к нашему примеру, мы можем создать и подогнать модель ARIMA:</p><pre><code class="language-python">decomposition = seasonal_decompose(df_log) \nmodel = ARIMA(df_log, order = (2, 1, 2))\nresults = model.fit(disp = -1)\nplt.plot(df_log_shift)\nplt.plot(results.fittedvalues, color = 'red')</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/ARIMA-------------.jpg" class="kg-image" alt loading="lazy" width="466" height="334"></figure><p>Посмотрим, как модель сравнивается с исходным временным рядом:</p><pre><code class="language-python">predictions_ARIMA_diff = pd.Series(results.fittedvalues, copy=True)\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_log = pd.Series(df_log['#Passengers'].iloc[0], index = df_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value = 0)\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(df)\nplt.plot(predictions_ARIMA)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/ARIMA--------.jpg" class="kg-image" alt loading="lazy" width="461" height="334"></figure><p>Учитывая, что у нас есть данные за каждый месяц за 12 лет и мы хотим спрогнозировать количество пассажиров на следующие 10, мы используем параметр 264 – (12 * 12) + (12 * 10) = 264.</p><pre><code class="language-python">results.plot_predict(1, 264)</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/ARIMA--------------.jpg" class="kg-image" alt loading="lazy" width="458" height="334"></figure><p><br>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1EN1FB2dszKoKsiRXsxWGXa8f0-WM0CJg?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/machine-learning-part-19-time-series-and-autoregressive-integrated-moving-average-model-arima-c1005347b0d7">Cory Maklin</a></p>		modiel-boksa-dzhienkinsa	2021-11-14		
159	Лемматизация (Lemmatization)		<p>Лемматизация – объединение слов с одним и тем же корнем или леммой, но с разными склонениями или производными значения для дальнейшего анализа как элемента. Цель состоит в том, чтобы выявить присутствие слова в любой из его форм в Текстовом блоке (Corpus) и, например, определить частоту его появления.</p><p>Например, лемматизировать слова «кошки», «кошек» и «кошка» означает привести к именительному падежу все эти слова и получить «кошка». Лемматизация активно используется в области Обработки естественного языка (NLP).</p><h3 id="%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%B8-%D1%81%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3">Лемматизация и стемминг</h3><p>В отличие от лемматизации, <a href="__GHOST_URL__/stemming/">Стемминг (Stemming)</a> выделяет грамматическую основу текста. В случае с "кошкой" он отбрасывает окончание "ой" и генерирует стем "кошк".</p><h3 id="%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5-%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5">Практическое применение</h3><p>Одним из широко известных приложений лемматизации является поиск информации для поисковых систем. Лемматизация позволяет системам сопоставлять документы по темам, позволяя поисковым системам отображать релевантные результаты и даже расширять их, чтобы включить другую информацию, которая также может оказаться полезной для читателей.</p><p>Лемматизация также применяется в кластеризации документов, когда пользователям необходимо находить документы по схожей тематике.</p><p>Лемматизация также полезна для улучшения результатов поисковой оптимизации. Поисковые системы, такие как Google, используют эту технологию для предоставления пользователям высокорелевантных результатов. Обратите внимание, что когда пользователи вводят запросы, поисковая система автоматически лемматизирует слова, чтобы понять смысл поискового запроса и дать релевантные и исчерпывающие результаты.</p><h3 id="%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-natasha">Лемматизация: natasha</h3><p>Понимаю, заголовок звучит весьма странно, однако natasha – это прекрасная библиотека, пригодная для широкого спектра задач NLP, в том числе для лемматизации. Посмотрим, как элегантно этот модуль позволяет приводить слова в тексте к исходному состоянию. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">!pip install natasha</code></pre><p>Импортируем некоторые составляющие:</p><pre><code class="language-python">import natasha\nfrom natasha import (\n    Segmenter,\n    MorphVocab,\n    \n    NewsEmbedding,\n    NewsMorphTagger,\n    NewsSyntaxParser,\n   \n    PER,\n    Doc\n)</code></pre><p>Инициируем экземпляры Segmentor, который разделяет текст на слова, MorphVocab, содержащий в себе слова-токены наподобие словаря, NewsMorphTagger, описывающий грамматические свойства слова, и NewsSyntaxParser, описывающий роли слов в предложениях:</p><pre><code class="language-python">segmenter = Segmenter()\nmorph_vocab = MorphVocab()\n\nemb = NewsEmbedding()\nmorph_tagger = NewsMorphTagger(emb)\nsyntax_parser = NewsSyntaxParser(emb)</code></pre><p>Создадим текстовый блок. Как видите, библиотека в некой мере ориентирована на анализ новостей:</p><pre><code class="language-python">text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\ndoc = Doc(text)</code></pre><p>Сегментируем абзац на предложения и посмотрим на первые два с помощью <code>display(doc.sents[:2])</code>:</p><pre><code class="language-python">doc.segment(segmenter)\ndisplay(doc)\ndisplay(doc.sents[:2])\ndisplay(doc.tokens[:5])</code></pre><p>Функция <code>display(doc.tokens[:5])</code> позволит посмотреть первые пять слов:</p><pre><code class="language-python">Doc(text='Посол Израиля на Украине Йоэль Лион признался, чт..., tokens=[...], sents=[...])\n[DocSent(stop=218, text='Посол Израиля на Украине Йоэль Лион признался, чт..., tokens=[...]),\n DocSent(start=219, stop=257, text='Свое заявление он разместил в Twitter.', tokens=[...])]\n[DocToken(stop=5, text='Посол'),\n DocToken(start=6, stop=13, text='Израиля'),\n DocToken(start=14, stop=16, text='на'),\n DocToken(start=17, stop=24, text='Украине'),\n DocToken(start=25, stop=30, text='Йоэль')]</code></pre><p>С помощью морфологии – науке о формах слов, изучим каждое отдельное слово:</p><pre><code class="language-python">doc.tag_morph(morph_tagger)\ndoc.parse_syntax(syntax_parser)\ndisplay(doc.tokens[:5])</code></pre><p>Отобразим первые пять слов и их свойства – идентификатор (id), роль в предложении (nsubj – подлежащее), часть речи (noun – существительное), прочие интересные свойства (anim – одушевленное, nom – нарицательный, masc – мужского рода, sing – в единственном числе):</p><pre><code class="language-python">[DocToken(stop=5, text='Посол', id='1_1', head_id='1_7', rel='nsubj', pos='NOUN', feats=&lt;Anim,Nom,Masc,Sing&gt;),\n DocToken(start=6, stop=13, text='Израиля', id='1_2', head_id='1_1', rel='nmod', pos='PROPN', feats=&lt;Inan,Gen,Masc,Sing&gt;),\n DocToken(start=14, stop=16, text='на', id='1_3', head_id='1_4', rel='case', pos='ADP'),\n DocToken(start=17, stop=24, text='Украине', id='1_4', head_id='1_1', rel='nmod', pos='PROPN', feats=&lt;Inan,Loc,Fem,Sing&gt;),\n DocToken(start=25, stop=30, text='Йоэль', id='1_5', head_id='1_1', rel='appos', pos='PROPN', feats=&lt;Anim,Nom,Masc,Sing&gt;)]</code></pre><p>И наконец, лемматизируем текст:</p><pre><code class="language-python">for token in doc.tokens:\n    token.lemmatize(morph_vocab)\n    \n{_.text: _.lemma for _ in doc.tokens[:10]}</code></pre><p>Список получился внушительный, потому выведем только слайс из первых десяти элементов:</p><pre><code class="language-python">{',': ',',\n 'Израиля': 'израиль',\n 'Йоэль': 'йоэль',\n 'Лион': 'лион',\n 'Посол': 'посол',\n 'Украине': 'украина',\n 'на': 'на',\n 'признался': 'признаться',\n 'пришел': 'прийти',\n 'что': 'что'}</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1OYAJqCTNG3J1qXVLbmipjMs-jLjUs8eO?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.techslang.com/definition/what-is-lemmatization/">techslang.com</a></p>		liemmatizatsiia	2021-11-21		
160	Сравнение конструкторов чат-ботов		<p>Чат-бот – виртуальный ассистент, имитирующий с помощью <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a> способности живого собеседника.</p><p>Сравним некоторые конструкторы ботов с помощью небольшого перечня характеристик.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/11/chat-bot-interface.jpg" class="kg-image" alt loading="lazy" width="697" height="221" srcset="__GHOST_URL__/content/images/size/w600/2021/11/chat-bot-interface.jpg 600w, __GHOST_URL__/content/images/2021/11/chat-bot-interface.jpg 697w"></figure><p>Редактор потока (Flowchart) – это специфическое представление логики общения, иерархическая диаграмма:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/11/image-11.png" class="kg-image" alt loading="lazy" width="1024" height="504" srcset="__GHOST_URL__/content/images/size/w600/2021/11/image-11.png 600w, __GHOST_URL__/content/images/size/w1000/2021/11/image-11.png 1000w, __GHOST_URL__/content/images/2021/11/image-11.png 1024w"><figcaption>Редактор потока DialogFlow CX</figcaption></figure><p>Тестирование подразумевает, что сценарист чат-бота может опробовать результат прямо в консоли.</p><p>"Легкость установки" – это субъективная оценка легкости добавления ассистента на сайт, например, с помощью скрипта в заголовке сайта.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/11/chat-bot-elements-1.jpg" class="kg-image" alt loading="lazy" width="737" height="414" srcset="__GHOST_URL__/content/images/size/w600/2021/11/chat-bot-elements-1.jpg 600w, __GHOST_URL__/content/images/2021/11/chat-bot-elements-1.jpg 737w"></figure><p>Быстрые ответы (Suggestion Chips) – это кнопки с предполагаемыми распространенными ответами:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/11/image-12.png" class="kg-image" alt loading="lazy" width="409" height="208"><figcaption>Предполагаемые ответы пользователя</figcaption></figure><p>Проверка введенных данных необходима, чтобы исключить ошибку при вводе, скажем, номера телефона. Как правило, это маска ввода:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/11/image-13.png" class="kg-image" alt loading="lazy" width="405" height="302"></figure><p>Еще один важный раздел – сервисы, для которых настроена интеграция:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/11/chat-bot-integrations.jpg" class="kg-image" alt loading="lazy" width="588" height="414"></figure><p>Полная сравнительная таблица для 49 конструкторов <a href="https://chatimize.com/chatbot-platform-comparison/">здесь</a>.</p>		sravneniye-konstruktorov-chat-botov	2021-11-28		
161	Модель Хольта-Винтера (Holt-Winters' Model)		<p>Модель Хольта-Винтера (алгоритм тройного экспоненциального сглаживания) – метод прогнозирования <a href="__GHOST_URL__/vriemiennoi-riad/">Временных рядов (Time Series)</a>, учитывающий тренд, Cезонность (Seasonality) и <a href="__GHOST_URL__/shum/">Шум (Noise)</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/12/image-1.png" class="kg-image" alt loading="lazy" width="707" height="492" srcset="__GHOST_URL__/content/images/size/w600/2021/12/image-1.png 600w, __GHOST_URL__/content/images/2021/12/image-1.png 707w"><figcaption>Прогноз модели HWM. Источник: staesthetic.wordpress.com</figcaption></figure><p>Анализ временных рядов – это наиболее широко используемая область <a href="__GHOST_URL__/nauka-o-dannykh/">Науки о данных (Data Science)</a> и <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>; он использует исторические данные, чтобы определить тенденцию. На прогнозируемые значения могут влиять определенные внешние факторы, которые известны как <a href="__GHOST_URL__/priediktor/">Предиктор (Predictor Variable</a>), например, на продажи продукта влияет скидка, или температура зависит от влажности и скорости ветра и т.д.</p><p>Специалисту по анализу данных доступен ряд алгоритмов прогнозирования временных рядов, но выбор зависит от бизнес-задач и <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>. Для анализа могут использоваться простые методы прогнозирования временных рядов, таких как Скользящее среднее (Moving Average), Экспоненциальное сглаживание (Exponential Smoothing), <a href="__GHOST_URL__/modiel-boksa-dzhienkinsa/">Модель Бокса — Дженкинса (ARIMA)</a> и т.д. Более продвинутое прогнозирование подразумевает <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокое обучение (Deep Learning)</a> и <a href="__GHOST_URL__/riekurrientnaia-nieirosiet/">Рекуррентные нейросети (RNN)</a>, Долгую краткосрочную память (LSTM), <a href="a">Экстремальный градиентный бустинг (XGBoost)</a>, Градиентный бустинг (GB) и т.д.</p><p>Экспоненциальное сглаживание Холта-Винтера, названное в честь двух его авторов – Чарльза Холта (Charles Holt) и Питера Винтера (Peter Winter), является одним из старейших методов анализа временных рядов, который учитывает тенденции и сезонность при прогнозировании. Этот метод имеет три основных аспекта: среднее значение с учетом тренда и сезонности. Эти три аспекта представляют собой три типа экспоненциального сглаживания, поэтому метод также известен как тройное экспоненциальное сглаживание. Разберем подробно каждый из аспектов.</p><p>Простое экспоненциальное сглаживание, как следует из названия, используется для прогнозирования, когда набор данных не имеет тенденций или сезонности.</p><p>Метод сглаживания Холта, также известный как <em>линейное экспоненциальное сглаживание</em>, является широко известной моделью сглаживания для данных, имеющих тенденцию. Метод сглаживания Винтера позволяет учитывать сезонность при прогнозировании наряду с трендом. Следовательно, метод Холта-Винтера учитывает среднее значение, а также тенденцию и сезонность при прогнозировании временных рядов.</p><h3 id="%D1%82%D1%80%D0%BE%D0%B9%D0%BD%D0%BE%D0%B5-%D1%8D%D0%BA%D1%81%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5-%D1%81%D0%B3%D0%BB%D0%B0%D0%B6%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-statsmodels">Тройное экспоненциальное сглаживание: statsmodels</h3><p>Давайте посмотрим, как работает метод, с помощью примера. У нас есть хронология посещаемости веб-сайта за несколько дней, давайте попробуем предсказать количество посетителей на следующие 3 дня. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import matplotlib\nfrom matplotlib import pyplot as plt\nfrom pylab import rcParams\n\nimport pandas as pd\nimport seaborn as sns\n\nimport statsmodels\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.seasonal import seasonal_decompose \nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing   \nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# Зададим стандартный размер холста графика\nrcParams['figure.figsize'] = 20, 5</code></pre><p>Импортируем данные об уникальных посетителях сайта:</p><pre><code class="language-python">visitors = pd.read_csv('https://www.dropbox.com/s/5uidyjqy8dijae4/daily-website-visitors.csv?dl=1', sep = ';', index_col = 'Date', parse_dates = True)\nvisitors = visitors.rename(columns = {'Unique Visits': 'Уникальных посетителей'})\nvisitors.head()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/--------------2021-12-05---14.57.09.png" class="kg-image" alt loading="lazy" width="674" height="546" srcset="__GHOST_URL__/content/images/size/w600/2021/12/--------------2021-12-05---14.57.09.png 600w, __GHOST_URL__/content/images/2021/12/--------------2021-12-05---14.57.09.png 674w"></figure><p>Заполним пробелы в столбце посещаемости, если таковые имеются, и построим Ящик с усами (Boxplot), чтобы увидеть Выбросы (Outlier) – значительно отстоящие от основного массива Наблюдения (Observation):</p><pre><code class="language-python">visitors['Уникальных посетителей'].fillna(value = visitors ['Уникальных посетителей'].mean(), inplace = True)\nsns.boxplot(x = visitors['Уникальных посетителей'])</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2021/12/halt-winters-model-boxplot.jpg" class="kg-image" alt loading="lazy" width="1220" height="401" srcset="__GHOST_URL__/content/images/size/w600/2021/12/halt-winters-model-boxplot.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/12/halt-winters-model-boxplot.jpg 1000w, __GHOST_URL__/content/images/2021/12/halt-winters-model-boxplot.jpg 1220w" sizes="(min-width: 1200px) 1200px"></figure><p>Рассчитаем <a href="__GHOST_URL__/standartizovannaia-otsienka/">Стандартизованную оценку (Z-Score)</a> для каждого наблюдения, то есть меру удаленности от <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднего значения (Mean)</a>:</p><pre><code class="language-python">visitors['Стандартизованная оценка'] = (visitors['Уникальных посетителей'] - visitors['Уникальных посетителей'].mean()) / visitors['Уникальных посетителей'].std(ddof = 0)\nvisitors.head()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/--------------2021-12-05---15.02.06.png" class="kg-image" alt loading="lazy" width="1150" height="546" srcset="__GHOST_URL__/content/images/size/w600/2021/12/--------------2021-12-05---15.02.06.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/--------------2021-12-05---15.02.06.png 1000w, __GHOST_URL__/content/images/2021/12/--------------2021-12-05---15.02.06.png 1150w" sizes="(min-width: 720px) 720px"></figure><p>Сгруппируем данные по месяцам с помощью метода <code>resample()</code> и отобразим результат на графике:</p><pre><code class="language-python">visitors = visitors.resample('MS').sum()\nvisitors[['Уникальных посетителей']].plot(title = 'Ежемесячная посещаемость')</code></pre><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/12/halt-winters-model-monthly.jpg" class="kg-image" alt loading="lazy" width="1261" height="415" srcset="__GHOST_URL__/content/images/size/w600/2021/12/halt-winters-model-monthly.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/12/halt-winters-model-monthly.jpg 1000w, __GHOST_URL__/content/images/2021/12/halt-winters-model-monthly.jpg 1261w"></figure><p>Разложим временной ряд на компоненты:</p><pre><code class="language-python">decompose_result = seasonal_decompose(visitors['Уникальных посетителей'], model = 'multiplicative')\ndecompose_result.plot()\nplt.show()</code></pre><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/12/halt-winters-model-seasonal.jpg" class="kg-image" alt loading="lazy" width="1520" height="438" srcset="__GHOST_URL__/content/images/size/w600/2021/12/halt-winters-model-seasonal.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/12/halt-winters-model-seasonal.jpg 1000w, __GHOST_URL__/content/images/2021/12/halt-winters-model-seasonal.jpg 1520w"></figure><p>Разделим датасет на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные данные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>:</p><pre><code class="language-python">train_visitors = visitors[:55]\ntest_visitors = visitors[55:]</code></pre><p>Построим график, отображающий прогноз посещаемости:</p><pre><code class="language-python">fitted_model = ExponentialSmoothing(train_visitors['Уникальных посетителей'], trend = 'mul', seasonal = 'mul', seasonal_periods = 2).fit()\ntest_predictions = fitted_model.forecast(25)\ntrain_visitors['Уникальных посетителей'].plot(legend = True, label = 'Тренировочные данные')\ntest_visitors['Уникальных посетителей'].plot(legend = True, label = 'Тестовые данные')\ntest_predictions.plot(legend = True, label = 'Прогноз')</code></pre><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2021/12/halt-winters-model-forecast.jpg" class="kg-image" alt loading="lazy" width="1261" height="401" srcset="__GHOST_URL__/content/images/size/w600/2021/12/halt-winters-model-forecast.jpg 600w, __GHOST_URL__/content/images/size/w1000/2021/12/halt-winters-model-forecast.jpg 1000w, __GHOST_URL__/content/images/2021/12/halt-winters-model-forecast.jpg 1261w"></figure><h3 id="%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%B0">Ограничения метода</h3><p>Несмотря на то, что метод Холта-Винтера дает отличный результат, он все же имеет определенные недостатки. Одним из основных ограничений этого алгоритма является <em>мультипликативная характеристика сезонности: </em>модель работает, когда у нас есть временные рамки с очень небольшими суммами. Временной ряд с точками, имеющими значения 10 и 1, провоцирует фактическую разницу в 9 пунктов, то есть первое число больше второго на 1000%. Более того, сезонность, которая тоже является относительным понятием, может резко исказить прогнозы, что и произошло с нашим прогнозом посещаемости.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1Q1rMfXRb6bc3ig_a6xf-wIgnBpvsZrcR?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2021/08/holt-winters-method-for-time-series-analysis/">Snehal_bm</a></p>		modiel-kholta-vintiersa	2021-12-05		
162	Площадь под ROC-кривой (AUC ROC)		<p>Площадь под ROC-кривой (Area Under Curve – площадь под кривой, Receiver Operating Characteristic – рабочая характеристика приёмника) – это метрика оценки для задач Бинарной классификации (Binary Classification). Площадь под кривой (AUC) является мерой способности классификатора различать классы и используется в качестве сводки кривой ROC:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/12/AUC-ROC.jpg" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>ROC-кривая (фиолетового цвета)</figcaption></figure><p>Представьте: вы создали свою <a href="__GHOST_URL__/modiel/">Модель (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, но что же дальше? Вам необходимо оценить ее и оценить качество, чтобы затем Вы могли решить, стоит ли она внедрения. Вот тут-то и пригодится кривая AUC ROC.</p><p>Название может показаться скучным, но оно просто говорит о том, что мы вычисляем «Площадь под кривой» (AUC) «рабочей характеристики приёмника (ROC). Пока много неясного, но мы подробно рассмотрим, что означают эти термины, и все встанет на свои места.</p><p>ROC-кривая помогает визуализировать, насколько хорошо работает классификатор машинного обучения. Хотя она работает только для задач двоичной классификации, ближе к концу мы увидим, как расширить его, чтобы решать проблемы Мультиклассовой классификации (Multi-Class Classification).</p><p>Мы также затронем такие темы, как Чувствительность (Sensitivity) и Специфичность (Specificity), поскольку это ключевые вспомогательные термины. </p><h3 id="%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA">Матрица ошибок</h3><p>Из <a href="__GHOST_URL__/matritsa-oshibok/">Матрицы ошибок (Confusion Matrix)</a> мы можем вывести некоторые важные показатели. Это показатель успешности классификации, где классов два или более. Это таблица с 4 различными комбинациями сочетаний прогнозируемых и фактических значений.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/confusion-matrix.jpg" class="kg-image" alt loading="lazy" width="568" height="189"></figure><p>Давайте рассмотрим значения ячеек (истинно позитивные, ошибочно позитивные, ошибочно негативные, истинно негативные) с помощью "беременной" аналогии.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/confusion-matrix-error-types.png" class="kg-image" alt loading="lazy" width="512" height="512"></figure><p><br><strong>Истинно позитивное предсказание (True Positive, сокр. TP)</strong><br>Вы предсказали положительный результат, и женщина действительно беременна.</p><p><strong>Истинно отрицательное</strong> <strong>предсказание (True Negative, TN)</strong><br>Вы предсказали отрицательный результат, и мужчина действительно не беременен.</p><p><strong>Ошибочно положительное</strong> <strong>предсказание (ошибка типа I, False Positive, FN)</strong><br>Вы предсказали положительный результат (мужчина беременен), но на самом деле это не так.</p><p><strong>Ошибочно отрицательное</strong> <strong>предсказание (ошибка типа II, False Negative, FN)</strong><br>Вы предсказали, что женщина не беременна, но на самом деле она беременна.</p><h3 id="%D1%87%D1%83%D0%B2%D1%81%D1%82%D0%B2%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Чувствительность</h3><p>Очевидно, желательны более высокий уровень TP и более низкий уровень FN. Чувствительность говорит нам, какая часть положительного класса была правильно классифицирована:</p><!--kg-card-begin: markdown--><p>$$Чувствительность = \\frac{TP}{TP + FN}$$<br>\n$$TP\\space{}{–}\\space{количество}\\space{истинно}\\space{позитивных}\\space{предсказаний,}$$<br>\n$$FN\\space{}{–}\\space{количество}\\space{ошибочно}\\space{отрицательных}\\space{предсказаний}$$</p>\n<!--kg-card-end: markdown--><h3 id="%D1%81%D0%BF%D0%B5%D1%86%D0%B8%D1%84%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C-tpr">Специфичность (TPR)</h3><p>Специфичность говорит нам, какая часть отрицательного класса была правильно классифицирована:</p><!--kg-card-begin: markdown--><p>$$Специфичность = \\frac{TN}{TN + FP}$$<br>\n$$TN\\space{}{–}\\space{количество}\\space{истинно}\\space{негативных}\\space{предсказаний,}$$<br>\n$$FP\\space{}{–}\\space{количество}\\space{ошибочно}\\space{позитивных}\\space{предсказаний}$$</p>\n<!--kg-card-end: markdown--><p>Если взять тот же пример, что и в случае с чувствительностью, специфичность будет означать определение доли здоровых людей, которые были правильно идентифицированы моделью.</p><h3 id="%D0%B4%D0%BE%D0%BB%D1%8F-%D0%BD%D0%B5%D0%B2%D0%B5%D1%80%D0%BD%D0%BE-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%BD%D0%B5%D0%B3%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D1%8B%D1%85-%D0%BD%D0%B0%D0%B1%D0%BB%D1%8E%D0%B4%D0%B5%D0%BD%D0%B8%D0%B9">Доля неверно классифицированных негативных наблюдений</h3><p><a href="__GHOST_URL__/dolia-lozhnykh-polozhitielnykh-klassifikatsii/">Доля ложных положительных классификаций (FPR)</a> сообщает нам, какая часть отрицательного класса была неправильно классифицирована классификатором:</p><!--kg-card-begin: markdown--><p>$$FPR = 1 - Специфичность$$</p>\n<!--kg-card-end: markdown--><p>Из этих показателей чувствительность и специфичность, пожалуй, самые важные, и позже мы увидим, как они используются для построения оценочной метрики. Но перед этим давайте разберемся, почему вероятность предсказания лучше, чем предсказание целевого класса напрямую.</p><h3 id="%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%BF%D1%80%D0%BE%D0%B3%D0%BD%D0%BE%D0%B7%D0%BE%D0%B2">Вероятность прогнозов</h3><p>Модель классификации может использоваться для непосредственного определения класса <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> или прогнозирования ее <em>вероятности</em> принадлежности к разным классам. Последнее, как ни странно, дает нам больше контроля над результатом. Мы можем задать наш собственный порог для интерпретации результата. Иногда это более благоразумно, чем просто построить совершенно новую модель!</p><p>Установка различных пороговых значений для классификации положительного класса непреднамеренно изменит чувствительность и специфичность модели. И один из этих пороговых значений, вероятно, даст лучший результат, чем другие, в зависимости от того, стремимся ли мы снизить количество ложноотрицательных или ложноположительных результатов. Взгляните на таблицу ниже:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/12/AUC-ROC-2.jpg" class="kg-image" alt loading="lazy" width="522" height="646"><figcaption>Попробуйте рассчитать часть долей самостоятельно</figcaption></figure><p>Показатели меняются с изменением пороговых значений. Мы можем генерировать различные матрицы путаницы и сравнивать различные метрики, которые обсуждали в предыдущем разделе. Но это было бы неразумно. Вместо этого мы можем создать график между некоторыми из этих показателей, чтобы легко увидеть, какой порог дает наилучший результат. Кривая AUC-ROC решает именно эту проблему!</p><p>ROC-кривая – это кривая вероятности, которая отображает отношение TPR к FPR при различных пороговых значениях и по существу отделяет «сигнал» от «шума». Площадь под кривой (AUC) является мерой способности классификатора различать классы. Чем выше площадь под кривой, тем лучше производительность модели.</p><p>Когда AUC равен единице, тогда классификатор может правильно различать все положительные и отрицательные точки класса:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-5.png" class="kg-image" alt loading="lazy" width="205" height="201"></figure><p>Однако, если бы AUC была равна 0, тогда классификатор предсказывал бы все отрицательные значения как положительные, а все положительные – как отрицательные.</p><p>Когда 0,5 &lt; AUC &lt; 1, высока вероятность, что классификатор сможет различить положительные и отрицательных значения класса. Это так, потому что классификатор может обнаруживать больше истинно положительных и истинно отрицательных результатов, чем ложно отрицательных и ложно положительных результатов:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-6.png" class="kg-image" alt loading="lazy" width="205" height="201"></figure><p>Когда AUC = 0,5, тогда классификатор не может различать положительные и отрицательные баллы класса. Это означает, что либо классификатор предсказывает случайный класс, либо существует один постоянный класс для всех точек данных:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-7.png" class="kg-image" alt loading="lazy" width="205" height="201"></figure><p>Таким образом, чем выше значение AUC для классификатора, тем лучше его способность различать положительные и отрицательные классы.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-%D0%BA%D1%80%D0%B8%D0%B2%D0%B0%D1%8F-auc-roc">Как работает кривая AUC-ROC?</h3><p>На ROC-кривой более высокое значение по оси X указывает на бо́льшее количество ложных, чем истинно отрицательных "срабатываний". В то время как более высокое значение по оси Y указывает на большее количество истинно положительных, чем ложно отрицательных результатов. Итак, выбор порога зависит от способности балансировать между "ложными срабатываниями" и "ложными отрицаниями".</p><p>Давайте копнем немного глубже и поймем, как наша кривая будет выглядеть для разных пороговых значений и как будут меняться специфичность и чувствительность.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-8.png" class="kg-image" alt loading="lazy" width="267" height="252"></figure><p>Мы можем попытаться понять этот график, сгенерировав матрицу путаницы для каждой точки, соответствующей порогу, и поговорим о производительности нашего классификатора:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/AUC-ROC-confusion-matrix.jpg" class="kg-image" alt loading="lazy" width="261" height="280"></figure><p>В точке А графика выше чувствительность самая высокая, а специфичность – самая низкая. Это означает, что все представители положительного класса классифицируются правильно, а все представители отрицательного – неправильно. Фактически, любая точка на синей линии соответствует ситуации, когда TPR равен FPR.</p><p>Все точки над этой линией соответствуют ситуации, когда доля правильно классифицированных точек, принадлежащих к положительному классу, больше, чем доля неправильно классифицированных точек, принадлежащих к отрицательному классу.</p><p>Хотя точка B имеет ту же чувствительность, что и точка A, у нее более высокая специфичность. Это означает, что количество баллов ложно отрицательного класса ниже по сравнению с предыдущим порогом. Это указывает на то, что данный порог лучше предыдущего.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-9.png" class="kg-image" alt loading="lazy" width="299" height="195"></figure><p>Чувствительность в точке C выше, чем в D при той же специфичности. Это означает, что для того же количества неправильно классифицированных представителей отрицательного класса классификатор предсказал большее количество баллов положительного класса. Следовательно, порог в точке C лучше, чем в точке D.</p><p>Теперь, в зависимости от того, сколько неправильно классифицированных очков мы хотим допустить для нашего классификатора, мы будем выбирать между точкой B или C.</p><p>Точка E – это место, где специфичность становится самой высокой:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-10.png" class="kg-image" alt loading="lazy" width="156" height="194"></figure><p>Это означает, что модель не содержит ложных срабатываний. Модель может правильно классифицировать все отрицательные записи! Мы бы выбрали эту точки, если бы наша задача заключалась в генерации рекомендации Spotify.</p><p>Следуя этой логике, можете ли вы угадать, где на графике будет находиться точка, соответствующая идеальному классификатору? Да! Она будет в верхнем левом углу графика ROC, соответственно координатам (0, 1). Именно здесь чувствительность и специфичность будут наивысшими, и классификатор правильно классифицирует все положительные и отрицательные наблюдения.</p><h3 id="auc-roc-scikit-learn">AUC-ROC: Scikit-learn</h3><p>Давайте посмотрим, как кросс-валидация реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score</code></pre><p>Сгенерируем игрушечный <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> из 1000 наблюдений, каждое из которых принадлежит одному из двух классов. Разделим его на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные данные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a> в пропорции 3 к 7:</p><pre><code class="language-python">X, y = make_classification(n_samples = 1000, n_classes = 2, n_features = 20, random_state = 27)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 27)</code></pre><p>Инициируем модели <a href="__GHOST_URL__/loghistichieskaia-rieghriessiia/">Логистической регрессии (Logistic Regression)</a> и <a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метода K-ближайших соседей (kNN</a>). Затем обучим модели и сгенерируем предсказания классов для тестовой выборки:</p><pre><code class="language-python">model1 = LogisticRegression()\nmodel2 = KNeighborsClassifier(n_neighbors = 4)\n\nmodel1.fit(X_train, y_train)\nmodel2.fit(X_train, y_train)\n\npred_prob1 = model1.predict_proba(X_test)\npred_prob2 = model2.predict_proba(X_test)</code></pre><p>Конечно, мы можем вручную проверить чувствительность и специфичность для каждого порога, однако предпочту дать scikit-learn сделать всю работу за нас. В этой прекрасной библиотеке есть очень мощный метод <code>roc_curve()</code>, который вычисляет ROC для классификатора за считанные секунды! Он возвращает значения FPR, TPR и пороговые значения:</p><pre><code class="language-python">fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:, 1], pos_label = 1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:, 1], pos_label = 1)\n\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label = 1)</code></pre><p>Дело за малым: рассчитаем площади под кривой:</p><pre><code class="language-python">auc_score1 = roc_auc_score(y_test, pred_prob1[:, 1])\nauc_score2 = roc_auc_score(y_test, pred_prob2[:, 1])\n\nprint(auc_score1, auc_score2)</code></pre><p>Модель логистической регрессии более эффективна в данной ситуации, чем метод K-ближайших соседей:</p><pre><code class="language-python">0.9762374461979914 0.9233769727403157</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1PjFG96exMov9uRNAnkkJu70LVY7soILg?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/">Aniruddha Bhandari</a></p>		roc-krivaia	2021-12-12		
163	Градиентный бустинг (GB)		<p><br>Градиентный бустинг (Градиент-бустинг, Gradient Tree Boosting, Gradient Boosting Machine - GBM) – это метод <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> для задач <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression</a>) и <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>, который создает прогнозирующую <a href="__GHOST_URL__/modiel/">Модель (Model)</a> в форме Ансамбля (Ensemble) слабых алгоритмов прогнозирования, обычно <a href="__GHOST_URL__/dierievo-rieshienii/">Деревьев решений (Decision Tree)</a>.</p><p>Несмотря на то, что GBM широко используется, многие практики по-прежнему рассматривают его как <a href="__GHOST_URL__/chiernyi-iashchik/">Черный ящик (Black Box)</a> и просто запускают модели с использованием предварительно созданных библиотек. Цель этой статьи – упростить предположительно сложный алгоритм и помочь читателю интуитивно понять его. </p><h3 id="%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D1%8C-%D0%B1%D1%8D%D0%B3%D0%B3%D0%B8%D0%BD%D0%B3-%D0%B8-%D0%B1%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3">Ансамбль, бэггинг и бустинг</h3><p>Когда мы пытаемся предсказать <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a> с помощью любого метода машинного обучения, основными причинами различий между фактическими и прогнозируемыми значениями являются Шум (Noise), <a href="__GHOST_URL__/dispiersiia/">Дисперсия (Variance)</a> и <a href="__GHOST_URL__/smieshchieniie/">Смещение (Bias)</a>. Ансамбль помогает уменьшить влияние этих факторов (кроме шума, который является неснижаемой ошибкой).</p><p>Ансамбль – это просто набор <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a>, которые собираются вместе с помощью, например, среднего значения всех прогнозов, чтобы дать окончательный. Причина, по которой мы используем ансамбли, заключается в том, что множество разных методов, пытающихся предсказать одну и ту же целевую переменную, будут выполнять свою работу лучше, чем один. Методы ансамбля подразделяются на <a href="__GHOST_URL__/beghghingh/">Бэггинг (Bagging)</a> и <a href="https://helenkapatsa.ghost.io/ghost/#/editor/post/5fd4fdb7d1e7890039b7ee59/">Бустинг (Boosting)</a>.</p><p>Бэггинг – это простой метод объединения, в котором мы создаем множество независимых моделей и группируем их, используя определенные методы усреднения моделей (например, средневзвешенное или нормальное среднее).</p><p>Обычно мы берем случайную <a href="__GHOST_URL__/vyborka/">Выборку (Sample)</a> для каждой модели, чтобы все они мало отличались друг от друга. Каждая модель будет использовать разные наблюдения. Поскольку для генерации окончательного результата с помощью этого метода требуется много независимых алгоритмов, он снижает погрешность за счет уменьшения дисперсии. Примером бэггинг-ансамбля является <a href="__GHOST_URL__/sluchainyi-lies/">Случайный лес (Random Forest)</a>.</p><p>Бустинг – это ансамблевый метод, в котором алгоритмы применяются последовательно. Этот метод использует логику, в которой последующие модели учатся на ошибках предыдущих. Следовательно, <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> имеют неодинаковую вероятность появления в последующих моделях, а наблюдения с наибольшей ошибкой появляются чаще. (Таким образом, наблюдения выбираются не на основе процесса начальной загрузки, а на основе ошибки). Из используемых методов перечислим деревья решений, регрессоры, классификаторы и т.д. Поскольку новые алгоритмы учатся на ошибках, совершенных предшественниками, требуется меньше времени / итераций, чтобы приблизиться к фактическим прогнозам. Но мы должны тщательно выбирать критерии остановки, иначе это может привести к <a href="__GHOST_URL__/pierieobuchieniie/">Переобучению (Overfitting)</a>. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/12/image-11.png" class="kg-image" alt loading="lazy" width="700" height="270" srcset="__GHOST_URL__/content/images/size/w600/2021/12/image-11.png 600w, __GHOST_URL__/content/images/2021/12/image-11.png 700w"><figcaption>Бэггингу, в отличие от бустинга, характерна последовательность</figcaption></figure><p>Цель любого алгоритма <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Контролируемого обучения (Supervised Learning)</a> – определить <a href="__GHOST_URL__/funktsiia-potieri/">Функцию потерь (Loss.Function)</a> и минимизировать ее. Давайте посмотрим, как работает GBM. </p><p>Логика GBM проста. Я полагаю, что читаюший эту статью знаком с простой <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессией (Linear Regression)</a>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/12/image-12.png" class="kg-image" alt loading="lazy" width="2000" height="1323" srcset="__GHOST_URL__/content/images/size/w600/2021/12/image-12.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/image-12.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/image-12.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/12/image-12.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Простейшая линейная регрессия</figcaption></figure><p>Основное ее предположение состоит в том, что сумма <a href="__GHOST_URL__/ostatok/">Остатков (Residual)</a> – разницей между фактическим и спрогнозированным значениями, равна 0, то есть они распределены случайным образом вокруг нуля:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-13.png" class="kg-image" alt loading="lazy" width="700" height="427" srcset="__GHOST_URL__/content/images/size/w600/2021/12/image-13.png 600w, __GHOST_URL__/content/images/2021/12/image-13.png 700w"></figure><p>Интуиция алгоритма повышения градиента заключается в том, чтобы многократно ориентироваться на остатки, не укладывающиеся в принцип "сумма остатков равна нулю" и укреплять модель с помощью <em>слабых</em> прогнозов. Как только мы достигаем стадии, когда остатки не имеют какого-либо паттерна, который можно было бы смоделировать, мы прекращаем моделирование остатков (иначе это может привести к переобучению). Алгоритмически мы минимизируем нашу функцию потерь.</p><p>В итоге,</p><ul><li>Сначала мы моделируем с помощью простых методов и анализируем результат на предмет ошибок. Эти ошибки означают точки данных, которые трудно вписать в существующую модель.</li><li>Затем, в более поздних моделях, мы особенно сосредотачиваемся на тех данных, которые трудно "уложить".</li><li>В конце мы группируем все методы, присваивая каждому из них вес.</li></ul><h3 id="gbm-scikit-learn">GBM: Scikit-learn</h3><p>Давайте посмотрим, как кросс-валидация реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><!--kg-card-begin: markdown--><p>from numpy import mean<br>\nfrom numpy import std<br>\nfrom sklearn.datasets import make_classification<br>\nfrom sklearn.ensemble import GradientBoostingClassifier<br>\nfrom sklearn.model_selection import cross_val_score<br>\nfrom sklearn.model_selection import RepeatedStratifiedKFold<br>\nfrom matplotlib import pyplot</p>\n<!--kg-card-end: markdown--><p>Сгенерируем набор данных из 1000 наблюдений, 10 <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a> и инициируем модель. Параметр <code>n_redundant</code> позволяет сгенерировать пять малозначимых переменных, которые сильно коррелируют теми, что обладают высокой Важностью признаков (Feature Importance). Мы будем совершенствовать модель с помощью k-блочной кросс-валидации (k-Fold Cross Validation) и соответствующего класса <code>RepeatedStratifiedKFold</code>:</p><!--kg-card-begin: markdown--><p>X, y = make_classification(n_samples = 1000, n_features = 10, n_informative = 5, n_redundant = 5, random_state = 1)</p>\n<p>model = GradientBoostingClassifier()<br>\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)<br>\nn_scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')<br>\nprint('Точность измерений: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))</p>\n<!--kg-card-end: markdown--><p>Обучим модель на имеющихся данных:</p><!--kg-card-begin: markdown--><p>model = GradientBoostingClassifier()<br>\nmodel.fit(X, y)</p>\n<!--kg-card-end: markdown--><p>Определим класс для нового наблюдения 'row': числа в списке – это просто значения десяти признаков.</p><!--kg-card-begin: markdown--><p>row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]<br>\nyhat = model.predict(row)<br>\nprint('Предсказание: %d' % yhat[0])</p>\n<!--kg-card-end: markdown--><p>Модель соотносит row с классом "1":</p><!--kg-card-begin: markdown--><p>Предсказание: 1</p>\n<!--kg-card-end: markdown--><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1nMNTj2uQ6TktZMgyQzmpxQpfa5HBqDwf?usp=sharing">здесь</a>.</p><p>Авторы оригинальных статей: <a href="https://blog.mlreview.com/gradient-boosting-from-scratch-1e317ae4587d">ML Review</a>, <a href="https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/">Jason Brownlee</a></p>		gradiientnyi-bustingh-2	2021-12-19		
164	Парадокс Симпсона (Simpson Paradox)		<p>Парадокс Симпсона — статистический парадокс, согласно которому фактор, больше проявляющийся при любых фоновых условиях, чем противоположный ему, проигрывает менее эффективному, но относительно часто встречающемуся фактору.</p><p>Познакомлю вас с понятиями категоризации данных и парадокс Симпсона. Для начала давайте посмотрим что за метод будет использоваться сегодня. </p><p>Небольшая преамбула: метод <code>value_counts()</code>, который мы сейчас будем использовать, определяет уникальные значения и возвращает таблицу, где подсчитывает, сколько раз они встретились в серии, причем в нисходящем порядке. У этого метода есть несколько параметров, такие как нормализация, сортировка, отображение восходящем порядке, категоризация и исключение ячейки с отсутствующим значением. Посмотрите, в правой части слайда мы создаем серию под названием <code>index</code> и присваиваем ей набор целочисленных значений и включаем одно отсутствующее, затем, используя метод <code>value_counts()</code>, мы определяем сколько раз то или иное число встретилась в этой серии. Допустим, цифра 3 встретилась дважды. Теперь мы хотим определить удельный вес каждого значения в этой серии, используя <code>normalize = True</code>, просим пересчитать результирующую серию и отобразить не количество упоминаний, а пропорцию. Если суммировать все цифры второго столбца, то мы получим единицу, то есть цифра 3 занимает 40 процентов место в этой серии.</p><p>Для начала импортируем все необходимые библиотеки. Импортируем модуль pandas.plotting для отображения графиков. Импортируем SciPy и набор статистических методов. Импортируем matplotlib и модуль pyplot. используем директиву <code>%matplotlib inline</code>, чтобы график строился все в той же оболочки Colab, но отображался как картинка:</p><!--kg-card-begin: markdown--><p>import numpy as np<br>\nimport csv</p>\n<p>import pandas as pd<br>\nimport pandas.plotting</p>\n<p>import scipy.stats</p>\n<p>import matplotlib<br>\nimport matplotlib.pyplot as plt</p>\n<p>from ipywidgets import interact, widgets</p>\n<p>%matplotlib inline</p>\n<!--kg-card-end: markdown--><p>Создадим переменную <code>smoking</code> и вычитаем с помощью метода <code>read_csv()</code> данные о влиянии курения на продолжительность жизни:</p><!--kg-card-begin: markdown--><p>smoking = pd.read_csv('<a href="https://www.dropbox.com/s/jjjhzvnackds0h9/%D1%83%D0%B8%D0%BA%D1%85%D0%B5%D0%BC.csv?dl=1">https://www.dropbox.com/s/jjjhzvnackds0h9/уикхем.csv?dl=1</a>')</p>\n<!--kg-card-end: markdown--><p>С помощью ранее изученного нами метода <code>value_counts()</code> определим количество выживших и невыживших, обратившись к параметру outcome (выживаемость):</p><!--kg-card-begin: markdown--><p>pd.DataFrame(smoking.outcome.value_counts())</p>\n<!--kg-card-end: markdown--><p>Итак, на момент окончания исследования из всей контрольной группы выживших девятьсот сорок пять человек, не выживших – 369:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/--------------2021-12-25---22.41.23.png" class="kg-image" alt loading="lazy" width="130" height="105"></figure><p>Отобразим в процентах то есть пропорциях с помощью нормализации. С помощью метода <code>groupby()</code>, метода <code>value_counts()</code>, нормализации:</p><!--kg-card-begin: markdown--><p>bysmoker = smoking.groupby('smoker').outcome.value_counts(normalize = True)<br>\nbysmoker.unstack()</p>\n<!--kg-card-end: markdown--><p>На момент окончания исследования исследования живых курильщиков 76%:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/--------------2021-12-25---22.43.04.png" class="kg-image" alt loading="lazy" width="243" height="135"></figure><p>Сейчас пока не очень понятно, но я вам покажу, как сделать эти данные нагляднее и делать выводы. Мы категоризируем группы респондентов, используя специальный метод <code>pd.cut()</code>, и разделим группы корреспондентов на такие диапазоны: от 0 до 30 лет, от 31 до 40, от 41 до 50, от 51-64, 64 и старше:</p><!--kg-card-begin: markdown--><p>smoking['ageGroup'] = pd.cut(smoking.age,[0, 30, 40, 53, 64, 120], labels = ['0-30', '31-40', '41-52', '53-63', '64+'])<br>\nsmoking['ageGroup'].head()</p>\n<!--kg-card-end: markdown--><p>Посмотрим, что получилось:</p><!--kg-card-begin: markdown--><p>0     0-30<br>\n1     0-30<br>\n2      64+<br>\n3      64+<br>\n4    53-63<br>\nName: ageGroup, dtype: category<br>\nCategories (5, object): ['0-30' &lt; '31-40' &lt; '41-52' &lt; '53-63' &lt; '64+']</p>\n<!--kg-card-end: markdown--><p>Теперь отобразим количество выживших курильщиков и не курильщиков в каждый из возрастных групп. Создадим переменную <code>byage</code>, посчитаем уникальные значения в каждой из возрастных категорий и нормализуем это все, то есть отобразим пропорции:</p><!--kg-card-begin: markdown--><p>byage = smoking.groupby(['ageGroup', 'smoker']).outcome.value_counts(normalize = True)<br>\nbyage.unstack().drop('Dead', axis = 1)</p>\n<!--kg-card-end: markdown--><p>Смотрите, здесь взаимосвязь между курением и выживаемости уже более очевидна: если на молодых людей в возрасте от 0 до 30 и от 31 до 40 курение почти никак не влияет, то в трех остальных возрастных группах шансы выжить уже значительно меньше:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/--------------2021-12-25---22.44.54.png" class="kg-image" alt loading="lazy" width="240" height="416"></figure><p>Причем самой опасной возрастной группой является диапазон с 53 до 64 лет. Это явление получило название "Парадокс Симпсона": разбив данные на категории, мы наконец-то увидели взаимосвязь между выживаемостью и курением. </p><p>Сделаем эти данные нагляднее, построим несколько графиков.<br>Начнем с двойной гистограмма. Она у нас размером 10 на 4. Первый график будет занимать на холсте место, определяемое этими тремя параметрами. Мы используем метод <code>value_counts()</code> и построим гистограмму, обозначаемую словом bar, окрасим ее, определим название:</p><!--kg-card-begin: markdown--><p>plt.figure(figsize = (10, 4))<br>\nplt.subplot(1, 2, 1);<br>\nsmoking.smoker.value_counts().plot(kind = 'bar');</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-14.png" class="kg-image" alt loading="lazy" width="293" height="254"></figure><p>Зададим внешний вид второго графика, изменив немного цвета, название, местоположение на холсте:</p><!--kg-card-begin: markdown--><p>plt.figure(figsize = (10, 4))<br>\nplt.subplot(1, 2, 2);<br>\nsmoking.smoker.value_counts().plot(kind = 'bar');</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-15.png" class="kg-image" alt loading="lazy" width="293" height="254"></figure><p>Теперь построим круговую диаграмму. Копируем код выше и изменим тип графика на круговой (<code>pie</code>):</p><!--kg-card-begin: markdown--><p>plt.figure(figsize = (10, 4))<br>\nplt.subplot(1, 2, 1);<br>\nsmoking.smoker.value_counts().plot(kind = 'pie', colors =['C0', 'C1']);<br>\nplt.title = 'Выживших'</p>\n<p>plt.subplot(1, 2, 2);<br>\nsmoking.smoker.value_counts().plot(kind = 'pie', colors = ['C2', 'C3']);<br>\nplt.title = 'Курильщиков'</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-16.png" class="kg-image" alt loading="lazy" width="550" height="231"></figure><!--kg-card-begin: markdown--><p>bysmoker.plot(kind = 'bar')</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-17.png" class="kg-image" alt loading="lazy" width="372" height="309"></figure><p>Ярлыки путают карты, мешают воспринимать данные, потому немного видоизменим внешний вид этой гистограммы:</p><!--kg-card-begin: markdown--><p>bysmoker.unstack().plot(kind = 'bar', stacked = True)</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-18.png" class="kg-image" alt loading="lazy" width="372" height="268"></figure><p>Параметр <code>stacked = true</code>, состыковывает эти данные в высотную шкалу, где легко можно судить о пропорции. Теперь визуализируем возрастную группировку:</p><!--kg-card-begin: markdown--><p>byage.unstack().plot(kind = 'bar', stacked = True)</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/12/image-19.png" class="kg-image" alt loading="lazy" width="372" height="311"></figure><p>Как вы видите, отрыв от предыдущей группы наиболее ярко выражен в возрастных группах "53 - 64" и "64 и старше" лет.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1fYtVbamX9gxicgdpsCPk_HlECw-O_TJs?usp=sharing">здесь</a>.</p>		paradoks-simpsona	2021-12-25		
165	Кластер (Cluster)		<p>Кластер – совокупность наблюдений, объединенных схожим значением того или иного <a href="__GHOST_URL__/priznak/">Признака (Feature)</a>. Центральное понятие методов Кластеризации (Clustering), во время которых изыскиваются способы группировать точки данных в сколь угодно многомерных пространствах на основании их подобности. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/01/image.png" class="kg-image" alt loading="lazy" width="905" height="656" srcset="__GHOST_URL__/content/images/size/w600/2022/01/image.png 600w, __GHOST_URL__/content/images/2022/01/image.png 905w" sizes="(min-width: 720px) 720px"><figcaption>Три кластера в двумерном пространстве © <em>Jason Feng</em></figcaption></figure>		klaster	2022-01-05		
166	Токен (Token)		<p>Лексема – набор слов, их частей или символов как основной компонент Токенизации (Tokenization), то есть преобразования значимой части данных. Значимый набор слов, например, четверостишие, разбирается на слова и символы, не имеющие смысла в виде токена, но позволяющие ссылаться на этот текст для решения некоторых задач Машинного обучения (ML), например, Анализа эмоциональной окраски (Sentiment Analysis).</p><p>Пример. Ниже приведен фрагмент книги Чарльза Диккенса «Повесть о двух городах»:</p><p><em>It was the best of times,</em><br><em>It was the worst of times,</em><br><em>It was the age of wisdom,</em><br><em>It was the age of foolishness,</em></p><p>В этом небольшом примере давайте рассматривать каждую строку как отдельный «документ», а все четверостишие – как Корпус (Corpus) документов.</p><p>Теперь мы можем составить токен, и состоит он из 11 слов:</p><ul><li>“it”</li><li>“was”</li><li>“the”</li><li>“best”</li><li>“of”</li><li>“times”</li><li>“worst”</li><li>“age”</li><li>“wisdom”</li><li>“foolishness”</li><li><em>","</em></li></ul>		token	2022-03-05		
167	Ковариация (Covariance)		<p>Ковариация – мера взаимосвязи двух случайных величин, измеряющая общее отклонение двух случайных величин от их ожидаемых значений. Метрика оценивает, в какой степени переменные изменяются вместе. Другими словами, это мера <a href="__GHOST_URL__/dispiersiia/">Дисперсии (Variance)</a> между двумя переменными. Однако метрика не оценивает зависимость между ними. Ковариация рассчитывается согласно формуле:</p><!--kg-card-begin: markdown--><p>$$Cov(X, Y) = \\frac{Σ(X_i - \\overline{X})(Y_j - \\overline{Y})}{n}, где$$<br>\n$$X\\space{–}\\space{первая}\\space{переменная,}$$<br>\n$$X_i\\space{–}\\space{i-й}\\space{элемент}\\space{переменной}\\space{X}$$<br>\n$$\\overline{X}\\space{–}\\space{среднее}\\space{значение}\\space{переменной}\\space{X}$$<br>\n$$Y\\space{–}\\space{вторая}\\space{переменная,}$$<br>\n$$Y_i\\space{–}\\space{i-й}\\space{элемент}\\space{переменной}\\space{Y}$$<br>\n$$\\overline{Y}\\space{–}\\space{среднее}\\space{значение}\\space{переменной}\\space{Y}$$<br>\n$$n\\space{–}\\space{число}\\space{наблюдений}\\space{в}\\space{выборке}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Джон — инвестор. Его портфель в первую очередь отслеживает показатели S&amp;P500, и Джон хочет добавить акции ABC Corp. Прежде чем добавить акции в свой портфель, он хочет оценить ковариацию между акциями и S&amp;P500.</p><p>Джон не хочет увеличивать несистематический риск своего портфеля. Таким образом, он не заинтересован в том, чтобы иметь в портфеле ценные бумаги, имеющие тенденцию к движению в одном направлении.</p><p>Джон может рассчитать ковариацию между акциями ABC Corp. и S&amp;P 500, выполнив следующие шаги:</p><ol><li>Получить данные: сначала Джон сопоставляет среднюю стоимость ABC Corp. и индекса S&amp;P500 за год:</li></ol><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/image-2.png" class="kg-image" alt loading="lazy" width="450" height="504"></figure><p>2. Рассчитать средние цены для каждого актива. Для индекса это значение составляет 2,044.80, для ABC Corp. – 109,2.</p><p>3. Для каждой ценной бумаги найдите разницу между каждым значением и средней ценой (A и B):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/image-1.png" class="kg-image" alt loading="lazy" width="1052" height="648" srcset="__GHOST_URL__/content/images/size/w600/2022/01/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2022/01/image-1.png 1000w, __GHOST_URL__/content/images/2022/01/image-1.png 1052w" sizes="(min-width: 720px) 720px"></figure><p>4. Перемножьте результаты (A * B). Сумма этих произведений – ковариация.</p><p>Положительная ковариация указывает на то, что цена акции ABC Corp. и S&amp;P500 имеют тенденцию двигаться в одном направлении. Отрицательная ковариация показывала бы, что две переменные имеют тенденцию двигаться в противоположных направлениях. В отличие от <a href="__GHOST_URL__/korrieliatsiia/">Корреляции (Correlation)</a>, ковариация измеряется в единицах. </p><p>Одним из наиболее распространенных приложений понятия является метод диверсификации, использующий ковариацию между активами в портфеле. Выбирая активы, которые не демонстрируют высокой положительной ковариации друг с другом, можно частично устранить несистематический риск.</p><h3 id="%D0%BA%D0%BE%D0%B2%D0%B0%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D1%8F-%D0%B8-%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F">Ковариация и корреляция</h3><p>Ковариация и корреляция в первую очередь оценивают взаимосвязь между переменными. Ближайшей аналогией связи между ними является связь между дисперсией  и <a href="__GHOST_URL__/standartnoie-otklonieniie/">Cтандартным отклонением (Standard Deviation)</a>.</p><p>Используя ковариацию, мы можем оценить направление взаимосвязи (независимо от того, имеют ли переменные тенденцию "двигаться в тандеме" или демонстрируют обратную зависимость). Однако это не указывает ни на силу связи, ни на зависимость между переменными.</p><p>С другой стороны, корреляция измеряет силу связи между переменными. Корреляция является масштабированной мерой ковариации без единиц измерения.</p><p>Автор оригинальной статьи: <a href="https://corporatefinanceinstitute.com/resources/knowledge/finance/covariance/">corporatefiancialinstitute.com</a></p>		kovariatsiia	2022-01-16		
168	Вебхук (Webhook)		<p>Вебхук (веб-коллбэк) – способ предоставить вашей системе информацию в реальном времени от другого программного обеспечения (API). К примеру, если Ваш <a href="__GHOST_URL__/chat-bot/">Чат-бот (Chatbot)</a> помогает пользователям забронировать столик ресторане, то с помощью обращения в базе данных эти записи можно создавать, обновлять и удалять. </p><p>Пример. Посмотрим, как работает вебхук, связывающий <a href="__GHOST_URL__/dialogflow/">DialogFlow </a>и базу данных MySQL на языке PHP. Нам предстоит развернуть некоторое ПО на своем компьютере.</p><h3 id="%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-%D0%BB%D0%BE%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE-%D1%81%D0%B5%D1%80%D0%B2%D0%B5%D1%80%D0%B0">Настройка локального сервера</h3><p>Для начала установим свежую версию локального сервера XAMPP. Это ПО позволит сделать наш скрипт исполняемым при обращении к последнему в адресной строке браузера и доступным веб-консоли DialogFlow в целом. Сервер можно скачать на сайте <a href="https://www.apachefriends.org/ru/index.html">apachefriend.org</a>. Если вы работаете на Windows, используйте стандартный установочный путь 'C://xampp'. После установки утилита выглядит следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/image-3.png" class="kg-image" alt loading="lazy" width="499" height="450"></figure><p>Если вы используете Windows, запустите Apache и MySQL кнопками Start:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/-------mysql.PNG" class="kg-image" alt loading="lazy" width="1001" height="609" srcset="__GHOST_URL__/content/images/size/w600/2022/01/-------mysql.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/-------mysql.PNG 1000w, __GHOST_URL__/content/images/2022/01/-------mysql.PNG 1001w" sizes="(min-width: 720px) 720px"></figure><p>Запустим локальный сервер кнопкой Start, затем во вкладке Services запустим MySQL кнопкой Start All:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/xampp-vm-cakephp-1-b65e7ada.png" class="kg-image" alt loading="lazy" width="500" height="450"></figure><p>Во вкладке Network выделим localhost:8080 и запустим его кнопкой Enable:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/image-6.png" class="kg-image" alt loading="lazy" width="500" height="450"></figure><p>Во вкладке Volumes нажмем Mount, а затем Explore. Откроется проводник с папкой сетевого тома, который доступен только при запущенном сервере XAMPP. В эту директорию мы и отправим наш скрипт, связывающий MySQL и DialogFlow</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/image-5.png" class="kg-image" alt loading="lazy" width="498" height="446"></figure><p>В открывшейся директории найдем папку htdocs и сделаем ее избранной папкой в нашем проводнике. Затем создадим внутри нее папку 'RestaurantReservation' и откроем в редакторе кода. Я буду использовать VSCode.</p><p>В этом разделе дело за малым – дать возможность DialogFlow подступиться к нашему локальному серверу. С этим поможет утилита ngrok, скачаем ее по <a href="https://ngrok.com/download">ссылке</a>. Распаковываем архив и перемещаем исполняемый одноименный файл в нашу директорию RestaurantReservation.</p><p>macOS: откроем терминал в VSCode и запустим ngrok командой <code>ngrok http 8080</code>.</p><p>Windows: откроем приложение и запустим переадресацию с помощью <code>ngrok http 8080</code>. Стоит использовать именно этот стандартный для XAMPP порт 8080, чтобы DF смог добраться до нашего скрипта.</p><p>Мы получили https-ссылку <a href="https://dd24-188-234-79-4.ngrok.io">https://dd24-188-234-79-4.ngrok.io</a>, ее вскоре и будем использовать:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/ngrok.PNG" class="kg-image" alt loading="lazy" width="1322" height="717" srcset="__GHOST_URL__/content/images/size/w600/2022/01/ngrok.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/ngrok.PNG 1000w, __GHOST_URL__/content/images/2022/01/ngrok.PNG 1322w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-%D0%B1%D0%B0%D0%B7%D1%8B-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Настройка базы данных</h3><p>Чтобы связка с БД работала, нам предстоит наладить стандартное соединение с MySQL, которую мы в XAMPP уже запустили. Для этого создадим файл db.php и зальем в него стандартные настройки  (они могут слегка разниться в зависимости от версии программы) – тип сервера, логин и пароль для подключения, база по умолчанию и хост (localhost – это зарезервированное вашим ПК доменное имя). С помощью встроенной функции mysqli_connect() мы и производим подключение, используя переменные выше как аргументы. Если соединение по какой-то причине установить не удалось, функция die() отобразит сообщение с причиной ошибки:</p><pre><code class="language-php">$mySQLserver = "localhost";\n$mySQLuser = "root";\n$mySQLpassword = "";\n$mySQLdefaultdb = "DialogFlowRestaurantReservation";\n$host = "localhost";\n\n$link = mysqli_connect($mySQLserver, $mySQLuser, $mySQLpassword,$mySQLdefaultdb) or die (mysql_error());</code></pre><p>Теперь перейдем в браузерную админку MySQL нажатием кнопки Admin в XAMPP или переходом по адресу <a href="http://localhost:8080/phpmyadmin/index.php?route=/sql&amp;server=1&amp;db=dialogflow&amp;table=users&amp;pos=0">http://localhost:8080/phpmyadmin/index.php</a> (или <a href="http://localhost/phpmyadmin/">http://localhost/phpmyadmin/</a>) и создадим базу данных. Нажав кнопку "Создать БД", мы попадем в окно настроек, где задать стоит только имя базы данных DialogFlowRestaurantReservation. Жмем "Создать".</p><p>Переходим во вкладку SQL, вставляем код запроса (полный код <a href="https://drive.google.com/file/d/1lGpv9Y8taL8YLw1ZfZ82G81jk0w4PND2/view?usp=sharing">здесь</a>), создающего таблицу users с бронированиями и несколько примеров и жмем "Вперед":</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/----------------.PNG" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2022/01/----------------.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/----------------.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/----------------.PNG 1600w, __GHOST_URL__/content/images/size/w2400/2022/01/----------------.PNG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Теперь при нажатии на название базы в меню слева мы сможем перейти к полученной таблице, в которую добавили три примера:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/--------users.PNG" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2022/01/--------users.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/--------users.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/--------users.PNG 1600w, __GHOST_URL__/content/images/size/w2400/2022/01/--------users.PNG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Таблица отныне способна воспринимать наши скрипты, создающие бронирования. </p><h3 id="%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5-%D1%81%D0%BA%D1%80%D0%B8%D0%BF%D1%82%D0%B0">Создание скрипта</h3><p> Создадим в папке RestaurantReservation файл MakeReservation.php:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/---------------.PNG" class="kg-image" alt loading="lazy" width="1851" height="1052" srcset="__GHOST_URL__/content/images/size/w600/2022/01/---------------.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/---------------.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/---------------.PNG 1600w, __GHOST_URL__/content/images/2022/01/---------------.PNG 1851w" sizes="(min-width: 1200px) 1200px"></figure><p>Для начала включим в скрипт db.php, производящий соединение с MySQL:</p><pre><code class="language-php">include("db.php");</code></pre><p>Функция <code>sendMessage()</code> отправит сообщение от имени бота:</p><pre><code class="language-php">function sendMessage($parameters) {\n\techo json_encode($parameters);\n}</code></pre><p>Мы будем сохранять текст пользовательского ответа во временный файл add.txt, и чтобы вычленить из него данные полей в корректном формате, создадим функцию <code>debug_text()</code>:</p><pre><code class="language-php">function debug_text($filename, $contentdebug) {\n\t$myfile = fopen($filename, "w") or die("Не удается открыть файл!");\n\tfwrite($myfile, $contentdebug);\n\tfclose($myfile);\n}</code></pre><p>Следующий игрок – функция <code>add_data()</code>, непосредственно добавляющая в БД новую запись, оставленную в файле:</p><pre><code class="language-php">function add_data($stand_number, $first_name, $last_name,\n$stand_size, $date_purchased, $phone_number, $mySQLserver, $mySQLdefaultdb, $mySQLuser, $mySQLpassword) {\n\n$sql = "insert into users (`stand_number`, `first_name`, `last_name`,\n `stand_size`, `date_purchased`, `phone_number`, `date_purchased`) values\n(\n'$stand_number', '$first_name', '$last_name',\n'$stand_size', '$date_purchased', '$phone_number')";\n\n$filename = "add.txt";\n$contentdebug = "sql=".$sql;\ndebug_text($filename, $contentdebug);\n\n\t$last_id=0;\n\ttry {\n\t\t\t$conn = new PDO("mysql:host = $mySQLserver; dbname = $mySQLdefaultdb", $mySQLuser, $mySQLpassword);\n\t\t\t$conn-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\t\t\t$conn -&gt; exec($sql);\n\t\t\t$last_id = $conn -&gt; lastInsertId();\n\t\t}\n\tcatch(PDOException $e)\n\t{\n\t\t$last_id = 0;\n\t}\n\t\n\treturn $last_id;\n}</code></pre><p>Дата записи генерируется автоматически, так что сгенерируем значение этого поля автоматически с помощью встроенных функций PHP. Установим часовой пояс по умолчанию, выбрать свой вы можете по <a href="https://www.php.net/manual/ru/timezones.europe.php">ссылке</a>:</p><pre><code class="language-php">date_default_timezone_set("Europe/Moscow");\n$CurrentDay = date("Y/m/d");\n$CurrentHour = date("H:i:sa");\n$DateTime = $CurrentDay. " ".$CurrentHour;</code></pre><p>Создадим команду для получения названия action-функции из диалогового массива, который перенаправляется на этот сервер:</p><pre><code class="language-php">$update_response = file_get_contents("php://input");\n$update = json_decode($update_response, true);\n$varresultaction = $update["queryResult"]["action"];</code></pre><p>На самом деле общение бота и человека – это обмен немаленькими JSON-массивами, и умение читать их позволит вам настраивать какую угодно логику. Чтобы добраться до диагностической информации, после старта диалога в консоли нажмите кнопку Diagnostic Info:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/diagnostic-info.PNG" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2022/01/diagnostic-info.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/diagnostic-info.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/diagnostic-info.PNG 1600w, __GHOST_URL__/content/images/size/w2400/2022/01/diagnostic-info.PNG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Разверзнутся хляби небесные и мы получим полезнейшую информацию о происходящем у DialogFlow под капотом. Вкладка 'Raw API Response' покажет полное состояние диалога на момент последней реплики, с идентификатором сессии, Контекстами (Context), параметрами и проч. В разделе "Создание скрипта" мы уже научились вычленять из массива любые параметры. Выделяем мы их из вкладки 'Fulfillment Request':</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/dialogflow.cloud.google.com_--3-.png" class="kg-image" alt loading="lazy" width="1438" height="871" srcset="__GHOST_URL__/content/images/size/w600/2022/01/dialogflow.cloud.google.com_--3-.png 600w, __GHOST_URL__/content/images/size/w1000/2022/01/dialogflow.cloud.google.com_--3-.png 1000w, __GHOST_URL__/content/images/2022/01/dialogflow.cloud.google.com_--3-.png 1438w" sizes="(min-width: 720px) 720px"></figure><p>Вкладка 'Fulfillment Status' даст понять, исполнился ли вебхук и укажет на причину ошибки в случае чего. </p><p>Научим скрипт выделять реплику пользователя из массива ответа:</p><pre><code class="language-php">$messages = $update["queryResult"]["queryText"];\n$session = $update["session"];</code></pre><p>Получим данные полей из ответного массива:</p><pre><code class="language-php">// Идентификатор записи\n$id = (isset($update["queryResult"]["parameters"]["id"]) ? $update["queryResult"]["parameters"]["id"] : null);\n\n// номер забронированного столика\n$standnumber = (isset($update["queryResult"]["parameters"]["standnumber"]) ? $update["queryResult"]["parameters"]["standnumber"] : null);\n\n// размер забронированного столика\n$standsize = (isset($update["queryResult"]["parameters"]["standsize"]) ? $update["queryResult"]["parameters"]["standsize"] : null);\n\n// имя, фамилию и телефон бронирующего\n$firstname = (isset($update["queryResult"]["parameters"]["firstname"]) ? $update["queryResult"]["parameters"]["firstname"] : null);\n\n$lastname = (isset($update["queryResult"]["parameters"]["lastname"]) ? $update["queryResult"]["parameters"]["lastname"] : null);\n\n$phonenumber = (isset($update["queryResult"]["parameters"]["phonenumber"]) ? $update["queryResult"]["parameters"]["phonenumber"] : null);\n\n// Сгенерируем дату бронирования\n$datepurchased = (isset($update["queryResult"]["parameters"]["datepurchased"]) ? $update["queryResult"]["parameters"]["datepurchased"] : null);\n// Уберем избыточную часть временного штампа\n$datepurchased = str_replace("T12:00:00+07:00", "", $datepurchased);</code></pre><p>Осталось только обработать некоторые возможные ошибки? На случай: если скрипт запускается с помощью адресной строки браузера, он не получит идентификатор сессии от DialogFlow, и мы обучаем систему реагировать на такие ситуации:</p><pre><code class="language-php">$ignore1 = "";\n$sessionunique = "";\n\nif ($session != "") {\n\t$list  = list($ignore1, $sessionunique) = explode('sessions/', $session);\n}\nelse {\n\t$nothing = "Нет активной сессии"; \n    sendMessage(array(\n        "source" =&gt; null,\n        "fulfillmentText" =&gt; $nothing\n        ));\n    exit;\n}\n</code></pre><p>В "эпицентре" скрипта – функция entry_data, отыскивающая последний ID в таблице MySQL, генерирующей новый, увеличены на единицу, и создающая запись на основе ответа юзера:</p><pre><code class="language-php">if ($varresultaction == "entry_data") {\n\t// Узнаем последний использованный ID в БД и назначим новой записи .// идентификатор, увеличенный на единицу\n    $last_id = add_data($standnumber, $firstname, $lastname, $standsize, $datepurchased, $phonenumber, $DateTime, $link, $mySQLserver, $mySQLdefaultdb, $mySQLuser, $mySQLpassword);\n\t$response = "Запись создана, номер заказа - ".$last_id;\n\tsendMessage(array(\n            "source" =&gt; null,\n            "fulfillmentText" =&gt; $response\n\t\t\t));\n\t\texit;\n}</code></pre><h3 id="%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-dialogflow">Настройка DialogFlow</h3><p>Чтобы создать бота, перейдите по адресу dialogflow.cloud.google.com и в раскрывающемся списке слева нажмите 'Create new agent':</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/-------------.PNG" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2022/01/-------------.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/-------------.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/-------------.PNG 1600w, __GHOST_URL__/content/images/size/w2400/2022/01/-------------.PNG 2400w" sizes="(min-width: 720px) 720px"></figure><p>Задайте ему следующие настройки:</p><ul><li>Agent name: RestaurantReservation (можно любое)</li><li>Default Language: Russian - ru</li><li>Default Time Zone: (GMT +3.00) Europe / Moscow</li></ul><p>Нажмите Create. Теперь перейдите во вкладку Export and Import в настройках (кнопка-шестеренка). Восстановите бота из архива, который можно скачать по <a href="https://drive.google.com/file/d/1Z-OzyjYSvR-QQm51K44ZTdb-0Npbx9gG/view?usp=sharing">ссылке</a>. </p><p>Теперь перейдем в раздел Fulfillment, разрешим Webhook и поместим https-ссылку с дополнением в виде названия папки и скрипта внутри (https://dd24-188-234-79-4.ngrok.io/RestaurantReservation/MakeReservation.php), сгенерированную ngrok в поле URL. Сохраним настройку нажатием Save.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/fulfillment-1.PNG" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2022/01/fulfillment-1.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/fulfillment-1.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/fulfillment-1.PNG 1600w, __GHOST_URL__/content/images/size/w2400/2022/01/fulfillment-1.PNG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>При импорте мы получили небольшой набор намерений: приветствие, инициация (Начать бронирование), создание записи в MySQL (Создать бронирование) и ошибка (Фолбэк). Чтобы начать диалог, наберем в консоли справа "Создать запись". Система перейдет в намерение "Начать бронирование" и предложит нам шаблон ответа:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/------------1.PNG" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2022/01/------------1.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/------------1.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/------------1.PNG 1600w, __GHOST_URL__/content/images/size/w2400/2022/01/------------1.PNG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Прежде чем отвечать на вопрос: заглянем в намерение "Создать бронирование" и обратим внимание на пару нюансов. В разделе Action and parameters можно заметить action-функцию entry_data,  которую мы ранее создавали в нашем скрипте. Она-то и будет собирать из реплики пользователя поля. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/dialogflow.cloud.google.com_.png" class="kg-image" alt loading="lazy" width="1271" height="1102" srcset="__GHOST_URL__/content/images/size/w600/2022/01/dialogflow.cloud.google.com_.png 600w, __GHOST_URL__/content/images/size/w1000/2022/01/dialogflow.cloud.google.com_.png 1000w, __GHOST_URL__/content/images/2022/01/dialogflow.cloud.google.com_.png 1271w" sizes="(min-width: 720px) 720px"></figure><p>Главное – разрешить исполнение вебхуков в секции внизу:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/01/dialogflow.cloud.google.com_--1-.png" class="kg-image" alt loading="lazy" width="1271" height="346" srcset="__GHOST_URL__/content/images/size/w600/2022/01/dialogflow.cloud.google.com_--1-.png 600w, __GHOST_URL__/content/images/size/w1000/2022/01/dialogflow.cloud.google.com_--1-.png 1000w, __GHOST_URL__/content/images/2022/01/dialogflow.cloud.google.com_--1-.png 1271w" sizes="(min-width: 720px) 720px"></figure><p>Копируем шаблон и заполним поля: "Номер столика: 4 Имя: Helen Фамилия: Kapatsa Число гостей: 3 Дата бронирования: 2022-01-23 Телефон: 89627276031".  Готово, запись создана и доступна в админке:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/01/--------------.PNG" class="kg-image" alt loading="lazy" width="2000" height="1125" srcset="__GHOST_URL__/content/images/size/w600/2022/01/--------------.PNG 600w, __GHOST_URL__/content/images/size/w1000/2022/01/--------------.PNG 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/--------------.PNG 1600w, __GHOST_URL__/content/images/size/w2400/2022/01/--------------.PNG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Все файлы урока можно найти в <a href="https://drive.google.com/drive/folders/1iBwd91lx7zCLebICidfwDNxSwKf0Wx3I?usp=sharing">этой облачной папке</a>.</p>		viebkhuk	2022-01-23		
169	Утечка данных (Data Leakage)		<p>Утечка данных — это использование <a href="__GHOST_URL__/modiel/">Моделью (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения  (ML)</a> данных извне. Эта дополнительная информация может позволить модели узнать что-то, чего она в противном случае не знала бы, и, в свою очередь, сделать недействительной предполагаемую производительность создаваемого режима.</p><p>Утечка данных — большая проблема при разработке прогностических моделей. Целью прогнозного моделирования является разработка модели, которая делает точные прогнозы на основе новых данных, недоступных во время обучения. Это трудная проблема. Это сложно, потому что мы не можем оценить модель на чем-то, чего у нас нет.</p><p>Следовательно, мы должны оценить производительность модели на недоступных данных, обучив ее только на имеющихся.</p><p>Этот принцип лежит в основе <a href="__GHOST_URL__/kross-validatsiia/">Кросс-валидации (Cross Validation)</a>, или перекрастной проверки, и более сложных методов, которые пытаются уменьшить <a href="__GHOST_URL__/dispiersiia/">Дисперсию (Variance)</a>.</p><p>Утечка данных может привести к созданию чрезмерно оптимистичных, если не полностью неверных прогнозных моделей.</p><p>В компьютерной безопасности есть тема, называемая утечкой данных и предотвращением потери данных, которая не относится к одноименному понятию в ML.</p><p>Утечка данных — это серьезная проблема как минимум по трем причинам:</p><ul><li>Если вы проводите соревнование по Машинному обучению. Модели будут использовать неточные данные, и обобщать будут хуже.</li><li>Когда вы предоставляете данные: отмена <a href="__GHOST_URL__/anonimizatsiia/">Анонимизации (Anonymization)</a> и Обфускации (Obfuscation) может привести к неожиданному нарушению конфиденциальности.</li><li>Когда вы разрабатываете свои собственные прогностические модели. Возможно, вы создаете чрезмерно оптимистичные модели, которые практически бесполезны и не могут быть использованы в производстве.</li></ul><p>Как специалистов по машинному обучению, нас в первую очередь интересует последний случай.</p><h3 id="%D0%B5%D1%81%D1%82%D1%8C-%D0%BB%D0%B8-%D1%83-%D0%BC%D0%BE%D0%B5%D0%B9-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-%D1%83%D1%82%D0%B5%D1%87%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Есть ли у моей модели утечка данных?</h3><p>Простой способ узнать, что у вас есть утечка данных, — это достичь производительности, которая кажется слишком хорошей, чтобы быть правдой. Например, вы можете предсказывать лотерейные номера или выбирать акции с высокой точностью.</p><p>Утечка – больше проблема со сложными наборами данных, например:</p><ul><li>Наборы данных временных рядов при создании обучающих и тестовых наборов могут быть трудными.</li><li>Проблемы с <a href="__GHOST_URL__/graf/">Графами (Graph)</a>, где методы Сэмплирования (Sampling) будут трудноисполнимыми.</li><li>Аналоговые данные, такие как звук и изображения, где сэмплы хранятся в отдельных файлах, имеющих размер и временные метки.</li></ul><h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B-%D0%BC%D0%B8%D0%BD%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8-%D1%83%D1%82%D0%B5%D1%87%D0%BA%D0%B8-%D0%BF%D1%80%D0%B8-%D0%BF%D0%BE%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%B8%D0%B8-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9">Методы минимизации утечки при построении моделей</h3><p>Два хороших метода, которые вы можете использовать для минимизации утечки данных при разработке прогностических моделей, заключаются в следующем:</p><ul><li>Выполните подготовку данных в рамках кросс-валидации</li><li>Придержите проверочный набор данных для окончательной проверки работоспособности разработанных вами моделей</li></ul><p>Как правило, рекомендуется использовать оба этих метода.</p><h3 id="%D0%BF%D0%BE%D0%B4%D0%B3%D0%BE%D1%82%D0%BE%D0%B2%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%B2-%D1%80%D0%B0%D0%BC%D0%BA%D0%B0%D1%85-%D0%BF%D0%B5%D1%80%D0%B5%D0%BA%D1%80%D0%B5%D1%81%D1%82%D0%BD%D0%BE%D0%B9-%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B8">Подготовка данных в рамках перекрестной проверки</h3><p>Эффект заключается в <a href="__GHOST_URL__/pierieobuchieniie/">Переобучении (Overfitting)</a> <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a> и чрезмерно оптимистичной оценке производительности ваших моделей на неизвестных данных.</p><p>Например, если вы подвергаете весь набор <a href="__GHOST_URL__/standartizatsiia/">Стандартизации (Standartization)</a> или <a href="__GHOST_URL__/normalizatsiia/">Нормализации (Normalization)</a>, а затем оцените производительность своей модели с помощью перекрестной проверки, вы "совершили грех" утечки данных.</p><p>В процессе масштабирования данных было известно полное распределение данных в обучающем наборе данных при расчете коэффициентов масштабирования, таких как Минимум (Minimum) и Максимум (Maximum) или <a href="__GHOST_URL__/sriednieie-znachieniie/">Cреднее значение (Average)</a> и <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>. Это знание отразилось на масштабированных данныхи использовано при перекрестной проверке.</p><p>"Герметичная" оценка <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a> Машинного обучения в этой ситуации будет вычислять параметры для масштабирования данных на каждой стадии кросс-валидации и использовать эти параметры для тестирования в каждом цикле.</p><p>Реальность такова, что как <a href="__GHOST_URL__/data-saiientist/">Дата-сайентист (Data Scientist)</a> вы рискуете создать ситуацию утечки данных в любое время, когда готовите, очищаете данные, заполняете <a href="__GHOST_URL__/propusk/">Пропуски (Omission)</a>, удаляете <a href="__GHOST_URL__/vybros/">Выбросы (Outlier)</a> и т. д. Вы можете искажать данные в процессе их подготовки до такой степени, что создадите модель, которая хорошо работает на вашем «чистом» наборе данных, но будет совершенно непотребной в реальной ситуации.</p><p>В более общем плане, подготовка данных без утечек должна выполняться на каждом этапе цикла перекрестной проверки.</p><p>Вы можете ослабить это ограничение для некоторых проблем, например, если можете уверенно оценить распределение ваших данных, потому что у вас есть некоторые знания в предметной области.</p><p>Однако в целом хорошей идеей является повторная подготовка данных в рамках перекрестной проверки, включая выбор <a href="__GHOST_URL__/priznak/">Признака (Feature)</a>, удаление выбросов, Кодирование (Encoding), стандартизацию, нормализацию, Понижение размерности (Dimensionality Reduction) и многое другое.</p><blockquote>"Если вы выполняете выбор функций для всех данных, а затем выполняете перекрестную проверку, то тестовые данные в каждой части процедуры перекрестной проверки также использовались для выбора функций, и это то, что искажает анализ производительности".</blockquote><p>— Дикран Марсупиал (Dikran Marsupial)</p><h3 id="%D1%80%D0%B0%D0%B7%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BD%D0%B0%D0%B1%D0%BE%D1%80%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%B4%D0%BB%D1%8F-%D0%BA%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D0%B8">Разделение набора данных для кросс-валидации</h3><p>Другой, возможно, более простой подход — разделить <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> на тренировочные и <a href="__GHOST_URL__/validatsionnyye-dannyye/">Валидационные данные (Validation Data)</a>.</p><p>После того, как вы завершили процесс моделирования и фактически создали окончательную модель, оцените ее с помощью валидационных данных.</p><p>Это может дать вам проверку работоспособности, чтобы увидеть, не была ли ваша оценка производительности чрезмерно оптимистичной и не случилась ли утечка.</p><h3 id="5-%D1%81%D0%BE%D0%B2%D0%B5%D1%82%D0%BE%D0%B2-%D0%BF%D0%BE-%D0%B1%D0%BE%D1%80%D1%8C%D0%B1%D0%B5-%D1%81-%D1%83%D1%82%D0%B5%D1%87%D0%BA%D0%BE%D0%B9-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">5 советов по борьбе с утечкой данных</h3><ul><li><strong>Временная отсечка</strong>. Удалите все данные непосредственно перед интересующим событием, сосредоточившись на самом событии, а не на времени, когда произошло наблюдение.</li><li><strong>Добавьте шум</strong>. Добавьте случайный <a href="__GHOST_URL__/shum/">Шум (Noise)</a>, чтобы сгладить последствия возможной утечки.</li><li><strong>Удалить "протекающие" переменные</strong>. Оцените переменные, такие как номера учетных записей и идентификаторы и т.п., чтобы увидеть, являются ли эти переменные "негерметичными", и если да, удалите их. Если вы подозреваете, что переменная "негерметична", рассмотрите возможность ее удаления.</li><li><strong>Используйте </strong><a href="__GHOST_URL__/paiplain/"><strong>Пайплайны (<strong>Pipeline</strong>)</strong>.</a> Интенсивно используйте конвейерные архитектуры, которые позволяют выполнять последовательность шагов подготовки данных в рамках перекрестной проверки, например пакет Caret в R и Pipelines в Scikit-learn.</li><li><strong>Используйте валидационные данные</strong>. Придержите проверочный набор данных в качестве окончательной проверки работоспособности вашей модели перед ее использованием.</li></ul><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/data-leakage-machine-learning/">Jason Brownlee</a></p>		utiechka-dannykh	2022-01-30		
170	Анонимизация (Anonymization)		<p>Анонимизация (деидентификация) — это процесс защиты частной или конфиденциальной информации путем стирания или шифрования идентификаторов, которые хранимые данные с тем или иным человеком или организацией. Речь идет об именах, ИНН, адресах, банковских картах и т.д. </p><h3 id="%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B">Методы</h3><ul><li>Маскировка данных — скрытие данных с измененными значениями. Вы можете создать зеркальную версию базы данных и применить методы модификации, такие как перетасовка символов, шифрование и замена слов или символов. Например, вы можете заменить символ значения таким символом, как «*» или «x». Маскировка данных делает невозможным обратный инжиниринг или обнаружение.</li><li>Псевдонимизация — метод управления данными, при котором частные идентификаторы заменяются поддельными идентификаторами или псевдонимами, например, замена идентификатора «Джон Смит» на «Марк Спенсер». Псевдонимизация сохраняет статистическую точность и целостность данных, позволяя использовать измененные данные для обучения, разработки, тестирования и аналитики, защищая при этом конфиденциальность данных.</li><li>Обобщение — преднамеренно удаляет некоторые данные, чтобы сделать их менее идентифицируемыми. Данные могут быть преобразованы в набор диапазонов или широкую область с соответствующими границами. Вы можете удалить номер дома из адреса, но не удаляйте название дороги. Цель состоит в том, чтобы исключить некоторые идентификаторы, сохранив при этом некоторую точность данных.</li><li>Обмен данными  (перетасовка, перестановка) – метод, используемый для перестановки значений атрибутов набора данных, чтобы они не соответствовали исходным записям. Например, перетасовка дат рождения.</li><li>Пертурбация данных немного изменяет исходный набор данных, применяя методы, которые округляют числа и добавляют случайный шум. Диапазон значений должен быть пропорционален искажению. Небольшое искажение может привести к слабой анонимности, а большое – снизить полезность набора данных. Например, вы можете использовать основание 5 для округления таких значений, как возраст или номер дома, потому что оно пропорционально исходному значению. Вы можете умножить номер дома на 15, и значение может сохранить свою достоверность. Однако использование более радикального основания, такого как 15, может сделать значения возраста некорректными.</li><li>Синтезирование данных — алгоритмически созданная информация, не имеющая связи с реальной. Синтетические данные используются для создания искусственных наборов данных вместо изменения исходного набора данных или использования его как есть, что ставит под угрозу конфиденциальность и безопасность. Процесс включает в себя создание статистических моделей на основе шаблонов, найденных в исходном наборе данных. Вы можете использовать стандартные отклонения, медианы, линейную регрессию или другие статистические методы для создания синтетических данных.</li></ul><p>Автор оригинальной статьи: <a href="https://www.imperva.com/learn/data-security/anonymization/">imperva.com</a></p>		anonimizatsiia	2022-02-13		
171	Стемминг (Stemming)		<p>Стемминг – способ подготовки текста для использования в Модели (Model) Машинного обучения (ML), сокращение слов до своих грамматических основ (основа слова "Африки" – "Африк"). Основа слова – стем, не обязательно совпадает с корнем, он может включать и суффиксы. Это неизменяемая при склонении часть.</p><p>Алгоритмы стемминга обычно основаны на правилах: слово проходит через ряд условных предложений, которые определяют, как его сократить. Например, существует правило суффиксов: в английском языке «-ed» и «-ing» отрезают, чтобы сопоставить "cooking" и "cooked" с одной и той же основой "cook".</p><p>Не стоит путать стемминг с <a href="__GHOST_URL__/liemmatizatsiia/">Лемматизацией (Lemmatization)</a> – объединением слов с одним и тем же корнем или леммой, но с разными склонениями или производными значения для дальнейшего анализа.</p><h2 id="%D0%BF%D0%B5%D1%80%D0%B5%D1%81%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3-%D0%B8-%D0%BD%D0%B5%D0%B4%D0%BE%D1%81%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3">Перестемминг и недостемминг</h2><p>Поскольку стемминг обычно основан на эвристике, он далек от совершенства. На самом деле он "страдает" от двух проблем: пере- и недостемминга.</p><p>Перестемминг (англ. overstemming) происходит, когда слишком большая часть слова обрезается. Это может привести к бессмысленным стемам, где значение слова потеряно. Или же к тому, что совершенно неродственные слова будут приведены к одной и той же основе.</p><p>Возьмите четыре слова university (университет), universal (всобщий), universities (университеты) и universe (вселенная). Алгоритм стемминга, который преобразует эти четыре слова в основу 'univers', перегнул палку. Было бы неплохо, если university и universities продемонстрировали свою связь при стемминге, также как и universal и universe, причем попарно, но нет, основы получаются одинаковые. Соблюдение правил стемминга может привести к возникновению дополнительных проблем.</p><p>Недостемминг — противоположная проблема. Он происходит, когда у нас есть несколько слов, которые на самом деле являются формами друг друга. Было бы хорошо, если бы они все "разрешались" в одно древо родственных слов, но, к сожалению, этого не происходит. Это легко понять, если использовать алгоритм, генерирующий основы для слов data (данные) – dat, и datum (данная величина) – datu. Легко предположить: даваайте просто разрешим оба до стема 'dat'. Однако что тогда делать с date (дата)? Существует ли хорошее общее правило? Или мы просто применяем очень конкретное правило для очень конкретного примера?</p><p>Эти вопросы быстро становятся проблемами, когда дело доходит до стемминга. Применение новых правил и эвристик может быстро выйти из-под контроля. Решение одной или двух проблем с чрезмерным или недостаточным стеммингом может привести к появлению еще двух! Создание хорошего алгоритма стемминга — тяжелая работа.</p><h2 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D0%B0-%D1%81%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3%D0%B0">Примеры алгоритма стемминга</h2><p>Не будем вдаваться в подробности алгоритмов, составим предварительное впечатление:</p><ul><li><strong>Стеммер Портера</strong>: это старый алгоритм из 1980-х годов, и его главная задача — удалить общие окончания слов. Он не слишком сложный и развитие метода приостановлено. Как правило, это хороший базовый стеммер для начинающих, но на самом деле не рекомендуется использовать его для любого более-менее сложного приложения. Вместо этого он занимает свое место хорошего базового алгоритма поиска основ и гарантирует воспроизводимость. Это также очень щадящий алгоритм стемминга по сравнению с другими.</li><li><strong>Стеммер Snowball</strong> ("Снежок") также известен как алгоритм стемминга Porter2. Почти повсеместно считается, что он лучше, чем собрат выше по списку, даже сам Портер согласился с утверждением. Snowball более агрессивен. Многие его особенности, были спровоцированы проблемами портеровского стеммера. Разница между ними составляет около 5%.</li><li><strong>Ланкастерский стеммер</strong> — это еще один агрессивный алгоритм. Однако, если вы используете стеммер в NLTK, популярной библиотеки для <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a>, то легко добавить свои собственные правила. Одна претензия к ланкастерскому алгоритму заключается в том, что иногда он слишком агрессивен и действительно может приводить слова к странным основам. Просто убедитесь, что он делает то, что вы хотите, прежде чем использовать этот вариант!</li></ul><h2 id="%D1%81%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3-nltk">Стемминг: NLTK</h2><p> Посмотрим, как алгоритм приведения к основе реализован в NLTK. Для начала импортируем портеровский стеммпер и инициируем одну такую сущность. Затем приведем список слов words к их основам:</p><!--kg-card-begin: markdown--><p>from nltk.stem import PorterStemmer</p>\n<p>ps = PorterStemmer()</p>\n<p>words = [&quot;program&quot;, &quot;programs&quot;, &quot;programmer&quot;, &quot;programming&quot;, &quot;programmers&quot;]</p>\n<p>for w in words:<br>\nprint(w, &quot; : &quot;, ps.stem(w))</p>\n<!--kg-card-end: markdown--><p>Библиотека сгенерирует такие выходные данные:</p><!--kg-card-begin: markdown--><p>program  :  program<br>\nprograms  :  program<br>\nprogrammer  :  program<br>\nprogramming  :  program<br>\nprogrammers  :  program</p>\n<!--kg-card-end: markdown--><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8" rel="noopener noreferrer">Hunter Heidenreich</a></p>		stemming	2022-02-20		
172	Частичная автокорреляция (PACF)		<p>Частичная автокорреляция (Partial Autocorrelation) — это краткая характеристика взаимосвязи между <a href="__GHOST_URL__/nabliudieniie/">Наблюдением (Observation)</a> во <a href="__GHOST_URL__/vriemiennoi-riad/">Временном ряду (Time Series)</a> и наблюдениями на предыдущем отрезке времени с удалением взаимосвязей между промежуточными наблюдениями.</p><p>Графики Функции автокорреляции (ACF) и частичной автокорреляции широко используются в анализе и прогнозировании временных рядов.</p><p>Это графики, которые графически обобщают силу связи с наблюдением во временном ряду с наблюдениями на предыдущих временных шагах. Разница между автокорреляцией и частичной автокорреляцией может быть сложной и запутанной для новичков в Прогнозировании временных рядов (Time Series Forecasting).</p><p>Давайте посмотрим, как PACF реализован в библиотеке statsmodels. Для начала импортируем необходимые библиотеки. Matplotlib строит графики, а statsmodels с ее встроенными функциями вычисляет автокорреляции:</p><!--kg-card-begin: markdown--><p>import pandas as pd<br>\nimport matplotlib<br>\nfrom matplotlib import pyplot<br>\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf</p>\n<!--kg-card-end: markdown--><p>Загрузим набор данных и посмотрим на исходный временной ряд:</p><!--kg-card-begin: markdown--><p>series = pd.read_csv('<a href="https://www.dropbox.com/s/i8xs9myposohyp9/temperature.csv?dl=1">https://www.dropbox.com/s/i8xs9myposohyp9/temperature.csv?dl=1</a>')<br>\nmatplotlib.rcParams['figure.figsize'] = [20, 5]<br>\nseries.plot()<br>\npyplot.show()</p>\n<!--kg-card-end: markdown--><p>Исходные колебания обусловлены сменой сезонов (1 шаг равен дню):</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/02/pacf-pyplot-show.jpg" class="kg-image" alt loading="lazy" width="1236" height="387" srcset="__GHOST_URL__/content/images/size/w600/2022/02/pacf-pyplot-show.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/02/pacf-pyplot-show.jpg 1000w, __GHOST_URL__/content/images/2022/02/pacf-pyplot-show.jpg 1236w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F-%D0%B8-%D0%B0%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F">Корреляция и автокорреляция</h3><p>Статистическая <a href="__GHOST_URL__/korrieliatsiia/">Корреляция (Correlation)</a> характеризует силу связи между двумя Переменными (Variable). </p><p>Распределение значений каждой переменной может соответствовать распределению Гаусса (колоколообразной кривой):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/02/image-2.png" class="kg-image" alt loading="lazy" width="700" height="400" srcset="__GHOST_URL__/content/images/size/w600/2022/02/image-2.png 600w, __GHOST_URL__/content/images/2022/02/image-2.png 700w"></figure><p>Если это так, мы можем использовать Коэффициент корреляции Пирсона (Pearson Correlation Coefficient), чтобы обсчитать корреляцию между переменными.</p><p>Коэффициент корреляции Пирсона представляет собой число от -1 до 1, которое описывает прямую или обратную корреляцию соответственно. Нулевое значение указывает на отсутствие таковой.</p><p>Мы можем рассчитать корреляцию наблюдений временного ряда с предыдущими временными шагами – Лагами (Lags). Поскольку такая корреляция рассчитывается со значениями того же ряда в предыдущие моменты времени, это называется последовательной корреляцией или <a href="__GHOST_URL__/avtokorrieliatsiia/">Автокорреляцией (Autocorrelation)</a>.</p><p>График автокорреляции называется функцией автокорреляции (ACF), а также <a href="__GHOST_URL__/korrieloghramma/">Коррелограммой (Correlogram)</a>. Давайте же построим его с помощью <code>plot_acf()</code>:</p><!--kg-card-begin: markdown--><p>plot_acf(series.Temp)<br>\npyplot.show()</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/02/pacf-acf.jpg" class="kg-image" alt loading="lazy" width="1247" height="403" srcset="__GHOST_URL__/content/images/size/w600/2022/02/pacf-acf.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/02/pacf-acf.jpg 1000w, __GHOST_URL__/content/images/2022/02/pacf-acf.jpg 1247w" sizes="(min-width: 1200px) 1200px"></figure><p>Для читаемости число связей сокращено автоматически до 35 (из-за вручную заданной ширины полотна графика). Бледно-голубая фигура демонстрирует величину задержки между первым и каждым последующим наблюдением, потому высота ее растет по мере перехода к новому наблюдению. По умолчанию для него установлен доверительный интервал 95%, что предполагает, что значения корреляции за пределами этого кода, скорее всего, являются корреляцией, а не статистической случайностью.</p><p>Столбцы с шарообразными наконечниками – это своеобразная визитная карточка коррелограммы, высота столбца отображает степень корреляции. Поскольку значения температур не могут быть в обратной корреляции, то столбцы никогда не "отправятся" ниже нуля. Из бытового опыта мы знаем, что температура двух, скажем, летних соседствующих дней, как правило, меньше отличается от своих сильноудаленных зимних собратьев. Чуть позже мы узнаем, как оперирование разницей между днем и его лагом может изменить корреляцию.</p><h3 id="%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F-%D1%87%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D0%B9-%D0%B0%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D0%B8">Функция частичной автокорреляции</h3><p>Частичная автокорреляция (Partial Autocorrelation) — это краткая характеристика взаимосвязи между наблюдением во временном ряду и наблюдениями на предыдущем отрезке времени, когда влияние малой задержки устранено.</p><p>Автокорреляция состоит как из прямой, так и из косвенной корреляции. И именно последние пытается удалить функция частичной автокорреляции. Оттуда и название – "частичная".</p><p>В приведенном ниже примере вычисляется и строится функция частичной автокорреляции для первых 50 лагов в наборе данных с использованием <code>plot_pacf()</code>:</p><!--kg-card-begin: markdown--><p>plot_pacf(series.Temp, lags = 50)<br>\npyplot.show()</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/02/pacf.jpg" class="kg-image" alt loading="lazy" width="1239" height="403" srcset="__GHOST_URL__/content/images/size/w600/2022/02/pacf.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/02/pacf.jpg 1000w, __GHOST_URL__/content/images/2022/02/pacf.jpg 1239w" sizes="(min-width: 1200px) 1200px"><figcaption>Частичная автокорреляция для первых 50 лагов</figcaption></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1tvpBQsuma1KTg7bwzJsCepbceHPCw8zd?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/">Jason Brownlee</a></p>		chastichnaia-avtokorrieliatsiia	2022-02-26		
173	Моделирование оттока (Churn Modeling)		<p>Моделирование оттока (Churn Modeling, Churn Prediction) – одна из популярнейших задач <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, нацеленная на удержание пользователей с определенными поведением и характеристиками. Рассмотрим в качестве примера кейс американского провайдера услуг связи Telco. Мы будем использовать JavaScript.</p><p>«Предсказывайте поведение, чтобы удерживать клиентов. Вы можете анализировать все соответствующие данные о клиентах и ​​разрабатывать целенаправленные программы их удержания».</p><p><a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> имеет 7044 записи и 21 столбец:</p><ul><li>customerID: идентификатор клиента</li><li>Gender<strong>:</strong> является ли клиент мужчиной или женщиной</li><li>SeniorCitizen: является ли клиент пожилым гражданином (да, нет)</li><li>Partner: есть ли у клиента партнер (да, нет)</li><li>Dependents: есть ли у клиента иждивенцы (да, нет)</li><li>Tenure: количество месяцев, в течение которых клиент оставался в компании</li><li>PhoneService: есть ли у клиента телефония (да, нет)</li><li>MultipleLines: есть ли у клиента несколько линий (да, нет, нет телефонии)</li><li>InternetService: интернет-провайдер клиента (DSL, оптоволокно, нет)</li><li>OnlineSecurity: есть ли у клиента онлайн-безопасность (да, нет, нет интернет-сервиса)</li><li>OnlineBackup: есть ли у клиента онлайн-резервное копирование (да, нет, нет интернет-сервиса)</li><li>DeviceProtection: есть ли у клиента защита устройства (да, нет, нет интернет-сервиса)</li><li>TechSupport: есть ли у клиента техническая поддержка (да, нет, нет интернет-сервиса)</li><li>StreamingTV: есть ли у клиента потоковое телевидение (да, нет, нет интернет-сервиса)</li><li>StreamingMovies: есть ли у клиента потоковые фильмы (да, нет, нет интернет-сервиса)</li><li>Contract: срок контракта клиента (ежемесячно, один год, два года)</li><li>PaperlessBilling: есть ли у клиента безбумажный биллинг (да, нет)</li><li>PaymentMethod: способ оплаты клиента (электронный чек, чек по почте, автоматический банковский перевод, автоплатеж с карты)</li><li>MonthlyCharges: сумма, взимаемая с клиента ежемесячно</li><li>TotalCharges: общая сумма, списанная с клиента</li><li>Churn: ушел ли клиент или нет (да или нет)</li></ul><p>Мы будем использовать Papa Parse для загрузки данных:</p><!--kg-card-begin: markdown--><p>const prepareData = async () =&gt; {<br>\nconst csv = await Papa.parsePromise(<br>\n&quot;<a href="https://raw.githubusercontent.com/curiousily/Customer-Churn-Detection-with-TensorFlow-js/master/src/data/customer-churn.csv">https://raw.githubusercontent.com/curiousily/Customer-Churn-Detection-with-TensorFlow-js/master/src/data/customer-churn.csv</a>&quot;<br>\n);</p>\n<p>const data = csv.data;<br>\nreturn data.slice(0, data.length - 1);<br>\n};</p>\n<!--kg-card-end: markdown--><p>Обратите внимание, что мы игнорируем последнюю строку, так как она пуста.</p><h3 id="%D1%80%D0%B0%D0%B7%D0%B2%D0%B5%D0%B4%D0%BE%D1%87%D0%BD%D1%8B%D0%B9-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Разведочный анализ данных</h3><p>Давайте поизучаем наш набор данных. Сколько клиентов ушло?</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image.png" class="kg-image" alt loading="lazy" width="700" height="450" srcset="__GHOST_URL__/content/images/size/w600/2022/03/image.png 600w, __GHOST_URL__/content/images/2022/03/image.png 700w"></figure><p>Около 74% клиентов продолжают пользоваться услугами компании. У нас очень несбалансированный набор данных.</p><p>Влияет ли пол на вероятность ухода?</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image-1.png" class="kg-image" alt loading="lazy" width="700" height="450" srcset="__GHOST_URL__/content/images/size/w600/2022/03/image-1.png 600w, __GHOST_URL__/content/images/2022/03/image-1.png 700w"></figure><p>Кажется, нет. У нас примерно одинаковое количество клиентов женского и мужского пола. Как насчет возраста?</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image-2.png" class="kg-image" alt loading="lazy" width="700" height="450" srcset="__GHOST_URL__/content/images/size/w600/2022/03/image-2.png 600w, __GHOST_URL__/content/images/2022/03/image-2.png 700w"></figure><p>Около 20% клиентов – пожилые, и они гораздо чаще уходят.</p><p>Как долго клиенты остаются в компании?</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image-3.png" class="kg-image" alt loading="lazy" width="700" height="450" srcset="__GHOST_URL__/content/images/size/w600/2022/03/image-3.png 600w, __GHOST_URL__/content/images/2022/03/image-3.png 700w"></figure><p>Кажется, что чем дольше вы остаетесь, тем больше вероятность, что вы останетесь с Telco и дальше. </p><p>Как ежемесячные платежи влияют на отток?</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image-4.png" class="kg-image" alt loading="lazy" width="700" height="450" srcset="__GHOST_URL__/content/images/size/w600/2022/03/image-4.png 600w, __GHOST_URL__/content/images/2022/03/image-4.png 700w"></figure><p>Покупатель с низкими ежемесячными платежами (&lt;30 долларов США) с гораздо большей вероятностью будет удержан.</p><p>Как насчет общей суммы, взимаемой с клиента?</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image-5.png" class="kg-image" alt loading="lazy" width="700" height="450" srcset="__GHOST_URL__/content/images/size/w600/2022/03/image-5.png 600w, __GHOST_URL__/content/images/2022/03/image-5.png 700w"></figure><p>Чем выше общая списанная сумма средств, тем больше вероятность того, что этот клиент останется. </p><p>В нашем наборе данных всего 21 <a href="__GHOST_URL__/priznak/">Признак (Feature)</a>, и мы не просматривали их все. Тем не менее, мы нашли кое-что интересное.</p><p>Мы узнали, что столбцы SeniorCitizen, tenure, MonthlyCharges и TotalCharges в некоторой степени коррелируют со статусом оттока. Мы будем использовать их для нашей модели.</p><h3 id="%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">Глубокое обучение</h3><p><a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокое обучение (Deep Learning)</a> — это подраздел Машинного обучения, связанный с алгоритмами, основанными на структуре и функциях мозга – искусственными <a href="__GHOST_URL__/nieironnaia-siet/">Нейронными сетями (Neural Network)</a>.</p><p>Чтобы получить глубокую нейронную сеть, возьмите нейронную сеть с одним скрытым слоем (мелкая нейронная сеть) и добавьте больше слоев: это определение глубокой нейронной сети.</p><p>В глубоких нейронных сетях каждый слой обучается на выходных данных предыдущего слоя. Таким образом, мы можем создать иерархию функций-столбцов с возрастающей абстракцией и изучать сложные концепции.</p><p>Эти сети очень хорошо обнаруживают закономерности в необработанных данных (изображениях, текстах, видео- и аудиозаписях), а это самый большой объем данных, который у нас есть. Например, Deep Learning может взять миллионы изображений и разделить их на фотографии вашей бабушки, забавных кошек и вкусных тортов.</p><p>Глубокие нейронные сети – передовой метод решения целого ряда проблем, таких как распознавание изображений, их сегментация, распознавание звука, рекомендательные системы, обработка естественного языка и т. д.</p><p>Таким образом, глубокое обучение — это большие нейронные сети. Почему сейчас? Почему глубокое обучение не было практичным раньше?</p><ul><li>Для большинства реальных приложений глубокого обучения требуются большие объемы размеченных данных: для разработки беспилотного автомобиля могут потребоваться тысячи часов видео.</li><li>Обучающие модели с большим количеством параметров требуют значительных вычислительных мощностей: аппаратное обеспечение специального назначения в форма GPU и TPU предлагает массовые параллельные вычисления.</li><li>Крупные компании уже некоторое время хранят ваши данные: они хотят их монетизировать.</li><li>Мы узнали (вроде как), как инициализировать Веса (Weights) нейронов в моделях нейронной сети: в основном, используя небольшие случайные значения.</li><li>У нас есть лучшие методы Регуляризации (Regularization)</li></ul><p>И последнее, но не менее важное: у нас есть программное обеспечение, которое является производительным и простым в использовании. Такие библиотеки, как TensorFlow, PyTorch, MXNet и Chainer, позволяют специалистам-практикам разрабатывать, анализировать, тестировать и развертывать модели различной сложности, а также повторно использовать результаты работы других специалистов-практиков и исследователей.</p><h3 id="%D0%BF%D1%80%D0%BE%D0%B3%D0%BD%D0%BE%D0%B7%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BE%D1%82%D1%82%D0%BE%D0%BA%D0%B0-%D0%BA%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%B2">Прогнозирование оттока клиентов</h3><p>Давайте воспользуемся механизмом глубокого обучения, чтобы предсказать, какие клиенты собираются уйти. Во-первых, нам нужно выполнить некоторую предварительную обработку данных, поскольку многие столбцы являются категориальными.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%B4%D0%B2%D0%B0%D1%80%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F-%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Предварительная обработка данных</h3><p>Мы будем использовать все числовые (кроме customerID) и следующие категориальные признаки:</p><!--kg-card-begin: markdown--><p>const categoricalFeatures = new Set([<br>\n&quot;TechSupport&quot;,<br>\n&quot;Contract&quot;,<br>\n&quot;PaymentMethod&quot;,<br>\n&quot;gender&quot;,<br>\n&quot;Partner&quot;,<br>\n&quot;InternetService&quot;,<br>\n&quot;Dependents&quot;,<br>\n&quot;PhoneService&quot;,<br>\n&quot;TechSupport&quot;,<br>\n&quot;StreamingTV&quot;,<br>\n&quot;PaperlessBilling&quot;<br>\n]);</p>\n<!--kg-card-end: markdown--><p>Давайте создадим наборы данных для обучения и тестирования из наших данных:</p><!--kg-card-begin: markdown--><p>const [xTrain, xTest, yTrain, yTest] = toTensors(<br>\ndata,<br>\ncategoricalFeatures,<br>\n0.1<br>\n);</p>\n<!--kg-card-end: markdown--><p>Вот как мы создаем наши <a href="__GHOST_URL__/tenzor/">Тензоры (Tensor)</a> - многомерные массивы чисел:</p><!--kg-card-begin: markdown--><p>const toTensors = (data, categoricalFeatures, testSize) =&gt; {<br>\nconst categoricalData = {};<br>\ncategoricalFeatures.forEach(f =&gt; {<br>\ncategoricalData[f] = toCategorical(data, f);<br>\n});</p>\n<p>const features = [<br>\n&quot;SeniorCitizen&quot;,<br>\n&quot;tenure&quot;,<br>\n&quot;MonthlyCharges&quot;,<br>\n&quot;TotalCharges&quot;<br>\n].concat(Array.from(categoricalFeatures));</p>\n<p>const X = data.map((r, i) =&gt;<br>\nfeatures.flatMap(f =&gt; {<br>\nif (categoricalFeatures.has(f)) {<br>\nreturn categoricalData[f][i];<br>\n}</p>\n<pre><code>  return r[f];\n})\n</code></pre>\n<p>);</p>\n<p>const X_t = normalize(tf.tensor2d(X));</p>\n<p>const y = tf.tensor(toCategorical(data, &quot;Churn&quot;));</p>\n<p>const splitIdx = parseInt((1 - testSize) * data.length, 10);</p>\n<p>const [xTrain, xTest] = tf.split(X_t, [splitIdx, data.length - splitIdx]);<br>\nconst [yTrain, yTest] = tf.split(y, [splitIdx, data.length - splitIdx]);</p>\n<p>return [xTrain, xTest, yTrain, yTest];<br>\n};</p>\n<!--kg-card-end: markdown--><p>Во-первых, мы используем функцию <code>toCategorical()</code> для преобразования категориальных признаков в векторы с использованием <a href="__GHOST_URL__/bystroie-kodirovaniie/">Быстрого кодирования (One-Hot Encoding)</a>. Мы делаем это, конвертируя строковые значения в числа и используя <code>tf.oneHot()</code> для создания векторов.</p><p>Мы создаем двумерный тензор из наших признаков (категориальных и числовых) и нормализуем его. Другой тензор с горячим кодированием сделан из столбца Churn.</p><p>Наконец, мы разделяем набор на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные данные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые (Test Data)</a> и возвращаем результаты. Как мы кодируем категориальные переменные?</p><!--kg-card-begin: markdown--><p>const toCategorical = (data, column) =&gt; {<br>\nconst values = data.map(r =&gt; r[column]);<br>\nconst uniqueValues = new Set(values);</p>\n<p>const mapping = {};</p>\n<p>Array.from(uniqueValues).forEach((i, v) =&gt; {<br>\nmapping[i] = v;<br>\n});</p>\n<p>const encoded = values<br>\n.map(v =&gt; {<br>\nif (!v) {<br>\nreturn 0;<br>\n}<br>\nreturn mapping[v];<br>\n})<br>\n.map(v =&gt; oneHot(v, uniqueValues.size));</p>\n<p>return encoded;<br>\n};</p>\n<!--kg-card-end: markdown--><p>Сначала мы извлекаем вектор всех значений признака. Затем мы получаем уникальные значения и превращаем строковые значения в целочисленные.</p><p>Обратите внимание, что мы проверяем пропущенные значения и кодируем их как 0. Наконец, мы кодируем каждое значение.</p><p>Вот остальные служебные функции:</p><!--kg-card-begin: markdown--><p>// нормализованное_значение = (значение − минимум) / (максимум − минимум)<br>\nconst normalize = tensor =&gt;<br>\ntf.div(<br>\ntf.sub(tensor, tf.min(tensor)),<br>\ntf.sub(tf.max(tensor), tf.min(tensor))<br>\n);</p>\n<p>const oneHot = (val, categoryCount) =&gt;<br>\nArray.from(tf.oneHot(val, categoryCount).dataSync());</p>\n<!--kg-card-end: markdown--><h3 id="%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5-%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B9-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B9-%D1%81%D0%B5%D1%82%D0%B8">Создание глубокой нейронной сети</h3><p>Мы завершим построение и обучение нашей модели в функцию с именем <code>trainModel()</code>:</p><!--kg-card-begin: markdown--><p>const trainModel = async (xTrain, yTrain) =&gt; {<br>\n// Сейчас подробно рассмотрим, что внутри<br>\n...</p>\n<p>return model;<br>\n};</p>\n<!--kg-card-end: markdown--><p>Давайте создадим глубокую нейронную сеть, используя API последовательной модели в TensorFlow:</p><!--kg-card-begin: markdown--><p>const model = tf.sequential();<br>\nmodel.add(<br>\ntf.layers.dense({<br>\nunits: 32,<br>\nactivation: &quot;relu&quot;,<br>\ninputShape: [xTrain.shape[1]]<br>\n})<br>\n);</p>\n<p>model.add(<br>\ntf.layers.dense({<br>\nunits: 64,<br>\nactivation: &quot;relu&quot;<br>\n})<br>\n);</p>\n<p>model.add(tf.layers.dense({ units: 2, activation: &quot;softmax&quot; }));</p>\n<!--kg-card-end: markdown--><p>Наша глубокая нейронная сеть имеет два скрытых слоя с 32 и 64 нейронами соответственно. Каждый слой имеет Функция активации выпрямителя  (ReLU).</p><p>Время скомпилировать нашу модель:</p><!--kg-card-begin: markdown--><p>model.compile({<br>\noptimizer: tf.train.adam(0.001),<br>\nloss: &quot;binaryCrossentropy&quot;,<br>\nmetrics: [&quot;accuracy&quot;]<br>\n});</p>\n<!--kg-card-end: markdown--><p>Мы будем обучать нашу модель с помощью оптимизатора "Адаптивная оценка момента" (Adam) и измерять нашу ошибку с помощью бинарной <a href="__GHOST_URL__/kross-entropiia/">Кросс- энтропии (Cross-Entropy)</a>.</p><h3 id="%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">Обучение</h3><p>Наконец, мы передадим обучающие данные нашей модели и обучимся на них в течение 100 эпох, перемешаем данные и используем 10% для проверки. Визуализируем ход обучения с помощью tfjs-vis:</p><!--kg-card-begin: markdown--><p>const lossContainer = document.getElementById(&quot;loss-cont&quot;);</p>\n<p>await model.fit(xTrain, yTrain, {<br>\nbatchSize: 32,<br>\nepochs: 100,<br>\nshuffle: true,<br>\nvalidationSplit: 0.1,<br>\ncallbacks: tfvis.show.fitCallbacks(<br>\nlossContainer,<br>\n[&quot;loss&quot;, &quot;val_loss&quot;, &quot;acc&quot;, &quot;val_acc&quot;],<br>\n{<br>\ncallbacks: [&quot;onEpochEnd&quot;]<br>\n}<br>\n)<br>\n});</p>\n<!--kg-card-end: markdown--><p>Давайте обучим нашу модель:</p><!--kg-card-begin: markdown--><p>const model = await trainModel(xTrain, yTrain);</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/0_hf0-x8K95zp8OlBm.jpg" class="kg-image" alt loading="lazy" width="797" height="281" srcset="__GHOST_URL__/content/images/size/w600/2022/03/0_hf0-x8K95zp8OlBm.jpg 600w, __GHOST_URL__/content/images/2022/03/0_hf0-x8K95zp8OlBm.jpg 797w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/0_k0Uiaikl8XiNwIob.jpg" class="kg-image" alt loading="lazy" width="796" height="281" srcset="__GHOST_URL__/content/images/size/w600/2022/03/0_k0Uiaikl8XiNwIob.jpg 600w, __GHOST_URL__/content/images/2022/03/0_k0Uiaikl8XiNwIob.jpg 796w" sizes="(min-width: 720px) 720px"></figure><p>Похоже, что наша модель обучается в течение первых десяти эпох и выходит на плато после этого.</p><h3 id="%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B0-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8">Оценка модели</h3><p>Оценим нашу модель на тестовых данных:</p><!--kg-card-begin: markdown--><p>const result = model.evaluate(xTest, yTest, {<br>\nbatchSize: 32<br>\n});</p>\n<p>// Потери<br>\nresult[0].print();</p>\n<p>// Точность<br>\nresult[1].print();</p>\n<!--kg-card-end: markdown--><p>Модель имеет точность 79,2% на тестовых данных:</p><!--kg-card-begin: markdown--><p>Tensor 0.44808024168014526<br>\nTensor 0.7929078340530396</p>\n<!--kg-card-end: markdown--><p>Давайте посмотрим, какие ошибки он допускает, используя <a href="__GHOST_URL__/matritsa-oshibok/">Матрицу ошибок (Confusion Matrix)</a>:</p><!--kg-card-begin: markdown--><p>const preds = model.predict(xTest).argMax(-1);<br>\nconst labels = yTest.argMax(-1);<br>\nconst confusionMatrix = await tfvis.metrics.confusionMatrix(labels, preds);<br>\nconst container = document.getElementById(&quot;confusion-matrix&quot;);<br>\ntfvis.render.confusionMatrix(container, {<br>\nvalues: confusionMatrix,<br>\ntickLabels: [&quot;Retained&quot;, &quot;Churned&quot;]<br>\n});</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/0_y5Df-jhdAujAK3aF.jpg" class="kg-image" alt loading="lazy" width="797" height="405" srcset="__GHOST_URL__/content/images/size/w600/2022/03/0_y5Df-jhdAujAK3aF.jpg 600w, __GHOST_URL__/content/images/2022/03/0_y5Df-jhdAujAK3aF.jpg 797w" sizes="(min-width: 720px) 720px"></figure><p>Похоже, что наша модель слишком самоуверенна в прогнозировании удержания клиентов. В зависимости от потребностей мы можем перенастроить модель и улучшить ее предсказательную способность.</p><p>Песочница, не требующая дополнительной настройки на момент написания статьи, можно скачать <a href="https://codesandbox.io/s/credit-default-prediction-with-tensorflow-js-jt5mz?fontsize=14&amp;file=/src/index.js:5099-5952">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/customer-churn-prediction-using-neural-networks-with-tensorflow-js-6b3dc8c21e7d">Venelin Valkov</a></p>		modielirovaniie-ottoka	2022-03-05		
174	Уклон (Slope)		<p>Уклон – мера наклона прямой, описывающей функцию <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regression)</a>. Наклон линии рассчитывается путем деления "подъема" на "пробег" (т.е. на сколько единиц прямая идет вверх, разделенная на число единиц, уходящих вправо). В модели <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> уклон говорит нам, насколько мы ожидаем, что значение <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a> Y возрастет, когда мы увеличим значение X на одну единицу.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/03/slope.jpg" class="kg-image" alt loading="lazy" width="859" height="415" srcset="__GHOST_URL__/content/images/size/w600/2022/03/slope.jpg 600w, __GHOST_URL__/content/images/2022/03/slope.jpg 859w"></figure><p>Наклон просто говорит нам, насколько круто наклонена линия. Наклон определяется как "подъем, деленный на пробег", а точка пересечения с осью Y говорит нам, где линия пересекает ось Y. Эта линия на рисунке описывается следующим уравнением:</p><p><code>y = 0.5 x + 2</code></p><p>Наклон 0,5 означает, что когда мы идем по этой линии, на каждую единицу, которую мы двигаем вправо, мы перемещаемся на 0,5 единицы вверх. Наклон может быть нулевым, если мы вообще не движемся вверх, или отрицательным, если мы движемся вниз. Однако многие линии могут иметь одинаковый наклон. Я могу провести линию, параллельную этой, и она также будет подниматься на 0,5 единицы за каждую единицу, которую она перемещает вправо. Вот тут-то и появляется точка пересечения с осью Y. Она говорит нам, где линия пересекает ось Y. Эта конкретная линия пересекает линию на высоте 2, так что это y-пересечение.</p><p>Другими словами, наклон линии говорит нам о направлении, в котором она указывает, а точка пересечения с осью Y говорит нам о местоположении линии.</p><p>Автор оригинальной статьи: <a href="https://livebook.manning.com/concept/machine-learning/slope">manning.com</a></p>		uklon	2022-03-11		
175	Шум (Noise)		<p>Шум – записи в наборе данных, не укладывающиеся в ту или иную концепцию <a href="www.helenkapatsa.ru/klassifikatsiia/">Классификации (Classification)</a>. Такие <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> вызваны человеческой ошибкой при создании <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> или иными причинами.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image-9.png" class="kg-image" alt loading="lazy" width="533" height="382"></figure><p>Реальные данные имеют ряд факторов, которые могут повлиять на вероятность появления шума. Его существование – неизбежная проблема, но это поддается решению.</p><p>Люди склонны совершать ошибки при сборе данных, а инструменты могут быть ненадежными: это и приводит к ошибкам. Шум в <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> может вызвать проблемы, поскольку <a href="__GHOST_URL__/alghoritm/">Алгоритм (Algorithm)</a> интерпретирует такие записи как закономерность и начнет обобщать на основании имеющихся данных.</p><p>Зашумленный набор наносит ущерб всему <a href="__GHOST_URL__/paiplain/">Пайплайну (Pipeline)</a>. Зашумленность измеряется как отношение чистых данных – сигнала к шуму. Существует много методов, используемых для искоренения шума.</p><h3 id="%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85-%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82">Анализ главных компонент</h3><p><a href="__GHOST_URL__/mietod-ghlavnykh-komponient/">Анализ главных компонент (PCA)</a> — это арифметический метод, преобразующий потенциально коррелирующие переменные в несвязанные. Термин «главные компоненты» относится именно к ним.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/03/image-10.png" class="kg-image" alt loading="lazy" width="500" height="198"></figure><p>PCA пытается устранить искаженные данные из сигнала, сохраняя при этом важные функции. Это геометрический и статистический метод, который уменьшает размерность входного сигнала или данные, проецируя их по различным осям. </p><p>Чтобы лучше понять, представьте, что точка в измерении XY проецируется вдоль оси X. Плоскость шума — ось Y теперь можно удалить. Это явление получило название Понижение размерности (Dimensionality Reduction). В результате, путем исключения осей, содержащих зашумленные данные, анализ главных компонент может минимизировать шум во входных данных.</p><h3 id="%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5-%D1%88%D1%83%D0%BC%D0%BE%D0%BF%D0%BE%D0%B4%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5">Глубокое шумоподавление</h3><p>Существуют так называемые "автокодировщики", крайне полезные для шумоподавления. Поскольку их можно обучить распознавать обнаружение шума в сигнале или данных, их можно использовать в качестве шумоподавителей, передавая им зашумленные данные и получая на выходе чистые. Автокодировщики состоят из двух частей: кодировщик, который как бы шифрует входные данные, и декодер, который преобразует информацию обратно в читаемое состояние.</p><p>Шумоподавляющий автокодировщик делает две вещи: он кодирует вводные данные, сохраняя при этом как можно больше деталей. Он также устраняет шум.</p><h3 id="%D1%81%D1%80%D0%B0%D0%B2%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9-%D0%B4%D0%B0%D1%82%D0%B0%D1%81%D0%B5%D1%82">Сравнительный датасет</h3><p>Предположим, вам нужно очистить зашумленный набор данных. Используя подход адаптивного шумоподавления, в этом методе используются два сигнала: один является целевым, а другой — свободным от шума.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D1%84%D1%83%D1%80%D1%8C%D0%B5-fourier-transform">Преобразование Фурье (Fourier Transform)</h3><p>Исследования показали, что если наш сигнал имеет структуру, то мы можем удалить из них шум напрямую. <a href="__GHOST_URL__/prieobrazovaniie-furie/">Преобразование Фурье (Fourier Transform)</a> используется именно для этого.</p><p>Если мы разложим сигнал на частотные зоны, то заметим, что большая часть информации о сигнале во временной области представлена ​​всего несколькими частотами. Поскольку шум непредсказуем, он будет рассеиваться по всем.</p><p>Согласно теории, мы можем отфильтровать большую часть зашумленных данных, сохранив частоты, содержащие наиболее важную информацию. Таким образом можно удалить зашумленные сигналы из набора данных.</p><p>Автор оригинальной статьи: <a href="https://deepchecks.com/glossary/noise-in-machine-learning/">deepchecks.com</a></p>		shum	2022-03-19		
176	Управляемый рекуррентный блок (GRU)		<p>Управляемый рекуррентный блок (GRU – Gated Recurrent Units, УРБ) – механизм <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a>, объединяющий кратковременную и долговременную виды памяти в одно состояние. Механизм создан в 2014 году, и это разновидность Рекуррентной нейронной сети (RNN), набирающая большую популярность. Давайте посмотрим, как она работает.</p><p>Вы все знаете, что базовая RNN страдает от проблемы с кратковременной памятью. Например, мы пытаемся сделать автозаполнение последнего слова для утверждения: «Дхавал ест самосу почти каждый день, нетрудно догадаться, что эта любимая кухня — <u>индийская</u>" (Dhaval eats samosa almost everyday, it shouldn't be hard to guess that his favorite cuisine is Indian). Итак, чтобы догадаться, что это индийская кухня, мы должны помнить про самосу (печёное тесто с начинкой), но у традиционного RNN более короткая память:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/03/--------------2022-03-26---16.48.24.png" class="kg-image" alt loading="lazy" width="2000" height="985" srcset="__GHOST_URL__/content/images/size/w600/2022/03/--------------2022-03-26---16.48.24.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/--------------2022-03-26---16.48.24.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/--------------2022-03-26---16.48.24.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/--------------2022-03-26---16.48.24.png 2400w"></figure><p>Так, например, когда нейросеть помнит только последние два слова, когда она автозаполняет предложение о кухне, то не может предсказать ее "индийское происхождение", ибо слов между "samosa" и "indian" целых 14. Если у нас есть подобная сеть, которая может запоминать важные ключевые слова, скажем: «Дхавал ест», остальные неважные слова она не помнит, и когда дело доходит до самосы, она запоминает это и он "несет" ее в долговременной памяти на протяжении всего пути, а затем, когда потребуется сделать предположение, что это за кухня, мы можетм использовать это воспоминание и сделать автозаполняющее предположение, что это индийская кухня.</p><p>LSTM обладает подобными возможностями: он может запоминать долговременно. GRU — это модифицированная, упрощенная версия LSTM, в которой долговременная и кратковременная память объединяется в так называемое Скрытое состояние (Hidden State). У него есть только скрытое состояние, которое может сочетать в себе как долговременную, так и кратковременную память:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/03/--------------2022-03-26---16.55.24.png" class="kg-image" alt loading="lazy" width="2000" height="764" srcset="__GHOST_URL__/content/images/size/w600/2022/03/--------------2022-03-26---16.55.24.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/--------------2022-03-26---16.55.24.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/--------------2022-03-26---16.55.24.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/--------------2022-03-26---16.55.24.png 2400w"></figure><p>Если вы заглянете в "ящик" GRU, в нем будет двое ворот: обновление и сброс:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/03/--------------2022-03-26---16.56.38.png" class="kg-image" alt loading="lazy" width="2000" height="947" srcset="__GHOST_URL__/content/images/size/w600/2022/03/--------------2022-03-26---16.56.38.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/--------------2022-03-26---16.56.38.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/--------------2022-03-26---16.56.38.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/--------------2022-03-26---16.56.38.png 2400w"></figure><p>Функция update-ворот — помнить. По сути, они знают, сколько памяти нужно сохранить, тогда как reset-ворота знают, сколько памяти нужно "забыть". Это звучит немного неясно, но на самом деле есть небольшая разница, и мы рассмотрим ее на примере. И есть еще один механизм, который мы вскоре изучим. </p><p>Итак, давайте вернемся к нашей исходной задаче <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a> о завершении предложения. Поэтому, когда у вас есть такое предложение, конечно, ответ "индийский". Возможно, вы помните автоподстановку в Gmail: когда мы что-то вводим, система пытается предсказать следующее слово. Итак, мы пытаемся решить ту же проблему, и нейросеть знает, что это индийская кухня, поскольку помнит слово "самоса".</p><p>Но когда у нас есть более длинное утверждение: 'Dhaval eats samosa almost everyday, it shouldn't be hard to guess that his favorite cuisine is Indian. His brother Bhavin hovewer is a lover of pasta and cheese that means Bhavin's favorite cuisine is Italian' (Дхавал ест самосу почти каждый день, нетрудно догадаться, что его любимая кухня – индийская. Его брат Бхавин, однако, – любитель пасты с сыром, что означает, что любимая кухня Бхавина – итальянская".</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/03/--------------2022-03-26---17.04.21.png" class="kg-image" alt loading="lazy" width="2000" height="561" srcset="__GHOST_URL__/content/images/size/w600/2022/03/--------------2022-03-26---17.04.21.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/--------------2022-03-26---17.04.21.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/--------------2022-03-26---17.04.21.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/--------------2022-03-26---17.04.21.png 2400w"></figure><p>Тогда автозаполнение употребит слово "итальянская". Когда вы говорите "кухня", то вы хотите вспомнить о самосе, потому что у вас это сохранилось в контексте. И когда вы идете дальше и когда добираетесь до предложения с макаронами, то знаете, что контекст изменился. Когда дело доходит до пасты, мы хотим как бы забыть о самосе. Точно так же, когда дело доходит до сыра, вы хотите сохранить воспоминание о пасте, поэтому теперь вы уже видите какую-то разницу, то здесь хотите забыть о самосе. </p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/03/--------------2022-03-26---17.09.29.png" class="kg-image" alt loading="lazy" width="2000" height="971" srcset="__GHOST_URL__/content/images/size/w600/2022/03/--------------2022-03-26---17.09.29.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/--------------2022-03-26---17.09.29.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/--------------2022-03-26---17.09.29.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/--------------2022-03-26---17.09.29.png 2400w"></figure><p>Ячейка, которую можно увидеть ниже – это GRU:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/03/--------------2022-03-26---17.10.21.png" class="kg-image" alt loading="lazy" width="2000" height="1057" srcset="__GHOST_URL__/content/images/size/w600/2022/03/--------------2022-03-26---17.10.21.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/--------------2022-03-26---17.10.21.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/--------------2022-03-26---17.10.21.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/--------------2022-03-26---17.10.21.png 2400w"></figure><p>Когда речь заходит о пасте, мы хотим забыть о самосе, и способ сделать это – использовать reset-ворота (также называемые гейтом, шлюзом]. Такой гейт принимает скрытое состояние и выполняет математическую операцию – Весовая функция (Weight Function), применяющую сигмоиду (Sigmoid) поверх нее. Значение r(t), которое мы получаем, является значением сброса. </p><p>Когда дело доходит до сыра, например, мы хотим сохранить воспоминание о макаронах. И это делается шлюзом обновления, так что он будет делать то же самое, что и шлюз сброса, но будет выполнять другую матоперацию. Например, если вы посмотрите на это математическое уравнение:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/03/--------------2022-03-26---17.16.57.png" class="kg-image" alt loading="lazy" width="2000" height="1057" srcset="__GHOST_URL__/content/images/size/w600/2022/03/--------------2022-03-26---17.16.57.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/--------------2022-03-26---17.16.57.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/--------------2022-03-26---17.16.57.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/--------------2022-03-26---17.16.57.png 2400w"></figure><p>Математическое уравнение остается тем же: взвешенная сумма. Мы применяем сигмоиду, поэтому z(t), которое вы получаете здесь, является значением обновления.</p><p>До сих пор это выглядит немного просто, когда вы берете значение шлюза обновления и значение шлюза сброса. Дальнейшее объяснение станет немного сложнее. Мы будем использовать произведение Адамара – произведение двух матриц – поэлементное произведение их элементов. </p><p>Я знаю, что это довольно сложно, но, может быть, вы сможете использовать эту информацию и получите представление о том, что это такое. Суть GRU заключается в том, что он объединяет кратковременную память и долговременную память в одно состояние, и является более эффективным с вычислительной точки зрения, чем LSTM.</p><p>Автор оригинальной статьи: <a href="https://youtu.be/tOuXgORsXJ4">codebasics</a></p>		upravliaiemyi-riekurrientnyi-blok	2022-03-26		
177	Важность признака (Feature Importance)		<p>Важность функции – техника присваивания очков полезности зависимым переменным – <a href="__GHOST_URL__/priediktor/">Предикторам (Predictor Variables)</a> в зависимости от того, насколько они способны спрогнозировать <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a>.</p><p>Оценка важности играеь важную роль в прогнозном моделировании, в том числе обеспечивает понимание данных, модели и создает предпосылки для Понижения размерности (Dimensionality Reduction) и выбора <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, которые могут повысить эффективность и действенность модели.</p><p>В этом руководстве вы узнаете о показателях важности функций для машинного обучения в Python.</p><p>Важности признаков могут быть рассчитаны для задач, связанных с прогнозированием числового значения, называемых <a href="__GHOST_URL__/rieghriessiia/">Регрессией (Regression)</a>, и задач, связанных с прогнозированием метки класса – <a href="__GHOST_URL__/klassifikatsiia/">Классификацией (Classification)</a>.</p><p>Оценки полезны и могут использоваться в ряде ситуаций:</p><ul><li><strong>Лучшее понимание данных:</strong> относительные оценки могут указать, какие функции могут быть наиболее релевантными для цели, и, наоборот, какие наименее релевантны. Это может быть интерпретировано экспертом в предметной области и использовано в качестве основы для сбора большего количества или других данных.</li><li><strong>Лучшее понимание модели: </strong>расчет коэффициентов важности дает представление о конкретной модели и о том, какие столбцы являются наиболее и наименее важными при прогнозировании. </li><li><strong>Уменьшение количества входных признаков: </strong>используя оценки важности, чтобы выбрать те функции, которые нужно удалить (обладатели самых низких оценок). Это может упростить моделирование, ускорить процесс и, в некоторых случаях, повысить производительность модели.</li></ul><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">Классификация</h3><p>Мы будем использовать функцию <code>make_classification()</code> для создания тестового набора данных Бинарной классификации (Binary Classification).</p><p>В <a href="__GHOST_URL__/dataset/">Датасете (Dataset)</a> будет 1000 примеров с 10 предикторами, пять из которых будут информативными, а остальные – избыточными. Мы внедрим элемент случайности, чтобы гарантировать получение одних и тех же результатов при каждом запуске кода. </p><!--kg-card-begin: markdown--><p>from sklearn.datasets import make_regression<br>\nfrom sklearn.linear_model import LinearRegression<br>\nfrom matplotlib import pyplot</p>\n<!--kg-card-end: markdown--><p>Сгенерируем игрушечный набор данных, параметр <code>n_informative</code> как раз и определяет, какие признаки являются важными:</p><!--kg-card-begin: markdown--><p>X, y = make_regression(n_samples = 1000, n_features = 10, n_informative = 5, random_state = 1)</p>\n<!--kg-card-end: markdown--><p>Обучим модель и выведем коэффициенты важности:</p><!--kg-card-begin: markdown--><p>model = LinearRegression()<br>\nmodel.fit(X, y)<br>\nimportance = model.coef_<br>\nfor i,v in enumerate(importance):<br>\nprint('Feature: %0d, Score: %.5f' % (i,v))</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Feature: 0, Score: 0.00000<br>\nFeature: 1, Score: 12.44483<br>\nFeature: 2, Score: -0.00000<br>\nFeature: 3, Score: -0.00000<br>\nFeature: 4, Score: 93.32225<br>\nFeature: 5, Score: 86.50811<br>\nFeature: 6, Score: 26.74607<br>\nFeature: 7, Score: 3.28535<br>\nFeature: 8, Score: -0.00000<br>\nFeature: 9, Score: 0.00000</p>\n<!--kg-card-end: markdown--><p>Отобразим результат с помощью столбчатой диаграммы:</p><!--kg-card-begin: markdown--><p>pyplot.bar([x for x in range(len(importance))], importance)<br>\npyplot.show()</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/04/image.png" class="kg-image" alt loading="lazy" width="1280" height="960" srcset="__GHOST_URL__/content/images/size/w600/2022/04/image.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image.png 1000w, __GHOST_URL__/content/images/2022/04/image.png 1280w" sizes="(min-width: 720px) 720px"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1EsaDuEXmdypyz6Fcuz8wP1UyM6Hymn41?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/what-is-deep-learning/">Jason Brownlee</a></p>		vazhnost-priznaka	2022-04-04		
178	N-грамма (N-gram)		<p>N-грамма — это последовательность из n слов, где n — дискретное число, которое может принимать значения от 1 до бесконечности. Например, слово «сыр» — это 1-грамма (униграмма). Сочетание «вкус сыра» состоит из двух слов, то есть является биграммой. Точно так же фраза «со вкусом сыра» состоит из трех слов и является триграммой. N-граммы широко применяются для упорядочивания текстовых данных в преддверии создания Модели (Model) <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/04/image-1.png" class="kg-image" alt loading="lazy" width="445" height="275"></figure><p>Проблема с моделированием текста заключается в том, что последний весьма беспорядочный, а большинство <a href="__GHOST_URL__/alghoritm/">Алгоритмов (Algorithm)</a> предпочитают входные данные фиксированной длины и формата.</p><p>Алгоритмы Машинного обучения не могут работать напрямую с необработанным текстом: его необходимо преобразовать в числа а точнее, в векторы чисел. При языковой обработке векторы выводятся из текстовых данных, чтобы отразить различные лингвистические свойства текста. Это называется извлечением или Кодированием (Encoding) признаков, например, <a href="__GHOST_URL__/mieshok-slov/">Мешок слов  (Bag of Words)</a>. При таком преобразовании каждое слово называются «граммой». </p>		n-ghramm	2022-04-07		
179	Тест Дики-Фуллера (ADF)		<p>Тест ADF (расширенный тест Дики – Фуллера) – проверка Статистической значимости (Statistical Significance), которая демонстрирует результаты проверки <a href="__GHOST_URL__/nulievaia-ghipotieza/">Нулевой гипотезы (Null Hypothesis)</a> и <a href="__GHOST_URL__/altiernativnaia-ghipotieza/">Альтернативной (Alternative Hypothesis)</a>. В результате мы получим <a href="__GHOST_URL__/p-znachieniie/">P-значение (P Value)</a>, из которого можно сделать вывод о <a href="__GHOST_URL__/statsionarnost/">Стационарности (Stationarity)</a> <a href="__GHOST_URL__/vriemiennoi-riad/">Временного ряда (Time Series)</a>. Был предложен в 1979 году Дэвидом Дики и Уэйном Фуллером.</p><p>Когда мы создаем прогнозирующую <a href="__GHOST_URL__/modiel/">Модель (Model)</a> для временных рядов, нам требуются <em>стационарные</em> временные ряды, то есть обладающие одинаковой <a href="__GHOST_URL__/kovariatsiia/">Ковариацией (Covariance)</a> <a href="__GHOST_URL__/vyborka/">Выборок (Sample)</a> одного размера. Ковариация – мера взаимосвязи двух случайных величин, измеряющая общее отклонение двух случайных величин от их ожидаемых значений. Метрика оценивает, в какой степени переменные изменяются вместе. Другими словами, это мера <a href="__GHOST_URL__/dispiersiia/">Дисперсии (Variance)</a> между двумя переменными.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2021/09/stationarity.png" class="kg-image" alt loading="lazy"><figcaption>Стационарный временной ряд (сверху) и нестационарный</figcaption></figure><p>Тестирование на стационарность часто используется в Авторегрессионном моделях (Autoregressive Model). Мы можем выполнять различные тесты, такие как Критерий KPSS, Тест Филлипса – Перрона (Phillips-Perron Test) и ADF – тема этой статьи. Мы рассмотрим логику, стоящую за тестом, и реализуем ее с помощью временного ряда.</p><p>Мы проведем ADF-тест с нестационарными данными о пассажирах авиакомпаний и стационарными данными о температуре.</p><h2 id="adf-statsmodels">ADF: statsmodels</h2><p>Для начала импортируем необходимые библиотеки:</p><!--kg-card-begin: markdown--><p>from statsmodels.tsa.stattools import adfuller<br>\nimport pandas as pd<br>\nimport numpy as np</p>\n<!--kg-card-end: markdown--><h3 id="%D0%BD%D0%B5%D1%81%D1%82%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D1%80%D0%BD%D1%8B%D0%B9-%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D0%BE%D0%B9-%D1%80%D1%8F%D0%B4">Нестационарный временной ряд</h3><p>Мы будем использовать Датасет (Dataset) о числе пассажиров одной авиакомпании:</p><!--kg-card-begin: markdown--><p>path = '<a href="https://www.dropbox.com/s/cdjjafehpd0tmbw/AirPassengers.csv?dl=1">https://www.dropbox.com/s/cdjjafehpd0tmbw/AirPassengers.csv?dl=1</a>'<br>\ndata = pd.read_csv(path)<br>\ndata.plot(figsize = (14, 8), title = 'Число пассажиров')</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/04/adf-passengers.jpg" class="kg-image" alt loading="lazy" width="907" height="565" srcset="__GHOST_URL__/content/images/size/w600/2022/04/adf-passengers.jpg 600w, __GHOST_URL__/content/images/2022/04/adf-passengers.jpg 907w"></figure><p>Теперь, когда у нас есть все, что нужно, мы можем выполнить ADF-тест:</p><!--kg-card-begin: markdown--><p>result = adfuller(data['#Passengers'], autolag='AIC')</p>\n<p>print('Критерий ADF: %f' % result[0])</p>\n<p>print('P-значение: %f' % result[1])</p>\n<p>print('Критические значения:')</p>\n<p>for key, value in result[4].items():<br>\nprint('\\t%s: %.3f' % (key, value))<br>\nif result[0] &lt; result[4][&quot;5%&quot;]:<br>\nprint (&quot;Нулевая гипотеза отвергнута – Временной ряд стационарен&quot;)<br>\nelse:<br>\nprint (&quot;Нулевая гипотеза не отвергнута – Временной ряд не стационарен&quot;)</p>\n<!--kg-card-end: markdown--><p>Вот такие мы получим метрики:</p><!--kg-card-begin: markdown--><p>Критерий ADF: 0.815369<br>\nP-значение: 0.991880<br>\nКритические значения:<br>\n1%: -3.482<br>\n5%: -2.884<br>\n10%: -2.579<br>\nНулевая гипотеза не отвергнута – Временной ряд не стационарен</p>\n<!--kg-card-end: markdown--><p>P-значение для временного ряда больше 5%, и, соответственно, нулевая гипотеза не отвергнута, а временной ряд нестационарен.</p><h3 id="%D1%81%D1%82%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D1%80%D0%BD%D1%8B%D0%B9-%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D0%BE%D0%B9-%D1%80%D1%8F%D0%B4">Стационарный временной ряд</h3><p>Загрузим данные о температуре в Австралии:</p><!--kg-card-begin: markdown--><p>path = '<a href="https://www.dropbox.com/s/i8xs9myposohyp9/temperature.csv?dl=1">https://www.dropbox.com/s/i8xs9myposohyp9/temperature.csv?dl=1</a>'<br>\ndata = pd.read_csv(path)<br>\ndata.plot(figsize = (14, 8), title = 'Температура')</p>\n<!--kg-card-end: markdown--><p>Это средние значения температуры за день в течение нескольких лет:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/04/adf-temperature.jpg" class="kg-image" alt loading="lazy" width="901" height="565" srcset="__GHOST_URL__/content/images/size/w600/2022/04/adf-temperature.jpg 600w, __GHOST_URL__/content/images/2022/04/adf-temperature.jpg 901w"></figure><p>Проверим этот временной ряд на стационарность:</p><!--kg-card-begin: markdown--><p>result = adfuller(data['Temp'], autolag = 'AIC')</p>\n<p>print('Критерий ADF: %f' % result[0])</p>\n<p>print('P-значение: %f' % result[1])</p>\n<p>print('Критические значения:')</p>\n<p>for key, value in result[4].items():<br>\nprint('\\t%s: %.3f' % (key, value))<br>\nif result[0] &gt; result[4][&quot;5%&quot;]:<br>\nprint (&quot;Нулевая гипотеза отвергнута – Временной ряд не стационарен&quot;)<br>\nelse:<br>\nprint (&quot;Нулевая гипотеза не отвергнута – Временной ряд стационарен&quot;)</p>\n<!--kg-card-end: markdown--><p>В результате мы видим, что P-значение, полученное в результате теста, меньше 0,05, поэтому мы собираемся отклонить нулевую гипотезу о нестационарности:</p><!--kg-card-begin: markdown--><p>Критерий ADF: -4.444805<br>\nP-значение: 0.000247<br>\nКритические значения:<br>\n1%: -3.432<br>\n5%: -2.862<br>\n10%: -2.567<br>\nНулевая гипотеза не отвергнута – Временной ряд стационарен</p>\n<!--kg-card-end: markdown--><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1EEfuqVA-Cts16yKoWH9lnwgPiwFFKaXv?usp=sharing">здесь</a>.</p>		tiest-diki-fulliera	2022-04-09		
180	k-блочная кросс-валидация (k-Fold Cross Validation)		<p>k-блочная кросс-валидация (k-блочная перекрестная проверка) – процедура, используемая для оценки навыков <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> на новых данных.</p><p>Она обычно используется для сравнения и выбора модели конкретной задачи прогнозного моделирования, потому что его легко понять и реализовать.</p><p>Существуют общие тактики, которые вы можете использовать для выбора значения k для вашего набора данных. Существуют широко используемые варианты перекрестной проверки, такие как стратифицированная и повторная кросс-валидация, которые доступны в scikit-learn.</p><p>Кросс-валидация, или перекрестная проверка — это процедура повторной выборки, используемая для оценки моделей машинного обучения на ограниченной <a href="__GHOST_URL__/vyborka/">Выборке (Sample)</a> данных.</p><p>Процедура имеет единственный параметр, называемый k, который означает количество групп, на которые должна быть разбита данная выборка данных. Таким образом, эту процедуру часто называют k-кратной перекрестной проверкой. Когда выбрано конкретное значение k, его можно использовать вместо k в ссылке на модель, например, k=10 становится 10-кратной перекрестной проверкой.</p><p>Это популярный метод, поскольку он прост для понимания и обычно приводит к менее предвзятой или менее оптимистичной оценке навыков модели, чем другие методы.</p><p>Общая процедура выглядит следующим образом:</p><ul><li>Перемешайте набор данных случайным образом</li><li>Разделите набор на k групп. Для каждой уникальной группы:</li><li>\tВыделите группу записей в качестве <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовых данных (Test Data)</a></li><li>\tВозьмите оставшиеся группы в качестве <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a></li><li>\tОбучите модель на тренировочных и оцените ее эффективность на тестовых данных</li><li>\tСохраните значение оценки и сбросьте модель до исходного состояния для следующей итерации</li><li>Установите средний уровень навыка модели.</li></ul><p>Важно отметить, что каждое наблюдение в выборке данных относится к отдельной группе и остается в этой группе на протяжении всей процедуры. Это означает, что каждому образцу предоставляется возможность использоваться в наборе удержания 1 раз и использоваться для обучения модели k-1 раз.</p><p>Этот подход предполагает случайное разделение набора наблюдений на k групп (складок) примерно одинакового размера. Первая складка рассматривается как проверочный набор, и обучение проходит на оставшихся k-1 частях.</p><p>Также важно, чтобы любая подготовка данных перед подгонкой модели происходила в выборке, а не в более широком наборе данных. Это также относится к любой настройке <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a>. Невыполнение этих условий может привести к <a href="__GHOST_URL__/utiechka-dannykh/">Утечке данных (Data Leakage) </a>и оптимистичной оценке навыков модели.</p><p>Несмотря на все усилия статистических методистов, пользователи часто делают недействительными свои результаты, непреднамеренно взглянув на тестовые данные.</p><p>Результаты k-блочной кросс-валидации часто суммируются со средним значением оценки навыков модели. Также рекомендуется включать меру дисперсии оценок навыков, например <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>.</p><h3 id="%D0%BA%D0%BE%D0%BD%D1%84%D0%B8%D0%B3%D1%83%D1%80%D0%B0%D1%86%D0%B8%D1%8F-k">Конфигурация k</h3><p>Значение k должно быть тщательно подобрано.</p><p>Плохо выбранное значение k может привести к неправильному представлению о навыках модели, например, оценка с высокой <a href="__GHOST_URL__/dispiersiia/">Дисперсией (Variance)</a>, которая может сильно измениться в зависимости от данных, используемых для соответствия модели, или высокое систематическое <a href="__GHOST_URL__/smieshchieniie/">Смещение (Bias)</a>.</p><p>Три общие тактики выбора значения для k заключаются в следующем:</p><ul><li>Представительная: значение для k выбирается таким образом, чтобы каждая обучающая / тестовая группа выборок данных была достаточно большой, чтобы быть статистически репрезентативной для более широкого набора данных.</li><li>k = 10:  значение, которое, как было установлено в ходе экспериментов, обычно гарантирует низкое смещение и небольшую дисперсию.</li><li>k = n: значение k фиксируется равным n, где n — это размер набора данных, чтобы дать каждой тестовой выборке возможность быть использованной в наборе данных удержания. Такой подход называется <a href="__GHOST_URL__/poeliemientnaia/">Поэлементной кросс-валидацией (LOOCV)</a>.</li></ul><p>Выбор k обычно равен 5 или 10, но формального правила нет. По мере того, как k становится больше, разница в размере между обучающим набором и подмножествами повторной выборки становится меньше. По мере уменьшения этой разницы смещение уменьшается.</p><p>Значение k = 10 очень распространено в области прикладного машинного обучения и рекомендуется, если вы пытаетесь выбрать значение для своего набора данных.</p><p>Подводя итог, можно сказать, что существует компромисс между смещением и дисперсией, связанный с выбором k в k-кратной кросс-валидации. Как правило, с учетом этих соображений выполняется k-кратная перекрестная проверка с использованием k = 5 или k = 10, поскольку эмпирически было показано, что эти значения дают оценки частоты ошибок теста, которые не страдают ни чрезмерно высоким смещением, ни очень высокой дисперсией.</p><p>Если выбрано значение k, не разделяющее выборку данных равномерно, то одна группа будет содержать оставшуюся часть примеров. Предпочтительно разделить выборку данных на k групп с одинаковым количеством выборок, чтобы все выборки оценок навыков модели были эквивалентны.</p><h3 id="k-%D0%B1%D0%BB%D0%BE%D1%87%D0%BD%D0%B0%D1%8F-%D0%BA%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F-sklearn">k-блочная кросс-валидация: Sklearn</h3><p>Давайте посмотрим, как k-блочная кросс-валидация реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><!--kg-card-begin: markdown--><p>import numpy as np<br>\nfrom numpy import array<br>\nfrom sklearn.model_selection import KFold</p>\n<!--kg-card-end: markdown--><p>Инициализируем два массива X и y, содержащие значения <a href="__GHOST_URL__/priediktor/">Предикторов (Predictor Variable)</a> и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>. Затем инициализируем сущность валидатор KFold, который разделит этот набор на две части:</p><!--kg-card-begin: markdown--><p>X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])<br>\ny = np.array([1, 2, 3, 4])<br>\nkf = KFold(n_splits=2)<br>\nkf.get_n_splits(X)<br>\nprint(kf)</p>\n<!--kg-card-end: markdown--><p>Система отобразит некоторые настройки валидатора, как то: режим случайной выборки и перемешивание:</p><!--kg-card-begin: markdown--><p>KFold(n_splits=2, random_state=None, shuffle=False)</p>\n<!--kg-card-end: markdown--><p>Обучим модель на имеющихся данных:</p><!--kg-card-begin: markdown--><p>for train_index, test_index in kf.split(X):<br>\nprint(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)<br>\nX_train, X_test = X[train_index], X[test_index]<br>\ny_train, y_test = y[train_index], y[test_index]</p>\n<!--kg-card-end: markdown--><p>Посмотрим, какие метки класса модель назначит тестовым данным. Поскольку классов всего 4 (0, 1, 2 и 3), то в каждой строке выходных данных должны фигурировать все они, что и происходит:</p><!--kg-card-begin: markdown--><p>TRAIN: [2 3] TEST: [0 1]<br>\nTRAIN: [0 1] TEST: [2 3]</p>\n<!--kg-card-end: markdown--><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1H7YR22ymNvoklP4JTItKIcO86Gi-lX_9?usp=sharing">здесь</a>.</p>		k-blochnaia-kross-validatsiia	2022-04-16		
181	MinMaxScaler		<p>MinMaxScaler – библиотека Scikit-learn, позволяющая произвести <a href="__GHOST_URL__/normalizatsiia/">Нормализацию (Normalization)</a> данных перед использованием в <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, то есть приведение <a href="__GHOST_URL__/chislovaia-pieriemiennaia/">Числовых переменных (Numeric Variable)</a> к диапазону от 0 до 1.</p><p>Многие <a href="__GHOST_URL__/alghoritm/">Алгоритмы (Algorithm)</a> Машинного обучения работают лучше, когда числовые входные переменные приводятся к определенному диапазону.</p><p>Двумя наиболее популярными методами масштабирования числовых данных перед моделированием являются нормализация и <a href="__GHOST_URL__/standartizatsiia/">Стандартизация (Standartization)</a>. Нормализация масштабирует каждую входную переменную до диапазона от нуля до единицы – диапазон значений, где мы имеем наибольшую точность. Стандартизация масштабирует каждую входную переменную отдельно путем вычитания среднего значения (так называемое центрирование) и деления на <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a>, чтобы <a href="__GHOST_URL__/sriednieie-znachieniie/">Среднее значение (Average)</a> равнялось нулю, а стандартное отклонение – единице.</p><p>Масштабирование данных является рекомендуемым этапом предварительной обработки при работе со многими алгоритмами машинного обучения. </p><p>Модели машинного обучения изучают сопоставление <a href="__GHOST_URL__/priediktor/">Предикторов (Predictor Variable)</a> с <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой (Target Variable)</a>.</p><p>Входные переменные – предикторы могут иметь разные единицы измерения (например, футы, километры и часы), что, в свою очередь означает разные масштабы. А это увеличивает сложность моделирования. Большие входные значения (например, разброс в сотни или тысячи единиц) могут привести к модели, которая изучает большие значения веса. Модель с большими значениями веса часто нестабильна, а это означает, что она может страдать от низкой производительности во время обучения и чувствительности к входным значениям, что приводит к более высокой ошибке обобщения.</p><p>Нормализация требует, чтобы вы знали или могли точно оценить минимальные и максимальные наблюдаемые значения. Возможно, вы сможете оценить эти значения по имеющимся у вас данным.</p><!--kg-card-begin: markdown--><p>$$x_{i,norm} = \\frac{x_i - x_{min}}{x_{max} - x_{min}}, где$$<br>\n$$x_{i,norm}\\space{–}\\space{нормализованный}\\space{элемент}\\space{признака,}$$<br>\n$$x_{min}\\space{–}\\space{наименьший}\\space{элемент}\\space{признака,}$$<br>\n$$x_i\\space{–}\\space{i-й}\\space{непреобразованный}\\space{элемент,}$$<br>\n$$x_{max}\\space{–}\\space{наибольший}\\space{элемент}$$</p>\n<!--kg-card-end: markdown--><p>Например, для набора данных мы могли бы предположить минимальное и максимальное наблюдаемые значения как 30 и -10. Затем мы можем нормализовать любое значение, например 18,8, следующим образом:</p><p>y = (18.8 – (-10)) / (30 – (-10)) = 28.8 / 40 = 0.72</p><p>Вы можете нормализовать свой набор данных с помощью объекта MinMaxScaler библиотеки scikit-learn.</p><h3 id="%D0%BD%D0%B0%D0%B1%D0%BE%D1%80-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-sonar">Набор данных Sonar</h3><p>Набор данных Sonar — это стандартный набор данных для двоичной классификации. Он включает 60 столбцов-предикторов и целевую переменную с двумя классами. В наборе данных 208 примеров, и классы достаточно сбалансированы.</p><p>Это набор данных классификации гидроакустических сигналов. Задача состоит в том, чтобы научить <a href="__GHOST_URL__/nieironnaia-siet/">Нейронную сеть (Neural Network)</a> различать отраженные сигналы сонара.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/04/image-3.png" class="kg-image" alt loading="lazy" width="2000" height="1095" srcset="__GHOST_URL__/content/images/size/w600/2022/04/image-3.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image-3.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/04/image-3.png 1600w, __GHOST_URL__/content/images/2022/04/image-3.png 2362w" sizes="(min-width: 720px) 720px"><figcaption>Проекция подземных пещер, созданная с помощью гидроакустических сигналов</figcaption></figure><p>Для начала импортируем необходимые библиотеки:</p><!--kg-card-begin: markdown--><p>import matplotlib.pyplot as plt</p>\n<p>from numpy import mean, std</p>\n<p>from pandas import read_csv, DataFrame<br>\nfrom pandas.plotting import scatter_matrix</p>\n<p>from sklearn.model_selection import cross_val_score<br>\nfrom sklearn.model_selection import RepeatedStratifiedKFold<br>\nfrom sklearn.neighbors import KNeighborsClassifier<br>\nfrom sklearn.pipeline import Pipeline<br>\nfrom sklearn.preprocessing import LabelEncoder<br>\nfrom sklearn.preprocessing import MinMaxScaler</p>\n<!--kg-card-end: markdown--><p>Теперь подгрузим датасет:</p><!--kg-card-begin: markdown--><p>plt.rcParams[&quot;figure.figsize&quot;] = (20, 20)</p>\n<p>url = &quot;<a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv">https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv</a>&quot;<br>\ndataset = read_csv(url, header = None)</p>\n<p>dataset.hist()<br>\npyplot.show()</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/04/minmaxscaler-features-distribution.jpg" class="kg-image" alt loading="lazy" width="1242" height="1217" srcset="__GHOST_URL__/content/images/size/w600/2022/04/minmaxscaler-features-distribution.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/04/minmaxscaler-features-distribution.jpg 1000w, __GHOST_URL__/content/images/2022/04/minmaxscaler-features-distribution.jpg 1242w"></figure><p>Для каждой входной переменной создается <a href="__GHOST_URL__/gistoghramma/">Гистограмма (Histogram)</a>. Многие переменные имеют асимметричное распределение. </p><p>Набор данных – хороший кандидат для масштабирующих преобразований, поскольку переменные имеют разные минимальные и максимальные значения, а также разные распределения.</p><p>Далее давайте подгоним и оценим модель на необработанном наборе данных. Разделим данные на предикторы и целевую переменную, назначим всем столбцам тип данных – число с плавающей запятой. Закодируем классы целевой переменной и инициируем модель K ближайших соседей. Проведем <a href="__GHOST_URL__/kross-validatsiia/">Кросс-валидацию (Cross Validation)</a> и посмотрим точность:</p><!--kg-card-begin: markdown--><p>data = dataset.values<br>\nX, y = data[:, :-1], data[:, -1]<br>\nX = X.astype('float32')<br>\ny = LabelEncoder().fit_transform(y.astype('str'))<br>\nmodel = KNeighborsClassifier()<br>\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)<br>\nn_scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')<br>\nprint('Точность: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))</p>\n<!--kg-card-end: markdown--><p>Модель показывает средние результаты Точности (Accuracy):</p><!--kg-card-begin: markdown--><p>Точность: 0.797 (0.073)</p>\n<!--kg-card-end: markdown--><p>Мы видим, что модель достигла средней точности классификации около 79,7 % и находится на уровне хорошей производительности.</p><p>Давайте теперь посмотрим, как на этот показатель повлияет нормализация сс MinMaxScaler. Инициализируем экземпляр класса MinMaxScaler и Мы вызываем функцию fit_transform(), передавая ей наш набор данных. Затем конвертируем массив обратно в <a href="__GHOST_URL__/datafrieim/">Датафрейм (Dataframe)</a>. </p><!--kg-card-begin: markdown--><p>data = dataset.values[:, :-1]<br>\ntrans = MinMaxScaler()<br>\ndata = trans.fit_transform(data)<br>\ndataset = DataFrame(data)</p>\n<p>plt.rcParams[&quot;figure.figsize&quot;] = (20, 20)</p>\n<p>dataset.hist()<br>\npyplot.show()</p>\n<!--kg-card-end: markdown--><p>Мы видим, что распределения были скорректированы и что минимальное и максимальное значения для каждой переменной теперь равны 0 и 1 соответственно:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/04/minmaxscaler-normalized-features-distribution.jpg" class="kg-image" alt loading="lazy" width="1242" height="1217" srcset="__GHOST_URL__/content/images/size/w600/2022/04/minmaxscaler-normalized-features-distribution.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/04/minmaxscaler-normalized-features-distribution.jpg 1000w, __GHOST_URL__/content/images/2022/04/minmaxscaler-normalized-features-distribution.jpg 1242w"></figure><p>Создаются гистограммы переменных, хотя распределения не сильно отличаются от их исходных распределений, показанных в предыдущем разделе.</p><p>Графики гистограмм преобразованных MinMaxScaler входных переменных для набора данных сонара<br>Графики гистограмм преобразованных MinMaxScaler входных переменных для набора данных сонара</p><p>Далее давайте оценим ту же модель KNN, что и в предыдущем разделе, но в данном случае с преобразованным набором данных:</p><!--kg-card-begin: markdown--><p>trans = MinMaxScaler()<br>\nmodel = KNeighborsClassifier()<br>\npipeline = Pipeline(steps = [('t', trans), ('m', model)])<br>\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)<br>\nn_scores = cross_val_score(pipeline, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')<br>\nprint('Точность: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))</p>\n<!--kg-card-end: markdown--><p>Запустив пример, мы видим, что преобразование MinMaxScaler приводит к повышению производительности до 81,3%:</p><!--kg-card-begin: markdown--><p>Точность: 0.813 (0.085)</p>\n<!--kg-card-end: markdown--><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1VUhxv2tGQySlTYxcXim3Ad4LutnAYX_T#scrollTo=VDFV6Tqcw3Op">здесь</a>.</p>		minmaxscaler	2022-04-23		
182	Пулинг (Pooling)		<p>Пул — понижение размерности изображения, применяемой во избежание <a href="__GHOST_URL__/pierieobuchieniie/" rel="noopener noreferrer">Переобучения (Overfitting)</a><a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модели (Model)</a><a href="__GHOST_URL__/mashinnoie-obuchieniie/" rel="noopener noreferrer">Машинного обучения (ML)</a>. Самый распространенный вид пулинга "2 х 2" отбрасывает три пиксела из четырех:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/05/image.png" class="kg-image" alt loading="lazy" width="314" height="182"></figure><p>Роль пулинга состоит в том, чтобы уменьшить разрешение объекта, но сохранить данные, необходимые для классификации. Объединение в пул значительно снижает стоимость вычислений.</p><h3 id="%D1%82%D0%B5%D1%85%D0%BD%D0%B8%D0%BA%D0%B8">Техники</h3><p>Существует множество техник объединения:</p><ul><li><strong>Объединение по максимумам (Max Pooling)</strong>, при котором мы берем наибольшее из значений пикселей сегмента:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/05/image-1.png" class="kg-image" alt loading="lazy" width="314" height="182"></figure><ul><li><strong>Объединение средних значений (Mean Pooling)</strong>, при котором мы берем наибольшее из значений пикселей сегмента:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/05/pooling-mean.jpg" class="kg-image" alt loading="lazy" width="479" height="317"></figure><ul><li><strong>Объединение средних значений (Average Pooling)</strong>, при котором мы берем наибольшее из значений пикселей сегмента</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/05/pooling-avg.jpg" class="kg-image" alt loading="lazy" width="1300" height="579" srcset="__GHOST_URL__/content/images/size/w600/2022/05/pooling-avg.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/05/pooling-avg.jpg 1000w, __GHOST_URL__/content/images/2022/05/pooling-avg.jpg 1300w" sizes="(min-width: 720px) 720px"><figcaption>Пулинг с шагом (stride), равным 2</figcaption></figure><p>Автор оригинальной статьи: <a href="https://www.kaggle.com/questions-and-answers/59502">Pavan Sanagapati</a></p>		pulingh	2022-05-01		
183	Доля ложных положительных классификаций (FPR)		<p>Доля ложных положительных классификаций (False Positive Rate) – число предсказаний, ошибочно классифицированных <a href="__GHOST_URL__/modiel/">Моделью (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> как утвердительные. Является составляющей <a href="__GHOST_URL__/matritsa-oshibok/">Матрицы ошибок (Confusion Matrix)</a>.</p><p>Матрица ошибок – это показатель успешности классификации, где классов два или более. Это таблица с 4 различными комбинациями сочетаний прогнозируемых и фактических значений.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/matrix-confusion-2.png" class="kg-image" alt loading="lazy" width="564" height="185"></figure><p>Давайте рассмотрим значения ячеек (истинно позитивные, ошибочно позитивные, ошибочно негативные, истинно негативные) с помощью "беременной" аналогии.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/confusion-matrix-error-types.png" class="kg-image" alt loading="lazy" width="512" height="512"></figure><p><br><strong>Истинно позитивное предсказание (True Positive, сокр. TP)</strong><br>Вы предсказали положительный результат, и женщина действительно беременна.</p><p><strong>Истинно отрицательное</strong> <strong>предсказание (True Negative, TN)</strong><br>Вы предсказали отрицательный результат, и мужчина действительно не беременен.</p><p><strong>Ошибочно положительное</strong> <strong>предсказание (ошибка типа I, False Positive, FP)</strong><br>Вы предсказали положительный результат (мужчина беременен), но на самом деле это не так.</p><p><strong>Ошибочно отрицательное</strong> <strong>предсказание (ошибка типа II, False Negative, FN)</strong><br>Вы предсказали, что женщина не беременна, но на самом деле она беременна.</p><p>Давайте разберемся в матрице ошибок с помощью арифметики.</p><p>Пример. Мы располагаем датасетом пациентов, у которых диагностируют рак. Зная верный диагноз (столбец целевой переменной "Y на самом деле"), хотим усовершенствовать диагностику с помощью модели Машинного обучения. Модель получила тренировочные данные, и на тестовой части, состоящей из 7 записей (в реальных задачах, конечно, больше) и изображенной ниже, мы оцениваем, насколько хорошо прошло обучение.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/confusion-matrix-example-1.png" class="kg-image" alt loading="lazy" width="615" height="416" srcset="__GHOST_URL__/content/images/size/w600/2020/12/confusion-matrix-example-1.png 600w, __GHOST_URL__/content/images/2020/12/confusion-matrix-example-1.png 615w"></figure><p><br>Модель сделала свои предсказания для каждого пациента и записала вероятности от 0 до 1 в столбец "Предсказанный Y". Мы округляем эти числа, приводя их к нулю или единице, с помощью порога, равного 0,6 (ниже этого значения – ноль, пациент здоров). Результаты округления попадают в столбец "Предсказанная вероятность": например, для первой записи модель указала 0,5, что соответствует нулю. В последнем столбце мы анализируем, угадала ли модель. </p><p>Теперь, используя простейшие формулы, мы рассчитаем <a href=" (Recall)">Отзыв (Recall)</a>, точность результата измерений (Precision), <a href="__GHOST_URL__/tochnost-izmierienii/">Точность измерений (Accuracy)</a>, и наконец поймем разницу между этими метриками.</p><h3 id="%D0%BE%D1%82%D0%B7%D1%8B%D0%B2">Отзыв</h3><p>Из всех положительных значений, которые мы предсказали правильно, сколько на самом деле положительных? Подсчитаем, сколько единиц в столбце "Y на самом деле" (4), это и есть сумма TP + FN. Теперь определим с помощью "Предсказанной вероятности", сколько из них диагностировано верно (2), это и будет TP. </p><!--kg-card-begin: markdown--><p>$$Отзыв = \\frac{TP}{TP + FN} = \\frac{2}{2 + 2} = \\frac{1}{2}$$</p>\n<!--kg-card-end: markdown--><h3 id="%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D1%80%D0%B5%D0%B7%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D1%82%D0%B0-%D0%B8%D0%B7%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D0%B9-precision">Точность результата измерений (Precision)</h3><p>В этом уравнении из неизвестных только FP. Ошибочно диагностированных как больных здесь только одна запись.</p><!--kg-card-begin: markdown--><p>$$Точность\\spaceрезультата\\spaceизмерений = \\frac{TP}{TP + FP} = \\frac{2}{2 + 1} = \\frac{2}{3}$$</p>\n<!--kg-card-end: markdown--><h3 id="%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C-%D0%B8%D0%B7%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D0%B9-accuracy">Точность измерений (Accuracy)</h3><p>Последнее значение, которое предстоит экстраполировать из таблицы – TN. Правильно диагностированных моделью здоровых людей здесь 2.</p><!--kg-card-begin: markdown--><p>$$Точность\\spaceизмерений = \\frac{TP + TN}{Всего\\spaceзначений} = \\frac{2 + 2}{7} = \\frac{4}{7}$$</p>\n<!--kg-card-end: markdown--><h3 id="f-%D0%BC%D0%B5%D1%80%D0%B0-%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D1%82%D0%B5%D1%81%D1%82%D0%B0">F-мера точности теста</h3><p>Эти метрики полезны, когда помогают вычислить F-меру – конечный показатель эффективности модели.</p><!--kg-card-begin: markdown--><p>$$F-мера = \\frac{2 * Отзыв * Точность\\spaceизмерений}{Отзыв + Точность\\spaceизмерений} = \\frac{2 * \\frac{1}{2} * \\frac{2}{3}}{\\frac{1}{2} + \\frac{2}{3}} = 0,56$$</p>\n<!--kg-card-end: markdown--><p>Наша скромная модель угадывает лишь 56% процентов диагнозов, и такой результат, как правило, считают промежуточным и работают над улучшением точности модели.</p>		dolia-lozhnykh-polozhitielnykh-klassifikatsii	2022-05-08		
184	Класс (Class)		<p>Класс – метка, характеризующая принадлежность <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> к тому или иному виду характеристики. К примеру, в <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a> пациентов на здоровых и больных раком, классами как раз и будут два вида пациентов – здоровые и больные.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/05/image-4.png" class="kg-image" alt loading="lazy" width="473" height="334"><figcaption>3 кластера наблюдений становятся классами</figcaption></figure>		klass	2022-05-15		
185	Преобразование Фурье (Fourier Transform)		<p>Преобразование Фурье — это математическая функция, которая разлагает сигнал, являющийся функцией времени, на составляющие его частоты. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/05/image-5.png" class="kg-image" alt loading="lazy" width="2000" height="1378" srcset="__GHOST_URL__/content/images/size/w600/2022/05/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2022/05/image-5.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/05/image-5.png 1600w, __GHOST_URL__/content/images/2022/05/image-5.png 2078w" sizes="(min-width: 720px) 720px"><figcaption>"Чистый" сигнал обозначен красным цветом</figcaption></figure><p>Преобразование Фурье также называют обобщением ряда Фурье. Этот термин также может применяться как к представлению в частотной области, так и к используемой математической функции. Преобразование Фурье помогает расширить ряд Фурье на непериодические функции, что позволяет рассматривать любую функцию как сумму простых синусоид.</p><p>Преобразование Фурье активно используется для шумоподавления. Согласно теории, мы можем отфильтровать большую часть данных, содержащих <a href="__GHOST_URL__/shum/">Шум (Noise)</a>, сохранив частоты, содержащие наиболее важную информацию. </p><p>Автор оригинальной статьи: <a href="https://www.techopedia.com/definition/7292/fourier-transform#:~:text=The%20Fourier%20transform%20is%20a%20mathematical%20function%20that%20decomposes%20a,complex%20valued%20function%20of%20frequency.">techopedia.com</a></p>		prieobrazovaniie-furie	2022-05-21		
186	Цепное правило\t(Chain Rule)		<p>Цепное правило (общее правило произведения) – способ вычисления совместного распределения набора случайных величин с использованием только условных вероятностей. Метод используется в качестве основы для Метод обратного распространения ошибки (Backpropagation) и при создании байесовских сетей.</p><p>Эта простая цепочка вероятности и случайных величин выражается как:</p><!--kg-card-begin: markdown--><p>$$Р(А, В) = Р(В | А) Р(А), где$$<br>\n$$P\\space–\\spaceвероятность$$<br>\n$$A,B\\space–\\spaceсобытия$$</p>\n<!--kg-card-end: markdown--><p>Простой пример – вероятность выбора выигрышных лотерейных билетов из разных шляп. Внутри шляпы 1 находится 1 выигрышный и 2 проигрышных билета. Шляпа 2 имеет 1 выигрышный и 3 проигрышных билета.</p><p>Первый случайный выбор шляпы будет событием A. Ваши шансы выбрать шляпу 1: P(A) = P(~A) = 1/2.</p><p>Событие B — это шанс выудить выигрышный билет. В случае с первой шляпой шанс получить выигрышный билет равен P(B | A) = 1/3.</p><p>Событие «А, Б» — это своеобразное пересечение выбора шляпы 1 и выигрышного билета. Использование цепного правила для вероятности показывает, что ваши шансы получить выигрышный билет в Событии А составляют 16,5% (33% x 50%).</p><p>Автор оригинальной статьи: <a href="https://deepai.org/machine-learning-glossary-and-terms/chain-rule#:~:text=What%20is%20a%20Chain%20Rule,and%20in%20creating%20Bayesian%20networks.">deepai.org</a></p>		tsiepnoie-pravilo	2022-05-27		
187	Евклидово расстояние (Euclidean Distance)		<p>Евклидово расстояние — это расстояние между двумя точками. Формула Евклидова расстояния выводится с использованием теоремы Пифагора. Рассчитывается по формуле:</p><!--kg-card-begin: markdown--><p>$$d^2 = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2},\\spaceгде$$<br>\n$$d\\space-\\spaceЕвклидово\\space{расстояние}$$<br>\n$$x,\\space{y}\\space-\\spaceкоординаты\\space{точек}\\space{в}\\space{двумерном}\\space{пространстве}$$</p>\n<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/06/euclidian-distance.jpg" class="kg-image" alt loading="lazy" width="678" height="626" srcset="__GHOST_URL__/content/images/size/w600/2022/06/euclidian-distance.jpg 600w, __GHOST_URL__/content/images/2022/06/euclidian-distance.jpg 678w"><figcaption>d – Евклидово расстояние</figcaption></figure><p>Если вспомнить теорему Пифагора, согласно которой квадрат гипотенузы равен сумме квадратов катетов, то Евклидово расстояние и будет квадратным корнем из суммы квадратов катетов:</p><!--kg-card-begin: markdown--><p>$$AB^2 = AC^2 + BC^2,\\spaceгде$$<br>\n$$AB\\space-\\space{гипотенуза}\\space{или}\\space{Евклидово}\\space{расстояние,}$$<br>\n$$AC,\\space{BC}\\space-\\spaceкатеты$$</p>\n<!--kg-card-end: markdown--><p>Некоторые <a href="__GHOST_URL__/alghoritm/">Алгоритмы (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, такие как <a href="__GHOST_URL__/mietod-k-sriednikh/">Метод k-средних (K-Means)</a> вычисляют расстояние между точками, чтобы в конце концов кластеризовать наблюдения:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/05/k-means-borders-1.png" class="kg-image" alt loading="lazy" width="490" height="375"></figure><p>Пример. Найдем расстояние между точками P(3, 2) и Q(4, 1).</p><!--kg-card-begin: markdown--><p>$$d^2 = \\sqrt{(4 – 3)^2 + (1 – 2)^2} = \\sqrt{2}$$</p>\n<!--kg-card-end: markdown--><p>Автор оригинальной статьи: <a href="https://www.cuemath.com/euclidean-distance-formula/">cuemath.com</a></p>		ievklidovo-rasstoianiie	2022-06-02		
188	Двойник-моделирование (Look-Alike Modeling)		<p>Look-Alike моделирование — один из самых популярных методов увеличения размеров рекламных сегментов для увеличения охвата рекламодателей. Facebook внедрил моделирование Look Alike на своей платформе в 2013 году, и несколько поставщиков рекламных технологий предлагают версию моделирования Look Alike в своих продуктах. Однако, как мы покажем в этом посте, модели Look Alike на практике часто приводят к хрупким и неточным сегментам. Множество других подходов к машинному обучению (ML), включая классификацию и повышение уровня, почти всегда обеспечивают более высокую производительность по сравнению с моделями Look Alike. Из-за этих ограничений мы считаем, что Look Alike следует использовать с осторожностью, только когда другие методы недоступны.</p><p>Что такое похожие сегменты?</p><p>Как следует из названия, цель модели Look Alike — найти аудиторию, которая «выглядит» похожей на известный набор пользователей. Создание базового сегмента Look Alike обычно включает в себя указание двух частей информации:</p><p>Исходный набор: на каком наборе пользователей должна основываться аудитория Look Alike?<br>Размер сегмента: Насколько большой должна быть аудитория? Большая аудитория означает более широкий охват, но меньше общего сходства с исходным набором.<br>Например, рекламодатель может использовать набор из 1000 известных домовладельцев (начальный набор) для создания аудитории из 50 000 похожих друг на друга (размер сегмента), похожих на этих домовладельцев.</p><p>Ограничения похожих моделей</p><p>Моделирование Look Alike приобрело популярность благодаря своей простоте и доступности, но имеет существенные ограничения, которые часто снижают эффективность рекламы.</p><p>С технической точки зрения моделирование Look Alike обычно выполняется с помощью метода полууправляемого машинного обучения, известного как обучение PU. Это означает, что модели Look Alike изучаются с использованием информации о пользователях в начальном наборе (т. е. положительном наборе) без учета пользователей, которые не принадлежат к начальному набору (отрицательные метки).</p><p>Это помогает упростить использование моделей Look Alike — все, что нужно, — это исходный набор положительных меток для построения сегмента. Но это также означает, что модели Look Alike склонны к предвзятости, если вы не внимательно относитесь к тому, как вы определяете свой сегмент. В частности, модель Look Alike будет искать любую черту, общую для пользователей в начальном наборе, даже если эта черта не уникальна для этого набора.</p><p>Для наглядности рассмотрим пример. Допустим, коммерческий банк только что запустил новый продукт для бизнес-кредитов и хотел бы разместить рекламу на нашем сайте для сегмента из 10 000 владельцев малого бизнеса (SBO). Вот данные, которыми мы располагаем:</p><p>На сайте 1 млн активных пользователей<br>Из этого миллиона пользователей 100 тысяч зарегистрировали профиль и указали свою должность при регистрации.<br>Из этих 100 000 пользователей 2 000 указали, что они являются владельцами малого бизнеса (SBO).<br>Наша цель — использовать начальный набор из этих 2 000 SBO для создания модели Look Alike, которая (а) анализирует поведенческие модели начального набора на месте и (б) находит 8 000 других похожих пользователей. Но вот проблема: владение малым бизнесом — не единственная черта, которую разделяют пользователи начального набора; они также все зарегистрированные пользователи. Поскольку зарегистрированные пользователи могут вести себя совсем иначе, чем незарегистрированные, наша модель может учесть это несоответствие и предсказать аудиторию Look Alike, полностью состоящую из зарегистрированных пользователей, независимо от того, являются ли они владельцами малого бизнеса.</p><p>Этот тип ошибки распространен в практических сценариях, связанных с моделированием Look Alike — модель найдет характеристику, которая является общей для начального набора, но не имеет отношения к цели, которую мы хотели бы достичь. Показатели машинного обучения могут показывать высокую эффективность обучения, но кампания будет работать плохо в реальном мире.</p><p>Другие подходы на основе ML могут помочь обойти эти ограничения моделирования Look Alike, что приведет к лучшим результатам кампании. Среди этих подходов — классификация, прогнозирование конверсии и повышение, которые мы рассмотрим в оставшейся части этой статьи.</p><p>Альтернатива № 1: Классификация</p><p>Классификация — это распространенный тип машинного обучения, который стремится разделить две группы данных: положительный набор и отрицательный набор. Применительно к рекламному контексту классификация похожа на Look Alike в том смысле, что ее цель состоит в том, чтобы предсказать, какие пользователи похожи на начальный сегмент. Но, изучая информацию о том, кто входит и не входит в этот сегмент, классификация обычно приводит к лучшим результатам.</p><p>Вернемся к нашему вышеприведенному примеру с рекламой банка для SBO. Моделирование Look Alike было недостаточным, потому что наш начальный набор содержал скрытую черту в виде зарегистрированных пользователей. Эта черта оказалась доминирующим фактором с точки зрения того, что объединяет поведение исходного набора, поэтому гораздо более слабый сигнал, связанный с владением малым бизнесом, был проигнорирован.</p><p>Скажем, вместо этого мы используем модель классификации -</p><p>Положительный набор: зарегистрированные пользователи с должностью, равной «владельцам малого бизнеса».<br>Отрицательный набор: зарегистрированные пользователи с должностью, не равной «владельцам малого бизнеса».<br>Включая информацию о других зарегистрированных пользователях, которые, как мы знаем, не являются SBO, мы заставляем модель различать пользователей на основе</p>		dvoinik-modielirovaniie-look-alike-modeling	2023-09-27		
189	Перцептрон (Perceptron)		<p>Перцептрон (Perceptron) — это <a href="__GHOST_URL__/alghoritm/">Алгоритм (Algorithm)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> для <a href="__GHOST_URL__/kontroliruiemoie-obuchieniie/">Контролируемого обучения (Supervised Learning)</a> различных задач Бинарной классификации (Binary Classification). Кроме того, перцептрон также понимается как блок <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сети (Neural Network)</a>, который помогает обрабатывать входные данные.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/06/image-1.png" class="kg-image" alt loading="lazy" width="712" height="355" srcset="__GHOST_URL__/content/images/size/w600/2022/06/image-1.png 600w, __GHOST_URL__/content/images/2022/06/image-1.png 712w"><figcaption>Входные взвешенные x и выходные данные y перцептрона</figcaption></figure><p>Это первый шаг к изучению технологий машинного обучения и глубокого обучения, который состоит из набора Весов (Weights), входных значений x и выходных значений y. </p><p>В середине 19 века г-н Франк Розенблатт изобрел персептрон для выполнения определенных вычислений для определения возможностей входных данных. Этот алгоритм позволяет нейронам запоминать элементы и обрабатывать их один за другим во время подготовки. </p><p>Модель персептрона также считается одним из лучших и простейших типов искусственных нейронных сетей. Следовательно, мы можем рассматривать ее как однослойную нейронную сеть с четырьмя основными параметрами, то есть входными значениями, весами и смещением, чистой суммой и функцией активации. </p><p>Автор оригинальной статьи: <a href="https://www.javatpoint.com/perceptron-in-machine-learning">javatpoint.com</a></p>		piertsieptron	2022-06-08		
190	T-критерий Стюдента (T-Test)		<p>T-критерий Стюдента (T-Test, T-критерий) — это статистический тест, который используется для сравнения <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Средних арифметических (Mean)</a> двух групп. Он часто используется при проверке <a href="__GHOST_URL__/nulievaia-ghipotieza/">Нулевой гипотезы (Null Hypothesis)</a>, чтобы определить, например, действительно лечение влияет на группу пациентов.</p><p>T-Test можно использовать только при сравнении средних значений двух групп (так называемое попарное сравнение). Если вы хотите сравнить более двух групп или выполнить несколько попарных сравнений, используйте ANOVA или апостериорный тест.</p><p>Тест предполагает, что данные:</p><ul><li>независимы</li><li>распределены (приблизительно) нормально</li><li>имеют одинаковую величину дисперсии в каждой сравниваемой группе (т. н. однородность дисперсии)</li></ul><p>Если ваши данные не соответствуют этим предположениям, вы можете попробовать непараметрическую альтернативу t-критерию, например, Тест знаковых рангов Уилкоксона (Wilcoxon Signed-Rank Test) для данных с неравными <a href="__GHOST_URL__/dispiersiia/">Дисперсиями (Variance)</a>.</p><h3 id="%D0%BA%D0%B0%D0%BA%D0%BE%D0%B9-%D1%82%D0%B8%D0%BF-t-%D1%82%D0%B5%D1%81%D1%82%D0%B0-%D1%81%D0%BB%D0%B5%D0%B4%D1%83%D0%B5%D1%82-%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D1%8C">Какой тип T-теста следует использовать?</h3><p>При выборе типа T-теста вам необходимо учитывать две вещи: относятся ли сравниваемые группы к одной <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population)</a> или к двум разным.</p><p>Одновыборочный, двухвыборочный или парный T-критерий? Если <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a> берут начало из одной генеральной совокупности (например, измерение до и после экспериментального лечения), выполните Парный T-тест (Paired T-Test).</p><p>Если группы происходят из двух разных совокупностей (например, людей из двух разных городов), выполните Независимый T-тест (Independent T-Test).</p><p>Если есть одна группа, сравниваемая со стандартным значением (например, сравнивающая кислотность жидкости с нейтральным pH 7), выполните Одновыборочный T-тест (One-Sample T-Test).</p><h3 id="%D0%BA%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9-%D1%81%D1%82%D1%8C%D1%8E%D0%B4%D0%B5%D0%BD%D1%82%D0%B0-scipy">Критерий Стьюдента: SciPy</h3><p>Посмотрим, как библиотека SciPy рассчитывает значение критерия и что означают эти цифры. Для начала импортируем необходимые библиотеки:</p><!--kg-card-begin: markdown--><p>import numpy as np<br>\nfrom scipy import stats</p>\n<!--kg-card-end: markdown--><p>Возьмем за пример оценки студента за перый и второй семестр:</p><!--kg-card-begin: markdown--><p>semester_1 = (45, 45, 45, 50, 55, 80)<br>\nsemester_2 = (62, 55, 55, 65, 68, 70)</p>\n<p>stats.ttest_ind(semester_1, semester_2)</p>\n<!--kg-card-end: markdown--><p>Мы получаем ничто иное, как разность между средними арифметическими первой и второй выборок, равное ~1.099:</p><!--kg-card-begin: markdown--><p>Ttest_indResult(statistic=1.099305186099593, pvalue=0.30361296704535845)</p>\n<!--kg-card-end: markdown--><p>Теперь, определим <a href="__GHOST_URL__/degrees-of-freedom/">Степени свободы (Degrees of Freedom)</a>: вычтем единицу из размера выборки (6 – 1 = 5). <code>scipy.stats</code> вежливо рассчитал для нас <a href="__GHOST_URL__/p-znachieniie/">P-значение (P-Value)</a> – вероятность появления экстремального <a href="__GHOST_URL__/nabliudieniie/">Наблюдения (Observation)</a> при условии истинности нулевой гипотезы.</p><p>Например, если p-значение составляет около 0,9, т. е. 90%, это указывает на то, что полученное T-значение с вероятностью является случайным наблюдением. С другой стороны, если p-значение составляет около 0,025, т. е. 2,5%, t-значение является значимым. В нашем случае, P-значение составляет около 30%, что означает неверность нулевой гипотезы "студент не спрогрессировал во втором семестре".</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, <a href="https://colab.research.google.com/drive/1ZYs813Qf9tzf8st367m5PZKoxJ9xjDeQ?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.scribbr.com/statistics/t-test/#:~:text=What%20is%20a%20t%2Dtest,means%20is%20different%20from%20zero.">scribbr.com</a></p>		t-kritierii-stiudienta-t-test	2022-06-19		
191	Отбор признаков (Feature Selection)		<p>Отбор признаков — это процесс идентификации и выбора подмножества <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>, наиболее релевантных <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a>.</p><p>Самый простой случай отбора – числовые <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a> и числовая целевая переменная в случае <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a>. Здесь легче рассчитать силу связи между каждой входной переменной и целевой. Мы и  будем использовать синтетический набор данных в качестве основы для регрессионной <a href="__GHOST_URL__/modiel/">Модели (Model)</a>. Задача регрессии — предсказать числовое значение целевого признака. </p><p>Функцию <code>make_regression()</code> библиотеки scikit-learn можно использовать для генерации набора данных регрессии. Он обеспечивает контроль над количеством <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>, признаков и, что важно, количеством релевантных и избыточных входных признаков. </p><p>Для начала импортируем необходимые библиотеки:</p><!--kg-card-begin: markdown--><p>from matplotlib import pyplot<br>\nfrom matplotlib.pyplot import figure</p>\n<p>from sklearn.datasets import make_regression<br>\nfrom sklearn.feature_selection import SelectKBest, chi2, f_regression<br>\nfrom sklearn.model_selection import train_test_split</p>\n<!--kg-card-end: markdown--><p>Мы сгенерируем <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> с 1000 записями, каждая из которых имеет 100 предикторов, из которых 10 являются информативными, а остальные 90 избыточными:</p><!--kg-card-begin: markdown--><p>X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)</p>\n<!--kg-card-end: markdown--><p>Корреляция (Correlation)  — мера взаимосвязи переменных друг с другом. Возможно, наиболее распространенной мерой корреляции является Коэффициент корреляция Пирсона (Pearson Correlation Coefficient), который предполагает <a href="__GHOST_URL__/normalnoie-raspriedielieniie/">Нормальное распределение (Normal Distribution)</a> для каждой переменной и сообщает об их линейной зависимости.</p><p>Показатели линейной корреляции обычно представляют собой значение от -1 до 1, где 0 означает отсутствие связи. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/06/image-2.png" class="kg-image" alt loading="lazy" width="288" height="164"></figure><p>Библиотека scikit-learn обеспечивает реализацию корреляционной статистики в функции <code>f_regression()</code>. Эту функцию можно использовать в стратегии выбора признаков, например, при выборе k самых важных признаков (наибольшего значения) с помощью класса <code>SelectKBest</code>.</p><p>Например, мы можем определить класс <code>SelectKBest</code> для использования функции <code>f_regression()</code> и выбрать все функции, а затем преобразовать обучающие и тестовые наборы. Установим параметр k, равное 10, чтобы SelectKBest отобрал десять самых важных признаков:</p><!--kg-card-begin: markdown--><p>fs = SelectKBest(score_func=f_regression, k=10)<br>\nfs.fit(X_train, y_train)<br>\nX_train_fs = fs.transform(X_train)<br>\nX_test_fs = fs.transform(X_test)</p>\n<!--kg-card-end: markdown--><p>Отобразим степень важности каждого признака:</p><!--kg-card-begin: markdown--><p>for i in range(len(fs.scores_)):<br>\nprint('Признак %d: %f' % (i, fs.scores_[i]))</p>\n<p>pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)</p>\n<!--kg-card-end: markdown--><p>Мы не будем перечислять баллы для всех 100 входных переменных, так как это займет слишком много места:</p><!--kg-card-begin: markdown--><p>Признак 0: 0.009419<br>\nПризнак 1: 1.018881<br>\nПризнак 2: 1.205187<br>\nПризнак 3: 0.000138<br>\nПризнак 4: 0.167511<br>\nПризнак 5: 5.985083<br>\nПризнак 6: 0.062405<br>\nПризнак 7: 1.455257<br>\nПризнак 8: 0.420384<br>\nПризнак 9: 101.392225<br>\nПризнак 10: 0.387091<br>\n...</p>\n<!--kg-card-end: markdown--><p>Тем не менее, мы можем видеть, что некоторые переменные имеют более высокие баллы, чем другие.</p><p>Мы также построили гистограмму коэффициентов важности:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/06/feature-selection.jpg" class="kg-image" alt loading="lazy" width="1245" height="478" srcset="__GHOST_URL__/content/images/size/w600/2022/06/feature-selection.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/06/feature-selection.jpg 1000w, __GHOST_URL__/content/images/2022/06/feature-selection.jpg 1245w"></figure><p>Сократив число столбцов <a href="__GHOST_URL__/datafrieim/">Датафрейма (Dataframe)</a> до 10 самых важных признаков, мы и получим оптимизированный датасет для построения модели.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/13V5azHvyp9ugltlmVOH8I_Hc_mLBHB3A?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/what-is-deep-learning/">Jason Brownlee</a></p>		otbor-priznakov	2022-06-22		
192	Анализ тональности текста (Sentiment Analysis)		<p>Анализ тональности текста (анализ настроений, анализ мнений) — это классическая задача <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, позволяющая идентифицировать, извлекать и количественно оценивать текстовые данные для облегчения <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a> и работы с ними. К примеру, если речь идет о Текстовом блоке (Corpus) комментариев к продуктам, Sentiment Analysis способен, среди прочих, с помощью определения эмоциональной окраски:</p><ul><li>Определить основные причины популярности тех или иных наименований </li><li>Определить неучтенные потребности пользователей, аспекты или особенности товара, вызвавшие отрицательные эмоции</li><li>Быстро найти жалобы среди большого потока комментариев</li><li>Сгенерировать комментарии для непродающихся публикаций</li><li>Быстро найти спам</li><li>Сгруппировать похожие товары</li><li>Определить вызывающие проблемы процессы, например, взаимодействие со службой техподдержки</li><li>Лечь в основу рекомендательной системы и т.д.</li></ul><p>Это измеримый способ понять и проанализировать общественное восприятие различных идей и концепций или недавно запущенного продукта.</p><p>Мы используем различные инструменты <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a> и анализа текста, чтобы выяснить, что может быть субъективной информацией. Нам нужно идентифицировать, извлечь и количественно оценить такие детали из текста для облегчения классификации и работы с данными.</p><p>Понимание эмоций и анализ настроений играют огромную роль в системах рекомендаций на основе совместной фильтрации. Группировка людей, имеющих сходную реакцию на определенный продукт, и показ им похожих продуктов. Например, рекомендовать фильмы людям, группируя их с другими людьми, имеющими схожие взгляды на определенное шоу или фильм.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D1%82%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B8">Как работает анализ тональности?</h3><p>Обработка естественного языка — это основная концепция, на которой строится анализ настроений. Это ветвь <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a>, работающая с текстами, дающая машинам возможность "понимать" предложения и использовать эту способность. </p><p>В зависимости от объема текстового блока Sentiment Analysis можно разделить на анализ на уровне документа, предложения и фразы.</p><p>Выделяют два основных метода анализа мнений:</p><ul><li><strong>Анализ тональности на основе словарей эмоций</strong>: существует предопределенный список слов для каждого настроения; текст или документ сопоставляют с такими списками. Затем алгоритм определяет, какой тип слов или какая <a href="__GHOST_URL__/poliarnost/">Полярность (Polarity)</a> преобладает в нем. Этот тип анализа настроений на основе правил прост в реализации, но ему не хватает гибкости и он не учитывает контекст.</li><li><strong>Автоматический анализ тональности</strong> в основном основан на контролируемых алгоритмах Машинного обучения и на самом деле очень полезен для понимания сложных текстов. Алгоритмы в этой категории включают <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a>, <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейную регрессию (Linear Regression)</a>, <a href="__GHOST_URL__/riekurrientnaia-nieirosiet/">Рекуррентную нейросеть (RNN)</a> и ее подвиды. В этой статье мы будем использовать рекуррентную нейронную сеть.</li></ul><h3 id="%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0-%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%B8%D0%B9">Система анализа настроений</h3><p>Рекуррентные нейронные сети – это еще и способ превратить текстовые данные в последовательные, то есть такие, что каждая последующая запись зависит от предыдущей. Рекуррентные нейронные сети имеют собственную память и запоминают входные данные, которые были переданы каждому узлу. В Нейронных сетях прямого распространения (Feed Forward Neural Network) информация – входные данные, перемещается вперед и никогда не перемещаются назад ни в каких узлах. Поскольку у таких сетей нет памяти, они не запоминают предыдущие вводы:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/07/image.png" class="kg-image" alt loading="lazy" width="864" height="502" srcset="__GHOST_URL__/content/images/size/w600/2022/07/image.png 600w, __GHOST_URL__/content/images/2022/07/image.png 864w" sizes="(min-width: 720px) 720px"></figure><p>RNN же учитывает, какие вводные данные она получила, также как и предыдущие итерации ввода. Рекуррентная нейронная сеть создает копию выходных данных и зацикливает ее отправку обратно.</p><p>Например, когда предложение передается через нейросеть прямого распределения, она принимает слово за словом, пока не достигнет последнего. Она не помнит, что выдавали до этого. Но РНН также знает предыдущие входные данные и, таким образом, может предсказать, что может произойти дальше. Идеально для последовательных данных!</p><p>Рекуррентные нейронные сети не новы, они были впервые представлены в 1980-х годах, но стали очень популярными с ростом <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокого обучения (Deep Learning)</a> и его использования в последовательных данных. Тем не менее, у Рекуррентных нейросетей есть свой собственный набор проблем, основной из которых является Проблема исчезающего градиента (Vanishing Gradient Problem). </p><p>Ответом на этот вопрос является Долгая краткосрочная память (LSTM). Это особый тип модели RNN, который может изучать долгосрочные зависимости. Она сделана, чтобы "помнить" долгосрочные данные. Что делает LSTM особенной, так это дополнительная ячейка памяти, которая является повторяющимся состоянием, и каждая из них имеет несколько Гейтов (Gate), которые контролируют поток информации в ячейку памяти и из нее:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/07/image-1.png" class="kg-image" alt loading="lazy" width="960" height="661" srcset="__GHOST_URL__/content/images/size/w600/2022/07/image-1.png 600w, __GHOST_URL__/content/images/2022/07/image-1.png 960w" sizes="(min-width: 720px) 720px"><figcaption>Входной, выходной гейты и гейт "забывания"</figcaption></figure><p>Входные ворота (Input Gate) используются для обновления состояния ячейки, Гейт забывания (Forget Gate) решает, какую информацию следует сохранить, а какую следует отбросить. Выходной гейт (Output Gate) определяет значения для следующих скрытых гейтов.</p><h3 id="%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%B8%D0%B9-pytorch">Анализ настроений: PyTorch</h3><p>Теперь, когда у нас есть общее представление о концепции, попробуем реализовать такую модель с помощью PyTorch. Мы создадим простой классификатор тональности, который будет классифицировать, были ли отзывы пользователей о фильме на IMDB положительными, отрицательными или нейтральными. </p><p><a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>, который мы будем использовать, представляет собой 50 000 отзывов на фильмы. Это набор данных для Двоичной классификации (Binary Classification), в котором каждый обзор классифицируется как положительный или отрицательный. </p><p>Установим библиотеку <code>torchtext</code>, из которой и возьмем датасет:</p><pre><code class="language-python">!pip install torchtext==0.10.0</code></pre><p>Импортируем необходимые библиотеки:</p><pre><code class="language-python">import random\nimport spacy\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchtext\nfrom torchtext.legacy import data\nfrom torchtext.legacy import datasets</code></pre><p>Мы собираемся использовать метод <code>data.field</code>, чтобы решить, как данные должны быть предварительно обработаны. Параметры, которые мы туда передаем, будут определять предварительную обработку:</p><pre><code class="language-python">seed = 42\n\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntxt = data.Field(tokenize = 'spacy',\n                  tokenizer_language = 'en_core_web_sm',\n                  include_lengths = True)\n\nlabels = data.LabelField(dtype = torch.float)</code></pre><p>Первый параметр, наш <code>tokenizer_language</code> (который определяет, как предложения будут разбиты на токены) — это модуль <code>en_core_web_sm</code> для английского языка библиотеки spacy. По умолчанию он просто разделит текст с помощью пробелов.</p><p>Мы также устанавливаем случайное начальное число <code>seed</code>, которое может принимать любое значение, но мы упоминаем его только для целей воспроизводимости (чтобы и при верстке статьи, и при самостоятельном запуске кода читателем результаты были идентичны). Вы можете изменить его или даже опустить без какого-либо существенного эффекта. </p><p>Мы также будем использовать <code>cuda</code>, а также доступный нам Центральный процессор (CPU).</p><p>Загрузим набор данных и разделим его на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>. Набор уже разделен.</p><pre><code class="language-python">train_data, test_data = datasets.IMDB.splits(txt, labels)</code></pre><p>Далее мы выделим из тренировочных данных еще и <a href="__GHOST_URL__/validatsionnyye-dannyye/">Валидационные данные (Validation Data)</a>:</p><pre><code class="language-python">train_data, valid_data = train_data.split(random_state = random.seed(seed))</code></pre><p>Мы дополнительно ограничим количество слов, которые освоит модель, до 25000:</p><pre><code class="language-python">num_words = 25_000\n\ntxt.build_vocab(train_data, \n                 max_size = num_words, \n                 vectors = "glove.6B.100d", \n                 unk_init = torch.Tensor.normal_)\n\nlabels.build_vocab(train_data)</code></pre><p>Это позволит выбрать наиболее часто используемые 25000 слов из набора данных и использовать их для обучения. Так мы значительно сократим работу модели без реальной потери точности.</p><p>Воспользуемся преимуществами <a href="__GHOST_URL__/pakiet/">Пакетов (Batch)</a>, то есть разделим данные на части по 64 записи. Если этого не сделать, памяти не хватит:</p><pre><code class="language-python">btch_size = 64\n\ntrain_itr, valid_itr, test_itr = data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = btch_size,\n    sort_within_batch = True,\n    device = device)</code></pre><p>Теперь подготовим модель и определим ее архитектуру. Мы используем многослойный двунаправленный LSTM RNN для нашей задачи. Это означает, что будет несколько слоев нейросети, наложенных друг на друга.</p><p>Двунаправленная RNN имеют то преимущество, что охватывает больше контекста, чем однонаправленная сеть. Например, если модель должна угадать следующее слово в предложении, она сделает это на основе предыдущих знаний. Но в двунаправленной сети он также будет знать, что будет дальше, благодаря двум сетям, текущим как бы в противоположных направлениях и наложенных друг на друга. Предложение «я люблю машинное обучение» будет в первой сети выглядеть как "я", "люблю", "машинное", "обучение" а во второй — как "обучение", "машинное", "люблю", "я". Это обеспечивает более полный контекст, хоть и воспринимается как некое мошенничество.</p><p>Мы, проще говоря, разместим слова так, чтобы похожие слова были сгруппированы вместе:</p><pre><code class="language-python">class RNN(nn.Module):\n    def __init__(self, word_limit, dimension_embedding, dimension_hidden, dimension_output, num_layers, \n                 bidirectional, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(word_limit, dimension_embedding, padding_idx=pad_idx)\n        \n        self.rnn = nn.LSTM(dimension_embedding, \n                           dimension_hidden, \n                           num_layers=num_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        \n        self.fc = nn.Linear(dimension_hidden * 2, dimension_output)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n\n    def forward(self, text, len_txt):\n        \n        embedded = self.dropout(self.embedding(text))\n               \n\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, len_txt.to('cpu'))\n        \n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        \n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n\n        \n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n                            \n        return self.fc(hidden)</code></pre><p>Мы определяем параметры для модели и передаем их экземпляру класса <code>RNN</code>, который мы только что определили. Определяется количество входных параметров, скрытого слоя и выходного измерения, а также пропускной способности и логического значения двунаправленности. Мы также передаем индекс маркера площадки из словаря, который мы создали ранее.</p><p>Теперь мы зададим явно некоторые настройки нашей модели:</p><pre><code class="language-python">dimension_input = len(txt.vocab)\ndimension_embedding = 100\ndimension_hddn = 256\ndimension_out = 1\nlayers = 2\nbidirectional = True\ndroupout = 0.5\nidx_pad = txt.vocab.stoi[txt.pad_token]\n\nmodel = RNN(dimension_input, \n            dimension_embedding, \n            dimension_hddn, \n            dimension_out, \n            layers, \n            bidirectional, \n            droupout, \n            idx_pad)</code></pre><p>Затем мы получаем предварительно обученные веса вложений и копируем их в нашу модель, чтобы ей не нужно было изучать вложения, и мы могли напрямую сосредоточиться на текущей работе по изучению настроений, связанных с этими вложениями:</p><pre><code class="language-python">def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'Модель обладает {count_parameters(model):,} тренируемыми параметрами')</code></pre><pre><code class="language-python">&gt;&gt;&gt; Модель обладает 4,810,857 тренируемыми параметрами</code></pre><p>Предварительно обученные встраивающие веса размещаются вместо исходных:</p><pre><code class="language-python">pretrained_embeddings = txt.vocab.vectors\n\nprint(pretrained_embeddings.shape)</code></pre><pre><code class="language-python">&gt;&gt;&gt; torch.Size([25002, 100])</code></pre><pre><code class="language-python">model.embedding.weight.data.copy_(pretrained_embeddings)</code></pre><pre><code class="language-python">&gt;&gt;&gt; tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n        ...,\n        [ 0.2735, -0.1130,  0.2871,  ..., -0.8155, -0.0639,  0.9330],\n        [-1.1777, -0.1115, -0.1409,  ...,  0.8815,  0.1093,  1.1222],\n        [-0.8087,  0.4473,  0.0443,  ..., -1.2134,  0.4822,  0.0481]])</code></pre><pre><code class="language-python">unique_id = txt.vocab.stoi[txt.unk_token]\n\nmodel.embedding.weight.data[unique_id] = torch.zeros(dimension_embedding)\nmodel.embedding.weight.data[idx_pad] = torch.zeros(dimension_embedding)\n\nprint(model.embedding.weight.data)</code></pre><pre><code class="language-python">&gt;&gt;&gt; tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n        ...,\n        [ 0.2735, -0.1130,  0.2871,  ..., -0.8155, -0.0639,  0.9330],\n        [-1.1777, -0.1115, -0.1409,  ...,  0.8815,  0.1093,  1.1222],\n        [-0.8087,  0.4473,  0.0443,  ..., -1.2134,  0.4822,  0.0481]])</code></pre><p>Теперь мы определяем некоторые параметры модели, то есть <a href="__GHOST_URL__/optimizator/">Оптимизатор (Optimizer)</a>, который мы собираемся использовать, и критерий <a href="__GHOST_URL__/funktsiia-potieri/">Функции потери (Loss Function)</a>, который нам подходит. Мы выбрали Адаптивную оценку момента (Adam) для быстрой <a href="__GHOST_URL__/skhodimost-convergence/">Сходимости (Convergence)</a> модели, то есть сокращения ошибок. Размещаем модель и критерий на Графическом процессоре (GPU):</p><pre><code class="language-python">optimizer = optim.Adam(model.parameters())</code></pre><p>Теперь мы приступаем к необходимым функциям для обучения и оценки модели анализа настроений.</p><pre><code class="language-python">criterion = nn.BCEWithLogitsLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)</code></pre><pre><code class="language-python">def bin_acc(preds, y):\n   \n    predictions = torch.round(torch.sigmoid(preds))\n    correct = (predictions == y).float() \n    acc = correct.sum() / len(correct)\n    return acc</code></pre><p><code>bin_acc</code> — это функция двоичной точности, которую мы будем использовать для получения точности модели каждый раз.</p><pre><code class="language-python">def train(model, itr, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for i in itr:\n        \n        optimizer.zero_grad()\n        \n        text, len_txt = i.text\n        \n        predictions = model(text, len_txt).squeeze(1)\n        \n        loss = criterion(predictions, i.label)\n        \n        acc = bin_acc(predictions, i.label)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(itr), epoch_acc / len(itr)</code></pre><p>Определим функцию <code>train</code> для обучения и оценки моделей. Мы начинаем с циклического перебора количества <a href="__GHOST_URL__/epokha/">Эпох (Epoch)</a>. Число итераций в каждой эпохе зависит от размера пакета, который мы задали равным 64. Мы передаем текст в модель, получаем от нее прогнозы, вычисляем потери для каждой итерации, а затем используем Метод обратного распространения ошибки (Backpropagation).</p><p>Единственное существенное изменение в функции оценки по сравнению с функцией обучения заключается в том, что мы не распространяем потери в обратном направлении по модели и используем torch.nograd, что в основном означает отсутствие Градиентного спуска (Gradient Descent) при оценке.</p><pre><code class="language-python">def evaluate(model, itr, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for i in itr:\n\n            text, len_txt = i.text\n            \n            predictions = model(text, len_txt).squeeze(1)\n            \n            loss = criterion(predictions, i.label)\n            \n            acc = bin_acc(predictions, i.label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(itr), epoch_acc / len(itr)</code></pre><pre><code class="language-python">def epoch_time(start_time, end_time):\n    used_time = end_time - start_time\n    used_mins = int(used_time / 60)\n    used_secs = int(used_time - (used_mins * 60))\n    return used_mins, used_secs</code></pre><p>Мы создаем вспомогательную функцию <code>epoch_time</code> для расчета времени, которое требуется каждой эпохе для завершения своего запуска и печати. </p><pre><code class="language-python">num_epochs = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(num_epochs):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_itr, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_itr, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss &lt; best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut2-model.pt')\n    \n    print(f'Эпоха: {epoch+1:02} | Время на эпоху: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tТренировочные потери: {train_loss:.3f} | Тренировочная точность: {train_acc*100:.2f}%')\n    print(f'\\t Валидационные потери: {valid_loss:.3f} |  Валидационная точность: {valid_acc*100:.2f}%')</code></pre><p>Мы устанавливаем количество эпох равным 5, а затем начинаем наше обучение. Отобразим потери при обучении и проверке на каждом этапе, если нам нужно понять или построить кривую обучения на более позднем этапе. Мы сохраняем ту модель, которая имеет наименьшие потери при Валидации (Validation).</p><pre><code class="language-python">&gt;&gt;&gt;\nЭпоха: 01 | Время на эпоху: 0m 35s\n\tТренировочные потери: 0.649 | Тренировочная точность: 61.34%\n\t Валидационные потери: 0.583 |  Валидационная точность: 72.39%\nЭпоха: 02 | Время на эпоху: 0m 36s\n\tТренировочные потери: 0.561 | Тренировочная точность: 71.81%\n\t Валидационные потери: 0.460 |  Валидационная точность: 79.26%\nЭпоха: 03 | Время на эпоху: 0m 38s\n\tТренировочные потери: 0.549 | Тренировочная точность: 71.97%\n\t Валидационные потери: 0.358 |  Валидационная точность: 84.73%\nЭпоха: 04 | Время на эпоху: 0m 38s\n\tТренировочные потери: 0.432 | Тренировочная точность: 80.96%\n\t Валидационные потери: 0.353 |  Валидационная точность: 85.52%\nЭпоха: 05 | Время на эпоху: 0m 38s\n\tТренировочные потери: 0.313 | Тренировочная точность: 87.03%\n\t Валидационные потери: 0.299 |  Валидационная точность: 87.52%</code></pre><p>Загружаем сохраненную контрольную точку модели и тестируем ее на созданном ранее тестовом наборе:</p><pre><code class="language-python">model.load_state_dict(torch.load('tut2-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_itr, criterion)\n\nprint(f'Тестовые потери: {test_loss:.3f} | Тестовая точность: {test_acc*100:.2f}%')</code></pre><p>Во время пробного запуска модели анализа настроений Python мы достигли приличной оценки точности 87.42%:</p><pre><code class="language-python">&gt;&gt;&gt; Тестовые потери: 0.301 | Тестовая точность: 87.42%</code></pre><p>Мы также можем проверить модель на наших данных. Он обучен классифицировать обзоры фильмов на положительные, отрицательные и нейтральные, поэтому мы будем передавать ему соответствующие данные для проверки:</p><pre><code class="language-python">nlp = spacy.load('en_core_web_sm')\n\ndef pred(model, sentence):\n    model.eval()\n    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n    indexed = [txt.vocab.stoi[t] for t in tokenized]\n    length = [len(indexed)]\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(1)\n    length_tensor = torch.LongTensor(length)\n    prediction = torch.sigmoid(model(tensor, length_tensor))\n    return prediction.item()</code></pre><p>Мы загружаем англоязычный модуль spacy для токенизации данных, которые нам нужно передать модели. Вначале мы использовали встроенный <code>torch.text</code>, но здесь мы не используем пакеты, и предварительная обработка, которую нам нужно сделать, может быть выполнена библиотекой <code>spacy</code>. Для этого мы определяем функцию <code>pred</code> прогнозирования тональности. После предварительной обработки мы конвертируем данные в <a href="__GHOST_URL__/tenzor/">Тензоры (Tensor)</a> и готовим их к передаче в модель.</p><p>Мы определяем еще одну вспомогательную функцию, которая будет печатать тональность комментария на основе оценки, предоставленной моделью:</p><pre><code class="language-python">sent = ["Положительный", "Нейтральный", "Отрицательный"]\ndef print_sent(x):\n  if (x &lt; 0.3): print(sent[0])\n  elif (x &gt; 0.3 and x &lt; 0.7): print(sent[1])\n  else: print(sent[2])</code></pre><p>Теперь мы просто передаем любые данные и проверяем, что об этом думает модель:</p><pre><code class="language-python">print_sent(pred(model, "This film was average"))\nprint_sent(pred(model, "This film is horrible"))\nprint_sent(pred(model, "This film was great"))\nprint_sent(pred(model, "This was the best movie i have seen in a while. The cast was great and the script was awesome, and the direction just blew my mind"))</code></pre><pre><code class="language-python">&gt;&gt;&gt;\nНейтральный\nОтрицательный\nПоложительный\nПоложительный</code></pre><p>Мы успешно разработали модель анализа тональности Python, основанную на LSTM, которая является довольно надежной и очень точной. </p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1Yqe9eIcVlTOUq_jK1-q7wUG8fn_khGbv?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://data-flair.training/blogs/python-sentiment-analysis/">data-flair.training</a></p>		analiz-tonalnosti-tieksta	2022-07-02		
193	Полярность (Polarity)		<p>Полярность – характеристика текстового <a href="__GHOST_URL__/priznak/">Признака (Feature)</a> <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a>, описывающая эмоциональную окраску (отрицательная или положительная). К примеру, мы располагаем датасетом комментариев YouTube и хотим оценить, понравилось ли в целом пользователям видео или нет. Для этого каждому комментарию присваивается оценка в диапазоне [-1, 1], где '-1' соответствует отрицательной окраске текста, и в конечном итоге рассчитывается среднее значение полярности – некая итоговая оценка зрителей.</p><h3 id="%D0%BF%D0%BE%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C-textblob">Полярность: textblob</h3><p>Посмотрим, как работает автоматическая разметка текста в textblob. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">!pip install textblob</code></pre><pre><code class="language-python">import pandas as pd\nfrom textblob import TextBlob</code></pre><p>Теперь мы можем оценивать эмоциональную окраску любого комментария с помощью <code>sentiment.polarity</code>:</p><pre><code class="language-python">print(TextBlob('It\\'s more accurate to call it the M+ (1000) be..').sentiment.polarity)\nprint(TextBlob('Power is the disease.  Care is the cure.  Keep..').sentiment.polarity)</code></pre><p>Первый комментарий система оценила как в целом положительный, второй – как нейтральный:</p><pre><code class="language-python">&gt;&gt;&gt;\n0.45000000000000007\n0.0</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1OP5U_vdgb-DWMQAefHWCpjPei2EE0eH7?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.kaggle.com/code/breyannebroughton/youtube-comments-sentiment-analysis">Breyanne Broughton</a></p>		poliarnost	2022-07-06		
194	Cезонность (Seasonality)		<p>Сезонность — это характеристика Временного ряда (Time Series), в которой данные претерпевают регулярные и предсказуемые изменения, повторяющиеся каждый календарный год. Любые предсказуемые колебания или закономерности, которые повторяются или повторяются в течение одного года, называются сезонными.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/07/image-2.png" class="kg-image" alt loading="lazy" width="1200" height="600" srcset="__GHOST_URL__/content/images/size/w600/2022/07/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/image-2.png 1000w, __GHOST_URL__/content/images/2022/07/image-2.png 1200w" sizes="(min-width: 720px) 720px"><figcaption>Сезонность розничной торговли США, млрд. долл.</figcaption></figure><p>Сезонные эффекты отличаются от циклических эффектов, поскольку сезонные циклы наблюдаются в течение одного календарного года, в то время как циклические эффекты, такие как увеличение продаж из-за низкого уровня безработицы, могут охватывать периоды времени короче или длиннее одного календарного года.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B-%D1%81%D0%B5%D0%B7%D0%BE%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8">Примеры сезонности</h3><p>Существует множество различных случаев, когда можно наблюдать сезонность, связанную с регулярным переходом от одного времени года к другому.</p><p>Например, розничные продажи. Розничные продажи измеряют потребительские расходы и спрос и ежемесячно публикуются в отчетах Бюро переписи населения США. Данные колеблются в определенное время года, в основном во время сезона праздничных покупок. Этот период приходится на четвертый квартал года — с октября по декабрь. Многие ритейлеры сталкиваются с сезонными розничными продажами, наблюдая значительный скачок потребительских расходов в праздничный сезон.</p><p>Если вы живете в климате с холодной зимой и теплым летом, ваши расходы на отопление, вероятно, возрастут зимой и упадут летом. Вы ожидаете, что сезонность ваших расходов на отопление будет повторяться каждый год примерно в одно и то же время.</p><p>Еще одна область, на которую влияет сезонность, — это продажи солнцезащитных средств. Компания, которая продает средства для загара, отмечает, что летом продажи подскочат, поскольку спрос на их продукцию увеличивается. С другой стороны, компания, скорее всего, увидит значительное падение зимой.</p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/s/seasonality.asp#:~:text=Seasonality%20refers%20to%20predictable%20changes,analyze%20stocks%20and%20economic%20trends.">Will Kenton</a></p>		ciezonnost	2022-07-14		
195	Когорта (Cohort)		<p>Когорта — это группа людей, которые имеют некоторые общие демографические или статистические характеристики.</p><p>Этот термин обычно используется для обозначения людей, родившихся в определенное время, хотя группа может быть основана практически на любой общности, такой как профессия, уровень заработной платы, рост или географическое положение.</p><p>Например, в IT-среде считается, что люди из поколения Y (поколение миллениалов) и поколения Z по своей сути чувствуют себя комфортно с цифровыми технологиями. Их иногда называют "цифровыми с рождения", потому что они выросли в окружении персональных компьютеров и мобильных вычислительных устройств, таких как смартфоны и планшеты, и не знали другого мира. В таких средах часто предполагается, что поколение X, которое предшествовало этим двум, разбирается в технологиях на более глубоком уровне, потому что они выросли во время разработки и популяризации персональных компьютеров, когда приложения были менее автоматизированы.</p><p>Разделение населения на когорты помогает улучшить анализ данных во многих областях, включая статистику, бизнес-аналитику, маркетинг, управление взаимоотношениями с клиентами и управление персоналом. Для некоторых типов исследований предполагается, что когорты имеют больше общего, чем определяющая характеристика.</p><p>Слово "когорта" происходит от латинского слова cohors, где оно означало ограждение или замкнутую группу: Co (от com, что означает с) и hors (от hortus, что означает сад). В древние времена когорта была одним из десяти подразделений римского легиона. </p><p>Автор оригинальной статьи: <a href="https://www.techtarget.com/whatis/definition/cohort">Ivy Wigmore</a></p>		koghorta	2022-07-21		
196	Docker		<p>Docker – программное обеспечение, предоставляющее так называемые Контейнеры (Container) — это стандартные единицы программного обеспечения, которые упаковывают код и все его библиотеки-инструменты, чтобы приложение быстро и надежно запускалось из одной вычислительной среды в другую. Это автономный исполняемый пакет программного обеспечения, который включает в себя все необходимое для запуска приложения: код, среду выполнения, системные инструменты, системные библиотеки и настройки.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/08/--------------2022-08-07---23.13.56.png" class="kg-image" alt loading="lazy" width="976" height="784" srcset="__GHOST_URL__/content/images/size/w600/2022/08/--------------2022-08-07---23.13.56.png 600w, __GHOST_URL__/content/images/2022/08/--------------2022-08-07---23.13.56.png 976w" sizes="(min-width: 720px) 720px"></figure><p>Контейнерное программное обеспечение, доступное как для приложений на базе Linux, так и для Windows, всегда будет работать одинаково, независимо от инфраструктуры. Контейнеры изолируют программное обеспечение от его среды и обеспечивают его единую работу, несмотря на различия.</p><p>Автор оригинальной статьи: <a href="https://www.docker.com/resources/what-container/">docker.com</a></p>		docker	2022-08-07		
197	Теорема Байеса (Bayes Theorem)		<p>Теорема Байеса – математическая формула для определения условной вероятности. Теорема утверждает, что условная вероятность события, основанная на появлении другого события, равна вероятности второго события при первом событии, умноженной на вероятность первого события. Сейчас поясню.</p><!--kg-card-begin: markdown--><p>$$P(A|B) = \\frac{P(A⋂B)}{P(B)} = \\frac{P(A) x P(B|A)}{P(B)}, где$$<br>\n$$P(A|B)\\space{–}\\space{вероятность}\\space{гипотезы}\\space{A}\\space{при}\\space{наступлении}\\space{события}\\space{B,}$$<br>\n$$P(A)\\space{–}\\space{вероятность}\\space{гипотезы}\\space{A,}$$<br>\n$$P(B)\\space{–}\\space{вероятность}\\space{гипотезы}\\space{B,}$$<br>\n$$P(B|A)\\space{–}\\space{вероятность}\\space{гипотезы}\\space{B}\\space{при}\\space{наступлении}\\space{события}\\space{A,}$$<br>\n$$P(A⋂B)\\space{–}\\space{вероятность}\\space{A}\\space{и}\\space{B}\\space{одновременно}$$</p>\n<!--kg-card-end: markdown--><p>Пример. Представьте, что есть тест на наркотики, эффективность которого составляет 98%. Это означает, что в 98% случаев он показывает действительно положительный результат для человека, употребляющего вещества, и в 98% случаев он показывает истинно отрицательный результат для тех, кто "чист".</p><p>Далее предположим, что 0,5% людей употребляют наркотики. Если случайно выбранный человек получил положительный результат на наркотик, можно сделать следующий расчет, чтобы определить вероятность того, что человек действительно употребляет наркотик.</p><!--kg-card-begin: markdown--><p>(0,98 х 0,005) / [(0,98 х 0,005) + ((1 - 0,98) х (1 - 0,005))] = 0,0049 / (0,0049 + 0,0199) = 19,76%</p>\n<!--kg-card-end: markdown--><p>Теорема Байеса показывает, что даже если человек дал положительный результат в этом сценарии, вероятность того, что человек не принимает наркотики, составляет 80,24%.</p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/b/bayes-theorem.asp#:~:text=and%20disease%20control.-,What%20Does%20Bayes'%20Theorem%20State%3F,probability%20of%20the%20first%20event.">Adam Hayes</a></p>		tieoriema-baiiesa	2022-08-09		
198	Визуализация данных (Dataviz)		<p>Визуализация данных — это графическое представление информации. Используя диаграммы, графики и карты – инструменты визуализации данных предоставляют способ увидеть и понять тенденции, <a href="__GHOST_URL__/vybros/">Выбросы (Outlier)</a> и закономерности в данных.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/08/_data_visualization_definition.gif" class="kg-image" alt loading="lazy" width="900" height="600" srcset="__GHOST_URL__/content/images/size/w600/2022/08/_data_visualization_definition.gif 600w, __GHOST_URL__/content/images/2022/08/_data_visualization_definition.gif 900w"></figure><p>В мире <a href="__GHOST_URL__/bolshiie-dannyie/">Больших данных (Big Data)</a> технологии визуализации необходимы для анализа огромных объемов информации и принятия решений.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%B8%D0%BC%D1%83%D1%89%D0%B5%D1%81%D1%82%D0%B2%D0%B0-%D1%85%D0%BE%D1%80%D0%BE%D1%88%D0%B5%D0%B9-%D0%B2%D0%B8%D0%B7%D1%83%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Преимущества хорошей визуализации данных</h3><p>Наши глаза тянутся к цветам и узорам. Мы можем быстро отличить красный от синего, квадрат от круга. Наша культура визуальна, включая все, от искусства и рекламы до телевидения и фильмов. Визуализация данных — это еще одна форма изобразительного искусства, которая вызывает у нас интерес и удерживает наше внимание. Когда мы видим диаграмму, то быстро обнаруживаем тенденции и выбросы. Если мы можем что-то увидеть, то быстрее усваиваем это. Если вы когда-либо смотрели на огромную таблицу и не могли увидеть тенденцию, пока не визуализировали ее, то вы знаете, насколько действенной может быть визуализация.</p><p>По мере того, как «эпоха больших данных» набирает обороты, визуализация становится все более важным инструментом для осмысления триллионов строк данных, генерируемых каждый день. Визуализация данных помогает рассказывать истории, преобразовывая данные в более удобную для понимания форму, выделяя тенденции и отклонения. Хорошая визуализация рассказывает историю, удаляя шум из данных и выделяя полезную информацию. </p><p>Почему визуализация данных важна для любой карьеры</p><p>Трудно представить себе профессиональную отрасль, которая не выигрывает от того, что делает данные более понятными. Каждая область выигрывает от понимания данных: управление, финансы, маркетинг, история, потребительские товары, сфера услуг, образование, спорт и так далее. Хотя всегда будут поэтически восхвалять визуализацию данных, существуют практические, реальные приложения, которые неоспоримы. И, поскольку визуализация настолько распространена, это также один из самых полезных профессиональных навыков для развития. Чем лучше вы можете передать свои мысли визуально, будь то на панели инструментов или в виде слайдов, тем лучше вы сможете использовать эту информацию. Наборы навыков меняются, чтобы приспособиться к миру, управляемому данными. Для профессионалов становится все более ценным иметь возможность использовать данные для принятия решений и использовать визуальные эффекты. В то время как традиционное образование обычно проводит четкую грань между творческим "рассказыванием историй" и техническим анализом, современный профессиональный мир также ценит тех, кто может скрестить эти два понятия: визуализация данных находится прямо посередине анализа и визуального сторителлинга.</p><p>Когда вы думаете о визуализации данных, ваша первая мысль, вероятно, сразу же приходит к простым <a href="__GHOST_URL__/gistoghramma/">Гистограммам (Histogram)</a> или <a href="__GHOST_URL__/krughovaia-diaghramma/">Круговым диаграммам (Piechart)</a>. Простые графики — это только верхушка айсберга. Существует целый набор методов визуализации:</p><ul><li>Графики</li><li>Таблицы</li><li>Карты</li><li>Инфографика</li><li>Дашборды (Dashboard)</li></ul><p>Выделяют множество видов графиков, вот некоторые из них:</p><ul><li>Диаграмма области (Area Chart)</li><li>Столбцовая диаграмма (Bar Chart)</li><li>Ящик с усами (Boxplot)</li><li>Пузырьковая диаграмма (Bubble Cloud)</li><li>Пулевой график (Bullet Graph)</li><li>Картограмма (Cartogram)</li><li>Круговая диаграмма (Circle View)</li><li>Карта распространения (Dot Distribution Map)</li><li>Диаграмма Ганта (Gantt Chart)</li><li>Тепловая карта (Heat Map)</li><li>Таблица с тепловой картой (Highlight Table)</li><li>Гистограмма</li><li>Матрица (Matrix)</li><li>Сетевая диаграмма (Network)</li><li>Радиальная диаграмма (Polar Area)</li><li>Радиальное дерево (Polar Tree)</li><li><a href="__GHOST_URL__/tochechnaya-diagramma/">Точечная диаграмма (Scatterplot)</a></li><li>Потоковый граф (Streamgraph)</li><li>Сводная таблица (Pivot Table)</li><li>Таймлайн (Timeline)</li><li>Древовидная диаграмма (Treemap)</li><li>Облако слов (Word Cloud)</li></ul><p>Автор оригинальной статьи: <a href="https://www.tableau.com/learn/articles/data-visualization">tableau.com</a></p>		vizualizatsiia-dannykh	2022-08-21		
199	Двунаправленный кодировщик представлений трансформера (BERT)		<p>BERT (англ. <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers – двунаправленный кодировщик представлений трансформера) — это языковая модель, основанная на архитектуре <a href="__GHOST_URL__/transformiery/">Трансформер (Transformer)</a>, предназначенная для предобучения языковых Представлений (Representation) с целью их последующего применения в широком спектре задач <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a>.</p><p>Результаты исследоваий показывают, что языковая модель с двунаправленным обучением может иметь более глубокое понимание языкового контекста, чем однонаправленные. </p><p>В области компьютерного зрения исследователи неоднократно демонстрировали ценность трансферного обучения — предварительного обучения модели нейронной сети известной задаче, например ImageNet, а затем выполнения тонкой настройки — с использованием обученной нейронной сети в качестве основы. В последние годы исследователи показали, что подобная техника может быть полезна во многих задачах естественного языка.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-bert">Как работает BERT</h3><p>BERT использует Трансформер – механизм, который изучает контекстуальные отношения между словами в тексте. Он включает в себя два отдельных механизма — кодировщик, который считывает введенный текст, и декодер, который выдает прогноз. Поскольку целью BERT является создание языковой модели, необходим только механизм кодировщика. </p><p>В отличие от однонаправленных моделей, которые считывают вводимый текст последовательно (слева направо или справа налево), Transformer считывает сразу всю последовательность слов. Поэтому он считается двунаправленным, хотя правильнее было бы сказать, ненаправленным. Эта характеристика позволяет модели изучать контекст слова на основе всего его окружения.</p><p>В приведенной ниже диаграмме изображен Transformer. Вход представляет собой последовательность <a href="__GHOST_URL__/token/">Токенов (Token)</a> – слов, их частей или символов, которые сначала встраиваются в векторы, а затем обрабатываются в нейронной сети. Выход представляет собой последовательность векторов, в которой каждый вектор соответствует входной лексеме с тем же индексом.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/08/image.png" class="kg-image" alt loading="lazy" width="876" height="594" srcset="__GHOST_URL__/content/images/size/w600/2022/08/image.png 600w, __GHOST_URL__/content/images/2022/08/image.png 876w" sizes="(min-width: 720px) 720px"><figcaption>Архитектура BERT</figcaption></figure><h3 id="%D0%BA%D0%B0%D0%BA-bert-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-%D1%81-%D1%80%D1%83%D1%81%D1%81%D0%BA%D0%BE%D1%8F%D0%B7%D1%8B%D1%87%D0%BD%D1%8B%D0%BC-%D0%B4%D0%B0%D1%82%D0%B0%D1%81%D0%B5%D1%82%D0%BE%D0%BC">Как BERT работает с русскоязычным датасетом</h3><p>Поработаем с надстройкой BERT под названием dostoevsky и определим тональность того или иного комментария на форуме. У такого функционала большое число применений – от оперативного поиска жалоб и последующей реакции на них до подстегивания обсуждений на страницах новых товаров.</p><p>Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">!pip install dostoevsky</code></pre><p>Скачаем языковой пакет, содержащий набор русских слов, каждому из которых присвоена некая эмоциональная окраска:</p><pre><code class="language-python">!python -m dostoevsky download fasttext-social-network-model</code></pre><pre><code class="language-python">from dostoevsky.tokenization import RegexTokenizer\nfrom dostoevsky.models import FastTextSocialNetworkModel\n\nfrom google.auth import default\nfrom google.colab import auth, drive\nimport gspread\n\nimport pandas as pd</code></pre><p>Подгрузим датасет:</p><pre><code class="language-python">df = pd.read_csv('https://www.dropbox.com/s/q6t11s4inn6bvxg/forum_messages.csv?dl=1')df.head()</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/08/--------------2022-08-26---21.50.37.png" class="kg-image" alt loading="lazy" width="1184" height="272" srcset="__GHOST_URL__/content/images/size/w600/2022/08/--------------2022-08-26---21.50.37.png 600w, __GHOST_URL__/content/images/size/w1000/2022/08/--------------2022-08-26---21.50.37.png 1000w, __GHOST_URL__/content/images/2022/08/--------------2022-08-26---21.50.37.png 1184w"></figure><p>Инициируем сущность, токенизирующую комментарии на отдельно взятые единицы речи - слова, знаки препинания. Затем создадим экземпляр модели <code>FastTextSocialNetworkModel</code> и выделим для нее столбец с комментариями как объект. Отдельным столбцом polarity сохраним результаты:</p><pre><code class="language-python">tokenizer = RegexTokenizer()\nmodel = FastTextSocialNetworkModel(tokenizer=tokenizer)\nmessages = df['текст']\nresults = model.predict(messages, k=2)\ndf['polarity'] = results\ndf.head()</code></pre><p>Для каждого комментария мы получили словарь, описывающий негативность (negative), позитивность (positive), нейтральность (neutral). В практике автора статьи комментарии с высоким (более 0,5) коэффициентом нейтральности, как те, что расположены на строках № 1-5 стоит исключить из набора при дальнейшей классификации на негатив и позитив. </p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/08/--------------2022-08-26---22.02.06.png" class="kg-image" alt loading="lazy" width="1785" height="293" srcset="__GHOST_URL__/content/images/size/w600/2022/08/--------------2022-08-26---22.02.06.png 600w, __GHOST_URL__/content/images/size/w1000/2022/08/--------------2022-08-26---22.02.06.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/08/--------------2022-08-26---22.02.06.png 1600w, __GHOST_URL__/content/images/2022/08/--------------2022-08-26---22.02.06.png 1785w"></figure><p>Получается, из тех Наблюдений (Observation), что видны на экране, внимания достойна только вторая запись. И действительно, такие слова, как "нелепо" позволяют приписать этот комментарий к разряду негативных. Попробуйте запустить код в ноутбуке и посмотрите, какие еще комментарии dostoevsky причислил к отрицательным.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://drive.google.com/file/d/1OemUU7-tcZdWKEkbRwLMyq5ABL9cjSSQ/view?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">Rani Horev</a></p>		bert	2022-08-26		
200	ETL		<p>ETL (англ. Extract, Transform, Load – Извлечь, преобразовать и загрузить) группа процессов, происходящих при переносе данных из нескольких систем в одно хранилище.</p><p>Если у вас есть данные из нескольких источников, которые вы хотите перенести в централизованную базу данных, вам необходимо:</p><ul><li>Извлекать данные из исходного источника, будь то другая база данных или приложение</li><li>Преобразовывать информацию путем их очистки, очистки от дубликатов, объединения и других способов подготовки</li><li>Загружать результат в целевую базу</li></ul><p>Как правило, один инструмент ETL выполняет все три из этих шагов и является критически важной частью обеспечения полноты и пригодности данных, необходимых для отчетности, аналитики, а теперь и для <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Природа процесса сильно изменилась за последнее десятилетие.</p><h3 id="%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%B0%D1%8F-%D0%B8%D1%81%D1%82%D0%BE%D1%80%D0%B8%D1%8F">Краткая история</h3><p>ETL уходит своими корнями в 1970-е годы к появлению централизованных хранилищ данных. Но только в конце 1980-х и начале 1990-х годов, когда они заняли центральное место, мир ощутил потребность в специализированных инструментах, помогающих загружать данные в эти новые хранилища. Первым пользователям нужен был способ «извлекать» информацию из разрозненных систем, «преобразовывать» ее в целевой формат и «загружать». Первые инструменты ETL были примитивными, но они выполняли свою работу. Конечно, объем данных, которые они обрабатывали, был скромным по сегодняшним меркам.</p><p>По мере роста объема данных росли и хранилища данных, а программные инструменты ETL множились и становились все более сложными. Но до конца 20-го века хранение и преобразование данных осуществлялось в основном в локальных хранилищах. Однако произошло нечто, навсегда изменившее наш взгляд на хранение и обработку.</p><h3 id="%D0%BE%D0%B1%D0%BB%D0%B0%D1%87%D0%BD%D1%8B%D0%B5-%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F">Облачные вычисления</h3><p>Объем данных, которые мы генерируем и собираем, продолжает расти с экспоненциальной скоростью. Более того, у нас есть все более сложные инструменты, которые позволяют нам использовать все наши данные для получения реального представления о нашем бизнесе и клиентах. Традиционная инфраструктура хранилища данных не может масштабироваться для хранения и обработки такого большого объема данных — по крайней мере, неэффективно с точки зрения затрат и своевременно. Если мы хотим выполнять высокоскоростную, сложную аналитику и анализ всех наших данных, облако — единственное место, где это можно сделать. Облачные хранилища данных, такие как Amazon Redshift, Snowflake и Google BigQuery, могут бесконечно масштабироваться для размещения практически любого объема данных. Облачное хранилище данных также поддерживает Массовую параллельную обработку (MPP), которая позволяет координировать огромные рабочие нагрузки между горизонтально масштабируемыми кластерами вычислительных ресурсов. Локальные инфраструктуры просто не обладают такой скоростью или масштабируемостью. Облако меняет то, как мы обрабатываем данные, а также то, как мы определяем и доставляем ETL.</p><h3 id="%D0%BF%D0%BE%D1%87%D0%B5%D0%BC%D1%83-etl-%D0%B2%D0%B0%D0%B6%D0%B5%D0%BD">Почему ETL важен?</h3><p>Многие спрашивают, зачем нам ETL? Это все еще важно? Ответ – да. ETL имеет несколько бизнес-преимуществ, которые выходят за рамки простого извлечения, очистки, согласования и доставки данных из точки A (источник) в точку B (назначение):</p><ul><li>Контекст: ETL помогает предприятиям получить глубокий исторический контекст с данными</li><li>Укрепление: он обеспечивает консолидированное представление данных для упрощения анализа и составления отчетов</li><li>Производительность: это повышает производительность за счет повторяющихся процессов, не требующих сложного ручного кодирования</li><li>Точность: это повышает точность данных и возможности аудита, которые необходимы большинству предприятий для соблюдения правил и стандартов.</li></ul><p>Причина, по которой вам нужен ETL в облаке, такая же, как и в традиционном хранилище данных. Ваши данные по-прежнему должны быть доставлены в центральный репозиторий — теперь из большего количества источников, чем когда-либо, в структурированной и полуструктурированной форме. Эти огромные хранилища данных необходимо преобразовать в форматы, наиболее подходящие для анализа. ETL подготавливает данные для быстрого доступа и, таким образом, быстрого анализа. Данные должны быть собраны и подготовлены для использования в инструментах бизнес-аналитики, таких как программное обеспечение для визуализации данных, иначе в облаке они будут не более полезны, чем в необработанном формате в каком-либо центре обработки данных.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-etl">Как работает ETL?</h3><p>Традиционно ETL извлекает данные из одной или нескольких баз данных оперативной обработки транзакций. Такие приложения содержат большой объем транзакционных данных, которые необходимо преобразовать и интегрировать с операционными данными, чтобы их можно было использовать в дальнейшем.</p><p>Эти данные обычно извлекаются в промежуточную область, хранилище, которое находится между источником данных и целью данных. В этой промежуточной области инструмент ETL преобразует данные, очищает, объединяет и иным образом оптимизирует их для анализа.</p><p>Затем инструмент загружает данные в базу данных системы поддержки принятия решений (DSS), где группы бизнес-аналитики могут выполнять запросы к ним и представлять результаты и отчеты бизнес-пользователям, чтобы помочь им принимать решения и определять стратегию.</p><p>Часто, несмотря на полезность инструментов ETL, процесс ETL все еще несколько запутан и сложен, поскольку сами данные запутаны и сложны. Традиционный ETL по-прежнему требует от специалистов по работе с данными значительных трудозатрат в виде ручного кодирования, повторной обработки и других задач ручного обслуживания.</p><h3 id="%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9-etl-elt">Современный ETL (ELT)</h3><p>Для современной аналитики данных и ELT облако навсегда изменило все. Несмотря на то, что в локальных хранилищах данных все еще есть некоторая бизнес-аналитика и ELT, только облако обладает сочетанием скорости, масштабируемости и практичности, необходимым для обработки огромных объемов структурированных и полуструктурированных данных буквально из десятков или сотен источников.</p><p>Мощные облачные хранилища данных, такие как Amazon Redshift, Snowflake и Google BigQuery, не нуждаются во внешних ресурсах (например, в промежуточном сервере ETL) для выполнения преобразований. Данные можно анализировать из предварительно рассчитанных сводок, что еще больше упрощает и ускоряет процесс ETL (или, на данном этапе, ELT, поскольку данные загружаются, а затем преобразуются в облачном хранилище данных).</p><p>Преобразования и моделирование данных выполняются с помощью SQL — языка, общего для профессионалов в области бизнес-аналитики, специалистов по данным и аналитиков.</p><h3 id="%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8-etl">Задачи ETL</h3><p>ETL может быть невероятно сложным процессом, и у него есть некоторые неотъемлемые проблемы. Помните об этих проблемах и о том, как они могут повлиять на ваш бизнес, выбирая продукт ETL. Различные продукты используют разные подходы, но то, что вы выберете, зависит от ваших бизнес-требований и того, как вы используете данные.</p><ul><li>Задача №1 – масштабирование, одна из важнейших функций ETL. Количество данных, которые собирают компании, будет только расти. Теперь вы можете перемещать данные пакетами, но всегда ли это будет актуально для вашего бизнеса? Сколько заданий вы можете запустить? Вы должны иметь возможность масштабировать процессы и емкость ETL, если это возможно, до бесконечности. </li><li>№ 2 – точное преобразование данных, обеспечение точности и полноты преобразуемых данных. Ручное кодирование и изменения или отсутствие планирования и тестирования перед выполнением задания ETL иногда могут приводить к ошибкам, включая загрузку дубликатов, отсутствующие данные и другие проблемы. Инструмент ETL может уменьшить потребность в ручном кодировании и помочь сократить количество ошибок. Тестирование точности данных может помочь обнаружить несоответствия и дубликаты, а функции мониторинга могут помочь выявить случаи, когда вы имеете дело с несовместимыми типами данных и другими проблемами управления данными.</li><li>№ 3 – обработка разнообразных источников данных, которые растут в объеме. Одно предприятие может обрабатывать разнообразные данные из сотен или даже тысяч источников данных. Это могут быть структурированные и частично структурированные источники, источники в реальном времени, файлы CSV, бакеты Amazon S3, потоковые источники. Некоторые из этих данных лучше всего преобразовывать пакетами, в то время как для других лучше работает потоковое, непрерывное преобразование данных. Обработка каждого типа данных наиболее эффективным и практичным способом может оказаться огромной проблемой.</li></ul><p>Как используется ETL?</p><p>Существует несколько различных способов наиболее частого использования ETL:</p><ul><li>Хранилище: предприятия традиционно использовали ETL для сбора данных из различных источников, преобразования их в согласованный, готовый к аналитике формат и загрузки их в хранилище данных, где группы бизнес-аналитики могут анализировать их для бизнес-целей.</li><li>Облачная миграция: с появлением облачных вычислений предприятия переносят данные в облако в целом и, в частности, переносят данные в облачные хранилища данных, чтобы ускорить получение информации. Облачные инструменты ETL используют преимущества облака, в том числе скорость и масштабируемость, для загрузки данных непосредственно в облако и преобразования их в облачной инфраструктуре, что позволяет специалистам по данным экономить время и деньги.</li><li>Машинное обучение: хотя машинное обучение и <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственный интеллект (AI)</a> еще не стали обычным явлением в бизнесе, многие предприятия начинают изучать, как интегрировать их в аналитику и науку о данных. Облако — единственное практическое решение для крупномасштабных операций машинного обучения и ИИ. Кроме того, оба метода требуют больших хранилищ данных для построения и обучения аналитических моделей, а также для автоматизированного анализа данных. Облачные инструменты ELT (в отличие от традиционных ETL) необходимы как для переноса больших объемов данных в облако, так и для их преобразования для обеспечения готовности к аналитике.</li><li>Интеграция маркетинговых данных: сегодня клиенты взаимодействуют с компаниями по нескольким каналам, регистрируя множество взаимодействий и транзакций в день или даже в час. Маркетологам может быть сложно получить представление обо всех этих каналах, чтобы понять потребности и поведение клиентов. Программное обеспечение ETL может иметь решающее значение для сбора и интеграции данных о клиентах из электронной коммерции, социальных сетей, веб-сайтов, мобильных приложений и других платформ. Это также может помочь интегрировать другие контекстные данные, чтобы маркетологи могли применять гиперперсонализацию, улучшать взаимодействие с пользователем, предлагать стимулы и многое другое.</li><li>Интернет вещей: в настоящее время одним из самых быстрорастущих источников данных для бизнеса являются подключенные устройства и системы, являющиеся частью Интернета вещей. Говорим ли мы о носимых устройствах или встроенных датчиках в местах, транспортных средствах или оборудовании, IoT производит астрономические объемы вычислений. И эксперты ожидают, что этот объем будет расти на 28,5% в год в среднем до 2025 года. Технология ELT, особенно облачная ETL, будет абсолютно необходима для интеграции и преобразования данных из источников IoT.</li><li>Репликация (синхронизации) частей базы данных: часто это включает перемещение данных из локального хранилища данных в облачное хранилище данных, но по мере того, как все больше предприятий переходят в облако, это может означать переход от одной облачной инфраструктуры или поставщика облачных услуг к другому. Вот почему важно иметь инструмент ETL или ELT, который не только работает в облаке, но и гибок на нескольких облачных платформах.</li><li>Бизнес-аналитика: одна вещь, которая никогда не изменится, заключается в том, что предприятия должны анализировать данные, чтобы предоставлять бизнес-аналитику, которая позволяет менеджерам и заинтересованным сторонам принимать обоснованные решения. Чтобы эти решения были действительно обоснованными, они должны основываться на всех данных организации, а не только на том количестве, которое может обрабатывать устаревшая архитектура данных. Облачное хранилище данных становится важным элементом для анализа данных и бизнес-аналитики, поэтому облачная технология ETL также имеет решающее значение для управления информацией и позволяет ускорить получение информации.</li></ul><p>Автор оригинальной статьи: <a href="https://www.matillion.com/what-is-etl-the-ultimate-guide/">matillion.com</a></p>		etl	2022-09-01		
201	Vertica		<p>Vertica – база данных, хранящая информацию в колонках. По сравнению со строковыми Системами управления базами данных (DBMS), колоночная БД сокращает количество дисковых операций ввода-вывода, что делает ее идеальной для интенсивного чтения. Vertica читает только те столбцы, которые необходимы для ответа на запрос.</p><p>Основные функции Vertica:</p><ul><li><strong>Колоночное (столбчатое) хранилище и исполнение запросов</strong>: обеспечивается значительный прирост производительности, операций ввода-вывода, когда речь идет об аналитических рабочих нагрузках. Запрос считывает только те столбцы, которые необходимы для ответа на запрос.</li><li><strong>Загрузка и выполнение запросов в режиме реального времени</strong> благодаря высокому уровню параллелизма запросов и возможности одновременной загрузки новых данных в систему Vertica может загружать данные в 10 раз быстрее, чем традиционные базы данных с построчным хранилищем.</li><li><strong>Расширенная аналитика</strong> – набор расширенной аналитики в базе данных позволяет проводить аналитические вычисления ближе к данным. Это обеспечивает немедленные результаты из одного места без необходимости извлечения данных из отдельной среды.</li><li><strong>Дизайн баз данных и инструменты администрирования</strong> — эти функции позволяют настраивать и контролировать Vertica с минимальными усилиями администратора. </li><li><strong>Усовершенствованное сжатие:</strong> агрессивное кодирование и сжатие позволяют Vertica значительно повысить аналитическую производительность за счет сокращения ресурсов ЦП, памяти и дисковых операций ввода-вывода во время обработки. Vertica может уменьшить исходный размер данных на 90%, до 1/10 исходного размера, без потери информации или точности.</li><li><strong>Структурированные и полуструктурированные данные: </strong>в дополнение к традиционным структурированным таблицам базы данных Vertica предоставляет гибкие таблицы, которые позволяют загружать и анализировать полуструктурированные данные, например данные в формате JSON.</li><li><strong>Надежное и масштабируемое решение для параллельной обработки</strong>, обеспечивающее активную избыточность, автоматическую репликацию, аварийное переключение и восстановление.</li><li><strong>Развертывание в любом месте:</strong> запуск на физическом оборудовании, расположенном в вашем собственном (или совместно расположенном) центре обработки данных. Или запускайте на виртуальном оборудовании свои собственные виртуальные хосты или на основных облачных платформах (AWS, Azure и Google Cloud).</li><li><strong>Соединения с озерами данных</strong> из Apache Hadoop и Kafka, а также из многих других систем, используя стандартные клиентские библиотеки, такие как JDBC и ODBC.</li><li><strong>Управление и мониторинг:</strong> консоль управления на основе браузера позволяет создавать, импортировать и управлять базами данных Vertica с помощью удобного графического интерфейса.</li><li><strong>Динамическое масштабирование вашего кластера в соответствии с вашей рабочей нагрузкой</strong>, чтобы масштабировать кластер базы данных, чтобы справиться с возросшими рабочими нагрузками, или уменьшить их.</li></ul><p>Автор оригинальной статьи: <a href="https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/ConceptsGuide/Other/ConceptsGuide.htm">vertica.com</a></p>		vertica	2022-09-09		
202	Power BI: что это за система		<p>Power BI — это набор программных служб, приложений и соединителей, которые работают вместе, чтобы превратить ваши несвязанные источники данных в согласованные, интерактивные идеи. Ваши данные могут быть электронной таблицей Excel или набором облачных и локальных гибридных хранилищ данных. Power BI позволяет легко подключаться к источникам данных, визуализировать и обнаруживать важные данные, а также делиться ими с кем угодно.</p><p>Power BI состоит из нескольких элементов, которые работают вместе, начиная с этих трех основных элементов:</p><ul><li>Настольное приложение Windows под названием Power BI Desktop.</li><li>Интернет-служба SaaS (программное обеспечение как услуга), называемая службой Power BI.</li><li>Мобильные приложения Power BI для устройств Windows, iOS и Android.</li></ul><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/09/image.png" class="kg-image" alt loading="lazy" width="1200" height="600" srcset="__GHOST_URL__/content/images/size/w600/2022/09/image.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/image.png 1000w, __GHOST_URL__/content/images/2022/09/image.png 1200w"></figure><p>Эти три элемента предназначены для того, чтобы вы могли создавать бизнес-аналитику, делиться ею и использовать ее наиболее эффективно.</p><p>Например, вы можете использовать службу Power BI для просмотра отчетов и панелей мониторинга. Ваш коллега-аналитик может широко использовать Power BI Desktop или Power BI Report Builder для создания отчетов, а затем публиковать эти отчеты в службе Power BI, где вы их просматриваете. Другой коллега из отдела продаж может в основном использовать телефонное приложение Power BI для отслеживания хода выполнения квот продаж и получения подробной информации о новых потенциальных клиентах.</p><p>Если вы разработчик, вы можете использовать API-интерфейсы Power BI для передачи данных в наборы данных или для встраивания панелей мониторинга и отчетов в свои собственные настраиваемые приложения. </p><p>Автор оригинальной статьи: <a href="https://learn.microsoft.com/en-us/power-bi/fundamentals/power-bi-overview">microsoft.com</a></p>		power-bi	2022-09-18		
203	Что такое Hadoop, зачем и где применяется		<p>Hadoop — это платформа Apache с открытым исходным кодом, написанная на языке Java, которая позволяет выполнять распределенную обработку больших наборов данных в кластерах компьютеров с использованием простых моделей программирования. Приложение работает в среде, которая обеспечивает <em>распределенное</em> хранение и вычисления между кластерами компьютеров. Hadoop предназначен для масштабирования от одного сервера до тысяч машин, каждая из которых предлагает локальные вычисления и хранилище.</p><h3 id="%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0">Архитектура</h3><p>По своей сути Hadoop имеет два основных уровня, а именно:</p><ul><li>Слой обработки/вычисления (MapReduce)</li><li>Слой хранения (распределенная файловая система Hadoop)</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/09/image-1.png" class="kg-image" alt loading="lazy" width="400" height="472"></figure><h3 id="mapreduce">MapReduce</h3><p>MapReduce — это модель параллельного программирования для написания распределенных приложений, разработанная в Google для эффективной обработки больших объемов данных (многотерабайтных наборов данных) на больших кластерах (тысячи узлов) стандартного оборудования надежным и отказоустойчивым способом. </p><h3 id="hdfs">HDFS</h3><p>Распределенная файловая система Hadoop (HDFS) основана на файловой системе Google (GFS) и предоставляет систему, предназначенную для работы на обычном оборудовании. Она имеет много общего с существующими распределенными файловыми системами. Однако отличия от других распределенных файловых систем существенны. Она обладает высокой отказоустойчивостью и предназначена для развертывания на недорогом оборудовании, обеспечивает высокоскоростной доступ к данным приложений и подходит для приложений, имеющих большие наборы данных.</p><p>Помимо двух вышеупомянутых основных компонентов, среда Hadoop также включает в себя следующие два модуля:</p><ul><li>Hadoop Common — это библиотеки и утилиты Java, необходимые для других модулей Hadoop</li><li>Hadoop YARN — это платформа для планирования заданий и управления ресурсами кластера.</li></ul><h3 id="%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-hadoop">Как работает Hadoop?</h3><p>Довольно дорого создавать большие серверы с тяжелыми конфигурациями, которые выполняют крупномасштабную обработку, но в качестве альтернативы вы можете связать вместе множество обычных компьютеров с одним центральным процессором в единую функциональную распределенную систему, и практически кластерные машины могут считывать набор данных. параллельно и обеспечивают гораздо более высокую пропускную способность. Более того, это дешевле, чем один высокопроизводительный сервер. Таким образом, это первый мотивационный фактор использования Hadoop, который работает на кластерных и недорогих машинах.</p><p>Hadoop запускает код в кластере компьютеров. Этот процесс включает в себя следующие основные задачи:</p><ul><li>Данные изначально разделены на каталоги и файлы. Файлы разделены на блоки одинакового размера по 128M и 64M</li><li>Затем эти файлы распределяются по различным узлам кластера для дальнейшей обработки</li><li>HDFS, находящаяся поверх локальной файловой системы, контролирует обработку</li><li>Блоки реплицируются для обработки аппаратных сбоев</li><li>Проверяется успешность выполнения кода</li><li>Выполняется сортировка, которая происходит между этапами карты и сокращения</li><li>Отправка отсортированных данных на определенный компьютер</li><li>Создаются логи для отладки.</li></ul><p>Автор оригинальной статьи: <a href="https://www.tutorialspoint.com/hadoop/hadoop_introduction.htm">tutorialspoint.com</a></p>		hadoop	2022-09-25		
204	Лексема (Lexeme)		<p>Лексема — это последовательность буквенно-цифровых символов в <a href="__GHOST_URL__/token/">Токене (Token)</a>. Лексемы являются ключевыми словами в словарях. Лексема "играть", например, может принимать разные формы, такие как "играл", "игравший".</p><p>В контексте <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> лексемы являются частью входного потока, в котором идентифицируются токены. Лексема является одним из строительных блоков языка. Из лексем состоит лексикон.</p><p>Лексемы играют важную роль в <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработке естественного языка (NLP)</a>. Однако требования к точности здесь повышенные: один случайный или неуместный символ может ухудшить распознавание. Во время компиляции программы лексический анализ — это попытка компьютера разобраться в строках символов во входном потоке.</p><p>Каждая лексема анализируется на предмет ее полезности. Определенные шаблоны буквенно-цифровых строк составляют то, что компьютер распознает как токены. Эти токены могут быть идентификаторами, ключевыми словами, операторами, специальными символами или константами. Например, компьютер видит знак "*" (asterisk) как знак умножения и "2" как число.</p><p>Правильный синтаксис необходим для хорошего программирования. В то время как в человеческой речи можно обойтись сленгом, плохой грамматикой или неправильным произношением, компьютерный язык, как правило, более требователен.</p><p>Автор оригинальной статьи: <a href="https://www.techopedia.com/definition/8050/lexeme">techopedia.com</a></p>		lieksiema	2022-10-21		
205	Apache Spark		<p>Apache Spark — это технология кластерных вычислений, основанная на Hadoop MapReduce и расширяющая его модель, позволяя использовать интерактивные запросы и потоковую обработку. Главной особенностью Spark являются его кластерные вычисления в памяти, которые увеличивают скорость обработки приложения.</p><p>Spark предназначен для покрытия широкого спектра нагрузок, таких как пакетные приложения, итерационные алгоритмы, интерактивные запросы и потоковая передача. Помимо поддержки всех этих рабочих нагрузок в соответствующей системе, это снижает нагрузку на управление, связанную с обслуживанием отдельных инструментов.</p><p>Spark — это один из подпроектов Hadoop, разработанный Матеем Захария в 2009 году в AMPLab Калифорнийского университета в Беркли. Он был открыт в 2010 году под лицензией BSD. </p><p>Apache Spark имеет следующие функции.</p><ul><li>Скорость: Spark помогает запускать приложение в кластере Hadoop, до 100 раз быстрее в памяти и в 10 раз быстрее при работе на диске. Это возможно за счет уменьшения количества операций чтения / записи на диск. Он хранит данные промежуточной обработки в памяти.</li><li>Поддержка нескольких языков — Spark предоставляет встроенные API-интерфейсы на Java, Scala или Python. Поэтому вы можете писать приложения на разных языках. Spark предлагает 80 высокоуровневых операторов для интерактивных запросов.</li><li>Расширенная аналитика, поддержка SQL-запросов, потоковых данных, <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> и графических алгоритмов.</li></ul><p>Автор оригинальной статьи: <a href="https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm">tutorialspoint.com</a></p>		apache-spark	2022-10-29		
206	Ядерное сглаживание (KDE)		<p>Ядерное сглаживание (Kernel Density Estimation, KDE) – это процесс нахождения оценочной функции, которая пытается вывести характеристики <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population)</a> на основе конечного набора данных:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/11/image.png" class="kg-image" alt loading="lazy" width="640" height="480" srcset="__GHOST_URL__/content/images/size/w600/2022/11/image.png 600w, __GHOST_URL__/content/images/2022/11/image.png 640w"></figure><p>Задача сглаживания часто используется в <a href="__GHOST_URL__/nauka-o-dannykh/">Науке о данных (Data Science)</a>, поскольку этот метод позволяет создать сглаженную кривую. Эта функция особенно полезна при моделировании проектов и объектном моделировании.</p><p>KDE работает путем построения графика данных и начала создания кривой распределения. Кривая рассчитывается путем взвешивания расстояния между всеми точками в каждом конкретном месте распределения. Если есть больше точек, сгруппированных локально, оценка выше, так как вероятность увидеть точку в этом месте увеличивается. Потому KDE и расшифровывается как Kernel Density Estimation – буквально "Оценка плотности ядра". Функция ядра — это особый механизм, используемый для взвешивания точек в наборе данных. Пропускная способность ядра меняет свою форму. Более низкая полоса пропускания ограничивает объем функции и приводит к тому, что кривая оценки выглядит грубой и зубчатой. Настраивая параметры функции ядра (ширину полосы частот и амплитуду), можно изменить размер и форму оценки.</p><p>Метод может включается в процесс <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Например, нейронная сеть может начать обучаться корректировать свои оценки и давать более точные результаты. Поскольку процесс оценки повторяется, оценки ширины полосы и амплитуды постоянно обновляются, чтобы повысить точность расчетной кривой.</p><p>Автор оригинальной статьи: <a href="https://deepai.org/machine-learning-glossary-and-terms/kernel-density-estimation">deepai.org</a></p>		yadernoye-sglazhivaniye	2022-11-05		
207	Распознавание эмоций (Emotion Detection)		<p>Распознавание эмоций – классическая задача <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерного зрения (CV)</a>, процесс определения человеческих эмоций на изображении.</p><p>Если бы кто-то показал вам фотографию человека и попросил вас угадать, что он чувствует, скорее всего, у вас было бы довольно хорошее представление об этом. Что, если бы ваш компьютер мог делать то же самое? Что, если бы он мог стать еще лучше, чем вы? Кажется абсурдной мыслью, правда?</p><p>Три основных компонента обнаружения эмоций:</p><ul><li>Предварительная обработка изображения</li><li>Извлечение <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a></li><li>Классификация признаков</li></ul><h3 id="%D0%B2%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BB%D0%B8%D1%86%D0%B0-%D0%BD%D0%B0-%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B8">Выделение лица на изображении</h3><p>Распознавание лица — важный шаг в распознавании эмоций. Он удаляет части изображения, которые не имеют отношения к делу. Вот один из способов обнаружения лиц на изображениях.</p><pre><code class="language-python">import dlib\nimport numpy as np\nfrontalface_detector = dlib.get_frontal_face_detector()\ndef rect_to_bb(rect):\n    x = rect.left()\n    y = rect.top()\n    w = rect.right() - x\n    h = rect.bottom() - y\n    return (x, y, w, h)\ndef detect_face(image_url):\n    try:\n        url_response = urllib.request.urlopen(image_url)\n        img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n        image = cv2.imdecode(img_array, -1)\nrects = frontalface_detector(image, 1)\nif len(rects) &lt; 1:\n    return "No Face Detected"\nfor (i, rect) in enumerate(rects):\n    (x, y, w, h) = rect_to_bb(rect)\n    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\nplt.imshow(image, interpolation='nearest')\nplt.axis('off')\nplt.show()</code></pre><p>Другой способ сделать это — использовать предварительно обученную модель детектора лиц <code>dlib</code>, которая также используется в следующем сниппете.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/11/image-1.png" class="kg-image" alt loading="lazy" width="202" height="201"></figure><h3 id="%D0%B2%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D1%87%D0%B0%D1%81%D1%82%D0%B5%D0%B9-%D0%BB%D0%B8%D1%86%D0%B0">Выделение частей лица</h3><p>Ориентиры лица — это набор ключевых точек на изображениях лица человека. Точки определяются их координатами (x, y) на изображении. Эти точки используются для определения местоположения и представления выступающих областей лица, таких как глаза, брови, нос, рот и линия подбородка.</p><p>В качестве модели лицевых ориентиров мы использовали предварительно обученную модель обнаружения лицевых ориентиров <code>dlib</code>, которая обнаруживает 68 двумерных точек на лице человека.</p><p>Вы можете загрузить модель следующим образом:</p><pre><code class="language-python">import dlib\nimport numpy as np\nfrontalface_detector = dlib.get_frontal_face_detector()\nlandmark_predictor=dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')\ndef get_landmarks(image_url):\n    try:\n        url_response = urllib.request.urlopen(image_url)\n        img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n        image = cv2.imdecode(img_array, -1)\n    except Exception as e:\n        print ("Please check the URL and try again!")\n        return None,None\n    faces = frontalface_detector(image, 1)\n    if len(faces):\n        landmarks = [(p.x, p.y) for p in landmark_predictor(image, faces[0]).parts()]\n    else:\n        return None,None\n    \n    return image,landmarks\ndef image_landmarks(image,face_landmarks):\n    radius = -1\n    circle_thickness = 4\n    image_copy = image.copy()\n    for (x, y) in face_landmarks:\n        cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)\n        plt.imshow(image_copy, interpolation='nearest')\n        plt.axis('off')\n        plt.show()</code></pre><p>После использования модели ваш вывод должен выглядеть так:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/11/image-2.png" class="kg-image" alt loading="lazy" width="205" height="201"></figure><p>В этой модели специфическими ориентирами для черт лица являются:</p><p>Линия подбородка: 0–16<br>Брови: 17–26<br>Нос: 27–35<br>Левый глаз: 36–41<br>Правый глаз: 42–47<br>Рот: 48–67</p><p>Один из способов различить две эмоции — посмотреть, открыты ли у человека рот и глаза или нет. Мы можем найти евклидово расстояние между точками конкретно на рту, если человек удивлен, расстояние будет больше, чем было бы в спокойном состоянии.</p><h3 id="%D0%BF%D1%80%D0%B5%D0%B4%D0%B2%D0%B0%D1%80%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F-%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Предварительная обработка данных</h3><p>Прежде чем использовать данные, важно пройти ряд шагов, называемых предварительной обработкой. Это упрощает обработку данных. Мы будем использовать модифицированную версию набора данных <code>fer2013</code>, состоящую из пяти меток эмоций.</p><p>Набор данных хранится в файле формата csv. Каждое <a href="__GHOST_URL__/nabliudieniie/">Наблюдение (Observation)</a> имеет два атрибута:</p><ul><li>Пиксели изображения, хранящиеся в строковом формате</li><li>Целочисленное кодирование <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевой переменной (Target Variable)</a></li></ul><p>Всего имеется 20 000 черно-белых изображений пяти эмоций размером 48*48. </p><p>Целевые метки закодированы целым числом:<br>0 — -&gt; Злой<br>1 — -&gt; Счастливый<br>2 — -&gt; Грустный<br>3 — -&gt; Сюрприз<br>4 — -&gt; нейтральный</p><p>Загрузим <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>:</p><pre><code class="language-python">import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nlabel_map={"0":"ANGRY","1":"HAPPY","2":"SAD","3":"SURPRISE","4":"NEUTRAL"}\ndf = pd.read_csv("./ferdata.csv")\ndf.head()</code></pre><p>Этот набор данных содержит необработанные значения пикселей изображений.</p><h3 id="%D1%80%D0%B0%D0%B7%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Разделение данных</h3><p>Как обсуждалось в прошлый раз, данные необходимо разделить на два разных набора:</p><ul><li><a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные данные (Train Data)</a>: алгоритм будет обучаться на этой части датасета снова и снова, чтобы выполнить свою задачу.</li><li><a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>: алгоритм проверяют на этих данных, чтобы увидеть, насколько хорошо он работает.</li></ul><h3 id="%D1%81%D1%82%D0%B0%D0%BD%D0%B4%D0%B0%D1%80%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Стандартизация данных</h3><p>Стандартизация — это процесс помещения различных переменных в одну и ту же шкалу. Она масштабирует данные, чтобы привести среднее значение к нулю и стандартное отклонение к единице. Это преобразование центрирует данные.</p><pre><code class="language-python">from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)</code></pre><h2 id="%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B5-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8">Линейные модели</h2><h3 id="knn">kNN</h3><p><a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метод K-ближайших соседей (kNN)</a> — это непараметрический алгоритм обучения, то есть он не делает никаких предположений о распределении данных. В качестве данных использовались <a href="__GHOST_URL__/ievklidovo-rasstoianiie/">Евклидовы расстояния (Euclidean Distance)</a> между точками.</p><pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, Y_train)\npredictions = knn.predict(X_test)</code></pre><p>Чтобы вычислить <a href="__GHOST_URL__/tochnost-izmierienii/">Точность (Accuracy)</a>, используем встроенный модуль <code>accuracy_score()</code>:</p><pre><code class="language-python">from sklearn.metrics import accuracy_score\nprint(accuracy_score(Y_test, predictions)*100)\n</code></pre><p>Наша точность составила около 50%, поэтому мы попробовали несколько нелинейных моделей.</p><p>Вы можете попробовать ввести необработанные значения пикселей в модель и посмотреть, как это повлияет на точность модели.</p><h2 id="%D0%BD%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B5-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8">Нелинейные модели</h2><h3 id="%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D1%8B%D0%B5-%D0%BF%D0%B5%D1%80%D1%81%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD%D1%8B">Многослойные персептроны</h3><p><a href="__GHOST_URL__/mnoghosloinyi-piertsieptron/">Многослойный перцептрон (MLP)</a> — это разновидность <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a>. Они состоят из одного или нескольких слоев нейронов. Входной слой — это место, куда подаются данные, после которых может быть один или несколько скрытых слоев. Прогнозы поступают из выходного слоя.</p><pre><code class="language-python">from keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.losses import categorical_crossentropy\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.optimizers import Adam, SGD\nmlp_model = Sequential()\nmlp_model.add(Dense(1024, input_shape = (2304,), activation = 'relu', kernel_initializer='glorot_normal'))\nmlp_model.add(Dense(512, activation = 'relu', kernel_initializer='glorot_normal'))\nmlp_model.add(Dense(5, activation = 'softmax'))\nmlp_model.compile(loss=categorical_crossentropy, optimizer=SGD(lr=0.001), metrics=['accuracy']) \ncheckpoint = ModelCheckpoint('best_mlp_model.h5',verbose=1,\nmonitor='val_acc', save_best_only=True,mode='auto')\nmlp_history = mlp_model.fit(X_train, y_train, batch_size=batch_size,\nepochs=epochs, verbose=1, callbacks=[checkpoint], validation_data(X_test, y_test),shuffle=True)</code></pre><p>Наша точность с использованием MLP составила около 50%, она увеличилась, когда вместо значений пикселей мы использовали расстояния между ориентирами лица. Однако нам нужна еще более точная модель, поэтому мы решили использовать CNN.</p><h3 id="%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D0%B5-%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D0%B5%D1%82%D0%B8-cnn"><a href="__GHOST_URL__/sviortochnaia-nieironnaia-siet/">Сверточные нейронные сети (CNN) </a></h3><pre><code class="language-python">width, height = 48, 48\nX_train = X_train.reshape(len(X_train),height,width)\nX_test = X_test.reshape(len(X_test),height,width)\nX_train = np.expand_dims(X_train,3)\nX_test = np.expand_dims(X_test,3)\ncnn_model = Sequential()\ncnn_model.add(Conv2D(5000, kernel_size=(4, 4), activation='relu', padding='same', input_shape = (width, height, 1)))\ncnn_model.add(BatchNormalization())\ncnn_model.add(MaxPooling2D(pool_size=(3, 3), strides=(4, 4)))\ncnn_model.add(Dropout(0.2))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(2000, activation='relu'))\ncnn_model.add(Dropout(0.2))\ncnn_model.add(Dense(5, activation='softmax'))\ncheckpoint = ModelCheckpoint('best_cnn_model.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\ncnn_model.compile(loss=categorical_crossentropy,\noptimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999), \nmetrics=['accuracy'])\ncnn_history = cnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], \nvalidation_data=(X_test, y_test),shuffle=True)\ncnn_model.save('cnn_model.h5')</code></pre><p>Для повышения производительности вы можете настроить отсев, количество плотных слоев и активацию функции. Мы также использовали трансферное обучение с сверточную нейросеть под названием Группа визуальной геометрии (VGG), которая представляет собой предварительно обученную свёрточную нейронную сеть для классификации изображений.</p><h3 id="%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B0">Оценка</h3><p>Наилучшие результаты были получены при использовании VGG, которые были верны примерно в 68–70% случаев, но даже линейные модели работали очень хорошо. Хотя точность 50% не кажется чем-то достойным, это все же больше, чем если бы вы взяли картинку и присвоили ей метку эмоции наугад (что эквивалентно 20%-й точности).</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/emotion-detection-a-machine-learning-project-f7431f652b1f">Aarohi Gupta</a></p>		raspoznavaniie-emotsii	2022-11-12		
208	Распознавание объектов (Object Detection)		<p>Распознавание объектов – Распознавание эмоций – классическая задача <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерного зрения (CV)</a>, процесс узнавания различных предметов на изображении.</p><p>Давайте посмотрим, как задача решается с помощью Tensorflow. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import tensorflow as tf\nimport matplotlib.pyplot as plt</code></pre><p>Мы будем обучать нейронную сеть распознавать предметы одежды из <a href="__GHOST_URL__/dataset/">Датасета (Dataset)</a> под названием Fashion MNIST. Он содержит 70 000 предметов одежды в 10 различных категориях. Каждый предмет одежды представлен в виде изображения в оттенках серого 28x28. Вот примеры здесь:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/11/image-3.png" class="kg-image" alt loading="lazy" width="840" height="840" srcset="__GHOST_URL__/content/images/size/w600/2022/11/image-3.png 600w, __GHOST_URL__/content/images/2022/11/image-3.png 840w" sizes="(min-width: 720px) 720px"></figure><p>Данные Fashion MNIST доступны непосредственно в API <code>tf.keras</code>. Загрузим их:</p><pre><code class="language-python">mnist = tf.keras.datasets.fashion_mnist</code></pre><p>Разделим набор на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>:</p><pre><code class="language-python">(training_images, training_labels), (test_images, test_labels) = mnist.load_data()</code></pre><p>Как выглядят эти данные в понятном компьютеру табличном виде? Давайте выведем тренировочное изображение в виде таблицы пикселей с той или иной степенью белого по шкале от 0 до 255:</p><pre><code class="language-python">plt.imshow(training_images[0])\nprint(training_labels[0])\nprint(training_images[0])</code></pre><pre><code class="language-python">[[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.00392157 0.         0.         0.05098039 0.28627451 0.\n  0.         0.00392157 0.01568627 0.         0.         0.\n  0.         0.00392157 0.00392157 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n  0.21176471 0.         0.         0.         0.00392157 0.01176471\n  0.01568627 0.         0.         0.01176471]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n  0.56470588 0.48235294 0.09019608 0.         0.         0.\n  0.         0.04705882 0.03921569 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n  0.30196078 0.50980392 0.28235294 0.05882353]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.00392157\n  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n  0.55294118 0.34509804 0.6745098  0.25882353]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.00392157 0.00392157 0.00392157\n  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n  0.48235294 0.76862745 0.89803922 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n  0.8745098  0.96078431 0.67843137 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n  0.8627451  0.95294118 0.79215686 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.00392157 0.01176471 0.\n  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n  0.88627451 0.77254902 0.81960784 0.20392157]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.02352941 0.\n  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n  0.96078431 0.46666667 0.65490196 0.21960784]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.01568627 0.         0.\n  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n  0.85098039 0.81960784 0.36078431 0.        ]\n [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n  0.00784314 0.         0.         0.         0.         0.\n  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n  0.85490196 1.         0.30196078 0.        ]\n [0.         0.01176471 0.         0.         0.         0.\n  0.         0.         0.         0.24313725 0.56862745 0.8\n  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n  0.87843137 0.95686275 0.62352941 0.        ]\n [0.         0.         0.         0.         0.07058824 0.17254902\n  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n  0.91372549 0.93333333 0.84313725 0.        ]\n [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n  0.8627451  0.90980392 0.96470588 0.        ]\n [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n  0.87058824 0.89411765 0.88235294 0.        ]\n [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n  0.8745098  0.87843137 0.89803922 0.11372549]\n [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n  0.8627451  0.86666667 0.90196078 0.2627451 ]\n [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n  0.70980392 0.80392157 0.80784314 0.45098039]\n [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n  0.65490196 0.69411765 0.82352941 0.36078431]\n [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n  0.75294118 0.84705882 0.66666667 0.        ]\n [0.00784314 0.         0.         0.         0.25882353 0.78431373\n  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n  0.38823529 0.22745098 0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n  0.1372549  0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]]</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/11/image-4.png" class="kg-image" alt loading="lazy" width="251" height="248"></figure><p>Если мы обучаем нейронную сеть, по разным причинам будет проще, если мы будем рассматривать все значения как от 0 до 1, процесс, называемый «нормализация». К счастью, в Python такой список легко нормализовать без циклов. Мы делаем это так:</p><pre><code class="language-python">training_images  = training_images / 255.0\ntest_images = test_images / 255.0</code></pre><p>Почему существует два набора – тренировочный и тестовый? Идея состоит в том, чтобы иметь один набор данных для обучения, а затем еще один набор данных, который модель еще не видела, чтобы определить, насколько хорошо он будет классифицировать значения. В конце концов, модель и предназначена для распознавания объектов с данными из реального мира.</p><p>Давайте теперь спроектируем модель. Здесь довольно много новых концепций, но не волнуйтесь, вы их освоите:</p><pre><code class="language-python">model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n  tf.keras.layers.Dense(128, activation=tf.nn.relu), \n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])</code></pre><ul><li>Sequential: определяет ПОСЛЕДОВАТЕЛЬНОСТЬ слоев в нейронной сети.</li><li>Flatten: Помните ранее, когда наши изображения были квадратными, когда вы их распечатывали? Flatten просто берет этот квадрат и превращает его в одномерный набор.</li><li>Dense: добавляет слой нейронов. Каждому слою нейронов нужна функция активации, которая сообщает, что делать. Есть много вариантов, но пока используйте только эти.</li><li>Функция активации выпрямителя (ReLu) фактически означает «Если X&gt; 0 вернуть X, иначе вернуть 0» — так что он делает это, он только передает значения 0 или выше на следующий уровень в сети.</li><li><a href="__GHOST_URL__/softmaks/">Софтмакс (Softmax)</a> принимает набор значений и эффективно выбирает самое большое, поэтому, например, если выходные данные последнего слоя выглядят как [0,1, 0,1, 0,05, 0,1, 9,5, 0,1, 0,05, 0,05, 0,05], он сохраняет вы просматриваете его в поисках наибольшего значения и превращаете его в [0,0,0,0,1,0,0,0,0]. Это позволяет сделать код короче.</li></ul><p>Следующее, что нужно сделать — это построить модель. Мы делаем это, компилируя его с оптимизатором и Функцией потерь (Loss Function), как и раньше, а затем обучаете его, вызывая <code>model.fit()</code> и прося его сопоставить ваши данные обучения с вашими метками обучения, т.е. заставить его выяснить взаимосвязь между обучающие данные и их фактические метки, поэтому в будущем, если у вас будут данные, похожие на обучающие данные, он может сделать прогноз того, как эти данные будут выглядеть.</p><pre><code class="language-python">model.compile(optimizer = tf.keras.optimizers.Adam(),\n              loss = 'sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(training_images, training_labels, epochs=5)</code></pre><p>Обучение разделено на эпохи:</p><pre><code class="language-python">Epoch 1/5\n1875/1875 [==============================] - 13s 6ms/step - loss: 0.5011 - accuracy: 0.8236\nEpoch 2/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3728 - accuracy: 0.8648\nEpoch 3/5\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.3384 - accuracy: 0.8762\nEpoch 4/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3138 - accuracy: 0.8854\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.2932 - accuracy: 0.8917\n&lt;keras.callbacks.History at 0x7fb53ae2f6d0&gt;</code></pre><p>По завершении обучения мы увидим значение точности в конце последней эпохи. В данном случае это 0,8917. Это говорит о том, что точность вашей нейронной сети в классификации обучающих данных составляет около 89%. То есть она смогла правильно узнать предмет одежды в 89% случаев. </p><p>Но как это будет работать с неизвестными модели данными? Вот почему у нас есть тестовые изображения. Мы можем вызвать <code>model.evaluate()</code>, чтобы узнать о потерях. Давайте попробуем:</p><pre><code class="language-python">model.evaluate(test_images, test_labels)</code></pre><pre><code class="language-python">313/313 [==============================] - 1s 2ms/step - loss: 0.3577 - accuracy: 0.8744\n[0.35774385929107666, 0.8744000196456909]</code></pre><p>Точность равна примерно 87%. Как и ожидалось, модель, вероятно, не будет так хорошо работать с неизвестными доселе данными. Однако в целом результат можно считать удовлетворительным.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1eE3f8Rzfzw-jF0s5VD2RzCNNyG5xoHqW?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/yolo-object-detection-with-opencv-and-python-21e50ac599e9">Arun Ponnusamy</a></p>		raspoznavaniie-obiektov	2022-11-20		
209	Airflow		<p>Apache Airflow — это платформа с открытым исходным кодом для разработки, планирования и мониторинга рабочих процессов. Расширяемая среда Python Airflow позволяет создавать рабочие процессы, связанные практически с любой технологией. Веб-интерфейс помогает управлять состоянием ваших рабочих процессов. Airflow легко мастабируется.</p><p>Основная характеристика рабочих процессов Airflow заключается в том, что все рабочие процессы определены в коде Python:</p><ul><li>Динамичность: конвейеры Airflow настроены в виде кода Python.</li><li>Расширяемость: платформа Airflow содержит коннекторы для подключения к многочисленным технологиям, что позволяет легко адаптировать их к вашей среде.</li><li>Гибкость: встроенная параметризация процессов на механизме Jinja.</li></ul><p>Взгляните на следующий фрагмент кода:</p><pre><code class="language-python">from datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n# DAG – процесс, или набор таковых\nwith DAG(dag_id="demo", start_date=datetime(2022, 1, 1), schedule="0 0 * * *") as dag:\n\n    # Задачи в данном контексте – операторы\n    hello = BashOperator(task_id="hello", bash_command="echo hello")\n\n    @task()\n    def airflow():\n        print("airflow")\n\n    hello &gt;&gt; airflow()</code></pre><p>Здесь вы видите:</p><p>DAG-процесс под названием demo, запускается с 1 января 2022 года один раз в день. BashOperator выполняет bash-скрипт, и функция <code>airflow()</code>, определенная с помощью декоратора @task.</p><p>Airflow компилирует этот сценарий и выполняет задачи с заданным интервалом и в определенном порядке. Статус процесса demo виден в веб-интерфейсе:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/11/image-5.png" class="kg-image" alt loading="lazy" width="1337" height="463" srcset="__GHOST_URL__/content/images/size/w600/2022/11/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/image-5.png 1000w, __GHOST_URL__/content/images/2022/11/image-5.png 1337w"></figure><p>В этом примере демонстрируется простой сценарий Bash и Python, но эти задачи могут выполнять любой произвольный код, даже отправке электронного письма. Ниже – другое представление UI, описывающее статистику процесса:</p><figure class="kg-card kg-image-card kg-width-full"><img src="__GHOST_URL__/content/images/2022/11/image-6.png" class="kg-image" alt loading="lazy" width="1576" height="930" srcset="__GHOST_URL__/content/images/size/w600/2022/11/image-6.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/image-6.png 1000w, __GHOST_URL__/content/images/2022/11/image-6.png 1576w"></figure><p>Airflow — это платформа для оркестровки пакетных рабочих процессов. Если ваши процессы имеют четкое начало и конец и выполняются через равные промежутки времени, их можно запрограммировать процесс Airflow.</p><p>Если вы предпочитаете создание кода тонкой настройке в графическом интерфейсе, Airflow – инструмент для вас. Это означает, что:</p><ul><li>Процессы можно хранить в системе git, что позволяет пользоваться преимуществами этой системы версионирования</li><li>Процессы могут разрабатываться несколькими людьми одновременно</li><li>Могут быть написаны тесты для проверки функциональности</li><li>Список компонентов довольно обширный, и его можно пополнить.</li></ul><p>Богатая семантика планирования и выполнения позволяет легко определять сложные конвейеры, работающие через равные промежутки времени. Backfill-исполнение позволяет повторно запускать конвейер на исторических данных после внесения изменений в логику. А возможность повторного запуска частичных конвейеров после устранения ошибки помогает максимально повысить эффективность.</p><p>Пользовательский интерфейс Airflow предоставляет как подробные представления о конвейерах и отдельных задачах, так и обзор конвейеров с течением времени. Из интерфейса вы можете просматривать логи и управлять задачами, например, повторяя задачу в случае сбоя.</p><p>Открытый исходный код Airflow позволяет работать с компонентами, разработанными, протестированными и используемыми многими другими компаниями по всему миру. В активном сообществе вы можете найти множество полезных ресурсов в виде постов в блогах, статей, конференций, книг и многого другого. Вы можете общаться с другими сверстниками через несколько каналов, таких как Slack.</p><h3 id="%D0%BD%D0%B5%D0%B4%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BA%D0%B8-airflow">Недостатки Airflow</h3><p>Airflow был создан для ограниченных пакетных процессов. Хотя CLI и REST API позволяют запускать рабочие процессы, Airflow не был создан для бесконечно работающих рабочих процессов на основе событий. Airflow не является потоковым решением. Однако потоковая система, такая как Apache Kafka, часто работает вместе с Apache Airflow. Kafka можно использовать для приема и обработки в режиме реального времени, данные о событиях записываются в место хранения, а Airflow периодически запускает рабочий процесс, обрабатывающий пакет данных.</p><p>Если вы предпочитаете тонко настраивать программы в графическом интерфейсе, а не писать код, Airflow, вероятно, не является правильным решением. Веб-интерфейс призван максимально упростить управление рабочими процессами, а инфраструктура Airflow постоянно совершенствуется, чтобы сделать работу разработчиков максимально удобной. Однако философия Airflow заключается в том, чтобы использовать код.</p><p>Автор оригинальной статьи: <a href="https://airflow.apache.org/docs/apache-airflow/stable/">apache.org</a></p>		airflow	2022-11-27		
210	Распознавание лиц (Face Recognition)		<p>Распознавание лиц – классическая задача <a href="__GHOST_URL__/kompiutiernoie-zrieniie/">Компьютерного зрения (CV)</a>, выделение лиц на изображении. С примером нам поможет OpenCV — самая популярная библиотека в этой сфере. Первоначально написанная на C/C++, теперь она предоставляет коннектор и на Python.</p><p>OpenCV использует алгоритмы <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> для поиска лиц на изображении. Поскольку лица весьма разнообразны, не существует одного простого теста, который скажет вам, нашла ли библиотека лицо или нет. Однако алгоритмы разбивают задачу идентификации лица на тысячи более мелких задач, каждую из которых легко решить. </p><p>Для чего-то вроде лица у вас может быть 6000 или более классификаторов, и все они должны распознать части лица, чтобы оно было обнаружено. Но в этом и заключается проблема: для распознавания лиц алгоритм начинает с верхнего левого угла изображения и перемещается вниз по небольшим блокам данных, просматривая каждый блок, постоянно спрашивая: «Это лицо? … Это лицо? … Это лицо?» Поскольку в каждом блоке содержится 6000 или более тестов, вам могут потребоваться миллионы вычислений, которые повесят компьютер.</p><p>Чтобы обойти это, OpenCV использует <em>каскады, </em>своеобразный «ряд водопадов». Подобно серии водопадов, каскад OpenCV разбивает проблему обнаружения лиц на несколько этапов. Для каждого блока выполняется очень грубый и быстрый тест. Если это проходит, он выполняет немного более подробный тест и так далее. Алгоритм может иметь от 30 до 50 таких этапов или каскадов, и он обнаружит лицо только в том случае, если все этапы пройдены.</p><p>Преимущество заключается в том, что большая часть изображения вернет отрицательный результат на первых нескольких этапах, а это означает, что алгоритм не будет тратить время на проверку на нем всех 6000 признаков. Теперь обнаружение лиц можно выполнять не часами, а гораздо быстрее</p><p>Хотя теория может показаться сложной, на практике все довольно просто. Сами каскады — это просто набор файлов XML, содержащих данные OpenCV, используемые для обнаружения объектов. Мы инициализируем свой код желаемым каскадом, а затем он выполняет всю работу за нас.</p><p>Поскольку обнаружение лиц является таким распространенным случаем, OpenCV поставляется с рядом встроенных каскадов для обнаружения всего, от лиц и глаз до рук и ног. Есть даже каскады для других вещей, не имеющих отношения к человеческому телу. Например, если вы управляете магазином бананов и хотите отслеживать людей, крадущих бананы, для этого есть специальная конфигурация!</p><p>Давайте изучим код. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import cv2\nimport numpy as np\nimport pandas as pd\nimport sys\nimport urllib.request\nfrom google.colab.patches import cv2_imshow</code></pre><p>Сначала мы подгрузим каскадный файл и инициализируем каскадную конфигурацию:</p><pre><code class="language-python">cascPath = "/content/haarcascade_frontalface_default.xml"\nfaceCascade = cv2.CascadeClassifier(cascPath)</code></pre><p>Затем – изображение. После вычитки картинки мы обесцветим ее для сокращения требуемых вычислительных мощностей:</p><pre><code class="language-python">req = urllib.request.urlopen('https://www.dropbox.com/s/wbi8ubchu1qodtl/abba.png?dl=1')\narr = np.asarray(bytearray(req.read()), dtype=np.uint8)\nimage = cv2.imdecode(arr, -1) # 'Load it as it is'\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</code></pre><p>Функция <code>detectMultiScale</code> фактически определяет лицо и является ключевой частью нашего кода, поэтому давайте пройдемся по параметрам:</p><pre><code class="language-python">faces = faceCascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30),\n    flags = cv2.CASCADE_SCALE_IMAGE\n)\n\nprint("Найдено {0} лиц(-а).".format(len(faces)))\n\nfor (x, y, w, h) in faces:\n    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\ncv2_imshow(image)\ncv2.waitKey(0)</code></pre><p>Первый аргумент <code>gray</code> – изображение в градациях серого.</p><p>Второй <code>scale_factor</code> – масштабный фактор. Поскольку некоторые лица могут быть ближе к камере, они будут казаться больше, чем лица сзади. Коэффициент подбирается вручную и компенсирует это. </p><p>Алгоритм обнаружения использует движущееся окно для обнаружения объектов. <code>minNeighbors</code> определяет, сколько объектов будет обнаружено рядом с текущим, прежде чем он объявит найденное лицо. <code>minSize</code> тем временем определяет размер каждого окна.</p><p>Примечание. Я взял часто используемые значения для этих полей. В реальной жизни вы бы поэкспериментировали с различными значениями размера окна, коэффициента масштабирования и т.д., пока не нашли бы то, что лучше всего подходит для вас.</p><p>Функция возвращает список прямоугольников, в которых, по его мнению, найдено лицо. Далее мы ограничим прямоугольниками те места, где по мнению OpenCV, есть лицо.</p><p>Функция <code>faces</code> возвращает 4 значения <code>(x, y, w, h)</code>: расположение прямоугольника по осям x и y, а также ширину и высоту прямоугольника (w, h).</p><p>Мы используем эти значения для рисования прямоугольника с помощью встроенной функции <code>rectangle()</code>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/12/image.png" class="kg-image" alt loading="lazy" width="500" height="426"><figcaption>Найдено 4 лиц(-а)</figcaption></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1aEuGTHqBhdFbM9j5ZnDgziS_hFRQJ0rB?usp=sharing">здесь</a>. Предварительно скачайте и загрузите файл.xml в Colab <a href="https://www.dropbox.com/s/o0erlzv8hikey4t/haarcascade_frontalface_default.xml?dl=0">отсюда</a>.</p><p>Автор оригинальной статьи: <a href="https://realpython.com/face-recognition-with-python/">Shantnu Tiwari</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/12/frame.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		raspoznvaniie-lits	2022-12-03		
211	Кластеризация (Clustering)		<p>Кластеризация — это задача разделения совокупности или точек данных на несколько групп таким образом, чтобы точки данных в одних и тех же группах были более похожи на другие точки данных в той же группе и отличались от точек данных в других группах. Это в основном совокупность объектов на основе сходства и несходства между ними.</p><h3 id="%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-scikit-learn">Кластеризация: scikit-learn</h3><p>Давайте посмотрим, как кросс-валидация реализована в SkLearn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd  \n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE</code></pre><p>Загрузим бостонский датасет цен на дома из встроенных наборов данных. sklearn:</p><pre><code class="language-python">boston_dataset = load_boston()\nboston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\nboston.head()</code></pre><p>Создадим датафреймы X – <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a> и y – <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a> из этих данных.</p><pre><code class="language-python">X = pd.DataFrame(boston)\nY = boston_dataset.target</code></pre><p>Разобьем эти датафреймы на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые (Test Data)</a> данные с помощью функции <code>train_test_split()</code> так, чтобы размер тестовой выборки составлял 20% от всех данных:</p><pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)</code></pre><p>Масштабируем, то есть выполним <a href="__GHOST_URL__/standartizatsiia/">Стандартизацию (Standartization)</a> данные с помощью <code>StandardScaler</code>.  Такое преобразование необходимо, поскольку признаки датасета могут иметь большие различия между своими диапазонами, и для моделей Машинного обучения, основанных на вычислении дистанции между точками на графике как основу прогнозирования: <a href="__GHOST_URL__/mietod-k-blizhaishikh-sosiedie/">Метод k-ближайших соседей (kNN)</a>, <a href="__GHOST_URL__/mietod-opornykh-viektorov/">Метод опорных векторов (SVM)</a>, <a href="__GHOST_URL__/dierievo-rieshienii/">Дерево решений (Decision Tree)</a> и проч., это спровоцирует искаженное восприятие данных.</p><pre><code class="language-python">sc = StandardScaler()\nX_train_scaled = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\nX_test_scaled = pd.DataFrame(sc.transform(X_test), columns=X_test.columns)</code></pre><p>Инициируем объект – модель <code>KMeans</code> и разобьем данные из тренировочного набора на три <a href="__GHOST_URL__/klaster/">Кластера (Cluster)</a>, используя все признаки из датафрейма <code>X_train</code>:</p><pre><code class="language-python">kmeans = KMeans(n_clusters=3, random_state=42, max_iter=100).fit(X_train)</code></pre><p>Обучим модель и раскрасим получившиеся кластеры разными цветами:</p><pre><code class="language-python">labels_train = kmeans.fit_predict(X_train_scaled)\n\nplt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=labels_train)\nplt.show()</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2022/12/clustering.jpg" class="kg-image" alt loading="lazy" width="463" height="334"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://github.com/fitwist/kapatsa_gb_ds_libraries_hw/blob/lesson08/08.%20%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B1%D0%B5%D0%B7%20%D1%83%D1%87%D0%B8%D1%82%D0%B5%D0%BB%D1%8F/01-03.%20TSNE%2C%20KMeans.ipynb">здесь</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2022/12/image-1.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		klastierizatsiia	2022-12-10		
212	Оценка F1 (F1 Score)		<p>Оценка F1 (F-мера) – среднее значение <a href="__GHOST_URL__/tochnost-izmierienii/">Точности измерений (Accuracy)</a> и <a href="__GHOST_URL__/otzyv/">Отзыва (Recall)</a> с Весами (Weight) при наличии. F1 обычно более полезна, чем точность измерений, особенно если распределение классов неравномерно. Оценка F1 вычисляется по формуле:</p><!--kg-card-begin: markdown--><p>$$F_1 = \\frac{2 * Точность * Отзыв}{Точность + Отзыв}$$</p>\n<!--kg-card-end: markdown--><p>Когда мы получаем данные, то после очистки и предварительной обработки, первым делом передаем их в модель и, конечно же, получаем результат в виде вероятностей. Но как мы можем измерить эффективность нашей модели? Здесь <a href="__GHOST_URL__/matritsa-oshibok/">Матрица ошибок (Confusion Matrix)</a> и оказывается в центре внимания. </p><p>Матрица ошибок – это показатель успешности классификации, где классов два или более. Это таблица с 4 различными комбинациями сочетаний прогнозируемых и фактических значений.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/matrix-confusion-2.png" class="kg-image" alt loading="lazy" width="564" height="185"></figure><p>Давайте рассмотрим значения ячеек (истинно позитивные, ошибочно позитивные, ошибочно негативные, истинно негативные) с помощью "беременной" аналогии:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2020/12/confusion-matrix-error-types.png" class="kg-image" alt loading="lazy" width="512" height="512"></figure><p><br><strong>Истинно позитивное предсказание (True Positive, сокр. TP)</strong><br>Вы предсказали положительный результат, и женщина действительно беременна.</p><p><strong>Истинно отрицательное</strong> <strong>предсказание (True Negative, TN)</strong><br>Вы предсказали отрицательный результат, и мужчина действительно не беременен.</p><p><strong>Ошибочно положительное</strong> <strong>предсказание (ошибка типа I, False Positive, FN)</strong><br>Вы предсказали положительный результат (мужчина беременен), но на самом деле это не так.</p><p><strong>Ошибочно отрицательное</strong> <strong>предсказание (ошибка типа II, False Negative, FN)</strong><br>Вы предсказали, что женщина не беременна, но на самом деле она беременна.</p><p>Точность измерений вычисляется по формуле:</p><!--kg-card-begin: markdown--><p>$$Точность = \\frac{TP}{TP + FP}$$</p>\n<!--kg-card-end: markdown--><p>Отзыв вычисляется по формуле:</p><!--kg-card-begin: markdown--><p>$$Отзыв = \\frac{TP}{TP + FN}$$</p>\n<!--kg-card-end: markdown--><p>Теперь, когда у нас есть базовое представление о том, как рассчитывается F1, рассмотрим пример.</p><h3 id="%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B0-f1-sklearn">Оценка F1: SkLearn</h3><p>Давайте посмотрим, как SkLearn считает F1. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">from sklearn.metrics import f1_score</code></pre><p>Ниже мы объявим два списка –  истинные <a href="__GHOST_URL__/klass/">Классы (Class)</a> <code>y_true</code> шести <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> и предсказанные <code>y_pred</code>:</p><pre><code class="language-python">y_true = [0, 1, 2, 0, 1, 2]\ny_pred = [0, 2, 1, 0, 0, 1]</code></pre><p>Вызовем соответствующий модуль для вычисления оценки. Параметр <code>average</code> требуется, если у целей более двух классов, и <code>weighted</code> означает, что набор данных <em>несбалансированный</em>.</p><pre><code class="language-python">f1_score(y_true, y_pred, average='weighted')</code></pre><p>Метрика подспудно вычислила точность и отзыв, а затем выполнила подстановку в формулу F1:</p><pre><code class="language-python">0.26666666666666666</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1ZW64sxh5ZEW0SEGE2xlCrhqIcH2i_j0F?usp=sharing">здесь</a>.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/12/image-3.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		otsienka-f1	2022-12-18		
213	Компьютерное зрение (CV)		<p>Компьютерное зрение (Computer Vision) – область <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>, целью которой является разработка методов, помогающих компьютерам как бы понимать содержимое цифровых изображений.</p><p>Проблема компьютерного зрения кажется простой, потому что ее тривиально решают люди, даже очень маленькие дети. Тем не менее, это во многом остается нерешенной проблемой, основанной как на ограниченном понимании биологического зрения, так и на сложности зрительного восприятия в динамичном и почти бесконечно меняющемся физическом мире.</p><p>Смартфоны оснащены камерами, а снимать фото или видео и делиться ими никогда не было так просто, что привело к невероятному росту современных социальных сетей, таких как Instagram.</p><p>YouTube может быть второй по величине поисковой системой, и каждую минуту загружаются сотни часов видео, а каждый день просматриваются миллиарды видео.</p><p>Интернет состоит из текста и изображений. Индексировать и искать текст относительно просто, но для индексации и поиска изображений алгоритмам необходимо знать, что содержат изображения. В течение долгого времени содержание изображений было известно благодаря описанию и тегам, предоставленных автором.</p><p>Чтобы получить максимальную отдачу от графических данных, нам нужны компьютеры, чтобы «видеть» изображение и понимать его содержимое. Это тривиальная проблема для человека, даже маленького ребенка.</p><p>Человек может описать содержание фотографии, которую он видел однажды.<br>Человек может резюмировать видео, которое он видел только один раз.<br>Человек может узнать лицо, которое он видел только однажды.<br>Нам требуются, по крайней мере, те же возможности от компьютеров.</p><p>Одна конкретная проблема со зрением может быть легко решена с помощью статистического метода, созданного вручную, тогда как для другой может потребоваться большой и сложный набор обобщенных алгоритмов машинного обучения.</p><p>Компьютерное зрение как поле — это интеллектуальный рубеж. Как и любая граница, она захватывающая и неорганизованная, и часто нет надежного авторитета, к которому можно было бы обратиться. Многие полезные идеи не имеют теоретического обоснования, а некоторые теории бесполезны на практике.</p><p>Как правило, компьютерное зрение включает в себя разработку методов, которые пытаются воспроизвести возможности человеческого зрения.</p><p>Обработка изображений — это разновидность CV, процесс создания нового изображения из существующего. Это тип цифровой обработки сигналов, который не связан с пониманием содержания изображения, потому результаты работы такой <a href="__GHOST_URL__/modiel/">Модели (Model)</a> бывают весьма курьезными.</p><p>Подзадачи обработки изображений включают в себя:</p><ul><li>Нормализация фотометрических свойств изображения, таких как яркость или цвет</li><li>Обрезка границ изображения, например центрирование объекта на фотографии</li><li>Удаление цифрового шума из изображения, например, цифровых артефактов при слабом освещении</li></ul><p>Цель компьютерного зрения — извлечь полезную информацию из изображений. Это оказалось удивительно сложной задачей; за последние четыре десятилетия им были заняты тысячи умов, и, несмотря на это, мы все еще далеки от того, чтобы построить универсальную «видящую машину».</p><p>Компьютерное зрение кажется простым, возможно, потому, что оно не требует усилий для людей.</p><p>Первоначально считалось, что это тривиально простая задача, которую может решить студент, подключив камеру к компьютеру. После десятилетий исследований «компьютерное зрение» остается нерешенным, по крайней мере, с точки зрения соответствия возможностям человеческого зрения.</p><p>Заставить компьютер видеть было чем-то, что ведущие специалисты в области искусственного интеллекта считали на уровне сложности летнего студенческого проекта еще в шестидесятые годы. Спустя сорок лет задача все еще не решена и кажется сложнейшей.</p><p>Одна из причин заключается в том, что мы плохо понимаем, как работает человеческое зрение.</p><p>Изучение биологического зрения требует понимания органов восприятия, таких как глаза, а также интерпретации восприятия в мозге. Был достигнут значительный прогресс как в составлении схемы процесса, так и с точки зрения обнаружения приемов и сокращений, используемых системой, хотя, как и в любом исследовании, затрагивающем мозг, предстоит пройти еще долгий путь.</p><p>Перцептивные психологи потратили десятилетия, пытаясь понять, как работает зрительная система, и, хотя они могут изобретать оптические иллюзии, чтобы разобрать некоторые из ее принципов, полное решение этой загадки остается неуловимым.</p><p>Другая причина, по которой это такая сложная проблема, заключается в сложности, присущей визуальному миру.</p><p>Данный объект может быть виден с любого угла, при любых условиях освещения, с любым типом окклюзии от других объектов и так далее. Настоящая система технического зрения должна быть способна «видеть» в любой из бесконечного числа сцен и при этом извлекать что-то значимое.</p><p>Компьютеры хорошо работают с задачами с жесткими ограничениями, а не с открытыми неограниченными задачами, такими как визуальное восприятие.</p><p>Тем не менее, в этой области наблюдается прогресс, особенно в последние годы с массовыми системами оптического распознавания символов и распознавания лиц в камерах и смартфонах.</p><p>Компьютерное зрение находится на необычайной стадии своего развития. Сама тема существует с 1960-х годов, но только недавно стало возможным создавать полезные компьютерные системы, используя идеи компьютерного зрения.</p><p>Задачи компьютерного зрения:</p><ul><li><a href="__GHOST_URL__/raspoznavaniie-emotsii/">Распознавание эмоций (Emotion Detection)</a></li><li><a href="__GHOST_URL__/raspoznvaniie-lits/">Распознавание лиц (Face Recognition)</a></li><li>Генерация изображений (GAN)</li><li>Распознавание образов (Keypoints Detection): какие объекты изображены на этой фотографии?</li><li><a href="__GHOST_URL__/raspoznavaniie-obiektov/">Классификация объектов (Object Classification)</a>: какой тип объекта изображен на фотографии?</li><li>Распознавание объектов (Object Detection): где находятся объекты на фотографии?</li><li>Трекинг объектов (Object Tracking): где объект на фотографии?</li><li>Распознавание позы (Pose Detection)</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2022/12/image-4.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p><p>Автор оригинальной статьи: <a href="https://machinelearningmastery.com/what-is-computer-vision/">Jason Brownlee</a></p>		kompiutiernoie-zrieniie	2022-12-25		
214	Greenplum		<p>Greenplum — это база данных с массовой параллельной обработкой, построенная на PostgreSQL с возможностью масштабирования до рабочей нагрузки на уровне нескольких петабайт и обеспечивает доступ к кластеру мощных серверов, которые будут работать вместе в рамках единого интерфейса SQL, где мы можем просматривать все данные.</p><p>Greenplum – это база данных с открытым исходным кодом, разработанная компанией Pivotal, которая позже была приобретена VMware. Архитектура ПО была специально разработана для управления крупномасштабными хранилищами данных, предоставляя вам возможность распределять ваши данные по множеству серверов.</p><p>Чтобы лучше понять архитектуру Greenplum, давайте сначала рассмотрим, что такое база данных массивно-параллельной обработки (Massive Parallel Processing – MPP). При обработке больших объемов сложных данных велика вероятность того, что ваш сервер может начать "раздавливаться" из-за всех данных, которые он должен обрабатывать для получения результатов аналитики. Чтобы удовлетворить эту потребность в более быстрой обработке и обеспечении более быстрых результатов, многие организации рассматривают возможность использования базы данных MPP.</p><p>Система MPP использует несколько различных процессоров, которые работают независимо, используя свою собственную выделенную память и ресурсы, поэтому рабочая нагрузка распределяется между несколькими устройствами, а не только одним.</p><p>Обычно система MPP имеет один ведущий узел и один или несколько вычислительных узлов. Узел-лидер, называемый «мастером» в Greenplum, сообщает всем остальным узлам, называемым «сегментами», что делать, и объединяет их ответы для создания окончательного ответа.</p><p>Базы данных MPP масштабируются горизонтально, добавляя дополнительные вычислительные ресурсы (узлы), вместо того, чтобы беспокоиться об обновлении до более дорогих отдельных серверов (вертикальное масштабирование).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/01/image.png" class="kg-image" alt loading="lazy" width="1400" height="800" srcset="__GHOST_URL__/content/images/size/w600/2023/01/image.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/image.png 1000w, __GHOST_URL__/content/images/2023/01/image.png 1400w" sizes="(min-width: 720px) 720px"><figcaption>Архитектура Greenplum</figcaption></figure><p>Основываясь на архитектуре PostgreSQL, Greenplum фактически использует несколько экземпляров базы данных PostgreSQL одновременно в одном кластере. Пользователи PostgreSQL могут быстро ознакомиться с базой данных этого типа, так как многие функции, конфигурации и функциональные возможности аналогичны Greenplum и включают в себя функции, предназначенные для оптимизации работы PostgreSQL для задач и рабочих нагрузок бизнес-аналитики (BI).</p><p>Подобно PostgreSQL, Greenplum использует один главный сервер или хост, который является точкой входа в базу данных, принимающей соединения и SQL-запросы. Сегменты независимы, и каждый из них хранит часть данных. Вы можете использовать всего два хоста сегмента и масштабировать до неограниченной емкости. </p><p>Преимущества Greenplum:</p><ul><li><strong>Высокая производительность </strong>по сравнению с системами в оперативной памяти, которым требуется немало памяти для хранения своих данных, или системами, не основанными на СУБД, которые представляют собой механизмы обработки в памяти, выделяющие ОЗУ для каждого параллельного запроса. Высокая производительность Greenplum устраняет проблему, с которой сталкивается большинство РСУБД при масштабировании до петабайтных уровней данных, поскольку они могут линейно масштабироваться для эффективной обработки данных.</li><li><strong>Оптимизация SQL-запросов</strong> для крупномасштабных рабочих нагрузок с большими данными. Задействовав производительность, как мы уже говорили выше, Greenplum масштабирует аналитику в интерактивном и пакетном режимах до петабайтного масштаба без снижения производительности запросов. Оптимизатор запросов на основе затрат позволяет Greenplum распределять нагрузку между их различными сегментами и использовать все ресурсы системы параллельно для обработки запроса.</li><li><strong>Открытый исходный код:</strong> Greenplum — это проект хранилища данных с открытым исходным кодом, основанный на ядре PostgreSQL с открытым исходным кодом, который позволяет пользователям воспользоваться преимуществами десятилетий экспертной разработки PostgreSQL, а также целенаправленной настройкой Greenplum для приложений с большими данными. Greenplum может работать на любом сервере Linux, размещенном в облаке или локально, и может работать в любой среде.</li><li><strong>Полиморфное (разноструктурное) хранилище данных</strong> позволяет вам контролировать конфигурацию вашего хранилища таблиц и разделов, позволяя выполнять и сжимать файлы в нем в любое время. </li></ul><p>Автор оригинальной статьи: <a href="https://scalegrid.io/blog/what-is-greenplum-database/">ScaleGrid</a></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/image-1.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		greenplum	2023-01-01		
215	Гетероскедастиичность (Heteroscedasticity)		<p>Гетероскедастичность – допущение о "неодинаковости" <a href="__GHOST_URL__/dispiersiia/">Дисперсии (Variance)</a> переменной:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/01/image-2.png" class="kg-image" alt loading="lazy" width="901" height="402" srcset="__GHOST_URL__/content/images/size/w600/2023/01/image-2.png 600w, __GHOST_URL__/content/images/2023/01/image-2.png 901w" sizes="(min-width: 720px) 720px"><figcaption>Размах вариации справа непостоянен при разных значениях Y</figcaption></figure><p>Иными словами, разность между реальным и предсказанным значениями Y, скажем, <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regresion)</a> не остается в определенном известном диапазоне. Такой "разброс" не позволяет в принципе использовать такую <a href="__GHOST_URL__/modiel/">Модель (Model)</a>. Это проблема, потому нарушается базовое предположение о линейной регрессии: все ошибки должны иметь одинаковую дисперсию. </p><p>Самый простой способ узнать, присутствует ли гетероскедастичность, – построить график <a href="__GHOST_URL__/ostatok/">остатков (подробнее здесь</a>):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/image-4.png" class="kg-image" alt loading="lazy" width="504" height="382"></figure><p>Если наблюдения располагаются на графике остатков неравномерно (часто конусообразно), то гетероскедастичность есть.</p><p>Типичные причины гетероскедастичности:</p><ul><li>Большая дисперсия в переменной. Другими словами, когда наименьшее и наибольшее значения в переменной слишком экстремальны. Это также могут быть <a href="__GHOST_URL__/vybros/">Выбросы (Outlier)</a> – наблюдения, сильно удаленные от других.</li><li>Неверный выбор модели: если вы подгоните модель линейной регрессии к нелинейным данным, это приведет к гетероскедастичности.</li><li>Когда данные неправильно преобразованы</li><li>Когда в данных присутствует <a href="__GHOST_URL__/skoshiennost/">Cкошенность (Skewness)</a>:</li></ul><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2023/01/skewness.jpg" class="kg-image" alt loading="lazy" width="2000" height="704" srcset="__GHOST_URL__/content/images/size/w600/2023/01/skewness.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/01/skewness.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2023/01/skewness.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2023/01/skewness.jpg 2400w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="%D1%87%D0%B8%D1%81%D1%82%D0%B0%D1%8F-%D0%B8-%D0%BD%D0%B5%D1%87%D0%B8%D1%81%D1%82%D0%B0%D1%8F-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C"><br>Чистая и нечистая гетероскедастичность</h3><p>Теперь, по вышеуказанным причинам, гетероскедастичность может быть чистой или нечистой. Когда мы подбираем правильную модель (линейную или нелинейную) и если еще есть видимая закономерность в остатках, это называется <em>чистой</em> гетероскедастичностью.</p><p>Однако, если мы подбираем неправильную модель, а затем наблюдаем закономерность в остатках, то это случай <em>нечистой</em> гетероскедастичности. В зависимости от типа гетероскедастичности необходимо принимать меры по ее преодолению. На этот процесс влияет область, в котором мы работаем.</p><h3 id="%D1%8D%D1%84%D1%84%D0%B5%D0%BA%D1%82%D1%8B-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D0%B2-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%BC-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B8">Эффекты гетероскедастичности в машинном обучении</h3><p>Как мы обсуждали ранее, модель линейной регрессии делает предположение о наличии в данных <a href="__GHOST_URL__/gomoskiedastichnost/">Гомоскедастичности (Homoscedascicity)</a>, т.е. равномерно распределенных остатков. Если это предположение будет нарушено, мы не сможем доверять полученным результатам.</p><p>Если присутствует гетероскедастичность, то экземпляры с высокой дисперсией будут иметь большее влияние на прогноз.</p><p>Наличие гетероскедастичности делает коэффициенты менее точными, и, следовательно, правильные коэффициенты находятся дальше от значения генеральной совокупности.</p><h3 id="%D0%BA%D0%B0%D0%BA-%D0%BB%D0%B5%D1%87%D0%B8%D1%82%D1%8C-%D0%B3%D0%B5%D1%82%D0%B5%D1%80%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C">Как лечить гетероскедастичность?</h3><p>Если мы обнаружим наличие гетероскедастичности, то есть несколько способов справиться с ней. Во-первых, давайте рассмотрим пример, в котором у нас есть 2 переменные: население города и количество заражений COVID-19.</p><p>Будет огромная разница в количестве инфекций в крупных мегаполисах по сравнению с небольшими городами. Теперь первым шагом будет выявление источника гетероскедастичности. В нашем случае это переменная с большой дисперсией.</p><p>Существует несколько способов борьбы с гетероскедастичностью, но мы рассмотрим три:</p><ul><li><strong>Манипуляции с переменными</strong>: мы можем внести некоторые изменения в переменные, чтобы уменьшить влияние большой дисперсии на прогнозы. Один из способов – привести значения к процентам, то есть <a href="__GHOST_URL__/normalizatsiia/">нормализовать</a>. В нашем случае, изменим признак «Количество заражений» на «Долю заражений». Это поможет уменьшить дисперсию, так как совершенно очевидно, что количество инфекций в городах с большим населением будет большим.</li><li><strong>Взвешенная регрессия</strong> — модификация обычной регрессии, при которой точкам присваиваются определенные Веса (Weights) в соответствии с их дисперсией. Те, у которых большая дисперсия, получают маленькие веса, а те, у которых меньше дисперсии, получают большие веса.</li><li><strong>Преобразование данных</strong> – последнее средство, так как при этом мы теряем интерпретируемость и не можем легко объяснить, что показывает эта переменная. Яркий пример – Логарифмирование (Log Transformation) – действие, обратное возведению в степень. Логарифм числа x по основанию а это показатель степени, в которую нужно возвести а, чтобы получить x.</li></ul><p>Автор оригинальной статьи: <a href="https://www.upgrad.com/blog/homoscedasticity-in-machine-learning/">Pavan Vadapalli</a></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/image-7.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		gietieroskiedastiichnost-heteroscedasticity	2023-01-15		
216	Регулярное выражение (RegEx)		<p>Регулярное выражение (реджекс, регулярка) – специальное сочетание символов, позволяющее компьютеру отыскивать текст, подходящий заданному набору правил. Регулярки часть используются не только для поиска определенного текста, но и для его обработки. Во время Конструирования признаков (Feature Engineering), то есть выделения определенных данных из исходных и не очень удобных, этот уникальный инструмент часто помогает понять, есть ли в тексте тот или иной смыслонесущий отрезок.</p><p>При первой попытке понять регулярные выражения многим кажется, что это какой-то инопланетный язык. Однако освоение регулярных выражений может сэкономить вам тысячи часов, если вы работаете с текстом или вам нужно анализировать большие объемы данных. Эта система взаимодействия с текстом применяется в BI-аналитике (например, при построении SQL-запроса) и в программировании (например, при переименовании объекта в редакторе VSCode).</p><p>Компания Google вообще создала свой стандарт регулярных выражений (Google Re2) и использует его повсеместно в своих продуктах: непосредственно поисковой системе, базе данных BigQuery, конструкторе чат-ботов Dialogflow и т.д.</p><p>Прекрасно поможет разобраться с устройством RegEx сервис ihateregex.io. Попробуем создать регулярное выражение, которое бы позволило выискивать почты. Вот наш текстовый Корпус (Corpus), с почтами и другими частями:</p><blockquote>Lorem ipsum dolor sit amet, consectetur adipiscing elit, <a>geon@ihateregex.io</a> sed do <a>test@gmail.com</a>  eiusmod tempor incididunt ut labore et <a>mail@testing.com</a> dolore magna aliqua. <a>zenco@gmail.com</a> Ut enim ad minim veniam, quis nostrud <a>yzy.design@gmail.com</a> exercitation ullamco laboris nisi ut aliquip <a>xyxel@bk.ru</a> ex ea commodo consequat. <a>rybina.e@bk.ru</a> aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.</blockquote><p>Первое, что приходит на ум, когда видишь почты – это наличие собачки (@). С него и начнем конструировать регулярку:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.04.19.png" class="kg-image" alt loading="lazy" width="1480" height="730" srcset="__GHOST_URL__/content/images/size/w600/2023/01/--------------2023-01-22---12.04.19.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/--------------2023-01-22---12.04.19.png 1000w, __GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.04.19.png 1480w" sizes="(min-width: 720px) 720px"></figure><p>Стоит помнить, что некоторые символы в мире RegEx интерпретируются нестандартно, однако собачка к ним не относится. Есть небольшой перечень <em>токенов</em> – символов, которые создатели "зарезервировали", чтобы исключать те или иные символы, обозначить диапазон возможных значений и т.д. На этом примере с email мы познакомимся с некоторыми из них.</p><p>Если вам довелось работать с регулярками, а сгенерированное выражение работает криво, не расстраивайтесь. Скорее всего, среда, в которой вы работаете, явно не указывает, какой стандарт RegEx использует, и потому небольшие расхождения путают карты. Только практика позволит преодолеть первичное разочарование от сложности системы. Однако экономия времени при обработке текста может стать прекрасным вознаграждением за усилия.</p><p>Что еще мы знаем о почте? Что после собачки идет доменное имя, состоящее из относительно скромного числа букв и цифр (на самом деле, не более 63). </p><p>Конечно, второй собачки в почте быть уже не может, так что исключим этот символ с помощью токена "исключить" – ^ (<code>^@</code>). Кстати, чтобы внедрить сами символы, зарезервированные под токены, используйте наклонную экранирующую черту (например, <code>\\\\</code>). Помимо собачки в <em>доменном имени второго уровня</em> (той части, где владелец сайта проявляет индивидуальность – gmail, bk, mail, google и т.д.), помимо собачки нельзя использовать еще:</p><ul><li>пробелы (между <code>@</code> и <code>\\t</code> есть еще и пробел!)</li><li>табуляцию (\\t - 'tabulation')</li><li>перенос строки (\\n – 'new line / line feed')</li><li>сдвигание текстового курсора в начало строки (\\r - 'carriage return'). </li></ul><p>Так что учтем все эти четыре вещи и положим их в одну группу с помощью квадратных скобок, запрещающих все, что внутри них. Токен-крышечка, кстати, исключит <em>все</em>, что внутри квадратных скобок:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.44.13.png" class="kg-image" alt loading="lazy" width="1328" height="1272" srcset="__GHOST_URL__/content/images/size/w600/2023/01/--------------2023-01-22---12.44.13.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/--------------2023-01-22---12.44.13.png 1000w, __GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.44.13.png 1328w" sizes="(min-width: 720px) 720px"></figure><p>Обратите внимание, как скромно этот обновленный паттерн <code>[^@ <strong><strong>\\t\\r\\n</strong></strong>]</code><em> </em>захватывает лишь один символ после собачки. А надо-то все до точки! Вот эту логику "все до точки" мы сейчас и добавим. Обратите внимание на серо-зеленую диаграмму в нижней части скриншота: она расшифровывает логику регулярки и здорово помогает.</p><p>С символом <code>+</code> все просто: он значит присоединение чего угодно. В нашем случае, присоединить предстоит точку. Однако есть нюанс: точка на языке RegEx – это обозначение <em>любого символа. Просто вонзив <code>.</code> </em>в наш паттерн, мы обучим отыскивать любой символ, а нам нужна именно точка. Так что экранируем ее, добавим токен "наклонная черта" (<code>\\.</code>):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.38.47.png" class="kg-image" alt loading="lazy" width="1328" height="1248" srcset="__GHOST_URL__/content/images/size/w600/2023/01/--------------2023-01-22---12.38.47.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/--------------2023-01-22---12.38.47.png 1000w, __GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.38.47.png 1328w" sizes="(min-width: 720px) 720px"></figure><p>Неужто поломали регулярку?! Где-то берет, как нам надо, а где-то перебарщивает. Просто настало время добавить паттерн <em>корневого домена</em> (.com, .ru, .net и проч.). К нему применимы те же правила (нет собачек, табуляции, переноса строки, возврата каретки):</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.49.16.png" class="kg-image" alt loading="lazy" width="1328" height="1272" srcset="__GHOST_URL__/content/images/size/w600/2023/01/--------------2023-01-22---12.49.16.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/--------------2023-01-22---12.49.16.png 1000w, __GHOST_URL__/content/images/2023/01/--------------2023-01-22---12.49.16.png 1328w" sizes="(min-width: 720px) 720px"></figure><p>И снова излишняя скромность: паттерн захватил лишь по одному символу после точки. Квадратные скобки исключают всего пять, и захватывают минимальное число (то есть один) символов. Давайте зададим паттерну корневого домена длину. По современным стандартам вместо .net, .com и .ru может быть сочетание аж в 24 символа, но мы возьмем пять. Корневых доменов в один символ тоже не бывает, так что нас интересует диапазон от двух до пяти включительно <code>{2, 5}</code>:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/--------------2023-01-22---13.08.15.png" class="kg-image" alt loading="lazy" width="1328" height="1288" srcset="__GHOST_URL__/content/images/size/w600/2023/01/--------------2023-01-22---13.08.15.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/--------------2023-01-22---13.08.15.png 1000w, __GHOST_URL__/content/images/2023/01/--------------2023-01-22---13.08.15.png 1328w" sizes="(min-width: 720px) 720px"></figure><p>Так-то лучше! Половина пути пройдена, вы тоже вспотели?</p><p>Осталось отловить первую часть до почты. Она может быть очень и очень длинной (почты бывают до 320 символов длиной!), так что закладывать ограничений числа символов не будем. Снова повторим паттерн "без собачек, пробелов, табуляции, переносов строки, каретки" и добавим после него знак плюса:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/--------------2023-01-22---13.12.00.png" class="kg-image" alt loading="lazy" width="1544" height="1194" srcset="__GHOST_URL__/content/images/size/w600/2023/01/--------------2023-01-22---13.12.00.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/--------------2023-01-22---13.12.00.png 1000w, __GHOST_URL__/content/images/2023/01/--------------2023-01-22---13.12.00.png 1544w" sizes="(min-width: 720px) 720px"></figure><p>Вуаля! Даже те почты с точками выделены верно, как здорово! Попробуем смутить нашу регулярку и добавить почтоподобного мусора (<code>@test</code>, <code>email@gmail</code>) в наш текст:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/--------------2023-01-22---13.14.05.png" class="kg-image" alt loading="lazy" width="1544" height="1250" srcset="__GHOST_URL__/content/images/size/w600/2023/01/--------------2023-01-22---13.14.05.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/--------------2023-01-22---13.14.05.png 1000w, __GHOST_URL__/content/images/2023/01/--------------2023-01-22---13.14.05.png 1544w" sizes="(min-width: 720px) 720px"></figure><p>Паттерн доказал свою жизнеспособность, и это прекрасно. Если вы почувствовали себя в мире RegEx увереннее, попробуйте потренироваться и добавить группе доменного имени второго уровня ограничение по длине (2 - 63 символа). Тогда паттерн станет еще совершеннее и сможет отличить почты от других длинных частей текста.</p><p>Что поделать еще? Удобочитаемый перечень спецсимволов – токенов можно найти в разделе Quick Reference еще одного онлайн-сервиса <a href="https://regex101.com/">regex101.com</a>.  Там вы узнаете, как искать только буквы алфавита, числа, НЕчисла, любое сочетание любой длины между символами и т.д. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/image-8.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		rieghuliarnoie-vyrazhieniie	2023-01-22		
217	Средняя абсолютная ошибка (MAE)		<p>Средняя абсолютная ошибка (англ. Mean Absolute Error) – это степень несоответствия между фактическими и прогнозируемыми значениями. Вычисляется по формуле:</p><!--kg-card-begin: markdown--><p>$$MAE = \\frac{Σ|\\space{Реальное}\\space{значение}-\\space{Прогнозируемое}\\space{значение}|}{n}$$<br>\n$$n\\space{}{–}\\space{Число}\\space{наблюдений}$$</p>\n<!--kg-card-end: markdown--><p><em>Абсолютная</em> ошибка представляет собой разность между спрогнозированным и фактическим значениями. MAE — это среднее от таких ошибок, что помогает понять эффективность <a href="__GHOST_URL__/modiel/">Модели (Model)</a>.</p><p>MAE – весьма популярная метрика, поскольку значение ошибки легко интерпретируется, а не конвертировано в проценты или какие-либо другие единицы измерения.</p><p>Чем ближе MAE к нулю, тем точнее модель. Но MAE возвращается в том же масштабе значений, что и исходные данные. Однако для универсальности порой рассчитывают Среднюю абсолютную ошибку в процентах (MAPE). </p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80">Пример</h3><p>Давайте рассмотрим таблицу реального и предсказанного роста людей:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/01/image-9.png" class="kg-image" alt loading="lazy" width="906" height="844" srcset="__GHOST_URL__/content/images/size/w600/2023/01/image-9.png 600w, __GHOST_URL__/content/images/2023/01/image-9.png 906w" sizes="(min-width: 720px) 720px"></figure><p>Суммируем разности между реальным и предсказанным ростом и разделим на число <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>, т.е. семь:</p><!--kg-card-begin: markdown--><p>$$MAE = \\frac{8 + 20 + 5 + 2 + 3 + 6 + 10}{7} ≈ 7.71$$</p>\n<!--kg-card-end: markdown--><p>Средняя ошибка составляет около 7,71, что является хорошим значением, учитывая, что средний фактический рост составляет 170.</p><p>Автор оригинальной статьи: <a href="https://stephenallwright.com/interpret-mae/">Stephen Allwright</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/01/image-10.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		sriedniaia-absoliutnaia-oshibka	2023-01-29		
218	GPT2		<p>GPT2 (англ. Generative Pretrained Transformer – Генеративный предварительно обученный преобразователь) – это популярная модель <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокого обучения (Deep Learning)</a>, позволяющая генерировать текст на основе предложения-тезиса. Относится к группе <a href="__GHOST_URL__/transformiery/">Трансформеров (Transformer)</a>.</p><p>По мере того как модели-трансформеры становились лучше и приближались к созданию текста, который может сойти за человеческий текст, их обучающие наборы данных также выросли в размере. Например, базы TransformerXL, BERT-Base выросли до 340 миллионов параметров (слов как характеристик текста), BERT-Large – до 175 миллиардов параметров. Царь горы сегодня – модель Microsoft Megatron-Turing Natural Language Generation (MT-NLG) с 530 миллиардами параметров.</p><p>Хотя преобразователи широко используются в <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработке естественного языка (NLP)</a>, это не единственный вариант их использования. Трансформаторы были адаптированы для использования в секвенировании белков и ДНК, обработке изображений и видео и многих других сферах.</p><p>Обучение огромной современной модели-трансформера сопряжено с затратами, которые исчисляются десятками миллионов долларов, огромным потреблением энергии и экологическими эффектом. Это создает огромное препятствие на пути небольших предприятий и стартапов, которые хотят запустить свои собственные проекты, что и привело к появлению Hugging Face – предварительно обученных моделей.</p><h3 id="%D1%87%D1%82%D0%BE-%D1%82%D0%B0%D0%BA%D0%BE%D0%B5-hugging-face">Что такое Hugging Face?</h3><p>Всего за несколько лет трансформеры сообщества Hugging Face зарекомендовали себя как надежный поставщик NLP-решений. Hugging Face — это стартап, который изначально был основан как приложение для обмена сообщениями.</p><p>Теперь, сосредоточившись исключительно на трансформерах, компания предоставляет технологии NLP с открытым исходным кодом и тысячи предварительно обученных моделей. HF также предоставляет курсы, наборы данных и имеет большое сообщество. В 2019-2021 компания привлекла 55 миллионов долларов в виде венчурного финансирования.</p><h3 id="%D1%8D%D0%BA%D0%BE%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0-hugging-face">Экосистема Hugging Face</h3><p>Модели-трансформеры <em>не понимают</em> текст и потому преобразуют его в Векторы (Vector), <a href="__GHOST_URL__/matritsa/">Матрицы (Matrix)</a> и <a href="__GHOST_URL__/tenzor/">Тензоры (Tensor)</a>:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/02/scalar-vector-matrix-tensor.png" class="kg-image" alt loading="lazy" width="1008" height="357" srcset="__GHOST_URL__/content/images/size/w600/2023/02/scalar-vector-matrix-tensor.png 600w, __GHOST_URL__/content/images/size/w1000/2023/02/scalar-vector-matrix-tensor.png 1000w, __GHOST_URL__/content/images/2023/02/scalar-vector-matrix-tensor.png 1008w" sizes="(min-width: 720px) 720px"></figure><p>Поскольку речь идет о миллиардах столбцов для каждого слова, такой модели нужны немалые вычислительные мощности. Здесь нам помогает PyTorch и его CUDA (англ. Compute Unified Device Architecture) – архитектура для использования процессоров Nvidia.</p><h3 id="gpt2-%D0%B2-%D0%B4%D0%B5%D0%B9%D1%81%D1%82%D0%B2%D0%B8%D0%B8">GPT2 в действии</h3><p>Теперь, когда мы рассмотрели, что такое экосистема Hugging Face, сгенерируем текст с помощью GPT2. Хотя на смену GPT2 пришла GPT3, она по-прежнему остается мощным представителем своего класса. Для начала импортируем небходимые библиотеки:</p><pre><code class="language-python">!pip install torch</code></pre><p>И сама библиотека Transformers:‍</p><pre><code class="language-python">!pip install git+https://github.com/huggingface/transformers.git</code></pre><p>Импортируем установленные инструменты, а также встроенные дополнительные библиотеки. <code>random</code> и <code>numpy</code>  помогут добиться воспроизводимости результата:</p><pre><code class="language-python">import torch\nimport transformers\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel \nimport random\nimport numpy as np</code></pre><p>Инициируем экземпляр модели, которая сгенерирует нам небольшое сочинение на базе одного предложения. <code>GPT2Tokenizer</code> – это так называемый токенизатор, который разбивает<strong> </strong>текст на отдельные слова, сочетания слов и знаки препинания. <a href="__GHOST_URL__/token/">Токен (Token)</a> – это именно такой неделимый элемент текста. </p><p>Для предложения "What you don't want to be done to yourself, don't do to others" токенами будут: <code>what; you; do; n't; want; to; be; done; yourself; others</code> (10 штук).</p><pre><code class="language-python">gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2-large")\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large")</code></pre><p><code>random.manual_seed(seed)</code> даст нам генератор случайных чисел с заданными настройками <code>seed</code>. Переменная <code>seed</code>, в свою очередь, содержит в себе  случайное число от 0 до 13. <code>cuda.manual_seed(seed)</code> помогает графическому процессору принять настройки генератора случайных чисел. С помощью <code>torch.device()</code>  включим CUDA, чтобы сделать модель мощнее и быстрее.</p><pre><code class="language-python">seed = random.randint(0, 13)\nnp.random.seed(seed)\ntorch.random.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")</code></pre><p>Инициируем блок текста – опору для сочинения GPT2:</p><pre><code class="language-python">text = """All of this is right here, ready to be used in your favorite pizza recipes."""</code></pre><p>Разложим на токены это предложение с <code>gpt2_tokenizer.encode()</code>. Параметр <code>add_special_tokens=True</code> добавит специальные токены – знаки препинания, а также позволит собирать токены в равные бэтчи-коллекции:</p><pre><code class="language-python">input_ids = torch.tensor(gpt2_tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)</code></pre><p>"Выдадим" нашей модели "производственные мощности", то есть графический процессор с CUDA, и сгенерируем эссе с помощью <code>generate()</code>. Возможно, вы уже догадались, что <code>max_length=500</code> ограничивает длину эссе до 500 символов, но что же значат остальные параметры? </p><p><code>do_sample=True, top_k-20</code>: в данном контексте сэмплинг – это подстановка ряда возможных слов после каждого использованного. Как только модель напишет эссе, мы увидим, как она выбирает каждое новое слово.</p><p>Чем выше значение параметра <code>temperature</code>, тем реже модель будет "уходить в себя" и повторять предложения в сочинении. </p><pre><code class="language-python">gpt2_model.to(device)\n\noutputs = gpt2_model.generate(input_ids.to(device), max_length=500, do_sample=True, top_k=20, temperature=0.7)</code></pre><p>Выведем результат генерации текста:</p><pre><code class="language-python">print(gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True))</code></pre><p>Вот такое забавное эссе нам сгенерировала GPT2:</p><pre><code class="language-python">All of this is right here, ready to be used in your favorite pizza recipes. In the video above you'll see how to make a pizza crust without using oil, butter, or cheese. You'll also see how to make the perfect pizza crust without adding any toppings. If you want to make a pizza that's extra good, try our recipe for the BEST PIZZA EVER! Watch the video to see how to make a pizza crust without oil, butter, or cheese.\nYou'll also hear how to make a pizza crust that's extra good, plus how to make a pizza crust that's extra good with extra toppings! So, what's the difference between pizza crusts made without oil and pizza crusts made with oil and butter? \n\nHere's what a pizza crust without oil looks like: Here's what a pizza crust made with oil looks like: Here's what a pizza crust made with butter looks like: \n\nIf you want to make a pizza crust without using oil, butter, or cheese, you'll need to use a pizza pan with a 9×13 or larger dough pan. If you want to make a pizza crust with oil, butter, or cheese, you'll need to use a 10×13 or larger dough pan.\n\nHow to Make the Perfect Pizza Crust without Oil, Butter, or Cheese \nHere's how to make a pizza crust without oil, butter, or cheese.\n\n1- Using a pizza pan with a 9×13 or larger dough pan, make a round pie crust.\n2- Cut a small hole in the bottom of the pan so the crust doesn't stick when you're baking it.\n3- Cover the bottom of the pan with a paper towel to let the air out.\n4- Brush the bottom of the pan with a little olive oil.\n5- Place the pan on a baking sheet and bake for 10 minutes.\n6- Remove the pan from the oven and let it cool before slicing.\n7- While the crust is cooling, chop up the vegetables and herbs and mix them into your pizza crust.\n8- Take the dough out of the pan and cut it into small pieces.\n9- Place the small pieces on a baking sheet and cover with a paper towel.\n10- Bake for 10 minutes.</code></pre><p>Набравшись текстов из интернета, модель определяет, какие слова чаще всего используются после того или иного слова (вероятности в примере ниже рандомные):</p><ul><li>video → to (.9), from (.4), bottom (.3)</li><li>crust → is (.8), without (.6), does (.7), vegatables (.1)</li></ul><p>Вот так модель, не понимающая сути текста, и делает выбор о следующем слове.</p><p>Обратите внимание: в середине эссе модель "ушла в себя" и повторила некоторые предложения по несколько раз. Это исправляется подбором значения<code>temperature</code>. Попробуйте запустить модель с любым другим англоязычным предложением в переменной <code>text</code>, и вы удивитесь, как легко сочинять посредством ML.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/19xfXl6ocinDp2co_F5bGYweRze_r2o9K?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.qwak.com/post/an-introduction-to-hugging-face-transformers-for-nlp">qwak.com</a>, <a href="https://dejanbatanjac.github.io/gpt2-example/">Mostly on AI</a></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/02/image.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		transformers	2023-02-04		
219	Трансформеры (Transformers)		<p>Трансформер (англ. Transformers – преобразователь) — это разновидность <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a>, которая использует концепцию Внимания (Attention) и эффективнее предшественников обрабатывает массивы данных с множеством <a href="__GHOST_URL__/priznak/">Признаков (Feature)</a>. Популярными представителями класса Трансформер являются:</p><ul><li><a href="__GHOST_URL__/bert/">ДКПТ (BERT)</a></li><li><a href="__GHOST_URL__/transformers/">GPT</a></li><li>XLNet</li></ul><p>Еще говорят, что такие нейросети полагаются на собственное Внимание – новый термин в контексте <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>.  Что же такое внимание в контексте <a href="__GHOST_URL__/obrabotka-iestiestviennogho-iazyka/">Обработки естественного языка (NLP)</a>? Оно позволяет сосредоточиться на элементах входных данных, пока мы предсказываем следующее слово. Например, если наша модель сочиняет эссе и использует слово «румяна», высока вероятность, что за ним последует слова «красные». </p><p>Проще говоря, внимание помогает нам создавать подобные связи в пределах одного предложения. Посмотрите на следующий пример:</p><p>«Я наливал воду из бутылки в чашку, пока она не наполнилась».<br>это =&gt; чашка<br>«Я наливал воду из бутылки в чашку, пока она не опустела».<br>это =&gt; бутылка</p><p>Изменив слово «наполнилась» на «опустела», значение слова «она» изменилось. Если мы переводим такое предложение, нам нужно знать, к чему относится слово «оно».</p><p>По сравнению с последовательными моделями, например, <a href="__GHOST_URL__/riekurrientnaia-nieirosiet/">Рекуррентной нейросетью (RNN)</a>, трансформеры обеспечивают лучшие результаты при более эффективном использовании доступной вычислительной мощности. Архитектура Transformer также позволяет модели использовать преимущества мощных процедур параллельной обработки, доступных в графических процессорах, которые все чаще используются для приложений обучения НЛП.</p><p>До появления трансформеров приходилось обучать нейронные сети с помощью больших дорогостоящих размеченных <a href="__GHOST_URL__/dataset/">Датасетов (Dataset)</a>. Находя закономерности между элементами математически, трансформеры устраняют эту необходимость, делая доступными для Машинного обучения триллионы изображений и петабайты текстовых данных в сети.</p><p>Кроме того, математика, которую используют трансформеры, поддается параллельной обработке, поэтому эти модели могут работать быстро.</p><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/transformers-89034557de14">Ria Kulshrestha</a><br></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/02/image-1.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		transformiery	2023-02-08		
220	Экстремальный градиентный бустинг (XGBoost)		<p>XGBoost — это opensource-библиотека, обеспечивающая высокопроизводительную реализацию <a href="__GHOST_URL__/dierievo-rieshienii/">Деревьев решений (Decision Tree)</a>. </p><p>В этой статье узнаем, как работает <a href="__GHOST_URL__/gradiientnyi-bustingh-2/">Градиентный бустинг (GB)</a>, а затем рассмотрим пример на Python.</p><p>В обычном <a href="__GHOST_URL__/mashinnoie-obuchieniie/" rel="noopener noreferrer">Машинном обучении (ML)</a>, таком как дерево решений, мы просто обучаем <a href="__GHOST_URL__/modiel/" rel="noopener noreferrer">Модель (Model)</a> на наборе данных и используем ее для прогнозирования:</p><figure class="kg-card kg-image-card"><img src="https://avatars.dzeninfra.ru/get-zen_doc/1703615/pub_63f130774802ec70030063d4_63f13088cc3e826ad1360de4/scale_1200" class="kg-image" alt loading="lazy" width="408" height="276"></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/02/image-3.png" class="kg-image" alt loading="lazy" width="408" height="276"><figcaption>Дерево решений – выдача кредита</figcaption></figure><p>Мы можем немного поэкспериментировать с параметрами или дополнить данные, но в итоге мы по-прежнему используем ту же модель. Даже если мы строим Ансамбль (Ensemble) – комбинацию моделей, все модели обучаются по отдельности.</p><p>Бустинг же использует <em>итеративный</em> подход: технически это по-прежнему ансамбль, но при этом каждая новая модель обучается исправлять ошибки, допущенные предыдущими. Модели добавляются последовательно до тех пор, пока дальнейшие улучшения не станут невозможными.</p><blockquote>Gradient Boosting — это подход, при котором новые модели обучаются прогнозировать ошибки предыдущих моделей. </blockquote><p>Давайте посмотрим на XGBoost в деле. Для начала установим импортируем ее и другие необходимые библиотеки:</p><pre><code class="language-python">!pip install xgboost</code></pre><pre><code class="language-python">from sklearn import datasets\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score</code></pre><p>Мы будем использовать известный <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> цветов ириса:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/02/51518iris-img1.png" class="kg-image" alt loading="lazy" width="1000" height="447" srcset="__GHOST_URL__/content/images/size/w600/2023/02/51518iris-img1.png 600w, __GHOST_URL__/content/images/2023/02/51518iris-img1.png 1000w" sizes="(min-width: 720px) 720px"><figcaption>Виды ирисов и характеристики их цветков</figcaption></figure><pre><code class="language-python">iris = datasets.load_iris()\nX = iris.data\ny = iris.target</code></pre><p><a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевая переменная (Target Variable)</a> <code>y</code> – это числовое обозначение одного из трех видов ириса – Setosa (0), Versicolor (1) или Virginica (2). Разделим набор на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>, чтобы проверить качество обучения XGBoost. Тестовые данные составят 20% (0.2):</p><pre><code class="language-python">X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)</code></pre><p>Чтобы XGBoost мог использовать наши данные, нам нужно преобразовать их в определенный формат – DMatrix:</p><pre><code class="language-python">D_train = xgb.DMatrix(X_train, label=Y_train)\nD_test = xgb.DMatrix(X_test, label=Y_test)</code></pre><p>Теперь мы можем определить параметры нашего ансамбля:</p><pre><code class="language-python">param = {\n    'eta': 0.3, \n    'max_depth': 3,  \n    'objective': 'multi:softprob',  \n    'num_class': 3} \n\nsteps = 20</code></pre><p>Самыми простыми параметрами являются:</p><ul><li><code>max_depth</code>: максимальная глубина обучаемых деревьев решений</li><li><code>objective: 'multi:softprob'</code>: каждому наблюдению присваивается определенная <em>вероятность</em> принадлежности к каждому классу.</li><li><code>num_class</code>: количество классов в наборе данных</li></ul><p><code>steps</code> – количество этапов обучения. <code>eta</code> же требует особого внимания. </p><p>Согласно нашей теории, Gradient Boosting включает в себя последовательное создание и добавление деревьев решений. Новые деревья создаются для исправления остаточных ошибок.</p><p>Из-за характера ансамбля, т. е. нескольких моделей, объединенных в одну очень большую сложную модель, этот метод подвержен <a href="__GHOST_URL__/pierieobuchieniie/">Переобучению (Overfitting)</a>. <code>eta</code> дает нам шанс предотвратить это.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/02/image-7.png" class="kg-image" alt loading="lazy" width="1157" height="522" srcset="__GHOST_URL__/content/images/size/w600/2023/02/image-7.png 600w, __GHOST_URL__/content/images/size/w1000/2023/02/image-7.png 1000w, __GHOST_URL__/content/images/2023/02/image-7.png 1157w" sizes="(min-width: 720px) 720px"><figcaption>Недо-, пере- и сбалансированное обучения</figcaption></figure><p><code>eta</code> похожа на скорость обучения: вместо того, чтобы просто добавлять прогнозы новых деревьев в ансамбль, ансабль будет учитывать ошибки предыдущих деревьев, то есть назначать Веса (Weights) до 0,3.</p><p>Наконец-то мы можем обучить нашу модель:</p><pre><code class="language-python">model = xgb.train(param, D_train, steps)</code></pre><p>Давайте теперь проведем оценку:</p><pre><code class="language-python">preds = model.predict(D_test)\nbest_preds = np.asarray([np.argmax(line) for line in preds])\n\nprint("Точность результата измерений (Precision): {}".format(precision_score(Y_test, best_preds, average='macro')))\nprint("Отзыв (Recall): {}".format(recall_score(Y_test, best_preds, average='macro')))\nprint("Точность измерений (Accuracy): {}".format(accuracy_score(Y_test, best_preds)))</code></pre><p>Мы получили довольно высокие показатели, в реальном мире эффективные на 97%+ модели встречаются редко:</p><pre><code class="language-python">Точность результата измерений (Precision): 0.9722222222222222\nОтзыв (Recall): 0.9629629629629629\nТочность измерений (Accuracy): 0.9666666666666667\n</code></pre><p>Автор оригинальной статьи: <a href="https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7">George Seif</a></p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/19TeZPtRABODFyqVrP4D1g7uDK3A3KW4L?usp=sharing">здесь</a>.<br></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/02/image-2.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		ekstremalnyy-gradiyentnyy-busting	2023-02-18		
221	Двусторонний тест (TTT)		<p>Двусторонний тест (англ. Two-Tailed Test, TTT) – метод проверки <a href="__GHOST_URL__/vyborka/">Выборки (Sample)</a> на принадлежность определенному интервалу значений. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/02/image-8.png" class="kg-image" alt loading="lazy" width="1473" height="900" srcset="__GHOST_URL__/content/images/size/w600/2023/02/image-8.png 600w, __GHOST_URL__/content/images/size/w1000/2023/02/image-8.png 1000w, __GHOST_URL__/content/images/2023/02/image-8.png 1473w" sizes="(min-width: 720px) 720px"><figcaption>Критический регион: 2,5% с каждой стороны</figcaption></figure><p>Он используется при проверке <a href="__GHOST_URL__/nulievaia-ghipotieza/">Нулевой гипотезы (Null Hypothesis)</a> и проверке Статистической значимости (Statistical Significance): если проверяемая выборка попадает в одну из критических областей, вместо нулевой гипотезы принимается <a href="__GHOST_URL__/altiernativnaia-ghipotieza/">Альтернативная (Alternative Hypothesis)</a>.</p><p>Нулевая гипотеза: среднестатистическое количество уничтоженных мылом микробов равно 99%.<br>Альтернативная: Мыло в среднем уничтожает менее 99% процентов микробов.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2021/03/null-hypothesis_definitive_definitive.png" class="kg-image" alt loading="lazy" width="1000" height="588" srcset="__GHOST_URL__/content/images/size/w600/2021/03/null-hypothesis_definitive_definitive.png 600w, __GHOST_URL__/content/images/2021/03/null-hypothesis_definitive_definitive.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>Двусторонний тест – это сравнение среднего значения выборки со средним <a href="__GHOST_URL__/gienieralnaia-sovokupnost/">Генеральной совокупности (Population) </a>– всех имеющихся для исследования значений. Любая точка данных, которая находится вне заданных предела, называется диапазоном отклонения.</p><p>Не существует стандарта в отношении количества <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a>, которые находятся в допустимом диапазоне. Например, при создании фармацевтических препаратов, может быть установлен коэффициент в 0,001% или меньше. В случаях, когда точность менее критична, например, количество продуктов в среднем пакете супермаркета, может быть уместным уровень в 5%.</p><p>Двусторонний тест также можно использовать для контроля производства: если кондитерская фабрика определяет своей целью 50 конфет в пакете с приемлемым распределением от 45 до 55 конфет, любой пакет, где конфет менее 45 или более 55, считается отклонением и браком.</p><p>Чтобы механизмы упаковки считались точными, желательно в среднем 50 конфет в упаковке с соответствующим распределением. Кроме того, количество мешков, попадающих в диапазон отбраковки, должно находиться в пределах предела распределения вероятностей, который считается приемлемым в качестве коэффициента ошибок. Здесь нулевая гипотеза будет заключаться в том, что среднее значение равно 50, а альтернативная гипотеза будет заключаться в том, что оно не равно 50.</p><p>Если после проведения двустороннего теста z-показатель попадает в область отклонения, что означает, что отклонение слишком далеко от желаемого среднего значения, то для исправления ошибки может потребоваться корректировка установки или связанного с ней оборудования. Регулярное использование двусторонних методов тестирования может помочь гарантировать, что производительность останется в пределах допустимых пределов в долгосрочной перспективе.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80-ttt">Пример TTT</h3><p>Представим, что новый брокер Гамма-инвестиции утверждает, что его комиссия ниже, чем у вашего текущего брокера, Тета-инвестиции. Данные, полученные от независимой исследовательской фирмы, показывают, что <a href="__GHOST_URL__/sriednieie-znachieniie/#-">Среднее значение (Mean)</a> и <a href="__GHOST_URL__/standartnoie-otklonieniie/">Стандартное отклонение (Standard Deviation)</a> – мера разброса значений в наборе всех клиентов Тета, равна $18 и $6 соответственно.</p><p>Берется выборка из 100 тета-клиентов, и брокерские сборы рассчитываются с новыми ставками брокера Гамма. Если среднее значение выборки составляет 18,75 доллара, а стандартное отклонение выборки равно $6, можно ли сделать вывод о разнице в среднем счете между брокерами?</p><ul><li>H<sub>0</sub> (Нулевая гипотеза): среднее равно 18</li><li>H<sub>1</sub> (Альтернативная гипотеза): среднее вне диапазона 18 (это то, что мы хотим доказать)</li><li>Стандартное отклонение (критический регион): 2,5% с каждой стороны</li><li>Стандартизованная оценка (Z-Score) – метрика, характеризующая удаленность наблюдения от среднего значения генеральной совокупности. Рассчитывается следующим образом: </li></ul><p>(среднее значение выборки - среднее значение гипотезы) / (стандартное отклонение / кв. корень (количество наблюдений)) = (18,75 - 18) / (6 / (кв. корень (100)) = 1,25</p><p>Это расчетное значение Z<sub>2,5</sub> находится между двумя пределами: -1,96 и 1,96.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/02/image-9.png" class="kg-image" alt loading="lazy" width="750" height="355" srcset="__GHOST_URL__/content/images/size/w600/2023/02/image-9.png 600w, __GHOST_URL__/content/images/2023/02/image-9.png 750w" sizes="(min-width: 720px) 720px"></figure><p>Нет достаточных доказательств, что существует какая-либо существенная разница между комиссиями брокеров. Следовательно, нулевая гипотеза не может быть отвергнута. </p><h3 id="%D0%B4%D0%B2%D1%83%D1%81%D1%82%D0%BE%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D0%B9-%D1%82%D0%B5%D1%81%D1%82-scipy">Двусторонний тест: SciPy</h3><p>Давайте посмотрим, как TTT реализован в SciPy. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np\nfrom scipy import stats</code></pre><p>Вызовем встроенный метод <code>default_rng()</code> (англ. default random generator – генератор случайных чисел по умолчанию), чтобы в дальнейшем сгенерировать два набора случайных значений:</p><pre><code class="language-python">rng = np.random.default_rng()</code></pre><p>Создадим две случайные переменные <code>rvs1</code> и <code>rvs2</code> (<strong>r</strong>andom <strong>v</strong>ariable<strong>s</strong>) с помощью метода <code>norm.rvs()</code>. Параметр <code>loc=5</code> (location – локация) здесь – среднее значение, равное пяти. <code>scale=10</code> – стандартное отклонение (мера разброса), равное десяти. <code>size=500</code> – размер выборки, равный 500.<code>random_state=rng</code> – это способ генерирования случайных значений, необходимый для воспроизводимости эксперимента.</p><p>Выполним TTT, вызвав <code>ttest_ind()</code>:</p><pre><code class="language-python">rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\nrvs2 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\nstats.ttest_ind(rvs1, rvs2)</code></pre><p>Поскольку среднее значение и среднее отклонения равны, и настройка генератора случайных значений тоже, то значение метрики двустороннего теста <code>statistic</code> стремится к единице (~0.99). <code>pvalue</code> – это вероятность появления экстремального наблюдения в интервале от нуля до единицы.</p><pre><code class="language-python">Ttest_indResult(statistic=0.9946875233274011, pvalue=0.3201293684785449)</code></pre><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1oHoTkKFqZx54vROg2GOTAGl9jd-RPOTQ?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.investopedia.com/terms/t/two-tailed-test.asp">Adam Hayes</a></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/02/image-10.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		dvustoronnii-tiest	2023-02-26		
222	Коррелограмма (Correlogram)		<p>Коррелограмма (график функции автокорреляции) — это визуальный способ показать последовательную <a href="__GHOST_URL__/korrieliatsiia/">Корреляцию (Correlation)</a> – взаимосвязь данных, которые меняются с течением времени. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/03/image.png" class="kg-image" alt loading="lazy" width="1280" height="1705" srcset="__GHOST_URL__/content/images/size/w600/2023/03/image.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/image.png 1000w, __GHOST_URL__/content/images/2023/03/image.png 1280w" sizes="(min-width: 720px) 720px"><figcaption>Временной ряд (сверху) и его коррелограмма (снизу)</figcaption></figure><p><a href="__GHOST_URL__/chastichnaia-avtokorrieliatsiia/">Частичная автокорреляция (Partial Autocorrelation)</a> – это краткая характеристика взаимосвязи между <a href="__GHOST_URL__/nabliudieniie/">Наблюдением (Observation)</a> во <a href="__GHOST_URL__/vriemiennoi-riad/">Временном ряду (Time Series)</a> и наблюдениями на предыдущем отрезке времени.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-3.png" class="kg-image" alt loading="lazy" width="579" height="420"></figure><p>Ось Y на графике выше отвечает за автокорреляцию. Ось x показывает задержку. Итак, если <code>x = 1</code>, мы рассматриваем корреляцию декабря с ноябрем, ноября с октябрем и т. д. Если <code>x = 2</code>, у нас есть задержка в две единицы, и мы рассматриваем корреляцию декабря с октябрем, ноября с сентябрем, и т. д.</p><p>На этом графике выше видны высокие положительные корреляции, которые медленно снижаются с увеличением лагов. Это указывает на большую автокорреляцию, и вам нужно будет принять это во внимание при моделировании.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-4.png" class="kg-image" alt loading="lazy" width="584" height="421"></figure><p>На втором же графике корреляция очень низкая: значения по оси лежат в интервале [-0,10, +0,10]. При сильной корреляции значения находятся в интервалах [-1, -0.7], [+0.7, +1]. Закономерности нет.</p><p>Серые области — это доверительные интервалы (они говорят, является ли корреляция статистически значимой).</p><p>Коррелограммы нельзя использовать для измерения того, насколько велика эта автокорреляция.</p><p>С примером расчета автокорреляции и построения таких графиков с помощью <code>statsmodels</code> можно познакомиться в <a href="statsmodels">этой статье</a>.</p><p>Автор оригинальной статьи: <a href="https://www.statisticshowto.com/correlogram/">statisticshowto.com</a></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-5.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		korrieloghramma	2023-03-06		
223	Хранилище данных (DWH)		<p>Хранилище данных (DWH – Data Warehouse) — это система управления данными, предназначенный для Бизнес-аналитики (BI). Хранилища данных предназначены исключительно для выполнения запросов и часто содержат большие объемы исторических данных. Данные в хранилище данных обычно поступают из широкого круга источников, таких как логи приложений и приложения для данные учетных систем.</p><p>Хранилище данных централизует и объединяет большие объемы данных из нескольких источников. Его аналитические возможности позволяют организациям извлекать ценную информацию из своих данных для улучшения процесса принятия решений. Со временем он создает историческую запись, которая может оказаться бесценной для специалистов по обработке и анализу данных и бизнес-аналитиков. Благодаря этим возможностям хранилище данных можно считать «единым источником достоверной информации» организации.</p><p>Типичное хранилище данных часто включает следующие элементы:</p><ul><li>Реляционная база данных</li><li>Решение для <a href="преобразования">Извлечения, преобразования и загрузки (ETL)</a> данных</li><li>Инструменты статистического анализа</li><li>Инструменты визуализации данных</li><li>Другие, более сложные аналитические приложения, которые генерируют полезную информацию, применяя алгоритмы <a href="__GHOST_URL__/nauka-o-dannykh/">Науки о данных (DS)</a> и <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a></li></ul><p>К популярным хранилищам можно отнести:</p><ul><li>Amazon Redshift</li><li>Snowflake</li><li>Google Cloud BigQuery</li><li>Vertica</li><li>Greenplum</li></ul><p>Стоит отличать DWH от так называемого Озера данных (Data Lake). Хранилище данных содержит очищенные, обработанные и структурированные данные, готовые к анализу на основе предопределенных потребностей бизнеса. Во втором содержатся все данные организации в необработанном, неструктурированном виде и храниться они могут неограниченно долго. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-7.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		khranilishche-dannykh	2023-03-11		
224	Сходимость (Convergence)		<p>Cходимость (конвергенция) — это точка обучения модели, после которой изменения в Скорости обучения (Learning Rate) становятся ниже, а <a href="__GHOST_URL__/oshibka/">Ошибки (Error)</a> – разность между предсказанным и реальным значениями, сводятся к минимуму:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2023/03/image-8.png" class="kg-image" alt loading="lazy" width="1440" height="810" srcset="__GHOST_URL__/content/images/size/w600/2023/03/image-8.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/image-8.png 1000w, __GHOST_URL__/content/images/2023/03/image-8.png 1440w" sizes="(min-width: 1200px) 1200px"></figure><p>Скорость обучения — это гиперпараметр, который определяет, насколько сильно следует изменять модель в ответ на предполагаемую ошибку при каждом обновлении модели.</p><p>Противоположным сходимости явлением называют Несходимость (Divergence) – ситуация, при которой ошибка не стремится к минимуму при уменьшении скорости обучения.</p><p>Конвергенция может быть двух типов: глобальная или локальная, и она должна происходить с нисходящей тенденцией. Однако в различных процедурах моделирования очень редко можно увидеть, что модель сходится очень строго.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/03/image-10.png" class="kg-image" alt loading="lazy" width="527" height="503"><figcaption>Ранняя сходимость</figcaption></figure><p>Если мы говорим, что потеря равна нулю, то ряд, который мы называем сходящимся, является бесконечным: </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/03/image-11.png" class="kg-image" alt loading="lazy" width="628" height="556" srcset="__GHOST_URL__/content/images/size/w600/2023/03/image-11.png 600w, __GHOST_URL__/content/images/2023/03/image-11.png 628w"><figcaption>Бесконечный ряд: кривая ошибок асимптотична оси x и никогда не пересечет ее.</figcaption></figure><p>Потеря, равная нулю, — это идеальное условие, которого нельзя достичь, однако после сходимости скорость обучения может продолжать уменьшаться.</p><p>Изображение выше – пример сходимости: обучение модели после двадцатой итерации становится сходящимся, а ошибки после этой итерации уменьшаются и держатся в меньшем диапазоне.</p><p>Сходимость важна, поскольку характеризует успешность обучения</p><h3 id="%D0%BF%D1%80%D0%B8%D1%87%D0%B8%D0%BD%D1%8B-%D0%BD%D0%B5%D1%81%D1%85%D0%BE%D0%B4%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D0%B8">Причины несходимости</h3><p>Проще говоря, мы можем думать о сбое конвергенции как о состоянии, при котором мы не можем найти точку конвергенции на кривой обучения нейронной сети. Это прямо означает, что на кривой нет такой точки, которую можно было бы представить как начальную точку снижения и уменьшения ошибки. Мы можем понять сбой в конвергенции, взглянув на изображение ниже.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/03/image-12.png" class="kg-image" alt loading="lazy" width="834" height="509" srcset="__GHOST_URL__/content/images/size/w600/2023/03/image-12.png 600w, __GHOST_URL__/content/images/2023/03/image-12.png 834w" sizes="(min-width: 720px) 720px"><figcaption>Дивергенция модели: ошибки не удерживаются в меньшем диапазоне</figcaption></figure><p>На приведенном выше изображении мы видим, что ошибки уменьшаются по мере увеличения количества итераций, но мы не можем сказать, с какой точки ошибка изменяется в <em>меньшем</em> диапазоне. В такой ситуации можно сказать, что нейронная сеть не сошлась. Давайте посмотрим, почему это происходит.</p><h3 id="%D1%81%D1%85%D0%BE%D0%B4%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D1%8C-%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80-%D0%BD%D0%B0-sklearn">Сходимость: пример на SkLearn</h3><p>Давайте посмотрим сходимость на примере <a href="__GHOST_URL__/linieinaia-rieghriessiia/">Линейной регрессии (Linear Regression)</a> библиотеки scikit-learn. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split</code></pre><p>Сгенерируем игрушечный набор данных, состоящий из 75 <a href="__GHOST_URL__/nabliudieniie/">Наблюдений (Observation)</a> – строк в <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочных данных (Train Data)</a>, 150 строк в <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовых (Test Data)</a> и 500 <a href="__GHOST_URL__/priznak/">Признаках (Feature)</a> – столбцах. Среди прочих настроек заметим, что перемешивания <code>shuffle</code> нет, шум есть (<code>noise=1.0</code>). Переменная <code>coef</code> означает <a href="__GHOST_URL__/gipierparamietr-c/">Параметр регуляризации С (C Regularization Parameter)</a>, это своеобразная мера наказания модели за неверное предсказание. <code>coef</code> напрямую влияет на сходимость модели.</p><pre><code class="language-python">n_samples_train, n_samples_test, n_features = 75, 150, 500\nX, y, coef = make_regression(\n    n_samples=n_samples_train + n_samples_test,\n    n_features=n_features,\n    n_informative=50,\n    shuffle=False,\n    noise=1.0,\n    coef=True,\n)</code></pre><p>Разделим датасет на тренировочную и тестовую части в соотношениие 75:150:</p><pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False\n)</code></pre><p>Инициируем модель линейной регрессии <code>linear_model.ElasticNet</code> и позволим ей до 10 тысяч итераций. Scikit-learn также позволяет рассчитать оптимальный параметр регуляризации:</p><pre><code class="language-python">alphas = np.logspace(-5, 1, 60)\nenet = linear_model.ElasticNet(l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = np.argmax(test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint("Оптимальный параметр регуляризации: %s" % alpha_optim)\n\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_</code></pre><p>В данном случае, C-параметр равен:</p><pre><code class="language-python">Оптимальный параметр регуляризации: 0.000335292414924956</code></pre><p>Построим график, чтобы оценить, достигла ли модель состояния сходимости:</p><pre><code class="language-python">plt.figure(figsize=(20,6))\nplt.subplot(2, 1, 1)\nplt.semilogx(alphas, train_errors, label="Тренировочные данные")\nplt.semilogx(alphas, test_errors, label="Тестовые данные")\nplt.vlines(\n    alpha_optim,\n    plt.ylim()[0],\n    np.max(test_errors),\n    color="k",\n    linewidth=3,\n    label="Точка оптимума (тест)",\n)\nplt.legend(loc="upper right")\nplt.ylim([0, 1.2])\nplt.xlabel("Параметр регуляризации")\nplt.ylabel("Производительность")\n\n\nplt.show()</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2023/03/--------------2023-03-19---12.17.19.png" class="kg-image" alt loading="lazy" width="2000" height="345" srcset="__GHOST_URL__/content/images/size/w600/2023/03/--------------2023-03-19---12.17.19.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/--------------2023-03-19---12.17.19.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/03/--------------2023-03-19---12.17.19.png 1600w, __GHOST_URL__/content/images/2023/03/--------------2023-03-19---12.17.19.png 2390w" sizes="(min-width: 1200px) 1200px"></figure><p>На графике мы видим, то по достижении точки Оптимума (Optimum) – локального максимума модель резко начинает сокращать свои ошибки, ибо была "наказана" более всего. На значении коэффициента регуляризации 10<sup>-3</sup>, то есть <code>0.000335292414924956</code> наступила ранняя сходимость, которая повторится на значении <code>coef</code> более 10<sup>1</sup>.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://drive.google.com/file/d/19KkYixD86mi4nS1FX0EUj9FNYOiLZ1Iu/view?usp=sharing">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://analyticsindiamag.com/when-does-a-neural-network-fail-to-converge/#:~:text=check%20out%20here.-,Convergence%20in%20deep%20learning,training%20comes%20to%20a%20minimum.">Yugesh Verma</a>, <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_train_error_vs_test_error.html">scikit-learn.org</a></p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-13.png" class="kg-image" alt loading="lazy" width="300" height="300"></figure><p>Подари чашку кофе дата-сайентисту ↑</p>		skhodimost-convergence	2023-03-19		
225	Распознавание позы (Pose Detection)		<p>Распознавание позы — это задачи в области Компьютерного зрения (CV) – определение позы человека на изображении. Причина, по которой так много энтузиастов машинного обучения привлекает оценка позы, заключается в ее широком разнообразии приложений и полезности. </p><p>Обычно такое распознавание выполняется путем нахождения ключевых точек – суставов, изгибов тела, контрастных переходов цвета (например, от волос на голове к коже лица). Основываясь на этих ключевых точках, мы можем сравнивать различные позы и делать выводы. Это активно используется в области дополненной реальности, анимации, игр и робототехники.</p><p>Рассмотрим пример. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport keras\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.layers import Input, Lambda, Dense, Flatten\nfrom tensorflow.keras.applications import VGG16, InceptionResNetV2\nfrom keras import regularizers\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax</code></pre><p>Разделим <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>, который уже разделен на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>:</p><pre><code class="language-python">train_dir = '../input/yoga-poses-dataset/DATASET/TRAIN' #directory with training images\ntest_dir = '../input/yoga-poses-dataset/DATASET/TEST' #directory with testing images</code></pre><p>Речь пойдет о таких изображениях:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/pose-detection-yoga-poses.jpg" class="kg-image" alt loading="lazy" width="1231" height="1231" srcset="__GHOST_URL__/content/images/size/w600/2023/03/pose-detection-yoga-poses.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/03/pose-detection-yoga-poses.jpg 1000w, __GHOST_URL__/content/images/2023/03/pose-detection-yoga-poses.jpg 1231w" sizes="(min-width: 720px) 720px"></figure><p>Встроенная функция Keras <code>ImageDataGenerator()</code> собирает картинки в <a href="__GHOST_URL__/pakiet/">Пакеты (Batch)</a> для пошагового обучения. <code>width_shift_range</code> определяет, какого разрешения будет пакет, <code>horizontal_flip</code> – поворачивает картинки, если необходимо. <code>validation_split</code> резервирует часть изображений для дальнейшей проверки эффективности модели:</p><pre><code class="language-python">train_datagen = ImageDataGenerator(width_shift_range= 0.1,\n                                  horizontal_flip = True,\n                                  rescale = 1./255,\n                                  validation_split = 0.2)\ntest_datagen = ImageDataGenerator(rescale = 1./255,\n                                 validation_split = 0.2)</code></pre><p><code>flow_from_directory</code> применяет настройки бэтчинга выше и дополняет их. Здесь мы видим цветовую схему RGB, размер пакета <code>batch_size</code>, тип набора <code>subset</code> – тренировочный или валидационный:</p><pre><code class="language-python">train_generator =  train_datagen.flow_from_directory(directory = train_dir,\n                                          target_size = (224,224),\n                                          color_mode = 'rgb',\n                                          class_mode = 'categorical',\n                                          batch_size = 16,\n                                          subset = 'training')\nvalidation_generator  = test_datagen.flow_from_directory(directory = test_dir,\n                                        target_size = (224,224),\n                                        color_mode = 'rgb',\n                                        class_mode = 'categorical',\n                                        subset = 'validation')</code></pre><p>Система распознала, что у нас пять классов и почти тысяча изображений:</p><pre><code class="language-python">Found 866 images belonging to 5 classes.\nFound 92 images belonging to 5 classes.</code></pre><p>Инициируем экземпляр Последовательной модели (Sequential Model), что позволяет нам задавать настройки каждого слоя от начала до конца. Здесь мы задаем настройки <a href="__GHOST_URL__/pulingh/">Пулинга (Pooling)</a>, то есть как картинки будут "сжиматься", <a href="__GHOST_URL__/funktsiia-aktivatsii/">Функцию активации (Activation Function)</a>. Последняя помогает <a href="__GHOST_URL__/modiel/">Модели (Model)</a> освоить сложные закономерности в данных. <a href="__GHOST_URL__/sviortochnaia-nieironnaia-siet/">Сверточная нейронная сеть (CNN)</a>, вызываемая методом <code>Conv2D(),</code> специализируется на изображениях среди прочих видов данных и способна распознавать предметы на них):</p><pre><code class="language-python">model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding = 'Same', input_shape=(224, 224, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.25),\n    #tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding = 'Same'),\n    #tf.keras.layers.MaxPooling2D(2,2),\n    #tf.keras.layers.Dropout(0.25),\n    #tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding = 'Same'),\n    #tf.keras.layers.MaxPooling2D(2,2),\n    #tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding = 'Same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(256, (3,3), activation='relu',padding = 'Same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(5, activation='softmax')\n])</code></pre><p>С самого начала модель, скорее всего, свою наивысшую результативность не покажет, так что ей понадобится <a href="__GHOST_URL__/optimizator/">Оптимизатор (Optimizer)</a> – алгоритм, который помогает повысить производительность. Для модели, работающей с изображениями, подойдет <a href="__GHOST_URL__/kross-entropiia/">Кросс-энтропия (Cross Entropy)</a>. В этом случае рассчитывается средняя ошибка между реальным и предсказанным значениями. Обучение пройдет в 50 эпох:</p><pre><code class="language-python">optimizer = Adam(lr=0.001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer = optimizer,\n              metrics=['accuracy'])\nepochs = 50  \nbatch_size = 16</code></pre><p>Посмотрим сводные характеристики модели:</p><pre><code class="language-python">model.summary()</code></pre><p>Не будем особо вдаваться в подробности во имя удобоваримости материала, но упомянем, что у нашей модели несколько слоев, которые распознают объекты на изображении, причем между ними встречаются:</p><ul><li>Прослойки <code>MaxPooling2D</code>, сокращающие размеры картинок</li><li>Выпадающий слой (<code>Dropout</code> Layer) – некая маска, что сводит на нет вклад некоторых нейронов в следующий слой и оставляет без изменений все остальные во избежание Переобучения (Overfitting) </li><li>Уплощающие слои <code>Flatten</code>, которые схлапывают многомерные выходные данные в одномерные. Именно в таком формате следующий слой и сможет принять информацию и распознавать позы.</li><li>Плотностный слой <code>Dense</code>: рабочая лошадка в этом сэндвиче, принимает предобработанные предшественниками данные и распознает запрошенное:</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/--------------2023-03-26---18.27.50.png" class="kg-image" alt loading="lazy" width="1172" height="1654" srcset="__GHOST_URL__/content/images/size/w600/2023/03/--------------2023-03-26---18.27.50.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/--------------2023-03-26---18.27.50.png 1000w, __GHOST_URL__/content/images/2023/03/--------------2023-03-26---18.27.50.png 1172w" sizes="(min-width: 720px) 720px"></figure><p>Загрузим сжатые изображения директивой <code>ImageFile.LOAD_TRUNCATED_IMAGES</code>, выставленной в значение True:</p><pre><code class="language-python">from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True</code></pre><p>Парой ячеек выше мы задали настройки модели, теперь пришло время запустить ее обучение:</p><pre><code class="language-python">history = model.fit(train_generator, epochs = epochs,validation_data = validation_generator)</code></pre><p>Показатели сейчас начнут стремительно меняться:</p><pre><code class="language-python">Epoch 1/50\n55/55 [==============================] - 52s 833ms/step - loss: 6.2334 - accuracy: 0.2734 - val_loss: 1.6744 - val_accuracy: 0.2500\n\n...\n\nEpoch 50/50\n55/55 [==============================] - 34s 610ms/step - loss: 0.0571 - accuracy: 0.9901 - val_loss: 0.5342 - val_accuracy: 0.8696</code></pre><p>Всего за 50 эпох, то есть подходов, модель увеличила свою <a href="__GHOST_URL__/tochnost-izmierienii/">Точность измерений (Accuracy)</a> с 25% до 86%.</p><p>Заглянув в ноутбук, вы заметите, что точность 86% – не максимальная за всю историю обучения, бывало и 91%! Что же произошло? Давайте проиллюстрируем хронологию изменения Accuracy:</p><pre><code class="language-python">fig, ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nfig.set_size_inches(12,4)\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Training Accuracy vs Validation Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epoch')\nax[0].legend(['Train', 'Validation'], loc='upper left')\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Training Loss vs Validation Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epoch')\nax[1].legend(['Train', 'Validation'], loc='upper left')\n\nplt.show()</code></pre><p>Валидационные потери обозначаются на графике справа оранжевым цветом. Если добавить еще десяток-другой эпох с новыми данными, возможно, точность "устаканится", то есть ее разброс станет предсказуемым. Тогда модель достигнет <a href="__GHOST_URL__/skhodimost-convergence/">Сходимости (Convergence)</a>. Сейчас об этом судить рановато.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2023/03/pose-detection.jpg" class="kg-image" alt loading="lazy" width="809" height="365" srcset="__GHOST_URL__/content/images/size/w600/2023/03/pose-detection.jpg 600w, __GHOST_URL__/content/images/2023/03/pose-detection.jpg 809w"></figure><p>Проведем Оценку (Evaluation) модели, то есть рассчитаем ключевые показатели ее эффективности:</p><pre><code class="language-python">train_loss, train_acc = model.evaluate(train_generator)\ntest_loss, test_acc   = model.evaluate(validation_generator)\nprint("final train accuracy = {:.2f} , validation accuracy = {:.2f}".format(train_acc*100, test_acc*100))</code></pre><p>В данном случае, мы рассчитали точность <code>accuracy</code> и потери <code>loss</code>:</p><pre><code class="language-python">55/55 [==============================] - 29s 520ms/step - loss: 0.0158 - accuracy: 0.9965\n3/3 [==============================] - 4s 1s/step - loss: 0.5342 - accuracy: 0.8696\nfinal train accuracy = 99.65 , validation accuracy = 86.96</code></pre><p>В дальнейшем мы сможем подгружать новые изображения и запрашивать позу, как описано в этой <a href="https://www.kaggle.com/code/venkatkumar001/pose-prediction-generate-csv-keypoints-mediapipe">статье</a> с использованием тех же данных.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://www.kaggle.com/code/aayushmishra1512/yoga-pose-detection">здесь</a>.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2021/10/human-pose-estimation-using-machine-learning-in-python/">Ayush Gupta</a>, <a href="https://www.kaggle.com/code/aayushmishra1512/yoga-pose-detection">Aayush Mishra</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/03/image-18.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		raspoznavaniie-pozy	2023-03-26		
226	Оптимизатор (Optimizer)		<p>Оптимизатор — это метод повышения производительности <a href="__GHOST_URL__/modiel/">Модели (Model)</a> <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокого обучения (Deep Learning)</a>. Эти алгоритмы сильно влияют на <a href="__GHOST_URL__/tochnost-izmierienii/">Долю правильных ответов (Accuracy)</a> и скорость обучения.</p><p>При обучении модели глубокого обучения нам необходимо изменить Веса  (Weights) – коэффициенты, которые присваиваются каждому <a href="__GHOST_URL__/priznak/">Признаку-столбцу (Feature)</a> и передают важность этого соответствующего признака при прогнозировании. Более того, веса позволяют минимизировать <a href="__GHOST_URL__/funktsiia-potieri/">Функцию потерь (Loss Function)</a>. Чем меньше ее значение, тем ближе предсказание модели к реальным значениям.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/03/image-20.png" class="kg-image" alt loading="lazy" width="990" height="477" srcset="__GHOST_URL__/content/images/size/w600/2023/03/image-20.png 600w, __GHOST_URL__/content/images/2023/03/image-20.png 990w" sizes="(min-width: 720px) 720px"><figcaption>Веса w<sub>1</sub>...w<sub>n</sub>, характеризующие вклад признаков x<sub>1</sub>...x<sub>n</sub></figcaption></figure><p>Оптимизатор — это алгоритм, который изменяет атрибуты <a href="__GHOST_URL__/nieironnaia-siet/">Нейронной сети (Neural Network)</a>. Проблема выбора правильных весов для модели является сложной задачей, поскольку модель глубокого обучения обычно состоит из миллионов признаков. Возникает необходимость выбора подходящего алгоритма оптимизации.</p><p>Вы можете использовать различные оптимизаторы в модели машинного обучения, чтобы вносить изменения в свои веса и скорость обучения. Однако выбор лучшего из них зависит от назначения модели. Мы пробуем все виды и выбираем ту, которая показывает лучшие результаты. Сначала это может не быть проблемой, но при работе с сотнями гигабайт данных даже одна эпоха может занять значительное время. Так что случайный выбор алгоритма — это не что иное, как азартная игра с вашим драгоценным временем, что вы рано или поздно осознаете.</p><p>В этом руководстве рассматриваются различные оптимизаторы глубокого обучения:</p><ul><li>Градиентный спуск (GD)</li><li>Cтохастический градиентный спуск (SGD)</li><li>Cтохастический градиентный спуск с импульсом</li><li>Мини-пакетный градиентный спуск (MBGD)</li><li>Adagrad</li><li>Среднеквадратичное распространение (RMSProp)</li><li>AdaDelta</li><li>Adam</li></ul><p>К концу статьи мы сможем сравнить различные оптимизаторы и процедуры, на которых они основаны. Прежде чем продолжить, вот несколько терминов, с которыми мы познакомимся:</p><ul><li><a href="__GHOST_URL__/epokha/">Эпоха (Epoch)</a> — количество запусков алгоритма на всем наборе обучающих данных.</li><li>Сэмпл  — одна строка набора данных.</li><li><a href="__GHOST_URL__/pakiet/">Пакет (Batch)</a> — обозначает количество сэмплов, которые необходимо взять для обновления параметров модели.</li><li>Скорость обучения (Learning Rate) — это параметр, который предоставляет модели шкалу того, сколько весов модели следует обновлять.</li><li><a href="__GHOST_URL__/funktsiia-potieri/">Функция потерь (Loss Function)</a>: используется для расчета разницы между прогнозируемым и фактическим значениями и, как следствие, характеризует эффективность модели.</li><li>Веса / Смещение (Bias) — параметры, которые управляют сигналом между двумя нейронами.</li></ul><p>Теперь давайте рассмотрим каждый оптимизатор.</p><h3 id="%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9-%D1%81%D0%BF%D1%83%D1%81%D0%BA-gradient-descent">Градиентный спуск (Gradient Descent)</h3><p>GD считается популярным детищем среди оптимизаторов. Он последовательно изменяет значения для достижения локального минимума. Прежде чем двигаться дальше, у вас может возникнуть вопрос о том, что такое градиент.</p><p>Проще говоря, представьте, что вы держите мяч, лежащий наверху чаши. Когда вы отпускаете мяч, он летит вниз под действием силы тяжести и в конце концов оседает на дне чаши. Градиент оптимизирует маршрут мяча таким образом, чтобы достичь локального минимума – дна чаши, как можно быстрее.</p><p>Градиентный спуск работает следующим образом:</p><ul><li>Он начинает с некоторых коэффициентов и ищет способ сократить ошибки.</li><li>Он движется к меньшему весу и обновляет значение коэффициентов.</li><li>Процесс повторяется до тех пор, пока не будет достигнут локальный минимум. </li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/optimizer-gradient-descent.png" class="kg-image" alt loading="lazy" width="692" height="387" srcset="__GHOST_URL__/content/images/size/w600/2023/03/optimizer-gradient-descent.png 600w, __GHOST_URL__/content/images/2023/03/optimizer-gradient-descent.png 692w"></figure><p><br>Градиентный спуск лучше всего подходит для большинства целей. Однако у него есть и некоторые недостатки. Вычислять градиенты дорого, если размер данных огромен. </p><h3 id="%D1%81%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9-%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9-%D1%81%D0%BF%D1%83%D1%81%D0%BA-stochastic-gradient-descent">Стохастический градиентный спуск (Stochastic Gradient Descent)</h3><p>В конце предыдущего раздела вы узнали, почему использование градиентного спуска для массивных данных может быть не лучшим вариантом. Чтобы решить эту проблему, у нас есть <em>стохастический</em> градиентный спуск (SGD). Термин "стохастический" означает случайность, на которой основан алгоритм. В SGD вместо всего набора данных для каждой итерации, мы случайным образом выбираем пакеты – несколько образцов из набора.</p><p>Поскольку мы не используем весь <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a>, а его части для каждой итерации, путь, пройденный алгоритмом, полон <a href="__GHOST_URL__/shum/">Шума (Noise)</a>, т.е. не укладывающихся в закономерность наблюдений. Таким образом, SGD использует большее количество итераций для достижения локальных минимумов. За счет увеличения числа итераций увеличивается общее время вычислений. Но даже после увеличения количества итераций стоимость вычислений все равно меньше, чем у градиентного спуска. Таким образом, вывод состоит в том, что если данные огромны, а время вычислений является важным фактором, стохастический градиентный спуск следует предпочесть алгоритму пакетного градиентного спуска.</p><h3 id="%D1%81%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9-%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9-%D1%81%D0%BF%D1%83%D1%81%D0%BA-%D1%81-%D0%B8%D0%BC%D0%BF%D1%83%D0%BB%D1%8C%D1%81%D0%BE%D0%BC">Стохастический градиентный спуск с импульсом</h3><p>Мы узнали, что стохастический градиентный спуск получает более зашумленные предсказания, чем градиентный спуск. По этой причине для достижения оптимального минимума требуется более значительное количество итераций, и, следовательно, большее время вычислений. Чтобы решить эту проблему, мы используем стохастический градиентный спуск с импульсом.</p><p>Импульс помогает быстрее достигнуть <a href="__GHOST_URL__/skhodimost-convergence/">Сходимости (Convergence)</a> – точке, при которой разность между предсказанным и реальным значениями, сводится к минимуму. Стохастический градиентный спуск колеблется между любым направлением градиента и соответствующим образом обновляет веса. Скорость обучения должна уменьшаться с высоким импульсом.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-24.png" class="kg-image" alt loading="lazy" width="300" height="150"></figure><p>На изображении выше в левой части показан график сходимости алгоритма стохастического градиентного спуска. В то же время справа показан SGD с импульсом. На изображении вы можете сравнить путь, выбранный обоими алгоритмами, и понять, что использование импульса помогает достичь сходимости за меньшее время. Возможно, вы думаете об использовании большого импульса и скорости обучения, чтобы сделать процесс еще быстрее. Но помните, что при увеличении импульса увеличивается и возможность прохождения оптимального минимума. Это может привести к плохой точности и еще большим колебаниям.</p><h3 id="%D0%BC%D0%B8%D0%BD%D0%B8-%D0%BF%D0%B0%D0%BA%D0%B5%D1%82%D0%BD%D1%8B%D0%B9-%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9-%D1%81%D0%BF%D1%83%D1%81%D0%BA-mini-batch-gradient-descent">Мини-пакетный градиентный спуск (Mini-Batch Gradient Descent)</h3><p>В этом варианте градиентного спуска вместо того, чтобы брать все обучающие данные, для вычисления функции потерь используется только выборка. Поскольку мы используем пакет данных, а не весь набор данных, требуется меньше итераций. Вот почему алгоритм мини-пакетного градиентного спуска быстрее, чем алгоритмы стохастического градиентного спуска и алгоритмы пакетного градиентного спуска. Этот алгоритм более эффективен и надежен, чем более ранние варианты градиентного спуска. Поскольку в алгоритме используется пакетная обработка, все обучающие данные не нужно загружать в память, что делает процесс более эффективным для реализации. Кроме того, функция стоимости в мини-пакетном градиентном спуске более зашумлена, чем алгоритм пакетного градиентного спуска, но более гладкая, чем у алгоритма стохастического градиентного спуска. Из-за этого мини-пакетный градиентный спуск идеален и обеспечивает хороший баланс между скоростью и точностью.</p><p>Несмотря на все это, у алгоритма мини-пакетного градиентного спуска есть и недостатки. Ему нужен гиперпараметр «мини-пакетного размера», который необходимо настроить для достижения требуемой точности. Хотя размер партии 32 считается подходящим почти для каждого случая. Кроме того, в некоторых случаях это приводит к плохой конечной доле верных предсказаний. В связи с этим необходим подъем, чтобы искать и другие альтернативы.</p><h3 id="%D0%B0%D0%B4%D0%B0%D0%BF%D1%82%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9-%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9-%D1%81%D0%BF%D1%83%D1%81%D0%BA-adagrad">Адаптивный градиентный спуск (AdaGrad)</h3><p>Алгоритм адаптивного градиентного спуска немного отличается от своих собратьев. Это связано с тем, что он использует разные скорости обучения для каждой итерации. Изменение скорости обучения зависит от разницы параметров во время обучения. Чем больше изменяются параметры, тем менее заметны изменения скорости обучения. Несправедливо иметь одинаковое значение скорости обучения для всех функций. </p><p>Преимущество использования AdaGrad заключается в том, что он устраняет необходимость вручную изменять скорость обучения. Он более надежен, чем алгоритмы градиентного спуска и их варианты, и достигает сходимости на более высокой скорости.</p><p>Одним из недостатков AdaGrad является то, что он агрессивно и монотонно снижает скорость обучения. Нередко настает момент, когда скорость обучения становится чрезвычайно низкой. Модель в конечном итоге становится неспособной получить больше знаний, и, следовательно, точность модели ставится под угрозу.</p><h3 id="%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D0%B5%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D0%BE%D0%B5-%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-rmsprop">Среднеквадратичное распространение (RMSProp)</h3><p>Проблема с градиентами в том, что некоторые из них маленькие, а другие – огромны. Таким образом, определение единой скорости обучения может быть не лучшей идеей. RMSProp адаптирует размер шага индивидуально для каждого веса. В этом алгоритме два градиента сначала сравниваются по знакам. Если они имеют одинаковый знак, мы идем в правильном направлении и, следовательно, увеличиваем размер шага на небольшую долю. Если же они имеют противоположные знаки, мы должны уменьшить размер шага. Затем мы ограничиваем размер шага, и теперь мы можем перейти к обновлению веса.</p><p>Проще говоря, если существует параметр, из-за которого функция стоимости сильно колеблется, мы хотим наказать обновление этого параметра. Предположим, вы построили модель для классификации рыб. Модель опирается на фактор «цвет» в основном для различения рыб. Из-за этого он делает много ошибок. RMSProp наказывает параметр «цвет», чтобы модель полагалась и на другие признаки. Это предотвращает слишком быструю адаптацию алгоритма к изменениям параметра «цвет» по сравнению с другими параметрами. Алгоритм быстро сходится и требует меньшей настройки, чем алгоритмы градиентного спуска и их варианты.</p><p>Проблема с RMSProp заключается в том, что скорость обучения нужно задавать вручную, а предлагаемое значение подходит не для каждого приложения.</p><h3 id="adadelta">AdaDelta</h3><p>AdaDelta можно рассматривать как более надежную версию оптимизатора AdaGrad. Он предназначен для устранения существенных недостатков AdaGrad и RMSProp. Основная проблема с двумя вышеперечисленными оптимизаторами заключается в том, что начальную скорость обучения необходимо задавать вручную. Еще одна проблема — падающая скорость обучения, которая в какой-то момент становится бесконечно малой. Из-за этого через определенное количество итераций модель уже не может усваивать новые знания.</p><h3 id="adam">Adam</h3><p>Adam в данном контексте означает "adaptive moment estimation" (англ. адаптивная оценка момента). Этот алгоритм оптимизации является дальнейшим расширением стохастического градиентного спуска для обновления весов сети во время обучения. В отличие от поддержания единой скорости обучения посредством обучения в SGD, оптимизатор Adam обновляет скорость обучения для каждого веса сети индивидуально. Adam наследует функции Adagrad и RMSProp. </p><p>Этот оптимизатор имеет ряд преимуществ, благодаря которым он получил широкое распространение. Он считается эталоном в глубоком обучении и рекомендуется в качестве алгоритма оптимизации по умолчанию. Кроме того, алгоритм прост в реализации, отрабатывает быстрее, имеет низкие требования к памяти и требует меньше настроек, чем любой другой алгоритм оптимизации.</p><p>Если оптимизатор Adam использует хорошие свойства всех алгоритмов и является лучшим доступным оптимизатором, то почему бы вам не использовать Adam в каждом приложении? И зачем было углубляться в изучение других алгоритмов? Это потому, что даже у Адама есть недостатки. Он имеет тенденцию сосредотачиваться на более быстром времени вычислений, тогда как алгоритмы, такие как стохастический градиентный спуск, сосредотачиваются на точках данных. Вот почему такие алгоритмы, как SGD, лучше обобщают данные за счет низкой скорости вычислений. </p><p>Таким образом, алгоритмы оптимизации могут быть выбраны соответственно в зависимости от требований и типа данных.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-25.png" class="kg-image" alt loading="lazy" width="620" height="480" srcset="__GHOST_URL__/content/images/size/w600/2023/03/image-25.png 600w, __GHOST_URL__/content/images/2023/03/image-25.png 620w"></figure><h3 id="%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80%D1%8B-%D0%BD%D0%B0-%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B5">Оптимизаторы на практике</h3><p>Мы изучили достаточно теории, и теперь нам нужна практика. Пришло время попробовать то, что мы узнали, и сравнить результаты, выбрав разные оптимизаторы на простой нейронной сети. Мы будем обучать простую модель, используя несколько базовых слоев, сохраняя размер партии и эпохи одинаковыми, но применять разные оптимизаторы. Ради справедливости мы будем использовать значения по умолчанию для каждого оптимизатора.</p><p>Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K</code></pre><p>Загрузим набор данных:</p><pre><code class="language-python">(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape, y_train.shape)</code></pre><p>Подготовим <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочные (Train Data) </a>и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовые данные (Test Data)</a>:</p><pre><code class="language-python">x_train = x_train.reshape(x_train.shape[0],28,28,1)\nx_test =  x_test.reshape(x_test.shape[0],28,28,1)\ninput_shape = (28,28,1)\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /=255</code></pre><p>Создадим функцию <code>build_model()</code>, которая будет строить одни и те же модели, но с разными оптимизаторами:</p><pre><code class="language-python">batch_size = 64\nnum_classes = 10\nepochs = 10\n\ndef build_model(optimizer):\n  model=Sequential()\n  model.add(Conv2D(32,kernel_size (3,3),activation='relu',input_shape=input_shape))\n  model.add(MaxPooling2D(pool_size=(2,2)))\n  model.add(Dropout(0.25))\n  model.add(Flatten())\n  model.add(Dense(256, activation='relu'))\n  model.add(Dropout(0.5))\n  model.add(Dense(num_classes, activation='softmax'))\n  model.compile(loss=keras.losses.categorical_crossentropy, optimizer= optimizer, metrics=['accuracy'])\n  return model</code></pre><p>Обучим модель:</p><pre><code class="language-python">optimizers = ['Adadelta', 'Adagrad', 'Adam', 'RMSprop', 'SGD']\n\nfor i in optimizers:\n\n  model = build_model(i)\n\n  hist=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test,y_test))</code></pre><p>Мы запустили нашу модель с 64 пакетами и в 10 эпох. Попробовав разные оптимизаторы, мы получили довольно интересные результаты. </p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/www.analyticsvidhya.com_blog_2021_10_a-comprehensive-guide-on-deep-learning-optimizers_.png" class="kg-image" alt loading="lazy" width="1670" height="946" srcset="__GHOST_URL__/content/images/size/w600/2023/03/www.analyticsvidhya.com_blog_2021_10_a-comprehensive-guide-on-deep-learning-optimizers_.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/www.analyticsvidhya.com_blog_2021_10_a-comprehensive-guide-on-deep-learning-optimizers_.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/03/www.analyticsvidhya.com_blog_2021_10_a-comprehensive-guide-on-deep-learning-optimizers_.png 1600w, __GHOST_URL__/content/images/2023/03/www.analyticsvidhya.com_blog_2021_10_a-comprehensive-guide-on-deep-learning-optimizers_.png 1670w" sizes="(min-width: 720px) 720px"></figure><p>В приведенной выше таблице показаны точность проверки и потери в разные эпохи. Он также содержит общее время, затраченное моделью на 10 эпох для каждого оптимизатора. </p><ul><li>Оптимизатор Adam показывает наилучшую точность за приемлемое время.</li><li>RMSProp показывает такую же точность, но со сравнительно большим временем вычислений.</li><li>Удивительно, но алгоритм SGD занял меньше времени для обучения и также дал хорошие результаты. Но для достижения точности оптимизатора Adam SGD потребуется больше итераций, а значит, время вычислений увеличится.</li><li>SGD с импульсом показывает аналогичную SGD точность с неожиданно большим временем вычисления. Это означает, что значение полученного импульса необходимо оптимизировать.</li><li>AdaDelta показывает плохие результаты как по точности, так и по времени вычислений.</li></ul><p>Вы можете проанализировать точность каждого оптимизатора для каждой эпохи на графике ниже:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/03/image-26.png" class="kg-image" alt loading="lazy" width="600" height="200" srcset="__GHOST_URL__/content/images/2023/03/image-26.png 600w"></figure><p>Вот мы и подошли к концу этого всеобъемлющего руководства. Чтобы освежить вашу память, кратко рассмотрим каждый алгоритм оптимизации еще раз.</p><p>SGD — это очень простой алгоритм, который практически не используется в приложениях из-за низкой скорости вычислений. Еще одна проблема с этим алгоритмом — постоянная скорость обучения для каждой эпохи. </p><p>Adagrad работает лучше, чем стохастический градиентный спуск, как правило, из-за частых обновлений скорости обучения. Лучше всего использовать его для работы с разреженными данными, то есть содержащими пропуски. </p><p>RMSProp показывает результаты, аналогичные алгоритму градиентного спуска с импульсом, он просто отличается способом вычисления градиентов.</p><p>Наконец, оптимизатор Adam унаследовал хорошие черты RMSProp и других алгоритмов. Результаты этого оптимизатора, как правило, лучше, чем у любого другого алгоритма оптимизации, занимают меньшее время и требуют меньше параметров для настройки. Из-за всего этого Адам рекомендуется в качестве оптимизатора по умолчанию для большинства приложений. </p><p>Но к концу мы узнали, что даже у оптимизатора Adam есть недостатки. Кроме того, бывают случаи, когда такие алгоритмы, как SGD, могут быть полезны и работать лучше, чем оптимизатор Adam. Таким образом, крайне важно знать свои требования и тип данных, с которыми вы имеете дело, чтобы выбрать лучший алгоритм оптимизации и добиться выдающихся результатов.</p><p>Автор оригинальной статьи: <a href="https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/">Ayush Gupta</a></p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1ODKWhlqWV8ceYEdtCe_ZvHhlZE6dkjJ4?usp=sharing">здесь</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/03/image-19.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		solver	2023-03-31		
227	Коэффициент Джини (Gini Score)		<p>Коэффициент Джини (индекс Джини) – 1. (в статистике) показатель, характеризующий неравенство доходов населения. 2. (в машинном обучении) метрика, характеризующая эффективность некоторых моделей, таких как <a href="__GHOST_URL__/dierievo-rieshienii/">Дерево решений (Decision Tree)</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/04/image.png" class="kg-image" alt loading="lazy" width="785" height="625" srcset="__GHOST_URL__/content/images/size/w600/2023/04/image.png 600w, __GHOST_URL__/content/images/2023/04/image.png 785w" sizes="(min-width: 720px) 720px"><figcaption>40% населения обладает ~15% дохода, 80% – 60% дохода.</figcaption></figure><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80-%D1%80%D0%B0%D1%81%D1%87%D0%B5%D1%82%D0%B0-%D0%B4%D0%B6%D0%B8%D0%BD%D0%B8">Пример расчета Джини</h3><p>Давайте разберемся с расчетом Джини на простом примере. У нас есть в общей сложности 10 точек данных двух классов – красного и синего:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/04/image-1.png" class="kg-image" alt loading="lazy" width="560" height="512"></figure><p>При создании дерева решений нам нужно разделить эти наблюдения на две ветви. Рассмотрим следующее разделение на 5 красных и 5 синих. Предположим, мы делаем бинарное разделение по X = 200:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/04/image-2.png" class="kg-image" alt loading="lazy" width="560" height="512"></figure><p>Видно, что разделение выполнено правильно: у нас осталось две ветви по 5 красных (левая) и 5 синих (правая).</p><p>Но каков будет результат, если дерево произведет расщепление по X=250? У нас осталось две ветви, левая ветвь состоит из 5 красных и 1 синей, а правая ветвь состоит из 4 синих:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/04/image-3.png" class="kg-image" alt loading="lazy" width="560" height="512"></figure><p>Это называется несовершенным разделением. При обучении дерева решений мы используем индекс Джини как меру его эффективности.</p><p>Чтобы рассчитать этот показатель:</p><ul><li>Мы будем случайным образом выбирать любую точку данных из набора. </li><li>Случайным образом мы будем классифицировать ее в соответствии с распределением классов в данном наборе данных. В нашем наборе данных мы дадим любой точке вероятность 5/10 принадлежать красной группе и 5/10 – синей, поскольку существует пять точек данных каждого цвета.</li></ul><p>Теперь рассчитаем индекс Джини по формуле:</p><!--kg-card-begin: markdown--><p>$$G = \\sum_{i=1}^C p_{i} × (1 - p_{i}), где$$<br>\n$$G\\space–\\space{индекс}\\space{Джини},$$<br>\n$$C\\space{–}\\space{общее}\\space{количество}\\space{классов},$$<br>\n$$p_{i}\\space–\\space{вероятность}\\space{выбора}\\space{точки}\\space{данных}\\space{c}\\space{классом}\\space{i}$$</p>\n<!--kg-card-end: markdown--><p>В приведенном выше примере мы имеем два класса (C = 2) и 50% вероятность у точки принадлежать любому из двух классов (p<sub>1</sub>  = p<sub>2</sub> = 0,5). Следовательно, индекс Джини можно рассчитать как:</p><!--kg-card-begin: markdown--><p>$$G = p_{1} × (1 - p_{1})) + p_{2} × (1 - p_{2}) = 0.5 × (1 - 0.5) + 0.5 × (1 - 0.5) = 0.5$$</p>\n<!--kg-card-end: markdown--><p>Теперь давайте рассчитаем П<em>римесь</em> Джини (Gini Impurity). При идеальном разделении левая ветвь имеет только красные цвета, и, следовательно, ее примесь Джини:</p><!--kg-card-begin: markdown--><p>$$G = 1 × (1 - 1) + 0 × (1 - 0) = 0$$</p>\n<!--kg-card-end: markdown--><p>Правая ветвь также имеет только синие точки, и, следовательно, ее примесь Джини также равна нулю:</p><!--kg-card-begin: markdown--><p>$$G = 1 × (1 - 1) + 0 × (1 - 0) = 0$$</p>\n<!--kg-card-end: markdown--><p>Как левая, так и правая ветви нашего идеального сплита имеют вероятность попадания чужеродной точки, равную нулю. Примесь Джини, равная 0, является наилучшим возможным вариантом.</p><p>Автор оригинальной статьи: <a href="https://www.upgrad.com/blog/gini-index-for-decision-trees/">MK Gurucharan</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/04/image-5.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		koeffitsiient-dzhini	2023-04-06		
228	Машинное обучение vs. Глубокое обучение: в чем разница?		<p>Даже если вы не связаны с миром науки о данных, вы, вероятно, слышали термины <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственный интеллект (AI)</a>, <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинное обучение (ML)</a> и <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокое обучение (DL)</a>. Иногда они даже используются взаимозаменяемо. Несмотря на эту связь, каждый из этих терминов имеет свое особое значение, и это больше, чем просто модные словечки, используемые для описания беспилотных автомобилей.</p><p>В общих чертах, глубокое обучение — это подмножество машинного , а машинное обучение — подмножество искусственного интеллекта. Вы можете думать о них как о серии перекрывающихся концентрических кругов, где ИИ занимает наибольший объем, за ним следует машинное обучение, а затем глубокое:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2023/04/ml-vs-dl.jpg" class="kg-image" alt loading="lazy" width="1555" height="859" srcset="__GHOST_URL__/content/images/size/w600/2023/04/ml-vs-dl.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/04/ml-vs-dl.jpg 1000w, __GHOST_URL__/content/images/2023/04/ml-vs-dl.jpg 1555w" sizes="(min-width: 1200px) 1200px"></figure><p>Другими словами, глубокое обучение — это ИИ, но ИИ — это не глубокое обучение. </p><h3 id="%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-vs-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5">Глубокое обучение vs. машинное</h3><p>Оксфордский словарь определяет ИИ как «теорию и разработку компьютерных систем, способных выполнять задачи, которые обычно требуют человеческого интеллекта». Словарь Британника предлагает аналогичное определение: «способность цифрового компьютера или управляемого компьютером робота выполнять задачи, обычно связанные с разумными существами».</p><p>Машинное обучение — это ИИ, который может автоматически адаптироваться с минимальным вмешательством человека. Глубокое обучение — это подмножество машинного обучения, в котором используются искусственные нейронные сети для имитации процесса обучения человеческого мозга.</p><p>Взгляните на эти ключевые отличия:</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Машинное обучение</th>\n<th>Глубокое обучение</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Подмножество ИИ</td>\n<td>Подмножество машинного обучения</td>\n</tr>\n<tr>\n<td>Может обучаться на небольших наборах данных</td>\n<td>Требует больших объемов данных</td>\n</tr>\n<tr>\n<td>Требует большего вмешательства человека для исправления и обучения</td>\n<td>Самостоятельно учится на окружающей среде и прошлых ошибках</td>\n</tr>\n<tr>\n<td>Более короткая тренировка и более низкая точность</td>\n<td>Более длительная тренировка и более высокая точность</td>\n</tr>\n<tr>\n<td>Делает простые, линейные корреляции</td>\n<td>Делает нелинейные, сложные корреляции</td>\n</tr>\n<tr>\n<td>Может обучаться на ЦП (центральном процессоре)</td>\n<td>Для обучения требуется специализированный ГП (графический процессор)</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>Автор оригинальной статьи: <a href="https://www.coursera.org/articles/ai-vs-deep-learning-vs-machine-learning-beginners-guide">coursera.org</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/04/image-7.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		ml-vs-dl	2023-04-15		
229	Метод обратного распространения ошибки (BackProp)		<p>Обратное распространение (англ. Back Propagation) — это алгоритм, используемый для обучения <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a> путем вычисления градиентов <a href="__GHOST_URL__/funktsiia-potieri/">Функции потерь (Loss Function)</a> по отношению к весам. Веса (Weights) — это значения, которые присваиваются каждому <a href="__GHOST_URL__/priznak/">Признаку (Feature)</a> и передают его важность при прогнозировании.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/04/image-9.png" class="kg-image" alt loading="lazy" width="626" height="428" srcset="__GHOST_URL__/content/images/size/w600/2023/04/image-9.png 600w, __GHOST_URL__/content/images/2023/04/image-9.png 626w"><figcaption>После итерации обучения входным данным в обратном направлении назначаются новые веса</figcaption></figure><p>Это основная концепция <a href="__GHOST_URL__/glubokoye-obucheniye/">Глубокого обучения (DL)</a>. Основная идея BackPropagation заключается в том, что каждый последующий выходной слой подает новые, лучшие значения весов обратно на входной. Это позволяет поднять эффективность модели.</p><p>Вот пример того, как реализовать обратное распространение. Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import numpy as np</code></pre><p>В этом примере мы определяем простую нейронную сеть с одним входным слоем, одним скрытым слоем и одним выходным слоем. Входной слой имеет три узла, скрытый слой — четыре узла, а выходной слой — один узел. Мы определяем входные данные X и выходные данные y. </p><pre><code class="language-python">X = np.array([[0, 0, 1],\n              [0, 1, 1],\n              [1, 0, 1],\n              [1, 1, 1]])\n\ny = np.array([[0],\n              [1],\n              [1],\n              [0]])</code></pre><p>Мы используем Сигмоиду (Sigmoid) для активации для обоих слоев. <a href="__GHOST_URL__/funktsiia-aktivatsii/">Функция активации (Activation Function)</a> – это фрагмент программного кода, добавляемый в нейронную сеть, чтобы помочь ей изучить сложные закономерности данных. </p><pre><code class="language-python">def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)</code></pre><p>Мы также определяем гиперпараметры, включая скорость обучения и количество итераций.</p><p>Мы инициализируем веса сети, используя случайные значения от -1 до 1. Затем мы повторяем обучающие данные для указанного количества итераций.</p><pre><code class="language-python">learning_rate = 0.1\nnum_iterations = 10000\n\nnp.random.seed(1)\nweights_0 = 2 * np.random.random((3, 4)) - 1\nweights_1 = 2 * np.random.random((4, 1)) - 1</code></pre><p>Для каждой итерации мы выполняем прямой проход по сети, вычисляя выходные данные каждого слоя. Затем мы вычисляем ошибку между прогнозируемым и фактическим выходными значениями.</p><p>Мы выполняем обратный проход по сети, вычисляя градиенты ошибки относительно весов каждого слоя. После завершения всех итераций мы печатаем окончательный вывод нейронной сети.</p><pre><code class="language-python">for i in range(num_iterations):\n\n    layer_0 = X\n    layer_1 = sigmoid(np.dot(layer_0, weights_0))\n    layer_2 = sigmoid(np.dot(layer_1, weights_1))\n\n\n    error = y - layer_2\n\n    layer_2_delta = error * sigmoid_derivative(layer_2)\n    layer_1_error = layer_2_delta.dot(weights_1.T)\n    layer_1_delta = layer_1_error * sigmoid_derivative(layer_1)\n\n    weights_1 += learning_rate * layer_1.T.dot(layer_2_delta)\n    weights_0 += learning_rate * layer_0.T.dot(layer_1_delta)\n\nprint(layer_2)</code></pre><pre><code class="language-python">[[0.03499461]\n [0.95478633]\n [0.96201778]\n [0.05337798]]</code></pre><p>Обратное распространение ошибки — это мощный алгоритм для обучения нейронных сетей, но он может быть дорогостоящим для больших сетей и <a href="__GHOST_URL__/dataset/">Датасетов (Dataset)</a>.</p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1fYtVbamX9gxicgdpsCPk_HlECw-O_TJs?usp=sharing">здесь</a>.</p><p>Авторы материала: chatGPT, Елена Капаца<br></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/04/image-8.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		mietod-obratnogho-rasprostranieniia-oshibki	2023-04-23		
230	Виды метрик в Машинном обучении		<p>В <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a> используются различные метрики, которые позволяют оценить качество работы моделей машинного обучения. Некоторые из наиболее распространенных метрик:</p><ul><li><a href="__GHOST_URL__/sriedniekvadratichieskaia-oshibka/">Среднеквадратическая ошибка (MSE)</a> измеряет отклонение прогнозируемых значений от фактических и возводит разность в квадрат. Последнее необходимо, чтобы получить дистанцию между реальным и спрогнозированным: простая разность может быть отрицательным числом, и это исказит расчеты. Она часто используется для задач <a href="__GHOST_URL__/rieghriessiia/">Регрессии (Regression)</a>.</li><li><a href="__GHOST_URL__/sriedniaia-absoliutnaia-oshibka/">Средняя абсолютная ошибка (MAE)</a> измеряет среднюю разницы между прогнозируемыми и фактическими значениями числовой переменной. Она рассчитывается путем взятия абсолютной разницы (модуля числа) между каждым прогнозируемым и соответствующим ему фактическим значением, а затем среднего значения всех этих абсолютных различий.</li><li>Коэффициент детерминации (R-Squared) измеряет, насколько хорошо прогнозируемые значения соответствуют фактическим значениям. Она также часто используется для задач регрессии.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/04/--------------2023-04-27---14.53.47.png" class="kg-image" alt loading="lazy" width="1836" height="1254" srcset="__GHOST_URL__/content/images/size/w600/2023/04/--------------2023-04-27---14.53.47.png 600w, __GHOST_URL__/content/images/size/w1000/2023/04/--------------2023-04-27---14.53.47.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/04/--------------2023-04-27---14.53.47.png 1600w, __GHOST_URL__/content/images/2023/04/--------------2023-04-27---14.53.47.png 1836w" sizes="(min-width: 720px) 720px"><figcaption>Подсчет R<sup>2</sup>: взаимосвязь веса мыши и ее длины. Тонкие черные линии измеряют и возводят в квадрат. https://youtu.be/2AQKmw14mHM</figcaption></figure><ul><li><a href="__GHOST_URL__/tochnost-izmierienii/">Доля правильных ответов (Accuracy)</a> измеряет долю верных решений модели на тестовом наборе. Она часто используется для задач <a href="__GHOST_URL__/klassifikatsiia/">Классификации (Classification)</a>.</li><li>Точность (Precision) измеряет долю правильно предсказанных положительных ответов относительно всех предсказанных. Она также часто используется для задач классификации.</li><li><a href="__GHOST_URL__/otzyv/">Полнота (Recall)</a> измеряет долю правильно предсказанных положительных ответов относительно всех истинно положительных ответов в тестовом наборе данных. Она также часто используется для задач классификации.</li><li>F1-мера (F1-score) объединяет точность и полноту в одно число, и позволяет оценить баланс между этими двумя метриками. Она также часто используется для задач классификации.</li></ul><p>Это только некоторые из наиболее распространенных метрик. В зависимости от типа задачи и используемого алгоритма могут быть использованы и другие.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/04/image-10.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		vidy-mietrik	2023-04-27		
231	Виды валидации		<p>Кросс-валидация (Cross-Validation – CV) – это метод оценки <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a> в условиях небольшого объема данных. <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> разделяют на N равных частей разными способами, затем обучают на первой и вычисляют эффективность с помощью второй части. Затем дообучают на второй и снова обсчитывают эффективность на третьей. </p><p>Такая перекрестная проверка используется для обнаружения <a href="__GHOST_URL__/pierieobuchieniie/">Переобучения (Overfitting)</a>, т.е. неспособности распознать паттерн. Выделяют следующие виды кросс-валидации:</p><ul><li><strong>Удерживающая проверка (Hold-Out CV)</strong>: датасет делится на два набора: для обучения и для тестирования. Модель обучается на тренировочном наборе, а затем оценивается на тестовом.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-2.png" class="kg-image" alt loading="lazy" width="525" height="225"></figure><ul><li><strong>k-блочная перекрестная проверка (k-Fold CV)</strong>: в этом типе проверки набор данных делится на k подмножеств одинакового размера. Модель обучается k раз и каждый раз использует другое подмножество в качестве набора для тестирования и оставшиеся подмножества в качестве набора для обучения. </li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-3.png" class="kg-image" alt loading="lazy" width="525" height="525"></figure><ul><li><strong>Стратифицированный метод k-Fold проверки</strong> <strong>(Stratified k-Fold cross-validation) </strong> — это разновидность k-Fold CV, разработанная для эффективной работы в случаях дисбаланса классов. Например, в наборе данных о ценах на наручные часы может быть большее количество наручных часов в категории "Люкс". </li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-8.png" class="kg-image" alt loading="lazy" width="525" height="525"></figure><ul><li><strong>Повторная перекрестная проверка k-Fold (Repeated k-Fold CV)</strong> – наиболее надежным из всех методов CV, описанных в этой статье. Это разновидность k-Fold, но k здесь не является количеством подмножеств. Это количество раз, которое мы будем обучать модель. На каждой итерации мы будем случайным образом выбирать образцы по всему набору данных в качестве нашего тестового набора. Например, если мы решим, что 20% набора данных будут нашим тестовым набором, остальные 80% станут обучающим набором.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-11.png" class="kg-image" alt loading="lazy" width="525" height="525"></figure><ul><li><strong>Перекрестная проверка с исключением одного (Leave-One-Out Cross Validation – LOOCV)</strong>: это особый случай k-Fold, где k равно количеству экземпляров в наборе данных. В этом случае каждый экземпляр используется как тестовый набор один раз.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-16.png" class="kg-image" alt loading="lazy" width="564" height="440"></figure><ul><li><strong>Перекрестная проверка с исключением p записей (Leave-p-out cross-validation – LpOC)</strong> похожа на LOOCV с исключением одного: она создает все возможные обучающие и тестовые наборы, используя p записей в качестве тестового набора.</li><li><strong>Вложенная перекрестная проверка (Nested CV)</strong>: это комбинация k-Fold и Hold-Out: набор сначала делится на несколько подмножеств. Затем производится удерживающая проверка для каждого подмножества. Этот подход обеспечивает более надежную оценку производительности модели, но требует бо́льших вычислительных ресурсов.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-12.png" class="kg-image" alt loading="lazy" width="1323" height="945" srcset="__GHOST_URL__/content/images/size/w600/2023/05/image-12.png 600w, __GHOST_URL__/content/images/size/w1000/2023/05/image-12.png 1000w, __GHOST_URL__/content/images/2023/05/image-12.png 1323w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Перекрестная проверка с выходом из одной группы (Leave-One-Group-Out Cross-Validation – LOGO CV)</strong> – это вариант перекрестной проверки, где образцы сгруппированы в отдельные кластеры. Процесс перекрестной проверки выполняется путем исключения одной целой группы за раз для тестирования. Оставшиеся группы в это время используются для обучения модели. Этот процесс повторяется для каждой группы, и производительность модели усредняется.</li><li><strong>Кросс-валидация <a href="__GHOST_URL__/vriemiennoi-riad/">Временных рядов (Time Series)</a> (Time-Series Cross-Validation)</strong> выполняется на скользящей основе, т. е. начиная с небольшого подмножества данных в целях обучения, прогнозирования будущих значений. </li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/--------------2023-05-02---15.44.30.png" class="kg-image" alt loading="lazy" width="1222" height="696" srcset="__GHOST_URL__/content/images/size/w600/2023/05/--------------2023-05-02---15.44.30.png 600w, __GHOST_URL__/content/images/size/w1000/2023/05/--------------2023-05-02---15.44.30.png 1000w, __GHOST_URL__/content/images/2023/05/--------------2023-05-02---15.44.30.png 1222w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Блокирующая кросс-валидация временных рядов (Blocked Time-Series Cross-Validation). </strong>Первый метод может привести к <a href="__GHOST_URL__/utiechka-dannykh/">Утечке данных (Data Leakage)</a>, то есть заимствованию записей о дальнейшем времени. Модель будет наблюдать за будущими паттернами, чтобы прогнозировать. Вот почему была введена блокирующая перекрестная проверка.</li></ul><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/--------------2023-05-02---15.45.21.png" class="kg-image" alt loading="lazy" width="1222" height="696" srcset="__GHOST_URL__/content/images/size/w600/2023/05/--------------2023-05-02---15.45.21.png 600w, __GHOST_URL__/content/images/size/w1000/2023/05/--------------2023-05-02---15.45.21.png 1000w, __GHOST_URL__/content/images/2023/05/--------------2023-05-02---15.45.21.png 1222w" sizes="(min-width: 720px) 720px"></figure><p>Отдельным видом выделяют:</p><ul><li><strong>Бутстрэп (Bootstrap Cross-Validation – BS CV)</strong> выбирает записи случайным образом. В каждом подмножестве есть повторения из-за случайности выбора.</li></ul><p>Итоговая эффективность – это среднее всех итераций.</p><p>Автор изображений: <a href="https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right">Vladimir Lyashenko</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/05/image-17.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		vidy-validatsii	2023-05-02		
232	Виды функций активации		<p>Функция активации – это фрагмент программного кода, добавляемый в искусственную <a href="__GHOST_URL__/nieironnaia-siet/">Нейронную сеть (Neural Network)</a>, чтобы помочь ей изучить сложные закономерности данных. В сравнении с нейронами нашего мозга, функция активации решает, что должно быть запущено в следующий нейрон. Она принимает выходной сигнал из предыдущей ячейки и преобразует его в некоторую форму, которую можно использовать в качестве входных данных для следующей ячейки. </p><p>В <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинном обучении (ML)</a> такая непредсказуемость входных данных называется <em>нелинейностью</em>. Вот некоторые распространенные ее типы:</p><ul><li><strong>Сигмовидная функция (Sigmoid)</strong> сопоставляет любое входное значение со значением от 0 до 1 и часто используется в задачах Двоичной классификации (Binary Classification).</li><li><strong>Функция активации выпрямителя (ReLU)</strong> – это популярная функция активации. Она обнуляет любое входное значение ниже нуля. Остальные (выше нуля) остаются без изменений.</li><li><strong>Leaky ReLU</strong> (LreLU) случайным образом обнуляет некоторые близкие к нулю положительные значения.</li><li><strong>Softmax</strong> часто используется в выходном слое нейронной сети для задач мультиклассовой классификации. Он сопоставляет выходные значения с распределением вероятностей по классам.</li><li><strong>Функция в форме гиперболического тангенса (Tanh)</strong> похожа на сигмовидную функцию, но отображает входные значения в диапазоне от -1 до 1. Она часто используется в скрытых слоях нейронной сети.</li><li><strong>Swish</strong> – относительно новая функция активации, похожая на ReLU. Swish применяется в различных сложных областях – Классификация изображений (Image Classification) и Машинный перевод (Machine Translation). К привычной сигмоиде применяется еще и обучаемый параметр бэта: f (x) = x * sigmoid (β * x).</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/05/image-18.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		vidy-funktsii-aktivatsii	2023-05-09		
235	Adam		<p>Adam (Adaptive Moment Estimation – Адаптивная оценка момента) – один из самых популярных алгоритмов для дообучения <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (NN)</a>. Он был разработан в 2015 году на основе двух других методов – RMSprop и AdaGrad.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/05/image-21.png" class="kg-image" alt loading="lazy" width="1500" height="1200" srcset="__GHOST_URL__/content/images/size/w600/2023/05/image-21.png 600w, __GHOST_URL__/content/images/size/w1000/2023/05/image-21.png 1000w, __GHOST_URL__/content/images/2023/05/image-21.png 1500w" sizes="(min-width: 720px) 720px"><figcaption>Чем короче траектория, тем эффективнее метод регуляризации</figcaption></figure><p>Как и большинство методов Градиентного спуска (Gradient Descent), Adam может оказаться неэффективным, если параметры модели начнут сильно колебаться в процессе обучения. Для решения этой проблемы было разработано несколько методов Регуляризации (Regularization), которые помогают справиться с этой проблемой без использования других методов.</p><p></p><p>Одним из таких методов являются L1 и L2. Они основываются на добавлении нормы весов в функцию потерь, что заставляет модель быть более гладкой, уменьшая различия значений между весами. L1-регуляризация добавляет абсолютную норму весов, а L2-регуляризация добавляет в функцию потерь квадрат нормы весов.</p><p>Adam регуляризация добавляет L2-регуляризацию в обновление весов в каждом шаге. </p><p>Однако, часто применяют более продвинутые формы AdamW, в которых введены дополнительные коэффициенты регуляризации, чтобы балансировать L2-регуляризацию существующей функцией оптимизации.</p><p>Давайте посмотрим на пример кода оптимизатора AdamW с использованием библиотеки PyTorch для обучения классификационной модели на наборе данных CIFAR-10.</p><pre><code>import torch.optim as optim\nfrom torch.optim import lr_scheduler\n\n# Define your neural network architecture\n\nmodel = Net()\n\n# Define the optimizer\n\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n\n# Define the learning rate scheduler\n\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# Train the model\n\nfor epoch in range(num_epochs):\n\n    # Train the model and update the optimizer\n    \n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n    # Update the learning rate scheduler\n    \n    scheduler.step()\n\n    # Evaluate the model on validation set\n    \n    validate(model, val_loader)\n</code></pre><p>Этот пример демонстрирует, как определить архитектуру модели, определить оптимизатор и регуляризацию AdamW, использовать обучающий процесс и проверку с помощью функции потерь и валидации. Вы также можете использовать lr_scheduler для изменения скорости обучения в зависимости от количества эпох.</p><p>В целом, метод Adam является мощным инструментом для улучшения производительности нейронных сетей во время обучения на больших наборах данных. </p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1o-QAMQBqwSvdH3wDeOJGymOXx1vzwz2v?usp=sharing">здесь</a>.</p><p>Автор кода: <a href="https://www.projectpro.io/recipes/optimize-function-adam-pytorch">project.pro</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/05/image-22.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		adam	2023-09-27		
236	Тест на нормальность (Normality Test)		<p>Тест на нормальность – это статистический тест, который используется для определения, является ли распределение данных нормальным или нет. Нормальное распределение (Normal Distribution), является одним из наиболее распространенных типов распределения в статистике и характеризуется симметричным колоколообразным графиком.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/05/image-23.png" class="kg-image" alt loading="lazy" width="1095" height="732" srcset="__GHOST_URL__/content/images/size/w600/2023/05/image-23.png 600w, __GHOST_URL__/content/images/size/w1000/2023/05/image-23.png 1000w, __GHOST_URL__/content/images/2023/05/image-23.png 1095w" sizes="(min-width: 720px) 720px"></figure><p>Один из самых распространенных тестов на нормальность - тест Шапиро-Уилка (Shapiro-Wilk Test). Он определяет, насколько хорошо данные соответствуют нормальному распределению. </p><p>Коэффициент теста Шапиро-Уилка можно рассчитать следующим образом&amp; Допустим, что у нас есть выборка исходных данных: </p><p>X = {1, 3, 2, 5, 4}</p><h3 id="%D1%88%D0%B0%D0%B3-1-%D1%81%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%BF%D0%BE-%D0%B2%D0%BE%D0%B7%D1%80%D0%B0%D1%81%D1%82%D0%B0%D0%BD%D0%B8%D1%8E">Шаг 1: Сортировка данных по возрастанию</h3><p>X = {1, 2, 3, 4, 5}</p><h3 id="%D1%88%D0%B0%D0%B3-2-%D1%80%D0%B0%D1%81%D1%81%D1%87%D0%B8%D1%82%D1%8B%D0%B2%D0%B0%D0%B5%D0%BC-%D0%BA%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82%D1%8B-w-%D0%B8-a">Шаг 2: Рассчитываем коэффициенты W и a. </h3><p>Для этого, сначала присваиваем порядковый номер каждому значению данных</p><p>X<sub>rank</sub> = {1, 2, 3, 4, 5}</p><p>Затем рассчитываем значение статистики W:</p><p>W = 1 - ((∑(d<sub>i</sub>)<sup>2</sup>) / (∑(x<sub>rank,i</sub> - a)<sup>2</sup>))</p><p>где:</p><p>∑(d<sub>i</sub>)<sup>2</sup> – сумма всех значений X, каждый из которых возведен в квадрат.<br>a – коэффициент, вычисляемый на основе количества элементов в выборке<br>x<sub>rank, i</sub> – порядковые номер наблюдаемого значения</p><p>d<sub>i</sub> – разницы между наблюдаемыми и средними значениями. Для нашей выборки d<sub>i</sub> = {-2, -1, 0, 1, 2}, a = 0.375.</p><p>Тогда W = 1 - ((2<sup>2</sup> + 1<sup>2</sup> + 0<sup>2</sup> + 1<sup>2</sup> + 2<sup>2</sup>) / (10.2 - (5(0.375)<sup>2</sup>))) = 0.7</p><h3 id="%D1%88%D0%B0%D0%B3-3-%D1%81%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-w-%D1%81-%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%87%D0%BD%D1%8B%D0%BC-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC">Шаг 3: Сравнение значения W с табличным значением</h3><p>Для заданного уровня значимости и количества элементов в выборке можно определить табличное значение статистики Шапиро-Уилка. Если вычисленное значение W меньше табличного значения, то гипотеза о нормальности распределения не отвергается.<br>Например, для 5 элементов в выборке и уровня значимости 0,05 табличное значение равно 0,762. Таким образом, так как вычисленное значение W меньше табличного значения, мы не можем отвергнуть гипотезу о нормальности распределения данных.</p>		tiest-na-normalnost-normality-test	2023-09-27		
237	9 вещей, которые помогли мне стать дата-сайентистом		<p>В этой статье я расскажу о вещах, которые помогли мне получить свою первую работу в Data Science.</p><p>Нет никакого способа подсластить пилюлю: получить работу в области Науки о данных сложно и это потребует много работы. Эта статья поможет вам стратегически относиться к тому, на что вы тратите свое время и энергию. Делясь своим личным опытом, я хочу помочь вам отфильтровать шум и маркетинговую чепуху, которая окружает индустрию. Поскольку наука о данных – модная отрасль, нам часто пытаются что-то продать (например, курсы, подписки так далее), и, по моему опыту, это действительно затрудняет освоение профессии.</p><p>В этой статье я не буду вам ничего продавать. Моя цель – честно рассказать о своем личном опыте работы в надежде, что это поможет вам.</p><h3 id="%D0%BF%D0%B8%D1%88%D0%B8%D1%82%D0%B5-%D0%BA%D0%BE%D0%B4-%D0%BA%D0%B0%D0%B6%D0%B4%D1%8B%D0%B9-%D0%B4%D0%B5%D0%BD%D1%8C">Пишите код каждый день<br></h3><p>Первое, что мне помогло, – это программировать <em>каждый</em> день. Ну, может быть, не каждый: жизнь не столь легко предсказать и спланировать. Однако нет лучшего способа подготовиться. Создание кода – это именно то, чем вы будете заниматься большую часть времени.</p><p>Дополнительный бонус – возможность прощупать почву и посмотреть, действительно ли вы хотите освоить такую профессию. Многих привлекают большие зарплаты и ажиотаж вокруг таких технологий, как ChatGPT. Хотя это веские причины для интереса к DS, очень важно понять, действительно ли вы будете получать удовольствие от этой работы. Затрачивая всего 20 минут в день на программирование, вы быстро поймете, нравится ли вам работа, которую выполняют специалисты по данным.</p><p>Какой язык выучить? Я бы рекомендовала начать с Python. Потратьте некоторое время на то, чтобы привыкнуть к библиотекам pandas, numpy и matplotlib. Затем попробуйте scikit-learn с их молниеносными функциями model.fit() и model.predict(). Когда “вырастете” из таких моделей, вас наверняка заинтересует PyTorch или TensorFlow. Наряду с этим очень полезен SQL, который применяется для сбора и предварительной обработки данных.</p><h3 id="%D1%81%D1%82%D0%B0%D0%BD%D1%8C%D1%82%D0%B5-%E2%80%9C%D0%B3%D1%80%D0%B0%D0%B6%D0%B4%D0%B0%D0%BD%D1%81%D0%BA%D0%B8%D0%BC%E2%80%9D-%D0%B4%D0%B0%D1%82%D0%B0-%D1%81%D0%B0%D0%B9%D0%B5%D0%BD%D1%82%D0%B8%D1%81%D1%82%D0%BE%D0%BC">Станьте “гражданским” дата-сайентистом</h3><p>Не ждите, пока вы «официально» станете дата-сайентистом, вместо этого станьте “гражданским” специалистом по данным, чтобы анализ данных стал вашей повседневностью.</p><p>Вы можете добровольно взять на себя часть задач вашей текущей команды по отчетности. В своей команде, я стала “ответственной за данные”, предлагая людям помощь с ETL и верстая отчеты. Конечно, вы должны сбалансировать ваши повседневные обязанности и такое новое направление. Тогда это станет отличным способом повысить ценность команды, а также развить свои собственные навыки.</p><p>Если вы работаете в крупной компании, в которой уже есть команда Data Science, свяжитесь с кем-то из них, объясните свои амбиции и спросите, есть ли какие-либо проекты, которые вы могли бы поддерживать. Скорее всего, они будут более чем рады вашей поддержке, так как рук часто не хватает.</p><h3 id="%D0%BE%D0%B1%D1%89%D0%B0%D0%B9%D1%82%D0%B5%D1%81%D1%8C">Общайтесь</h3><p>Почему нетворкинг имеет значение? Разве я не могу просто учиться онлайн?</p><p>Когда-то я бы с вами согласилась Но дело в том, что многие советы, которые мы читаем в Интернете, не подойдут либо потому, что они (1) устарели, (2) не актуальны для вашего географического положения или отрасли, либо (3) делают предположения о читателе, которые не относятся к вам. Мир технологий быстро меняется, и крайне важно поговорить с местными специалистами, чтобы получить актуальное и персонализированное представление об отрасли. Я знаю, что это сложно, но очень важно и полезно. </p><p>Если вас пугает идея нетворкинга, не волнуйтесь, меня тоже. Но на самом деле это намного проще, чем вы думаете. Начните с подписки на некоторые узкоспециализированные ресурсы. Это отличный способ держать руку на пульсе. Я особенно рекомендую следить за Джейсоном Браунли на machinelearningmastery.com.</p><p>Если вы работаете в компании, в которой есть команда Data Science, обратитесь к паре человек в ней и спросите, не хотят ли они выпить кофе и поговорить о своем опыте. Если в вашей компании нет подходящего специалиста, обратитесь к специалистам по обработке и анализу данных на LinkedIn. Возможно, вам придется потратить время, прежде чем вы получите согласие. Ваше терпение будет вознаграждено.</p><p>Не беспокойтесь о своих ошибках. Помните, каждый великий специалист по данным когда-то был новичком. Люди, как правило, проявляют естественное сочувствие к тем, кто находится в ситуациях, в которых они когда-то были сами. Вы будете удивлены, как много людей готовы протянуть руку помощи.</p><h3 id="%D0%BF%D0%BE%D0%BB%D1%83%D1%87%D0%B8%D1%82%D0%B5-%D1%81%D0%BE%D0%BE%D1%82%D0%B2%D0%B5%D1%82%D1%81%D1%82%D0%B2%D1%83%D1%8E%D1%89%D1%83%D1%8E-%D0%BA%D0%B2%D0%B0%D0%BB%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8E-%D0%B8%D0%BB%D0%B8-%D1%81%D0%B5%D1%80%D1%82%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82">Получите соответствующую квалификацию или сертификат</h3><p>Я бы солгала, если бы сказала, что это не сыграло большой роли в моей карьере. Первым сертификатом, который я получила, был диплом факультета ИИ от GeekBrains. Это помогло мне получить мою первую работу Аналитика данных.</p><p>Вам не обязательно иметь техническое образование, чтобы работать в этой сфере. Более типичный путь (особенно если у вас уже есть другая специальность) – это пройти несколько онлайн-курсов и заполнять пробелы конкретными технологиями.</p><h3 id="%D0%BF%D1%80%D0%BE%D0%B9%D0%B4%D0%B8%D1%82%D0%B5-%D1%81%D1%82%D0%B0%D0%B6%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D1%83-%D0%B8%D0%BB%D0%B8-%D0%B4%D0%B2%D0%B5">Пройдите стажировку (или две)</h3><p>Если у вас нет реального опыта, может быть трудно убедить работодателей взять вас на работу. Вот где стажировки действительно могут помочь. Вы даже можете пройти неофициальную неоплачиваемую стажировку. Такая практика поможет убедить будущих работодателей, что у вас есть все необходимое, а также поможет расширить стек (например, git, Docker).<br></p><p>Вакансии стажировки есть на большинстве работных сайтов. Есть еще вариант — напрямую обратиться к более мелким и менее известным компаниям. Это часто срабатывает. Напишите дружеское электронное письмо, объяснив, что вы ищете и что вы можете предложить.<br></p><p>Признаю, неоплачиваемая стажировка подойдет не всем. Если вам такое не подходит, не расстраивайтесь: есть много других способов получить необходимый опыт.</p><h3 id="%D0%BF%D0%BE%D0%B4%D0%B4%D0%B5%D1%80%D0%B6%D0%B8%D0%B2%D0%B0%D0%B9%D1%82%D0%B5-%D1%81%D0%B5%D0%B1%D1%8F">Поддерживайте себя<br></h3><p>Для меня это означало акцентировать внимание на навыках общения. Такие soft skills очень актуальны в контексте науки о данных. Среди прочих отмечу сторителлинг – умение облачить тот или иной инсайт в обертку конкретной истории, что упростит понимание.</p><h3 id="%E2%80%A6-%D0%BD%D0%BE-%D0%BD%D0%B5-%D0%BF%D0%B5%D1%80%D0%B5%D0%BE%D1%86%D0%B5%D0%BD%D0%B8%D0%B2%D0%B0%D0%B9%D1%82%D0%B5-%D1%81%D0%B5%D0%B1%D1%8F">… но не переоценивайте себя</h3><p>Будьте реалистичны в оценке своих навыков. У вас может быть огромный опыт в смежной области. Возможно, все-таки придется сделать промежуточный шаг, прежде чем получить должность мечты: например, стажер или другая менее высокая должность. Это не означает отказ от своих амбиций.</p><h3 id="%D0%BF%D1%80%D0%BE%D1%8F%D0%B2%D0%B8%D1%82%D0%B5-%D1%82%D0%B5%D1%80%D0%BF%D0%B5%D0%BD%D0%B8%D0%B5">Проявите терпение</h3><p>Сменить профессию сложно, и на это уйдет время. Вы не доберетесь в пункт назначения за одну неделю. Немного терпения и настойчивости, и вы освоитесь в этой удивительной вселенной.</p><h3 id="%D0%BF%D0%BE%D0%BC%D0%BD%D0%B8%D1%82%D0%B5-%D1%87%D1%82%D0%BE-%D0%B6%D0%B8%D0%B7%D0%BD%D1%8C-%E2%80%93-%D0%BD%D0%B5%D1%87%D1%82%D0%BE-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B5%D0%B5-%D1%87%D0%B5%D0%BC-%D1%82%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE-%D0%BD%D0%B0%D1%83%D0%BA%D0%B0-%D0%BE-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">Помните, что жизнь – нечто большее, чем только наука о данных</h3><p>Используйте выходные для <em>настоящего</em> отдыха. Не стоит пренебрегать здоровьем или отношениями ради работы. Ни одна карьера не стоит того, чтобы жертвовать всем, даже карьера дата-сайентиста. </p><p>Спасибо за прочтение, я надеюсь, это было полезно. Дайте знать, что вы думаете!</p>		9-vieshchiei-kotoryie-pomoghli-mnie-stat-data-saiientistom	2023-05-27		
238	Kafka		<p>Kafka – это распределенная платформа обработки данных с открытым исходным кодом, разработанная Apache Software Foundation. Она предоставляет высокопроизводительный, масштабируемый и устойчивый к сбоям способ передачи данных между различными приложениями или сервисами.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/06/Apache_kafka_wordtype.svg.jpg" class="kg-image" alt loading="lazy" width="2000" height="1000" srcset="__GHOST_URL__/content/images/size/w600/2023/06/Apache_kafka_wordtype.svg.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/06/Apache_kafka_wordtype.svg.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2023/06/Apache_kafka_wordtype.svg.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2023/06/Apache_kafka_wordtype.svg.jpg 2400w" sizes="(min-width: 720px) 720px"></figure><p>Основной компонент Kafka – это "брокеры" (brokers), которые являются серверами, отвечающими за хранение и обработку данных. Брокеры Kafka работают в кластере, где каждый брокер отвечает за хранение и обработку определенной части данных. Кластер брокеров может масштабироваться горизонтально путем добавления или удаления брокеров, что позволяет обеспечить высокую пропускную способность и отказоустойчивость.</p><p>Kafka использует модель "постоянного потока" (persistent log), где данные хранятся в журнале, известном как "топики" (topics). Топик представляет собой именованную категорию, в которую записываются данные. Потребители (consumers) могут подписаться на топики и читать данные из них. Однако данные в Kafka сохраняются в топиках на определенный период времени, а не только для мгновенного доступа. Это позволяет строить высокоэффективные потоковые приложения и обеспечивает возможность повторной обработки данных.</p><p>Одной из ключевых особенностей Kafka является ее способность обрабатывать огромные объемы данных в режиме реального времени. Она способна обеспечить высокую пропускную способность и низкую задержку при передаче сообщений. Кроме того, Kafka обладает множеством возможностей, включая репликацию данных для обеспечения отказоустойчивости, масштабирование и управление потоками данных с помощью строительных блоков, таких как "продюсеры" (producers), "потребители" (consumers) и "потоки" (streams).</p><p>Kafka может использоваться в различных сценариях, таких как анализ данных, потоковая обработка, агрегация логов, метрики и многое другое. Она широко применяется в индустрии для построения масштабируемых и отказоустойчивых систем обработки данных в реальном времени.</p>		kafka	2023-06-03		
239	Бустинг (Boosting)		<p>Бустинг – это метод Ансамблевого (Ensemble) обучения, при котором несколько слабых моделей объединяются для создания сильной модели. Основная идея – обучать каждую следующую модель на ошибках предыдущих моделей. Один из популярных алгоритмов бустинга - XGBoost.</p><h3 id="xgboost-%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80">XGBoost: пример</h3><p>Для начала импортируем необходимые библиотеки:</p><pre><code class="language-python">import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb</code></pre><p>Бостонский датасет из напрямую доступных в scikit-learn убрали из соображений конфиденциальности, однако его все же можно подгрузить с cайта Carnegie Mellon Uneversity. Мы сразу же разделим данные на <a href="__GHOST_URL__/priediktor/">Предикторы (Predictor Variable)</a> – <code>data</code> и <a href="__GHOST_URL__/tsielievaia-pieriemiennaia/">Целевую переменную (Target Variable)</a> – <code>target</code>:</p><pre><code class="language-python">data_url = "http://lib.stat.cmu.edu/datasets/boston"\nraw_df = pd.read_csv(data_url, sep="\\s+", skiprows=22, header=None)\ndata = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\ntarget = raw_df.values[1::2, 2] # столбец 10</code></pre><p>Наш датасет прошел стадию Инжиниринга фичей (Feature Engineering) и, к сожалению, потерял в читаемости: теперь это набор числовых признаков без понятных названий:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/06/--------------2023-06-15---13.36.28.png" class="kg-image" alt loading="lazy" width="571" height="193"></figure><p>Вот какие признаки кроются за числами:</p><ul><li>0 – уровень преступности на душу населения по городам;</li><li>1 – доля земель под жилую застройку, зонированных под участки свыше 25 000 кв.м.;</li><li>2 – доля акров неторгового бизнеса на город;</li><li>3 – фиктивная переменная реки Чарльз (1, если участок граничит с рекой; 0 в противном случае);</li><li>4 – концентрация оксидов азота (частей на 10 миллионов);</li><li>5 – среднее количество комнат в жилом помещении;</li><li>6 – доля жилых единиц, построенных до 1940 г.;</li><li>7 – взвешенные расстояния до пяти центров занятости Бостона;</li><li>8 – индекс доступности к радиальным магистралям;</li><li>9 – полная ставка налога на имущество за 10 000 долларов США;</li><li>10 – Средняя стоимость домов, занимаемых владельцами, в 1000 долларов.</li></ul><p>Теперь разделим данные на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочную (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">тестовую (Test Data)</a> части в пропорции 8 на 2:</p><pre><code class="language-python">X, y = data, target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</code></pre><p>Обучим модель XGBoost (Extreme Gradient Boosting – "экстримальный градиентный бустинг"), которую "обернули" в название XGBRegressor в мире scikit-learn. Мы используем несколько параметров. <code>n_estimators</code> — число запусков модели (до определенной степени чем больше запусков, тем лучше). Снижая learning_rate, мы можем предотвратить <a href="__GHOST_URL__/pierieobuchieniie/">Переобучение (Overfitting)</a>. max_depth характеризует, насколько глубокими будут <a href="__GHOST_URL__/dierievo-rieshienii/">Деревья решений (Decision Tree)</a>, из которых и состоит XGBoost.</p><pre><code class="language-python">model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=3)\nmodel.fit(X_train, y_train)</code></pre><p>Сгенерируем объект y_pred с предсказаниями для тестового набора и вычислим при сопоставлении с реальными метками:</p><pre><code class="language-python">y_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint("Mean Squared Error:", mse)</code></pre><p>Среднеквадратичная ошибка равна 7.26, что само по себе не очень показательно, хотя модель справляется лучше своих соперниц. </p><pre><code class="language-python">&gt;&gt;&gt; Mean Squared Error: 7.266727280170309</code></pre><p>В <a href="https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset">этой замечательной статье на Kaggle</a> Prasad Perera сравнивает результативность GradientBoostingRegressor (нашей XGBRegressor) с классическим неансамблевым деревом решений (DecisionTreeRegressor), с Методом k-ближайших соседей (KNeighborsRegressor) и другими собратьями. Высокое положение оранжевого Ящика с усами (Boxplot), ответственного за XGBoost, доказывает, что модель "выиграла" у остальных в "амплитуде" MSE и в ее величине в целом.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/06/__results___28_1.jpg" class="kg-image" alt loading="lazy" width="1254" height="665" srcset="__GHOST_URL__/content/images/size/w600/2023/06/__results___28_1.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/06/__results___28_1.jpg 1000w, __GHOST_URL__/content/images/2023/06/__results___28_1.jpg 1254w" sizes="(min-width: 720px) 720px"></figure><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/10FafN4MDtoRVwEz_KFbwRsipBVdAeggp?usp=sharing">здесь</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/06/image-1.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		bustingh	2023-06-15		
240	Выпадающий слой (Dropout Layer)		<p>Dropout Layer – это один из методов Регуляризации (Regularization), применяемых в <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетях (NN)</a>.  Идея заключается в том, чтобы случайным образом обнулять (отключать) некоторые выходы (нейроны) в процессе обучения модели. Это помогает предотвратить переобучение, улучшить обобщающую способность модели и повысить ее устойчивость.</p><p>Метод был предложен в 2012 году в статье "Dropout: A Simple Way to Prevent Neural Networks from Overfitting" авторства Nitish Srivastava и других.</p><h3 id="%D0%B2%D1%8B%D0%BF%D0%B0%D0%B4%D0%B0%D1%8E%D1%89%D0%B8%D0%B9-%D1%81%D0%BB%D0%BE%D0%B9-keras">Выпадающий слой: Keras</h3><p>Посмотрим, как это работает. С примером нам поможет библиотека Keras. Для начала импортируем необходимые модули:</p><pre><code class="language-python"># mlp with dropout on the two circles dataset\nfrom sklearn.datasets import make_circles\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom matplotlib import pyplot</code></pre><p>Сгенерируем игрушечный двумерный <a href="__GHOST_URL__/dataset/">Датасет (Dataset)</a> на 100 строк, зашумленных на 10%:</p><pre><code class="language-python">X, y = make_circles(n_samples=100, noise=0.1, random_state=1)</code></pre><p>Разделим датасет на <a href="__GHOST_URL__/trienirovochnyie-dannyie/">Тренировочную (Train Data)</a> и <a href="__GHOST_URL__/tiestovyie-dannyie/">Тестовую (Test Data)</a> части:</p><pre><code class="language-python">n_train = 30\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]</code></pre><p>Инициируем модель с двумя плотностными (Dense) слоями, одним выпадающим (Dropout) и скомпилируем ее. <code>Dropout(0.4)</code> означает, что мы удаляем случайным образом 40% записей:</p><pre><code class="language-python"># define model\nmodel = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</code></pre><p>Обучим модель на наших данных:</p><pre><code class="language-python">history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)</code></pre><p>Посмотрим, как сильно тестовые данные "прибивают" <a href="__GHOST_URL__/tochnost-izmierienii/">Долю правильных ответов (Accuracy)</a>:</p><pre><code class="language-python">pyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()</code></pre><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/06/image-4.png" class="kg-image" alt loading="lazy" width="547" height="413"></figure><p>Выведем <a href="__GHOST_URL__/tochnost-izmierienii/">Долю правильных ответов (Accuracy)</a>:</p><pre><code class="language-python">_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))</code></pre><pre><code class="language-python">Train: 0.967, Test: 0.771</code></pre><p>Если запустить этот код повторно и закомментировать строку с выпадающий слоем, то график будет выглядеть следующим образом:</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/06/image-5.png" class="kg-image" alt loading="lazy" width="547" height="413"></figure><p>Точность после теста будет равна тем же 77,1%, однако обобщающая способность модели ухудшилась: колебания на первом графике гораздо лучше объясняют десятипроцентный шум в данных. </p><p>Автор оригинальной статьи: <a href="https://machinelearningknowledge.ai/keras-dropout-layer-explained-for-beginners/">Palash Sharma</a></p><p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1kWwB4WWqKsVUv4KApBHFcD2GUtIpFGZn?usp=sharing">здесь</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/06/image-3.png" class="kg-image" alt loading="lazy" width="300" height="300"><figcaption>Подари чашку кофе дата-сайентисту ↑</figcaption></figure>		vypadaiushchii-sloi	2023-06-25		
241	Нейронная машина Тьюринга (NTM)		<p>Нейронная машина Тьюринга (Neural Turing Machine, NTM) – это комбинация идей из области <a href="__GHOST_URL__/nieironnaia-siet/">Нейронных сетей (Neural Network)</a> и машины Тьюринга. NTM представляет собой модель <a href="__GHOST_URL__/iskusstviennyi-intielliekt/">Искусственного интеллекта (AI)</a>, которая расширяет возможности классической нейронной сети, добавляя к ней элементы памяти и адресации.</p><p>Основная идея NTM состоит в том, что она использует внешнюю память, к которой она может обращаться для чтения и записи данных. Эта память управляется нейронной сетью, которая может обучаться и принимать решения на основе полученных данных. В отличие от обычных нейронных сетей, которые имеют фиксированное количество входов и выходов, NTM может динамически изменять свою память и использовать ее для решения сложных задач.</p><p>Основная структура NTM состоит из контроллера, который управляет доступом к памяти, и чтения/записи в память. Контроллер представляет собой нейронную сеть, которая может обучаться с использованием методов градиентного спуска и обратного распространения ошибки. Он принимает на вход информацию о текущем состоянии системы и осуществляет операции чтения и записи в память.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/07/image.png" class="kg-image" alt loading="lazy" width="720" height="992" srcset="__GHOST_URL__/content/images/size/w600/2023/07/image.png 600w, __GHOST_URL__/content/images/2023/07/image.png 720w" sizes="(min-width: 720px) 720px"></figure><p>NTM позволяет решать задачи, требующие хранения и манипулирования большими объемами информации. Он нашел применение в областях машинного перевода, обработки естественного языка, распознавания образов и других задач, где необходимо использовать контекст и память для достижения лучших результатов.</p>		nieironnaia-mashina-tiuringha	2023-07-02		
242	Язык структурированных запросов (SQL)		<p>Язык структурированных запросов (англ. SQL – Structured Query Language) –это язык программирования, используемый для работы с базами данных. Он позволяет создавать, изменять и управлять базами данных, а также извлекать информацию из них. </p><p>Несмотря на солидный возраст языка и множество недостатков, до сих пор является общепринятым стандартом обращения с данными, в том числе и с большими. Для таких хранилищ, как Google BigQuery, Apache Hadoop даже создали <em>диалекты</em> языка SQL.</p><p>Стоит отличать SQL от MySQL. Второе – это продукт компании Oracle, база данных, использующая язык SQL.</p><h3 id="%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B-sql-%D0%B7%D0%B0%D0%BF%D1%80%D0%BE%D1%81%D0%BE%D0%B2">Примеры SQL-запросов</h3><p>Создание таблицы:</p><pre><code class="language-sql">CREATE TABLE Employees (\n    ID INT PRIMARY KEY,\n    Name VARCHAR(50),\n    Age INT,\n    Department VARCHAR(50)\n);\n</code></pre><p>Вставка данных в таблицу:</p><pre><code class="language-sql">INSERT INTO Employees (ID, Name, Age, Department)\nVALUES (1, 'John Doe', 30, 'IT'),\n       (2, 'Jane Smith', 35, 'Sales'),\n       (3, 'Mike Johnson', 40, 'HR');\n</code></pre><p>Выборка всех записей из таблицы:</p><pre><code class="language-sql">SELECT * FROM Employees;\n</code></pre><p>Выборка конкретных столбцов из таблицы:</p><pre><code class="language-sql">SELECT Name, Age FROM Employees;\n</code></pre><p>Фильтрация данных с использованием условия WHERE:</p><pre><code class="language-sql">SELECT * FROM Employees WHERE Age &gt; 30;\n</code></pre><p>Обновление данных в таблице:</p><pre><code class="language-sql">UPDATE Employees SET Department = 'Marketing' WHERE ID = 3;\n</code></pre><p>Удаление записей из таблицы:</p><pre><code class="language-sql">DELETE FROM Employees WHERE Age &lt; 30;\n</code></pre><p>Группировка данных с использованием функции GROUP BY:</p><pre><code class="language-sql">SELECT Department, COUNT(*) FROM Employees GROUP BY Department;\n</code></pre><p>SQL обладает большим количеством операторов, функций и возможностей для работы с данными в базе данных.</p>		yazyk-strukturirovannykh-zaprosov	2023-07-09		
243	ClickHouse		<figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/07/oksik306_young_woman_in_a_server_room_looking_towards_a_server_35e07f82-8fda-4eff-8895-bd387cc4c09e.png" class="kg-image" alt loading="lazy" width="1456" height="816" srcset="__GHOST_URL__/content/images/size/w600/2023/07/oksik306_young_woman_in_a_server_room_looking_towards_a_server_35e07f82-8fda-4eff-8895-bd387cc4c09e.png 600w, __GHOST_URL__/content/images/size/w1000/2023/07/oksik306_young_woman_in_a_server_room_looking_towards_a_server_35e07f82-8fda-4eff-8895-bd387cc4c09e.png 1000w, __GHOST_URL__/content/images/2023/07/oksik306_young_woman_in_a_server_room_looking_towards_a_server_35e07f82-8fda-4eff-8895-bd387cc4c09e.png 1456w" sizes="(min-width: 720px) 720px"><figcaption>Изображение: Midjourney</figcaption></figure><p>ClickHouse – это колоночная база данных с открытым исходным кодом, предназначенная для обработки и анализа больших объемов данных с высокой производительностью. Она была создана компанией Яндекс и стала популярным решением для хранения и обработки данных в реальном времени.</p><figure class="kg-card kg-image-card"><img src="__GHOST_URL__/content/images/2023/07/image-1.png" class="kg-image" alt loading="lazy" width="1200" height="627" srcset="__GHOST_URL__/content/images/size/w600/2023/07/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2023/07/image-1.png 1000w, __GHOST_URL__/content/images/2023/07/image-1.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>Колоночная база данных, в отличие от классической строковой, хранит данные в таблицах с огромным количеством столбцов, каждый из которых был бы рядом в строковой БД:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="__GHOST_URL__/content/images/2023/07/image-2.png" class="kg-image" alt loading="lazy" width="1200" height="810" srcset="__GHOST_URL__/content/images/size/w600/2023/07/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2023/07/image-2.png 1000w, __GHOST_URL__/content/images/2023/07/image-2.png 1200w" sizes="(min-width: 1200px) 1200px"></figure><p>ClickHouse специально оптимизирована для работы с запросами за очень короткое время. Она обладает мощными возможностями для агрегации, фильтрации и анализа данных, а также поддерживает распределенные вычисления для обеспечения масштабируемости.</p><p>Основные преимущества ClickHouse:</p><ul><li>Высокая производительность: ПО способно обрабатывать миллионы запросов в секунду и предоставляет результаты в реальном времени.</li><li>Сжатие данных: БД использует эффективные алгоритмы сжатия данных, что позволяет значительно сократить объем хранимых данных без потери производительности.</li><li>Горизонтальная масштабируемость: ClickHouse поддерживает распределенные вычисления и может масштабироваться на несколько серверов, обрабатывая данные параллельно для повышения производительности.</li><li>SQL-совместимость: ClickHouse поддерживает большую часть стандарта SQL, что делает ее легко интегрируемой с существующими инструментами и приложениями.</li><li>Поддержка репликации и отказоустойчивости: ClickHouse предоставляет механизмы резервного копирования данных.</li></ul><p>ClickHouse нашла широкое применение в различных сферах, включая аналитику в реальном времени, обработку событий, мониторинг, анализ больших данных и другие задачи, требующие эффективной обработки и анализа больших объемов информации.</p>		clickhouse	2023-07-16		
244	Алгоритм поиска по сетке (Grid Search)		<p>Алгоритм поиска по сетке – это метод подбора оптимальных гиперпараметров для <a href="__GHOST_URL__/modiel/">Модели (Model)</a> путем перебора всех возможных комбинаций значений <a href="__GHOST_URL__/gipierparamietr/">Гиперпараметров (Hyperparameter)</a> из заданного набора. Гиперпараметры – это параметры модели, которые не оптимизируются во время процесса обучения, а задаются до его начала. Их оптимальный выбор влияет на качество и обобщающую способность модели.</p><p>Допустим, мы создали Дерево решений (Decision Tree) для банковского кредитного датасета. С полным кодом модели вы можете ознакомиться в <a href="https://colab.research.google.com/drive/1fYtVbamX9gxicgdpsCPk_HlECw-O_TJs#scrollTo=MhKacA-rlbn_&amp;uniqifier=1">этом ноутбуке</a>. У первой версии дерева следующие характеристики эффективности:</p><pre><code class="language-python">&gt;&gt;&gt; print('Доля правильных ответов: %.3f' % tr.score(X_test, y_test))\n&gt;&gt;&gt; print('Доля правильных ответов во время кросс-валидации: %0.3f' % cv_tr)\n&gt;&gt;&gt; print('Точность результата измерений: %.3f' % precision_score(y_test, tr_pred))\n&gt;&gt;&gt; print('Полнота: %.3f' % recall_score(y_test, tr_pred))\n&gt;&gt;&gt; print('Оценка F1: %.3f' % f1_score(y_test, tr_pred))\n... Доля правильных ответов: 0.910\n... Доля правильных ответов во время кросс-валидации: 0.908\n... Точность результата измерений: 0.946\n... Полнота: 0.953\n... Оценка F1: 0.950</code></pre><p>Но мы хотим автоматически подобрать наилучшие параметры. С этим нам поможет модуль scikit-learn под названием GridSearchCV:</p><pre><code class="language-python">from sklearn.model_selection GridSearchCV</code></pre><p>Выберем параметры, значения которых будем перебирать и инициируем экземпляр <code>GridSearchCV</code>:</p><pre><code class="language-python">parameters = {'criterion':['gini','entropy'],\n              'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n             }\n\ndefault_tr = tree.DecisionTreeClassifier(random_state=25)\ngs_tree = GridSearchCV(default_tr, parameters, cv=10, n_jobs=-1,verbose=1)\ngs_tree.fit(X_train, y_train)</code></pre><p>Запустим обучение дерева решений, чтобы подобрать значения гиперпараметров:</p><pre><code class="language-python">gs_tree_pred = gs_tree.predict(X_test)</code></pre><p>Выведем оптимизированные значения:</p><pre><code class="language-python">&gt;&gt;&gt; print('Лучшие параметры дерева решений: {}'.format(gs_tree.best_params_))\n&gt;&gt;&gt; print('Доля правильных ответов: %0.3f' % (gs_tree.score(X_test,y_test)))\n&gt;&gt;&gt; print('Доля правильных ответов кросс-валидации: %0.3f' % gs_tree.best_score_)\n&gt;&gt;&gt; print('Точность: %.3f' % precision_score(y_test, gs_tree_pred))\n&gt;&gt;&gt; print('Полнота: %.3f' % recall_score(y_test, gs_tree_pred))\n&gt;&gt;&gt; print('F1-мера: %.3f' % f1_score(y_test, gs_tree_pred))\n\n... Лучшие параметры дерева решений: {'criterion': 'gini', 'max_depth': 5}\n... Доля правильных ответов: 0.915\n... Доля правильных ответов кросс-валидации: 0.913\n... Точность: 0.936\n... Полнота: 0.970\n... F1-мера: 0.953</code></pre>		alghoritm-poiska-po-sietkie	2023-07-23		
245	Кривая аппроксимации (Curve Fitting)		<p>Кривая аппроксимации (Curve Fitting) – это процесс нахождения математической функции (например, синусоиды), которая наилучшим образом описывает набор данных. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="__GHOST_URL__/content/images/2023/07/image-4.png" class="kg-image" alt loading="lazy" width="836" height="564" srcset="__GHOST_URL__/content/images/size/w600/2023/07/image-4.png 600w, __GHOST_URL__/content/images/2023/07/image-4.png 836w" sizes="(min-width: 720px) 720px"><figcaption>Изображение: statology.org</figcaption></figure><p>Она используется для поиска зависимостей между переменными и для прогнозирования значений на основе имеющихся данных.</p><p>Допустим, у нас есть набор данных – точки на плоскости, и мы хотим найти функцию, которая проходит через эти точки. Рассмотрим следующий пример:</p><p>Предположим, у нас есть следующие данные (x, y):</p><p>(x=1, y=3)<br>(x=2, y=5)<br>(x=3, y=7)<br>(x=4, y=9)<br>(x=5, y=11)</p><p>Давайте попробуем аппроксимировать эти данные с помощью линейной функции вида y = ax + b.</p><p>Мы можем использовать метод наименьших квадратов (Least Squares Method), чтобы подобрать параметры a и b таким образом, чтобы линия проходила максимально близко к нашим данным. В данном случае, это означает, что мы хотим найти значения a и b для уравнения y = ax + b, чтобы минимизировать сумму квадратов разницы между предсказанными значениями функции и исходными данными.</p><p>Решим систему уравнений методом наименьших квадратов:</p><ol><li>Для x=1: 3 = a * 1 + b</li><li>Для x=2: 5 = a * 2 + b</li><li>Для x=3: 7 = a * 3 + b</li><li>Для x=4: 9 = a * 4 + b</li><li>Для x=5: 11 = a * 5 + b</li></ol><p>Решая эту систему уравнений, мы можем найти оптимальные значения a ≈ 2 и b ≈ 1.</p><p>Таким образом, функция, которая аппроксимирует наши данные, будет y ≈ 2x + 1.</p><p>Эта линия наилучшим образом подходит к нашим исходным данным и может быть использована для прогнозирования значений y для новых значений x в пределах аппроксимации.</p>		podghonka-krivykh	2023-07-30		
246	MLOps		<p>MLOps (Machine Learning Operations) – это практики и инструменты для обеспечения полного жизненного цикла <a href="__GHOST_URL__/mashinnoie-obuchieniie/">Машинного обучения (ML)</a>. Эта концепция возникла как ответ на сложности, связанные с интеграцией ML в процессы разработки программного обеспечения. MLOps стремится обеспечить более гладкое и эффективное взаимодействие между данными, разработчиками и операционной инфраструктурой.</p><p>В основе концепции лежит идея, что разработка <a href="__GHOST_URL__/modiel/">Моделей (Model)</a> машинного обучения – это не только эксперименты с данными и алгоритмами, но и сложный процесс, который включает в себя сбор, подготовку и аннотацию данных, обучение моделей, их тестирование, развертывание в производственной среде и непрерывное обновление. MLOps уделяет внимание каждому этапу этого цикла и ставит целью автоматизацию и стандартизацию процессов.</p><p>Одной из ключевых практик MLOps является контроль версий моделей и данных. Аналогично тому, как разработчики программ используют системы контроля версий для отслеживания изменений в исходном коде, команды, занимающиеся машинным обучением, используют подобные инструменты для учета изменений в моделях, данных и конфигурациях обучения. Это позволяет обеспечить воспроизводимость результатов и управлять изменениями в эффективной и систематической манере.</p><p>Еще одной важной составляющей MLOps является автоматизация процессов развертывания моделей. Цель заключается в том, чтобы сделать их развертывание быстрым и надежным. Использование инструментов автоматизации позволяет сократить риск человеческой ошибки и обеспечить более прозрачное развертывание моделей в производственной среде.</p><p>Непрерывное обучение (Continuous Training) – это еще одна практика MLOps, которая подразумевает, что обучение моделей – это процесс, который не останавливается после развертывания. Вместо этого модели постоянно улучшаются и обновляются на основе новых данных. Это подходит для ситуаций, когда данные постоянно меняются, и модель должна оставаться актуальной и точной.</p><p>MLOps также способствует сближению разработчиков и операционных команд. Разработчики моделей и инженеры операций совместно работают над обеспечением эффективного развертывания и управления моделями в производственной среде. Это снижает риск возникновения проблем, связанных с интеграцией моделей в боевую инфраструктуру.</p><p>Внедрение практик MLOps может значительно улучшить процессы разработки и интеграции моделей машинного обучения, сократив время от идеи до внедрения на практике.</p>		mlops	2023-08-13		
247	Несбалансированный датасет (Imbalanced Dataset)	Несбалансированный датасет – набор данных, где количество примеров разных классов в данных существенно отличается. Например, в задаче бинарной классификации может быть всего два класса: положительный и отрицательный	<p>Несбалансированный датасет &ndash; набор данных, где&nbsp;количество примеров разных классов в данных существенно отличается. Например, в задаче бинарной классификации может быть всего два класса: положительный и отрицательный. Если в датасете 90% примеров принадлежат к классу "положительный", а только 10% к классу "отрицательный", то такой датасет считается несбалансированным.</p>\r\n<p>Давайте создадим небольшой несбалансированный датасет с использованием библиотеки NumPy. Предположим, у нас есть задача бинарной классификации с 2 признаками:</p>\r\n<pre>import numpy as np<br /><br /><br /><br /># Создаем 100 примеров с признаками<br /><br />np.random.seed(0)<br /><br />features = np.random.randn(100, 2)<br /><br /><br /><br /># Создаем метки классов: 90% принадлежат классу 0, 10% - классу 1<br /><br />labels = np.concatenate([np.zeros(90), np.ones(10)])<br /><br /><br /><br /># Перемешиваем данные<br /><br />indices = np.arange(100)<br /><br />np.random.shuffle(indices)<br /><br />features = features[indices]<br /><br />labels = labels[indices]<br /><br /><br /><br />print(f"Количество примеров класса 0: {np.sum(labels == 0)}")<br /><br />print(f"Количество примеров класса 1: {np.sum(labels == 1)}")</pre>\r\n<p>&nbsp;</p>\r\n<p>В этом примере у нас есть 100 примеров, из которых 90 принадлежат к классу 0, а 10 к классу 1, что делает датасет несбалансированным.</p>\r\n<p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/15WcMiAZRnYKSeZ8rLbn3dzeWvJt0_FIP?usp=sharing">здесь</a>.</p>		nesbalansirovannyy-dataset	2023-10-01	Основы	https://images.unsplash.com/photo-1612012060851-20f943c02d3d?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3200&q=80
248	Тепловая карта (Heat Map)	Тепловая карта (Heat Map) — это графическое представление данных, в котором значения в точках набора данных представлены цветами. Это помогает визуально выделить области с высокой или низкой концентрацией, что делает их анализ более интуитивным и понятным.	<p>Тепловая карта (Heat Map) &mdash; это графическое представление данных, в котором значения в точках набора данных представлены цветами. Это помогает визуально выделить области с высокой или низкой концентрацией, что делает их анализ более интуитивным и понятным.</p>\r\n<p>Тепловые карты особенно полезны для анализа распределения данных на географических картах, но их можно использовать и в других контекстах, например, для визуализации интенсивности использования веб-страниц или сенсоров в пространстве.</p>\r\n<h1>Как создать тепловую карту с использованием Python?</h1>\r\n<p>В Python существует множество библиотек для создания тепловых карт. Одна из самых популярных &mdash; seaborn. Для начала убедитесь, что у вас установлена эта библиотека:</p>\r\n<pre>pip install seaborn</pre>\r\n<p>Вот пример кода, демонстрирующий, как создать простую тепловую карту с использованием данных о точках на карте:</p>\r\n<p>&nbsp;</p>\r\n<pre>import seaborn as sns<br />import matplotlib.pyplot as plt<br />import pandas as pd<br /><br /><br /># Пример данных (координаты с интенсивностью)<br />data = [<br />  &nbsp; (1, 2, 3),<br />  &nbsp; (3, 4, 5),<br />  &nbsp; (5, 6, 7),<br />  &nbsp; (7, 8, 9),<br />]<br /><br /><br /># Создаем DataFrame из данных<br />df = pd.DataFrame(data, columns=['X', 'Y', 'Intensity'])<br /><br /><br /># Преобразуем DataFrame в матрицу<br />heatmap_data = df.pivot(index='Y', columns='X', values='Intensity')<br /><br /># Создаем тепловую карту с помощью seaborn<br />sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True)<br /># Отображаем карту<br />plt.show()</pre>\r\n<p>Этот код создаст простую тепловую карту с заданными данными:</p>\r\n<p><img src="data:image/png;base64,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" /></p>\r\n<p>Ось X и Y представляют координаты, а интенсивность представляет собой значения в каждой точке. Карту можно настроить, используя различные параметры, такие как цветовая карта (`cmap`), аннотации и форматирование значений.</p>\r\n<h2>Заключение</h2>\r\n<p>Тепловые карты предоставляют мощный инструмент для визуализации и анализа данных. Они особенно эффективны при работе с большими объемами информации или при анализе географических данных. В Python существует множество библиотек, позволяющих легко создавать и настраивать тепловые карты, включая `seaborn`, `matplotlib` и другие.</p>\r\n<p>Ноутбук, не требующий дополнительной настройки на момент написания статьи, можно скачать <a href="https://colab.research.google.com/drive/1SGxJ-162PIEdOun7qDamvaaq2q-uGPHZ?usp=sharing">здесь</a>.</p>\r\n<p>&nbsp;</p>		teplovaya-karta	2023-10-07	Основы	https://images.unsplash.com/photo-1518570932309-3f370342d0ff?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3573&q=80
\.


--
-- Data for Name: home_home; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.home_home (sno, title, meta, content, thumbnail, slug, "time") FROM stdin;
\.


--
-- Name: chunk_constraint_name; Type: SEQUENCE SET; Schema: _timescaledb_catalog; Owner: postgres
--

SELECT pg_catalog.setval('_timescaledb_catalog.chunk_constraint_name', 1, false);


--
-- Name: chunk_id_seq; Type: SEQUENCE SET; Schema: _timescaledb_catalog; Owner: postgres
--

SELECT pg_catalog.setval('_timescaledb_catalog.chunk_id_seq', 1, false);


--
-- Name: dimension_id_seq; Type: SEQUENCE SET; Schema: _timescaledb_catalog; Owner: postgres
--

SELECT pg_catalog.setval('_timescaledb_catalog.dimension_id_seq', 1, false);


--
-- Name: dimension_slice_id_seq; Type: SEQUENCE SET; Schema: _timescaledb_catalog; Owner: postgres
--

SELECT pg_catalog.setval('_timescaledb_catalog.dimension_slice_id_seq', 1, false);


--
-- Name: hypertable_id_seq; Type: SEQUENCE SET; Schema: _timescaledb_catalog; Owner: postgres
--

SELECT pg_catalog.setval('_timescaledb_catalog.hypertable_id_seq', 1, false);


--
-- Name: bgw_job_id_seq; Type: SEQUENCE SET; Schema: _timescaledb_config; Owner: postgres
--

SELECT pg_catalog.setval('_timescaledb_config.bgw_job_id_seq', 1000, false);


--
-- Name: auth_group_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_group_id_seq', 1, true);


--
-- Name: auth_group_permissions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_group_permissions_id_seq', 1, true);


--
-- Name: auth_permission_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_permission_id_seq', 36, true);


--
-- Name: auth_user_groups_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_user_groups_id_seq', 1, true);


--
-- Name: auth_user_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_user_id_seq', 2, true);


--
-- Name: auth_user_user_permissions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.auth_user_user_permissions_id_seq', 1, true);


--
-- Name: django_admin_log_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.django_admin_log_id_seq', 92, true);


--
-- Name: django_content_type_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.django_content_type_id_seq', 8, true);


--
-- Name: django_migrations_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.django_migrations_id_seq', 27, true);


--
-- Name: home_blog_sno_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.home_blog_sno_seq', 248, true);


--
-- Name: home_home_sno_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.home_home_sno_seq', 1, true);


--
-- Name: django_migrations idx_17475_django_migrations_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_migrations
    ADD CONSTRAINT idx_17475_django_migrations_pkey PRIMARY KEY (id);


--
-- Name: auth_group_permissions idx_17484_auth_group_permissions_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions
    ADD CONSTRAINT idx_17484_auth_group_permissions_pkey PRIMARY KEY (id);


--
-- Name: auth_user_groups idx_17490_auth_user_groups_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups
    ADD CONSTRAINT idx_17490_auth_user_groups_pkey PRIMARY KEY (id);


--
-- Name: auth_user_user_permissions idx_17496_auth_user_user_permissions_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions
    ADD CONSTRAINT idx_17496_auth_user_user_permissions_pkey PRIMARY KEY (id);


--
-- Name: django_admin_log idx_17502_django_admin_log_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log
    ADD CONSTRAINT idx_17502_django_admin_log_pkey PRIMARY KEY (id);


--
-- Name: django_content_type idx_17511_django_content_type_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_content_type
    ADD CONSTRAINT idx_17511_django_content_type_pkey PRIMARY KEY (id);


--
-- Name: auth_permission idx_17520_auth_permission_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_permission
    ADD CONSTRAINT idx_17520_auth_permission_pkey PRIMARY KEY (id);


--
-- Name: auth_group idx_17529_auth_group_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group
    ADD CONSTRAINT idx_17529_auth_group_pkey PRIMARY KEY (id);


--
-- Name: auth_user idx_17538_auth_user_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user
    ADD CONSTRAINT idx_17538_auth_user_pkey PRIMARY KEY (id);


--
-- Name: django_session idx_17545_sqlite_autoindex_django_session_1; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_session
    ADD CONSTRAINT idx_17545_sqlite_autoindex_django_session_1 PRIMARY KEY (session_key);


--
-- Name: home_home idx_17553_home_home_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.home_home
    ADD CONSTRAINT idx_17553_home_home_pkey PRIMARY KEY (sno);


--
-- Name: home_blog idx_17562_home_blog_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.home_blog
    ADD CONSTRAINT idx_17562_home_blog_pkey PRIMARY KEY (sno);


--
-- Name: idx_17484_auth_group_permissions_group_id_b120cbf9; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17484_auth_group_permissions_group_id_b120cbf9 ON public.auth_group_permissions USING btree (group_id);


--
-- Name: idx_17484_auth_group_permissions_group_id_permission_id_0cd325b; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX idx_17484_auth_group_permissions_group_id_permission_id_0cd325b ON public.auth_group_permissions USING btree (group_id, permission_id);


--
-- Name: idx_17484_auth_group_permissions_permission_id_84c5c92e; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17484_auth_group_permissions_permission_id_84c5c92e ON public.auth_group_permissions USING btree (permission_id);


--
-- Name: idx_17490_auth_user_groups_group_id_97559544; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17490_auth_user_groups_group_id_97559544 ON public.auth_user_groups USING btree (group_id);


--
-- Name: idx_17490_auth_user_groups_user_id_6a12ed8b; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17490_auth_user_groups_user_id_6a12ed8b ON public.auth_user_groups USING btree (user_id);


--
-- Name: idx_17490_auth_user_groups_user_id_group_id_94350c0c_uniq; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX idx_17490_auth_user_groups_user_id_group_id_94350c0c_uniq ON public.auth_user_groups USING btree (user_id, group_id);


--
-- Name: idx_17496_auth_user_user_permissions_permission_id_1fbb5f2c; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17496_auth_user_user_permissions_permission_id_1fbb5f2c ON public.auth_user_user_permissions USING btree (permission_id);


--
-- Name: idx_17496_auth_user_user_permissions_user_id_a95ead1b; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17496_auth_user_user_permissions_user_id_a95ead1b ON public.auth_user_user_permissions USING btree (user_id);


--
-- Name: idx_17496_auth_user_user_permissions_user_id_permission_id_14a6; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX idx_17496_auth_user_user_permissions_user_id_permission_id_14a6 ON public.auth_user_user_permissions USING btree (user_id, permission_id);


--
-- Name: idx_17502_django_admin_log_content_type_id_c4bce8eb; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17502_django_admin_log_content_type_id_c4bce8eb ON public.django_admin_log USING btree (content_type_id);


--
-- Name: idx_17502_django_admin_log_user_id_c564eba6; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17502_django_admin_log_user_id_c564eba6 ON public.django_admin_log USING btree (user_id);


--
-- Name: idx_17511_django_content_type_app_label_model_76bd3d3b_uniq; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX idx_17511_django_content_type_app_label_model_76bd3d3b_uniq ON public.django_content_type USING btree (app_label, model);


--
-- Name: idx_17520_auth_permission_content_type_id_2f476e4b; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17520_auth_permission_content_type_id_2f476e4b ON public.auth_permission USING btree (content_type_id);


--
-- Name: idx_17520_auth_permission_content_type_id_codename_01ab375a_uni; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX idx_17520_auth_permission_content_type_id_codename_01ab375a_uni ON public.auth_permission USING btree (content_type_id, codename);


--
-- Name: idx_17529_sqlite_autoindex_auth_group_1; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX idx_17529_sqlite_autoindex_auth_group_1 ON public.auth_group USING btree (name);


--
-- Name: idx_17538_sqlite_autoindex_auth_user_1; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX idx_17538_sqlite_autoindex_auth_user_1 ON public.auth_user USING btree (username);


--
-- Name: idx_17545_django_session_expire_date_a5c62663; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_17545_django_session_expire_date_a5c62663 ON public.django_session USING btree (expire_date);


--
-- Name: auth_group_permissions auth_group_permissions_group_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions
    ADD CONSTRAINT auth_group_permissions_group_id_fkey FOREIGN KEY (group_id) REFERENCES public.auth_group(id);


--
-- Name: auth_group_permissions auth_group_permissions_permission_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_group_permissions
    ADD CONSTRAINT auth_group_permissions_permission_id_fkey FOREIGN KEY (permission_id) REFERENCES public.auth_permission(id);


--
-- Name: auth_permission auth_permission_content_type_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_permission
    ADD CONSTRAINT auth_permission_content_type_id_fkey FOREIGN KEY (content_type_id) REFERENCES public.django_content_type(id);


--
-- Name: auth_user_groups auth_user_groups_group_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups
    ADD CONSTRAINT auth_user_groups_group_id_fkey FOREIGN KEY (group_id) REFERENCES public.auth_group(id);


--
-- Name: auth_user_groups auth_user_groups_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_groups
    ADD CONSTRAINT auth_user_groups_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.auth_user(id);


--
-- Name: auth_user_user_permissions auth_user_user_permissions_permission_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions
    ADD CONSTRAINT auth_user_user_permissions_permission_id_fkey FOREIGN KEY (permission_id) REFERENCES public.auth_permission(id);


--
-- Name: auth_user_user_permissions auth_user_user_permissions_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.auth_user_user_permissions
    ADD CONSTRAINT auth_user_user_permissions_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.auth_user(id);


--
-- Name: django_admin_log django_admin_log_content_type_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log
    ADD CONSTRAINT django_admin_log_content_type_id_fkey FOREIGN KEY (content_type_id) REFERENCES public.django_content_type(id);


--
-- Name: django_admin_log django_admin_log_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.django_admin_log
    ADD CONSTRAINT django_admin_log_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.auth_user(id);


--
-- Name: SCHEMA public; Type: ACL; Schema: -; Owner: postgres
--

REVOKE USAGE ON SCHEMA public FROM PUBLIC;
GRANT ALL ON SCHEMA public TO PUBLIC;


--
-- PostgreSQL database dump complete
--

